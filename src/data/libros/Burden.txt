10a. edici√≥n

AN√ÅLISIS

NUM√âRICO
Richard L. Burden ‚Ä¢ Douglas J. Faires ‚Ä¢ Annette M. Burden

An√°lisis num√©rico,
10 a. ed.
Richard L. Burden, J. Douglas Faires y
Annette M. Burden
Director Editorial para Latinoam√©rica:
Ricardo H. Rodr√≠guez
Editora de Adquisiciones para
Latinoam√©rica:
Claudia C. Garay Castro
Gerente de Manufactura para
Latinoam√©rica:
Antonio Mateos Mart√≠nez
Gerente Editorial de Contenidos
en Espa√±ol:
Pilar Hern√°ndez Santamarina
Gerente de Proyectos Especiales:
Luciana Rabuffetti
Coordinador de Manufactura:
Rafael P√©rez Gonz√°lez
Editora:
Ivonne Arciniega Torres
Dise√±o de portada:
Anneli Daniela Torres Arroyo
Imagen de portada:
¬© theromb/Shutterstock.com
Composici√≥n tipogr√°Ô¨Åca:
Tsuki Marketing S.A. de C.V.
Gerardo Larios Garc√≠a

Impreso en M√©xico
1 2 3 4 5 6 7 20 19 18 17

¬© D.R. 2017 por Cengage Learning Editores, S.A. de C.V.,
una Compa√±√≠a de Cengage Learning, Inc.
Corporativo Santa Fe
Av. Santa Fe n√∫m. 505, piso 12
Col. Cruz Manca, Santa Fe
C.P. 05349, M√©xico, D.F.
Cengage Learning¬Æ es una marca registrada
usada bajo permiso.
DERECHOS RESERVADOS. Ninguna parte de
este trabajo amparado por la Ley Federal del
Derecho de Autor, podr√° ser reproducida,
transmitida, almacenada o utilizada en
cualquier forma o por cualquier medio, ya sea
gr√°Ô¨Åco, electr√≥nico o mec√°nico, incluyendo,
pero sin limitarse a lo siguiente: fotocopiado,
reproducci√≥n, escaneo, digitalizaci√≥n,
grabaci√≥n en audio, distribuci√≥n en internet,
distribuci√≥n en redes de informaci√≥n o
almacenamiento y recopilaci√≥n en sistemas
de informaci√≥n a excepci√≥n de lo permitido
en el Cap√≠tulo III, Art√≠culo 27 de la Ley Federal
del Derecho de Autor, sin el consentimiento
por escrito de la Editorial.
Traducido del libro Numerical Analysis, Tenth Edition
Richard L. Burden, J. Douglas Faires, Annette M. Burden
Publicado en ingl√©s por Cengage Learning
¬© 2016, 2011, 2005
ISBN: 978-1-305-25366-7
Datos para catalogaci√≥n bibliogr√°Ô¨Åca:
Burden, Faires y Burden
An√°lisis num√©rico, 10a. ed.
ISBN: 978-607-526-411-0
Visite nuestro sitio en:
http://latinoamerica.cengage.com

An√°lisis num√©rico
D√âCIMA EDICI√ìN

Richard L. Burden
Youngstown University

J. Douglas Faires
Youngstown University

Annette M. Burden
Youngstown University

Traducci√≥n:
Mara Paulina Su√°rez Moreno
Traductora profesional

Revisi√≥n t√©cnica:
Wilmar Alberto D√≠az Ossa
M√°gister en matem√°ticas aplicadas
Profesor en la Universidad Distrital Francisco Jos√© de Caldas

Australia ‚Ä¢ Brasil ‚Ä¢ Corea ‚Ä¢ Espa√±a ‚Ä¢ Estados Unidos ‚Ä¢ Jap√≥n ‚Ä¢ M√©xico ‚Ä¢ Reino Unido ‚Ä¢ Singapur

Contenido
Prefacio

vii

1 Preliminares matem√°ticos y an√°lisis de error
1.1
1.2

Revisi√≥n de c√°lculo 2
Errores de redondeo y aritm√©tica computacional 11

1.3

Algoritmos y convergencia

1.4

Software num√©rico

22

28

2 Soluciones de las ecuaciones en una variable
2.1

2.3
2.4
2.5
2.6
2.7

77

Interpolaci√≥n y el polinomio de Lagrange 78
Aproximaci√≥n de datos y m√©todo de Neville 86
Diferencias divididas 91
Interpolaci√≥n de Hermite 99
Interpolaci√≥n de spline c√∫bico 105
Curvas param√©tricas 121
Software num√©rico y revisi√≥n del cap√≠tulo 126

4 Diferenciaci√≥n num√©rica e integraci√≥n
4.1
4.2
4.3

35

El m√©todo de bisecci√≥n 36
,WHUDFLyQGHSXQWR√ÄMR 
M√©todo de Newton y sus extensiones 49
An√°lisis de error para m√©todos iterativos 58
Convergencia acelerada 64
Ceros de polinomios y m√©todo de M√ºller 68
Software num√©rico y revisi√≥n del cap√≠tulo 76

3 Interpolaci√≥n y aproximaci√≥n polinomial
3.1
3.2
3.3
3.4
3.5
3.6
3.7

1

127

Diferenciaci√≥n num√©rica 128
Extrapolaci√≥n de Richardson 136
Elementos de integraci√≥n num√©rica 142
iii

iv

Contenido

4.4
4.5
4.6
4.7
4.8
4.9
4.10

Integraci√≥n num√©rica compuesta 150
Integraci√≥n de Romberg 156
M√©todos de cuadratura adaptable 162
Cuadratura gaussiana 168
Integrales m√∫ltiples 174
Integrales impropias 186
Software num√©rico y revisi√≥n del cap√≠tulo 191

5 Problemas de valor inicial para ecuaciones de
diferenciales ordinarias
5.1
5.2
5.3
5.4
5.5
5.6
5.7
5.8
5.9
5.10
5.11
5.12

193

Teor√≠a elemental de problemas de valor inicial 194
M√©todo de Euler 198
M√©todos de Taylor de orden superior 205
M√©todo Runge-Kutta 209
Control de error y m√©todo Runge-Kutta-Fehlberg 218
M√©todos multipasos 224
M√©todo multipasos de tama√±o de paso variable 236
M√©todos de extrapolaci√≥n 241
Ecuaciones de orden superior y sistemas de ecuaciones diferenciales 247
Estabilidad 254
Ecuaciones diferenciales r√≠gidas 262
Software num√©rico 268

6 M√©todos directos para resolver sistemas lineales
6.1
6.2
6.3
6.4
6.5
6.6
6.7

Sistemas de ecuaciones lineales 270
Estrategias de pivoteo 279
√Ålgebra lineal e inversi√≥n de matriz 287
Determinante de una matriz 296
Factorizaci√≥n de matriz 298
Tipos especiales de matrices 306
Software num√©rico 318

7 T√©cnicas iterativas en √°lgebra de matrices 319
7.1
7.2
7.3



7.7

Normas de vectores y matrices 320
Eigenvalores y eigenvectores 329
T√©cnicas iterativas de Jacobi y Gauss-Siedel 334
7pFQLFDVGHUHODMDFLyQSDUDUHVROYHUVLVWHPDVOLQHDOHV 
&RWDVGHHUURU\UH√ÄQDPLHQWRLWHUDWLYR 
(OPpWRGRGHJUDGLHQWHFRQMXJDGR 
Software num√©rico 366

269

Contenido

8 Teor√≠a de aproximaci√≥n 369
8.1
8.2
8.3
8.4
8.5
8.6
8.7

Aproximaci√≥n por m√≠nimos cuadrados discretos 370
Polinomios ortogonales y aproximaci√≥n por m√≠nimos cuadrados 378
Polinomios de Chebyshev y ahorro de series de potencia 385
Aproximaci√≥n de funci√≥n racional 393
Aproximaci√≥n polinomial trigonom√©trica 402
Transformadas r√°pidas de Fourier 410
Software num√©rico 419

9 Aproximaci√≥n de eigenvalores
9.1
9.2
9.3
9.4
9.5
9.6
9.7

421

√Ålgebra lineal y eigenvalores 422
Matrices ortogonales y transformaciones de similitud 428
El m√©todo de potencia 431
M√©todo de Householder 445
El algoritmo QR 452
Descomposici√≥n en valores singulares 462
Software num√©rico 474

10 Soluciones num√©ricas de sistemas de ecuaciones
no lineales

475

 3XQWRV√ÄMRVSDUDIXQFLRQHVGHYDULDVYDULDEOHV 
10.2 M√©todo de Newton 482
10.3 M√©todos cuasi-Newton 487
10.4 T√©cnicas de descenso m√°s r√°pido 492
10.5 Homotop√≠a y m√©todos de continuaci√≥n 498
10.6 Software num√©rico 504

11 Problemas de valor en la frontera para ecuaciones
diferenciales ordinarias

505

11.1 El m√©todo de disparo lineal 506
11.2 El m√©todo de disparo para problemas no lineales 512
 0pWRGRVGHGLIHUHQFLDV√ÄQLWDVSDUDSUREOHPDVOLQHDOHV 
 0pWRGRVGHGLIHUHQFLDV√ÄQLWDVSDUDSUREOHPDVOLQHDOHV 
11.5 El m√©todo de Rayleigh-Ritz 527
11.6 Software num√©rico 540

v

vi

Contenido

12 Soluciones num√©ricas para ecuaciones
diferenciales parciales

541

12.1 Ecuaciones diferenciales parciales el√≠pticas 544
12.2 Ecuaciones diferenciales parciales parab√≥licas 551
12.3 Ecuaciones diferenciales parciales hiperb√≥licas 562
 8QDLQWURGXFFLyQDOPpWRGRGHHOHPHQWRV√ÄQLWRV 
12.5 Software num√©rico 579

Material en l√≠nea
El siguiente material se encuentra disponible en l√≠nea:
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢

&RQMXQWRVGHHMHUFLFLRV
Preguntas de an√°lisis
Conceptos clave
Revisi√≥n de cap√≠tulo
Bibliograf√≠a
5HVSXHVWDVDHMHUFLFLRVVHOHFFLRQDGRV
√çndice
√çndice de algoritmos
Glosario de notaci√≥n
Trigonometr√≠a
*Ui√ÄFDVFRPXQHV

Ingrese a www.cengage.com, busque el libro por el ISBN e ingrese el siguiente c√≥digo de acceso:

Prefacio
Acerca del texto
Este texto se escribi√≥ para una secuencia de cursos sobre la teor√≠a y la aplicaci√≥n de t√©cnicas
de aproximaci√≥n num√©rica. Est√° dise√±ado principalmente para los estudiantes avanzados de
matem√°ticas y ciencias e ingenier√≠a de nivel b√°sico que han terminado por lo menos el primer a√±o de la sucesi√≥n de c√°lculo universitario est√°ndar. La familiaridad con los fundaPHQWRVGHiOJHEUDGHPDWULFHV\ODVHFXDFLRQHVGLIHUHQFLDOHVHV~WLOSHURH[LVWHVX√ÄFLHQWH
material introductorio sobre estos temas, por lo que los cursos relacionados con ellos no son
por fuerza requisitos previos.
Se han usado ediciones previas de An√°lisis num√©rico en distintas situaciones. En algunos casos, el an√°lisis matem√°tico subyacente al desarrollo de t√©cnicas de aproximaci√≥n
recibi√≥ m√°s √©nfasis que los m√©todos; en otros, el √©nfasis era inverso. El libro se ha utilizado
como referencia central para los cursos de nivel posgrado en ingenier√≠a, matem√°ticas, programas de ciencias de la computaci√≥n y cursos de primer a√±o sobre an√°lisis introductorio
LPSDUWLGRVHQXQLYHUVLGDGHVLQWHUQDFLRQDOHV+HPRVDGDSWDGRHOOLEURSDUDDMXVWDUQRVDORV
GLIHUHQWHVXVXDULRVVLQFRPSURPHWHUQXHVWURREMHWLYRRULJLQDO
Presentar t√©cnicas modernas de aproximaci√≥n; explicar c√≥mo, porqu√© y cu√°ndo se puede esperar que funcionen, y proporcionar las bases para m√°s estudios de an√°lisis num√©ULFR\FiOFXORFLHQWt√ÄFR
(OOLEURFRQWLHQHPDWHULDOVX√ÄFLHQWHSDUDDOPHQRVXQDxRFRPSOHWRGHHVWXGLRSHURHVperamos que muchas personas lo usen solamente para un curso de un solo plazo. En dichos
FXUVRVORVHVWXGLDQWHVDSUHQGHQDLGHQWL√ÄFDUORVWLSRVGHSUREOHPDVTXHUHTXLHUHQWpFQLFDV
QXPpULFDV SDUD VX VROXFLyQ \ REVHUYDQ HMHPSORV GH HUURU GH SURSDJDFLyQ TXH VH SXHGHQ
presentar al aplicar m√©todos num√©ricos. Ellos se aproximan con precisi√≥n a la soluci√≥n de
los problemas que no se pueden resolver exactamente y aprenden t√©cnicas comunes para
calcular las cotas del error para sus aproximaciones. El resto del texto sirve como referencia
para los m√©todos que no se consideran en el curso. El tratamiento tanto para los cursos de
WRGRXQDxRFRPRSDUDORVFXUVRVGHXQVRORSOD]RHVFRQVLVWHQWHFRQOD√ÄORVRItDGHOWH[WR
3UiFWLFDPHQWH WRGRV ORV FRQFHSWRV HQ HO WH[WR HVWiQ LOXVWUDGRV FRQ XQ HMHPSOR \ HVWD
HGLFLyQFRQWLHQHPiVGHHMHUFLFLRVSUREDGRVHQFODVHTXHYDQGHVGHDSOLFDFLRQHVIXQdamentales de m√©todos y algoritmos hasta generalizaciones y extensiones de la teor√≠a. AdePiVORVFRQMXQWRVGHHMHUFLFLRVLQFOX\HQYDULRVSUREOHPDVDSOLFDGRVGHGLYHUVDViUHDVGH
la ingenier√≠a, as√≠ como de la f√≠sica, la inform√°tica, la biolog√≠a y las ciencias econ√≥micas y
sociales. Las aplicaciones, seleccionadas de forma clara y concisa, demuestran la manera en
la que las t√©cnicas num√©ricas pueden y, a menudo, se aplican en situaciones de la vida real.
Se ha desarrollado una serie de paquetes de software, conocidos como Sistemas de
√Ålgebra Computacional (CAS) para producir c√°lculos matem√°ticos simb√≥licos. Maple¬©,
Mathematica¬© y MATLAB¬© destacan en el ambiente acad√©mico. Existen versiones para
estudiantes de estos paquetes de software a un precio razonable para la mayor√≠a de los

vii

viii

Prefacio

sistemas inform√°ticos. Adem√°s, ahora existe Sage, un sistema de c√≥digo abierto. La informaci√≥n sobre este sistema se puede encontrar en el sitio
http://www.sagemath.org
Aunque existen diferencias entre los paquetes, tanto de desempe√±o como de precio,
todos efect√∫an operaciones de c√°lculo y algebraicas est√°ndar.
/RVUHVXOWDGRVHQPXFKRVGHQXHVWURVHMHPSORV\HMHUFLFLRVVHKDQJHQHUDGRSRUPHGLR
de problemas para los que se puedenGHWHUPLQDUYDORUHVH[DFWRV\DTXHHVWRSHUPLWHPHMRUDU
el desempe√±o de la supervisi√≥n del m√©todo de aproximaci√≥n. Adem√°s, para muchas t√©cnicas
num√©ricas, el an√°lisis de error requiere limitar una derivada ordinaria o parcial superior de
una funci√≥n, lo cual puede ser una tarea tediosa y que no es especialmente instructiva una
vez que se dominan las t√©cnicas de c√°lculo. Por lo tanto, tener un paquete inform√°tico simb√≥lico puede ser muy √∫til en el estudio de las t√©cnicas de aproximaci√≥n porque, a menudo, las
soluciones exactas se obtienen f√°cilmente por medio de c√°lculos simb√≥licos. Las derivadas
se pueden obtener simb√≥licamente de manera r√°pida y, con frecuencia, un poco de perspectiva permite que el c√°lculo simb√≥lico tambi√©n ayude al proceso de limitaci√≥n.

Algoritmos y programas
En nuestra primera edici√≥n, presentamos una caracter√≠stica que en ese tiempo era innovadoUD\FRQWURYHUWLGD(QOXJDUGHSUHVHQWDUQXHVWUDVWpFQLFDVGHDSUR[LPDFLyQHQXQOHQJXDMH
GHSURJUDPDFLyQHVSHFt√ÄFR )2575$1GRPLQDEDHQHVDpSRFD GLPRVDOJRULWPRVHQXQ
pseudoc√≥digo que conducir√≠a a un programa bien estructurado en una variedad de lenguaMHV$OLQLFLRGHODVHJXQGDHGLFLyQOLVWDPRVORVSURJUDPDVHQOHQJXDMHVHVSHFt√ÄFRVHQHO
Manual del Instructor SDUDHOOLEUR\HOQ~PHURGHHVWRVOHQJXDMHVVHKDLQFUHPHQWDGRHQ
ODV HGLFLRQHV SRVWHULRUHV$KRUD FRGL√ÄFDPRV ORV SURJUDPDV \ HVWiQ GLVSRQLEOHV HQ OtQHD
HQYDULRVGHORVOHQJXDMHVGHSURJUDPDFLyQPiVFRPXQHV\KRMDVGHFiOFXOR&$67RGRV
estos se encuentran en el sitio web que acompa√±a al libro (consulte ‚ÄúComplementos‚Äù [disponibles s√≥lo para la edici√≥n en ingl√©s y se venden por separado]).
Para cada algoritmo existe un programa escrito en Fortran, Pascal, C y Java. Adem√°s,
FRGL√ÄFDPRVORVSURJUDPDVXVDQGR0DSOH0DWKHPDWLFD\0$7/$%(VWRGHEHUtDJDUDQWL]DU
TXHKD\DXQFRQMXQWRGHSURJUDPDVGLVSRQLEOHVSDUDODPD\RUtDGHORVVLVWHPDVGHFRPSXtadora comunes.
Todos los programas se ilustran con un programa muestra, correlacionado estrechamenWHFRQHOWH[WR(VWRSHUPLWHHMHFXWDULQLFLDOPHQWHHOSURJUDPDHQHOOHQJXDMHGHVXHOHFFLyQ
para observar el formato de entrada y salida. A continuaci√≥n, los programas se pueden moGL√ÄFDUSDUDRWURVSUREOHPDVDOUHDOL]DUFDPELRVPHQRUHV/RVIRUPDWRVGHHQWUDGD\VDOLGD
son, en la medida de lo posible, iguales en cada uno de los sistemas de programaci√≥n. Esto
permite al instructor usar los programas para analizarlos de manera gen√©rica, de manera independiente del sistema de programaci√≥n particular que use un solo estudiante.
/RVSURJUDPDVHVWiQGLVHxDGRVSDUDHMHFXWDUVHHQXQDFRPSXWDGRUDFRQFRQ√ÄJXUDFLRQHV
PtQLPDV\VHSURSRUFLRQDHQIRUPDWR$6&,,SDUDSHUPLWLU√ÅH[LELOLGDGGHXVR(VWRKDFHPRGL√ÄFDUORVPHGLDQWHFXDOTXLHUHGLWRURSURFHVDGRUGHSDODEUDVTXHFUHHDUFKLYRV$6&,,HVWiQdar. (En general, tambi√©n reciben el nombre de archivos ‚Äús√≥lo texto‚Äù.) Se incluyen archivos
5($'0(H[WHQVRVMXQWRFRQORVDUFKLYRVGHOSURJUDPDSRUORTXHODVSHFXOLDULGDGHVGH
diferentes sistemas de programaci√≥n se pueden abordar de manera individual. Los archivos
README se presentan tanto en formato ASCII como en archivos PDF. Puesto que se desarrolla software nuevo, los programas se actualizar√°n y colocar√°n en el sitio web del libro.
Para la mayor√≠a de los sistemas de programaci√≥n se necesita software adecuado, como un
compilador para Pascal, Fortran y C, o uno de los Sistemas de √Ålgebra Computacional (Maple,
Mathematica y MATLAB). Las implementaciones Java son una excepci√≥n. Usted necesita el
VLVWHPDSDUDHMHFXWDUORVSURJUDPDVSHUR-DYDVHSXHGHGHVFDUJDUJUDWLVGHVGHGLIHUHQWHVVLWLRV
ZHE/DPHMRUIRUPDGHREWHQHU-DYDHVXWLOL]DUXQPRWRUGHE~VTXHGDSDUDEXVFDUHOQRPEUH
seleccionar el sitio web de descarga y seguir las instrucciones provistas en el mismo.

Prefacio

ix

Nuevo en esta edici√≥n
La primera edici√≥n de este libro se public√≥ hace m√°s de 35 a√±os, en la d√©cada anterior a los
DYDQFHVPiVLPSRUWDQWHVHQWpFQLFDVQXPpULFDVORVFXDOHVUH√ÅHMDQODDPSOLDGLVSRQLELOLGDG
del equipo inform√°tico. En nuestras revisiones del libro, hemos a√±adido t√©cnicas nuevas, en
un intento por mantener nuestro trato actual. Para continuar con esta tendencia, realizamos
XQDVHULHGHFDPELRVVLJQL√ÄFDWLYRVHQHVWDHGLFLyQ
¬á 5HHVFULELPRVDOJXQRVGHORVHMHPSORVHQHOOLEURSDUDHQIDWL]DUPHMRUHOSUREOHPDTXHVH
va a resolver antes de proporcionar la soluci√≥n. Agregamos pasos adicionales a algunos
GHORVHMHPSORVSDUDPRVWUDUH[SOtFLWDPHQWHORVFiOFXORVUHTXHULGRVSDUDORVSULPHURVSDsos de los procesos de iteraci√≥n. Esto da a los lectores una forma de probar y depurar los
SURJUDPDVTXHHVFULEHQSDUDSUREOHPDVVLPLODUHVDORVHMHPSORV
¬á 'LYLGLPRVORVHMHUFLFLRVGHOFDStWXORHQFRPSXWDFLRQDOHVDSOLFDGRV\WHyULFRVSDUDSURSRUFLRQDUPiV√ÅH[LELOLGDGDOLQVWUXFWRUDODVLJQDUODWDUHD&DVLHQWRGDVODVVLWXDFLRQHV
FRPSXWDFLRQDOHVORVHMHUFLFLRVVHHPSDUHMDURQGHIRUPDSDUHLPSDU3XHVWRTXHORVSURblemas impares se resuelven en la √∫ltima parte del texto, si los problemas pares se asignan
FRPRWDUHDORVHVWXGLDQWHVSRGUiQWUDEDMDUORVSUREOHPDVLPSDUHV\YHUL√ÄFDUVXVUHVSXHVtas antes de resolver el problema par.
¬á $JUHJDPRVPXFKRVHMHUFLFLRVDSOLFDGRVQXHYRVDOWH[WR
‚Ä¢ Incluimos preguntas de an√°lisis despu√©s de cada secci√≥n del cap√≠tulo, principalmente para
uso del instructor en los cursos en l√≠nea.
‚Ä¢ Renombramos la √∫ltima secci√≥n de cada cap√≠tulo y la dividimos en cuatro subsecciones:
Software num√©rico, Preguntas de an√°lisis, Conceptos clave y Revisi√≥n del cap√≠tulo (que
se encuentran disponibles en l√≠nea). La mayor√≠a de las Preguntas de an√°lisis llevan al estudiante hacia √°reas modernas de investigaci√≥n en el desarrollo de software.
‚Ä¢ Reorganizamos partes del texto para facilitar la instrucci√≥n en l√≠nea.
‚Ä¢ Agregamos diapositivas en PowerPoint para complementar el material de lectura (disponibles s√≥lo para la versi√≥n en ingl√©s).
¬á $FWXDOL]DPRVHOPDWHULDOELEOLRJUi√ÄFRSDUDUH√ÅHMDUODVQXHYDVHGLFLRQHVGHORVOLEURVTXH
consultamos. Agregamos nuevas fuentes que antes no estaban disponibles.
Como siempre con nuestras revisiones, examinamos todos los enunciados para determinar si
HVWDEDQUHGDFWDGRVGHODPHMRUIRUPDUHODFLRQDGDFRQORTXHWUDWDPRVGHGHVFULELU

Complementos
Los autores crearon un sitio web que acompa√±a el libro, el cual contiene los materiales complementarios que se mencionan m√°s adelante. El sitio web se encuentra en
https://sites.google.com/site/numericalanalysis1burden/
es para estudiantes e instructores. Parte del material en el sitio web es s√≥lo para uso del
instructor. Los instructores pueden acceder al material protegido al contactar a los autores
para obtener la contrase√±a (disponibles s√≥lo para la versi√≥n en ingl√©s y se venden por
separado).

x

Prefacio

Algunos de los complementos tambi√©n se pueden obtener en
https://www.cengagebrain.com
mediante la b√∫squeda del ISBN (disponibles s√≥lo para la versi√≥n en ingl√©s).
1.

2.

3.

4.

5.

6.

7.
8.

Ejemplos del programa para estudiantes que contienen c√≥digo Maple, Matlab y
Excel para que los estudiantes lo utilicen para resolver problemas del texto. Est√°
organizado en forma paralela al texto, cap√≠tulo por cap√≠tulo. Se ilustran los comandos en estos sistemas. Los comandos se presentan en segmentos de programa muy
FRUWRVSDUDPRVWUDUODIRUPDGHUHVROYHUORVHMHUFLFLRVVLQSURJUDPDFLyQH[WHQVD
Conferencias para estudiantes que contienen una perspectiva adicional al contenido
del cap√≠tulo. Estas conferencias se escribieron principalmente para el aprendiz en
l√≠nea, pero pueden ser √∫tiles para estudiantes que toman el curso de manera tradicional.
Gu√≠a de estudio para el estudiante que contiene las soluciones de muchos de los
problemas. Los primeros dos cap√≠tulos de esta gu√≠a est√°n disponibles en el sitio
web del libro en formato PDF, por lo que los posibles usuarios pueden decir si los
HQFXHQWUDQVX√ÄFLHQWHPHQWH~WLOHV7RGDODJXtDVHSXHGHREWHQHUVyORDSDUWLUGHO
editor al llamar a Cengage Learning Customer & Sales Support al 1-800-354-9706
o al ordenar en l√≠nea en http://www.cengagebrain.com/.
Programas de algoritmo que son programas completos escritos en Maple, Matlab,
Mathematica, C, Pascal, Fortran y Java para todos los algoritmos en el texto. Estos
SURJUDPDV HVWiQ SUHYLVWRV SDUD HVWXGLDQWHV PiV H[SHULPHQWDGRV HQ OHQJXDMHV GH
programaci√≥n.
Diapositivas en PowerPoint para el instructor en formato PDF para uso del instructor, tanto para cursos tradicionales como en l√≠nea. Contacte a los autores para
obtener la contrase√±a.
Manual del instructor TXHSURYHHUHVSXHVWDV\VROXFLRQHVSDUDWRGRVORVHMHUFLFLRV
en el libro. Los resultados de los c√°lculos en el Manual del Instructor se regeneraron
para esta edici√≥n mediante los programas en el sitio web para garantizar la compatibilidad entre los diferentes sistemas de programaci√≥n. Contacte a los autores para
obtener la contrase√±a.
Pruebas muestra para el instructor para uso del instructor. Contacte a los autores
para obtener la contrase√±a.
(UUDWDV

Posibles sugerencias para el curso
An√°lisis num√©ricoHVWiGLVHxDGRSDUDSURSRUFLRQDU√ÅH[LELOLGDGDORVLQVWUXFWRUHVDOHOHJLUORV
temas, as√≠ como en el nivel de rigor te√≥rico y en el √©nfasis sobre las aplicaciones. Junto con
HVWRVREMHWLYRVSURSRUFLRQDPRVUHIHUHQFLDVGHWDOODGDVSDUDORVUHVXOWDGRVTXHQRVHGHPXHVtran en el texto y para las aplicaciones que se utilizan para se√±alar la importancia pr√°ctica
de los m√©todos. Las referencias de los textos citados son aquellas que se pueden encontrar
FRQPD\RUIDFLOLGDGHQODVELEOLRWHFDVXQLYHUVLWDULDV\VHDFWXDOL]DURQSDUDUH√ÅHMDUHGLFLRQHV
recientes. Tambi√©n incluimos citas de los documentos originales de investigaci√≥n cuando
sentimos que este material es accesible para nuestra audiencia prevista. Todos los materiales
consultados se han indexado en las ubicaciones adecuadas en el texto y se incluye la informaci√≥n sobre la convocatoria de la Biblioteca del Congreso para el material de referencia
para permitir localizarlo f√°cilmente al buscarlo en la biblioteca.
(OVLJXLHQWHGLDJUDPDGH√ÅXMRLQGLFDORVUHTXLVLWRVSUHYLRVGHOFDStWXOR/DPD\RUSDUWH
de las secuencias posibles que se pueden generar a partir de este diagrama fueron ense√±adas
por los autores en Youngstown State University.

Prefacio

xi

El material en esta edici√≥n debe permitir a los instructores preparar un curso universitario de √°lgebra lineal num√©rica para los estudiantes que no han estudiado an√°lisis num√©rico
antes. Esto se puede realizar al cubrir los cap√≠tulos 1, 6, 7 y 9.
Cap√≠tulo 1

Cap√≠tulo 2

Cap√≠tulo 10

Cap√≠tulo 6

Cap√≠tulo 7

Cap√≠tulo 3

Cap√≠tulo 8

Cap√≠tulo 4

Cap√≠tulo 5

Cap√≠tulo 9
Cap√≠tulo 11
Cap√≠tulo 12

Reconocimientos
Afortunadamente obtuvimos las impresiones de muchos de nuestros estudiantes y colegas
sobre ediciones anteriores de este libro y hemos tomado estos comentarios muy en serio. HePRVLQWHQWDGRLQFOXLUWRGDVODVVXJHUHQFLDVTXHFRPSOHPHQWDQOD√ÄORVRItDGHOOLEUR\HVWDPRV
sumamente agradecidos con todos aquellos que se han tomado el tiempo para contactarnos y
SURSRUFLRQDUQRVIRUPDVGHPHMRUDUODVYHUVLRQHVSRVWHULRUHV
Nos gustar√≠a agradecer especialmente a las siguientes personas, cuyas sugerencias han
sido √∫tiles para √©sta y otras ediciones previas.
Douglas Carter,
John Carroll, Dublin University
Yavuz Duman, T.C. Istanbul Kultur Universitesi
Neil Goldman,
Christopher Harrison,
Teryn Jones, Youngstown State University
Aleksandar Samardzic, University of Belgrade
Mikhail M. Shvartsman, University of St. Thomas
Dennis C. Smolarski, Santa Clara University
Dale Smith, Comcast
Nos gustar√≠a agradecer a la doctora Barbara T. Faires por su cooperaci√≥n al proporcionarnos los materiales necesarios para hacer posible esta revisi√≥n. Su gentileza durante estos
tiempos dif√≠ciles fue muy apreciada.

xii

Prefacio

Siguiendo nuestra pr√°ctica en las ediciones pasadas, hemos obtenido la ayuda de los
estudiantes de la Youngstown State University. Nuestro capaz asistente para esta edici√≥n
IXH7HU\Q-RQHVTXLHQWUDEDMyHQORVDSSOHWVGH-DYD1RVJXVWDUtDDJUDGHFHUD(GZDUG5
%XUGHQXQHVWXGLDQWHGHGRFWRUDGRHQ,QJHQLHUtDHOpFWULFDHQOD2KLR6WDWH8QLYHUVLW\TXLHQ
KD HVWDGR YHUL√ÄFDQGR WRGRV ORV SUREOHPDV GH DSOLFDFLyQ \ HO PDWHULDO QXHYR HQ HO WH[WR
Tambi√©n nos gustar√≠a expresar nuestra gratitud con nuestros colegas en la facultad y administraci√≥n de la Youngstown State University por darnos la oportunidad y las instalaciones
para completar este proyecto.
Nos gustar√≠a agradecer a algunas de las personas que hicieron contribuciones importantes a la historia de los m√©todos num√©ricos. Herman H. Goldstine escribi√≥ un libro excelente
titulado A History of Numerical Analysis from the 16th Through the 19th Century [Golds].
2WUDIXHQWHGHH[FHOHQWHFRQRFLPLHQWRPDWHPiWLFRKLVWyULFRHVHODUFKLYR0DF7XWRU+LVWRU\
RI0DWKHPDWLFVHQOD8QLYHUVLW\RI6W$QGUHZVHQ(VFRFLD)XHFUHDGRSRU-RKQ-2¬∑&RQnor y Edmund F. Robertson y tiene la direcci√≥n de internet
http://www-gap.dcs.st-and.ac.uk/‚àºhistory/
6HKDGHGLFDGRXQDFDQWLGDGLQFUHtEOHGHWUDEDMRDODFUHDFLyQGHPDWHULDOHQHVWHVLWLRZHE\
descubrimos que la informaci√≥n es invariablemente precisa. Finalmente, gracias a todos los
que contribuyen con Wikipedia, quienes han agregado su experiencia a ese sitio web para
TXHRWURVSXHGDQEHQH√ÄFLDUVHGHVXFRQRFLPLHQWR
Por √∫ltimo, gracias nuevamente a aquellos que han dedicado tiempo y esfuerzo en contactarnos durante a√±os. Ha sido grandioso escuchar a tantos estudiantes y docentes, quienes
utilizan nuestro libro para su primera exposici√≥n al estudio de m√©todos num√©ricos. Esperamos que esta edici√≥n contin√∫e con este intercambio y se sume al deleite de los estudiantes de
DQiOLVLVQXPpULFR6LWLHQHDOJXQDVXJHUHQFLDSDUDPHMRUDUHGLFLRQHVIXWXUDVGHOOLEURFRPR
siempre, agradeceremos sus comentarios. Cont√°ctenos f√°cilmente por correo electr√≥nico a
WUDYpVGHODVGLUHFFLRQHVHQXPHUDGDVPiVDEDMR
5LFKDUG/%XUGHQ
rlburden@ysu.edu
$QQHWWH0%XUGHQ
amburden@ysu.edu

Esta edici√≥n est√° dedicada a la memoria de
J. Douglas Faires.
Doug fue amigo, colega y coautor durante m√°s de 40 a√±os.
Se le echar√° mucho de menos.

CAP√çTULO

1

Preliminares matem√°ticos
y an√°lisis de error
Introducci√≥n
Al comenzar los cursos de qu√≠mica, estudiamos la ley del gas ideal,
PV = NRT,
que relaciona la presi√≥n P, el volumen V, la temperatura T y el n√∫mero de moles N de un
gas ‚Äúideal‚Äù. En esta ecuaci√≥n, R es una contante que depende del sistema de medici√≥n.
Suponga que se realizan dos experimentos para evaluar esta ley, mediante el mismo gas
en cada caso. En el primer experimento,

P = 1.00 atm,

V = 0.100 m3 ,

N = 0.00420 mol,

R = 0.08206.

La ley del gas ideal predice que la temperatura del gas es

T =

PV
(1.00)(0.100)
=
= 290.15 K = 17‚ó¶ C.
NR
(0.00420)(0.08206)

Sin embargo, cuando medimos la temperatura del gas, encontramos que la verdadera temperatura es 15‚ó¶C.

V1
V2

A continuaci√≥n, repetimos el experimento utilizando los mismos valores de R y N, pero
incrementamos la presi√≥n en un factor de dos y reducimos el volumen en ese mismo factor.
El producto PV sigue siendo el mismo, por lo que la temperatura prevista sigue siendo 17‚ó¶C.
Sin embargo, ahora encontramos que la temperatura real del gas es 19‚ó¶C.
1

2

CAP√çTULO 1

Preliminares matem√°ticos y an√°lisis de error

Claramente, se sospecha la ley de gas ideal, pero antes de concluir que la ley es inv√°lida
en esta situaci√≥n, deber√≠amos examinar los datos para observar si el error se puede atribuir
a los resultados del experimento. En este caso, podr√≠amos determinar qu√© tan precisos deber√≠an ser nuestros resultados experimentales para evitar que se presente un error de esta
magnitud.
El an√°lisis del error involucrado en los c√°lculos es un tema importante en an√°lisis num√©rico y se presenta en la secci√≥n 1.2. Esta aplicaci√≥n particular se considera en el ejercicio
26 de esa secci√≥n.
Este cap√≠tulo contiene una revisi√≥n breve de los temas del c√°lculo de una sola variable
que se necesitar√°n en cap√≠tulos posteriores. Un conocimiento s√≥lido de c√°lculo es fundamental para comprender el an√°lisis de las t√©cnicas num√©ricas y ser√≠a preciso efectuar una revisi√≥n m√°s rigurosa para quienes no han estado en contacto con este tema durante un tiempo.
Adem√°s, existe una introducci√≥n a la convergencia, el an√°lisis de error, la representaci√≥n
GHQ~PHURVHQOHQJXDMHGHPiTXLQD\DOJXQDVWpFQLFDVSDUDFODVL√ÄFDU\PLQLPL]DUHOHUURU
computacional.

1.1 Revisi√≥n de c√°lculo
L√≠mites y continuidad
Los conceptos de l√≠mite y continuidad de una funci√≥n son fundamentales para el estudio del
c√°lculo y constituyen la base para el an√°lisis de las t√©cnicas num√©ricas.
DeÔ¨Ånici√≥n 1.1

Una funci√≥n fGH√ÄQLGDHQXQFRQMXQWRX de n√∫meros reales que tiene el l√≠mite L a x0, escrita
como
l√≠m f (x) = L ,
x‚Üíx0

si, dado cualquier n√∫mero real Œµ > 0, existe un n√∫mero real Œ¥ > 0, de tal forma que

| f (x) ‚àí L| < Œµ,

siempre que

x‚ààX

y

0 < |x ‚àí x0 | < Œ¥.

FRQVXOWHOD√ÄJXUD

Figura 1.1

Œµ

y

y 5 f (x)

L 1e
L
L 2e

x0 2 d

x0

x0 1 d

x

1.1 Revisi√≥n de c√°lculo

DeÔ¨Ånici√≥n 1.2
Los conceptos b√°sicos de
c√°lculo y sus aplicaciones se
GHVDUUROODURQD√ÄQDOHVGHOVLJOR
XVII y a principios del XVIII, pero
los conceptos matem√°ticamente
precisos de l√≠mites y continuidad
se describieron hasta la √©poca
de Augustin Louis Cauchy
¬≤ +HLQULFK(GXDUG
+HLQH ¬≤ \.DUO
:HLHUVWUDVV ¬≤ D√ÄQDOHV
del siglo XIX.

DeÔ¨Ånici√≥n 1.3

3

Sea f XQDIXQFLyQGH√ÄQLGDHQXQFRQMXQWRX de n√∫meros reales y x0 ‚àà X. Entonces f es continua en x0 si
l√≠m f (x) = f (x0 ).
x‚Üíx0

La funci√≥n f es continua en el conjunto X si es continua en cada n√∫mero en X.
El conjunto de todas las funciones que son continuas en el conjunto X se denota como
C(X &XDQGRX es un intervalo de la recta real, se omiten los par√©ntesis en esta notaci√≥n. Por
ejemplo, el conjunto de todas las funciones continuas en el intervalo cerrado [a, b] se denota
como C[a, b]. El s√≠mbolo R denota el conjunto de todos los n√∫meros reales, que tambi√©n
tiene la notaci√≥n del intervalo (‚àí‚àû, ‚àû 3RUHVRHOFRQMXQWRGHWRGDVODVIXQFLRQHVTXHVRQ
continuas en cada n√∫mero real se denota mediante C(R o mediante C(‚àí‚àû, ‚àû 
El l√≠mite de una sucesi√≥nGHQ~PHURVUHDOHVRFRPSOHMRVVHGH√ÄQHGHPDQHUDVLPLODU
Sea {xn }‚àû
n=1XQDVXFHVLyQLQ√ÄQLWDGHQ~PHURVUHDOHV(VWDVXFHVLyQWLHQHHOl√≠mite x (converge a x VLSDUDFXDOTXLHU Œµ  0, existe un entero positivo N(Œµ) tal que |xn ‚àí x| < Œµ siempre
que n > N(Œµ /DQRWDFLyQ

l√≠m xn = x,

n‚Üí‚àû

o

xn ‚Üí x

en

n ‚Üí ‚àû,

VLJQL√ÄFDTXHODVXFHVLyQ{xn }‚àû
n=1 converge a x.
Teorema 1.4

Si fHVXQDIXQFLyQGH√ÄQLGDHQXQFRQMXQWRX de n√∫meros reales y x0 ‚àà X, entonces los siguientes enunciados son equivalentes:
a.

f es continua en x0;

b.

Si {xn }‚àû
n=1 es cualquier sucesi√≥n en X, que converge a x0, entonces
l√≠m n‚Üí‚àû f (xn ) = f (x0 ).

Se asumir√° que las funciones que consideraremos al analizar los m√©todos num√©ricos son
continuas porque √©ste es el requisito m√≠nimo para una conducta predecible. Las funciones
TXHQRVRQFRQWLQXDVSXHGHQSDVDUSRUDOWRSXQWRVGHLQWHUpVORFXDOSXHGHFDXVDUGL√ÄFXOWDdes al intentar aproximar la soluci√≥n de un problema.

Diferenciabilidad
/DVVXSRVLFLRQHVPiVVR√ÄVWLFDGDVVREUHXQDIXQFLyQSRUORJHQHUDOFRQGXFHQDPHMRUHVUHVXOWDGRVGHDSUR[LPDFLyQ3RUHMHPSORQRUPDOPHQWHXQDIXQFLyQFRQXQDJUi√ÄFDVXDYHVH
comportar√≠a de forma m√°s predecible que una con numerosas caracter√≠sticas irregulares. La
condici√≥n de uniformidad depende del concepto de la derivada.
DeÔ¨Ånici√≥n 1.5

Si f HVXQDIXQFLyQGH√ÄQLGDHQXQLQWHUYDORDELHUWRTXHFRQWLHQHx0. La funci√≥n f es diferenciable en x0 si
f (x) ‚àí f (x0 )
f (x0 ) = l√≠m
x‚Üíx0
x ‚àí x0
existe. El n√∫mero f (x0 ) recibe el nombre de derivada de f en x0. Una funci√≥n que tiene una
derivada en cada n√∫mero en un conjunto X es diferenciable en X.
La derivada de f en x0HVODSHQGLHQWHGHODUHFWDWDQJHQWHDODJUi√ÄFDGHf en (x0 , f (x0 )),
FRPRVHPXHVWUDHQOD√ÄJXUD

4

CAP√çTULO 1

Preliminares matem√°ticos y an√°lisis de error

Figura 1.2
y
La recta tangente tiene
una pendiente f 9(x0)

f (x 0)

(x 0, f (x 0))

y 5 f (x)

x0

Teorema 1.6
El teorema atribuido a Michel
5ROOH ¬≤ DSDUHFLy
en 1691 en un tratado poco
conocido titulado M√©thode pour
r√©soundre les √©galites (M√©todo
para resolver las igualdades).
Originalmente, Rolle criticaba
el c√°lculo desarrollado por Isaac
Newton y Gottfried Leibniz, pero
despu√©s se convirti√≥ en uno de
sus defensores.

Teorema 1.7

x

Si la funci√≥n f es diferenciable en x0, entonces f es continua en x0.
Los siguientes teoremas son de importancia fundamental al deducir los m√©todos para
estimaci√≥n del c√°lculo de error. Las pruebas de estos teoremas y los otros resultados sin referencias en esta secci√≥n se pueden encontrar en cualquier texto de c√°lculo est√°ndar.
El conjunto de todas las funciones que tienen derivadas continuas n en X se denota como
Cn(X y el conjunto de funciones que tienen derivadas de todos los √≥rdenes en X se denota
como C ‚àû (X ). Las funciones polinomial, racional, trigonom√©trica, exponencial y logar√≠tmica se encuentran en C ‚àû (X ), donde X FRQVLVWHHQWRGRVORVQ~PHURVSDUDORVTXHVHGH√ÄQHQ
las funciones. Cuando X es un intervalo de la recta real, de nuevo se omiten los par√©ntesis
en esta notaci√≥n.
(Teorema de Rolle)
Suponga que f ‚àà C[a, b] y f es diferenciable en (a, b 6Lf(a = f (b HQWRQFHVH[LVWHXQQ~mero c en (a, b) con f 9(c =  &RQVXOWHOD√ÄJXUD

Figura 1.3

y

f 9(c) 5 0
y 5 f (x)

f (a) 5 f (b)

a

Teorema 1.8

c

b

x

(Teorema del valor medio)
Si f ‚àà C[a, b] y f es diferenciable en (a, b  HQWRQFHV H[LVWH XQ Q~PHUR c en (a, b con
FRQVXOWHOD√ÄJXUD

f (c) =

f (b) ‚àí f (a)
.
b‚àía

1.1 Revisi√≥n de c√°lculo

5

Figura 1.4
y
L√≠neas paralelas
Pendiente f 9(c)

Pendiente

f (b) 2 f (a)
b2a

c

a

Teorema 1.9

y 5 f (x)

x

b

(Teorema del valor extremo)
Si f ‚àà C[a, b], entonces existe c1, c2 ‚àà [a, b] con f (c1 ‚â§ f (x ‚â§ f (c2 SDUDWRGDVODVx ‚àà [a, b].
Adem√°s, si f es diferenciable en (a, b HQWRQFHVVHSUHVHQWDQORVQ~PHURVc1 y c2 ya sea en
los extremos de [a, b] o donde f 9HVFHUR &RQVXOWHOD√ÄJXUD

Figura 1.5
y

y 5 f (x)

c2

a

Ejemplo 1

c1

b

x

Encuentre los valores m√≠nimo absoluto y m√°ximo absoluto de
f (x = 2 ‚àí ex + 2x
en los intervalos a >, 1] y b >, 2].
Soluci√≥n

Comenzamos por derivar f (x SDUDREWHQHU
f v(x = ‚àíex + 2.

f v(x = 0 cuando ‚àíex + 2 = 0 o de forma equivalente, cuando ex = 2. Al tomar el logaritmo
natural de ambos lados de la ecuaci√≥n obtenemos
ln (ex =OQ  Rx =OQ  ‚âà

6

CAP√çTULO 1

Preliminares matem√°ticos y an√°lisis de error

a)

Cuando el intervalo es [0, 1], el extremo absoluto debe ocurrir en f  , f OQ  R
f  $OHYDOXDUWHQHPRV

f (0) = 2 ‚àí e0 + 2(0) = 1
f (ln (2)) = 2 ‚àí eln (2) + 2 ln (2) = 2 ln (2) ‚âà 1.38629436112
f (1) = 2 ‚àí e + 2(1) = 4 ‚àí e ‚âà 1.28171817154.
Por lo tanto, el m√≠nimo absoluto de f (x  HQ > @ HV f   = 1 y el m√°ximo
absoluto es f OQ  =OQ  
b)

Cuando el intervalo es [1, 2], sabemos que f (x) = 0, por lo que el extremo
absoluto se presenta en f  \f  3RUORWDQWR f (2) = 2 ‚àí e2 + 2(2) = 6 ‚àí e2 ‚âà
.
‚àí1.3890560983
El m√≠nimo absoluto en [1, 2] es 6 ‚àí e2 y el m√°ximo absoluto es 1.
Observamos que

m√°x | f (x)| = |6 ‚àí e2 | ‚âà 1.3890560983.

0‚â§x‚â§2

En general, el siguiente teorema no se presenta en un curso de c√°lculo b√°sico, pero se
deriva al aplicar el teorema de Rolle sucesivamente a f, f , . . . ,\√ÄQDOPHQWHD f (n‚àí1) . Este
resultado se considera en el ejercicio 26.
Teorema 1.10

(Teorema generalizado de Rolle)
Suponga que f ‚àà C[a, b] es n veces diferenciable en (a, b 6Lf (x = 0 en los n + 1 n√∫meros
distintos a a ‚â§ x0 < x1 < 7 < xn ‚â§ b, entonces un n√∫mero c en (x0, xn \SRUORWDQWRHQ a, b 
existe con f (n (c = 0.
Tambi√©n utilizaremos con frecuencia el teorema del valor intermedio. A pesar de que
esta declaraci√≥n parece razonable, su prueba va m√°s all√° del alcance del curso habitual de
c√°lculo. Sin embargo, se puede encontrar en muchos textos de an√°lisis (consulte, por ejemSOR>)X@S 

Teorema 1.11

(Teorema del valor intermedio)
Si f ‚àà C[a, b] y K es cualquier n√∫mero entre f(a \f (b HQWRQFHVH[LVWHXQQ~PHURc en
(a, b SDUDHOFXDOf (c = K.
/D√ÄJXUDPXHVWUDXQDRSFLyQSDUDHOQ~PHURJDUDQWL]DGDSRUHOWHRUHPDGHOYDORU
intermedio. En este ejemplo, existen otras dos posibilidades.

Figura 1.6
y
f (a)

(a, f (a))
y 5 f (x)

K
f (b)

(b, f (b))
a

c

b

x

1.1 Revisi√≥n de c√°lculo

Ejemplo 2

7

Muestre que x5 ‚àí 2x +x2 ‚àí 1 = 0 tiene una soluci√≥n en el intervalo [0, 1].
Soluci√≥n &RQVLGHUH OD IXQFLyQ GH√ÄQLGD SRU f (x  = x5 ‚àí 2x + x2 ‚àí 1. La funci√≥n f es

continua en [0, 1]. Adem√°s,
f  = ‚àí1 < 0

y

0<1=f  .

Por lo tanto, el teorema del valor intermedio implica que existe un n√∫mero c, con 0 , c , 1,
para el cual c5 ‚àí 2c +c2 ‚àí 1 = 0.
Como se observa en el ejemplo 2, el teorema del valor intermedio se utiliza para determinar cu√°ndo existen soluciones para ciertos problemas. Sin embargo, no provee un medio
H√ÄFLHQWHSDUDHQFRQWUDUHVWDVVROXFLRQHV(VWHWHPDVHFRQVLGHUDHQHOFDStWXOR

Integraci√≥n
El otro concepto b√°sico del c√°lculo que se utilizar√° ampliamente es la integral de Riemann.
DeÔ¨Ånici√≥n 1.12
George Fredrich Berhard Riemann
¬≤ UHDOL]yPXFKRVGH
los descubrimientos importantes
SDUDFODVL√ÄFDUODVIXQFLRQHV
que tienen integrales. Tambi√©n
realiz√≥ trabajos fundamentales en
geometr√≠a y la teor√≠a de funciones
complejas y se le considera uno
de los matem√°ticos prol√≠feros del
siglo XIX.

La integral de Riemann de la funci√≥n f en el intervalo [a, b] es el siguiente l√≠mite, siempre
y cuando exista:
b
a

n

f (x) d x =

l√≠m

m√°x xi ‚Üí0

f (z i

xi ,

i=1

donde los n√∫meros x0, x1,7 , xn satisfacen a = x0 ‚â§ x1 ‚â§ 7 ‚â§ xn = b, donde 6xi = xi ‚àí xi‚àí1,
para cada i = 1, 2,7 , n, y zi se selecciona de manera arbitraria en el intervalo [ xi‚àí1 , xi ].
Una funci√≥n f que es continua en un intervalo [a, b] es tambi√©n Riemann integrable en
[a, b]. Esto nos permite elegir, para conveniencia computacional, los puntos xi se separar√°n
equitativamente en [a, b] para cada i = 1, 2, 7 , n, para seleccionar zi = xi. En este caso,
b
a

n

b‚àía
f (x) d x = l√≠m
f (xi ),
n‚Üí‚àû
n i=1

GRQGHORVQ~PHURVPRVWUDGRVHQOD√ÄJXUDFRPRxi, son xi = a + i(b ‚àí a n.

Figura 1.7
y
y 5 f (x)

a 5 x 0 x1

x2 . . . x i21 x i

...

x n21 b 5 x n

x

Se necesitar√°n otros dos resultados en nuestro estudio para an√°lisis num√©rico. El primero
es una generalizaci√≥n del teorema del valor promedio para integrales.

8

CAP√çTULO 1

Preliminares matem√°ticos y an√°lisis de error

Teorema 1.13

(Teorema del valor promedio para integrales)
Suponga que f ‚àà C[a, b], la integral de Riemann de g existe en [a, b], y g(x QRFDPELDGH
signo en [a, b]. Entonces existe un n√∫mero c en (a, b FRQ
b
a

f (x)g(x) d x = f (c)

b
a

g(x) d x.

Cuando g(x ‚â°HOWHRUHPDHVHOWHRUHPDGHOYDORUPHGLRSDUDLQWHJUDOHVeVWH
proporciona el valor promedio de la funci√≥n f sobre el intervalo [a, b] como (consulte la
√ÄJXUD

f (c) =

1
b‚àía

b
a

f (x) d x.

Figura 1.8
y
y 5 f (x)
f (c)

a

c

b

x

(QJHQHUDOODSUXHEDGHOWHRUHPDQRVHGDHQXQFXUVREiVLFRGHFiOFXORSHURVH
SXHGHHQFRQWUDUHQPXFKRVWH[WRVGHDQiOLVLV FRQVXOWHSRUHMHPSOR>)X@S

Polinomios y series de Taylor
(OWHRUHPD√ÄQDOHQHVWDUHYLVLyQGHFiOFXORGHVFULEHORVSROLQRPLRVGH7D\ORU(VWRVSROLQRmios se usan ampliamente en el an√°lisis num√©rico.
Teorema 1.14
%URRN7D\ORU ¬≤ 
describi√≥ esta serie en 1715
en el art√≠culo Methodus
incrementorum directa et inversa
(M√©todos para incrementos
directos e inversos). Isaac
Newton, James Gregory y
otros ya conoc√≠an algunos
casos especiales del resultado
y, probablemente, el resultado
mismo.

(Teorema de Taylor)
Suponga que f ‚àà C n [a, b], f (n +  existe en [a, b], y x0 ‚àà [a, b]. Para cada x ‚àà [a, b], existe un
n√∫mero Œæ(x) entre x0 y x con

f (x) = Pn (x) + Rn (x),
donde

Pn (x) = f (x0 ) + f (x0 )(x ‚àí x0 ) +
n

=

f (k) (x0 )
(x ‚àí x0 )k
k!
k=0

f (x0 )
f (n) (x0 )
(x ‚àí x0 )2 + ¬∑ ¬∑ ¬∑ +
(x ‚àí x0 )n
2!
n!

1.1 Revisi√≥n de c√°lculo

9

y

Rn (x) =
&ROLQ0DFODXULQ  HV
m√°s conocido como el defensor
del c√°lculo de Newton cuando
√©ste fue objeto de los ataques
LPSODFDEOHVGHORELVSR\√ÄOyVRIR
irland√©s George Berkeley.
Maclaurin no descubri√≥ la
serie que lleva su nombre; los
matem√°ticos del siglo ya la
conoc√≠an desde antes de que √©l
naciera. Sin embargo, concibi√≥
un m√©todo para resolver un
sistema de ecuaciones lineales
que se conoce como regla de
Cramer, que Cramer no public√≥
hasta 1750.

Ejemplo 3

f (n+1) (Œæ(x))
(x ‚àí x0 )n+1 .
(n + 1)!

Aqu√≠ Pn(x  HV OODPDGR HO n-√©simo polinomio de Taylor para f alrededor de x0 y Rn(x 
recibe el nombre de residuo (o error de truncamiento UHODFLRQDGRFRQPn(x 3XHVWRTXHHO
n√∫mero Œæ(x) en el error de truncamiento Rn(x GHSHQGHGHOYDORUGHx donde se eval√∫a el polinomio Pn(x HVXQDIXQFLyQGHODYDULDEOHx. Sin embargo, no deber√≠amos esperar ser capaces
de determinar la funci√≥n Œæ(x) de manera expl√≠cita. El teorema de Taylor simplemente garantiza
que esta funci√≥n existe y que su valor se encuentra entre x y x0. De hecho, uno de los problemas
comunes en los m√©todos num√©ricos es tratar de determinar un l√≠mite realista para el valor de
f (n+1) (Œæ(x)) cuando xVHHQFXHQWUDHQXQLQWHUYDORHVSHFt√ÄFR
/DVHULHLQ√ÄQLWDREWHQLGDDOWRPDUHOOtPLWHGHPn(x FRQIRUPH n ‚Üí ‚àû recibe el nombre
de serie de Taylor para f alrededor de x0. En caso de que x0 = 0, entonces al polinomio de
Taylor con frecuencia se le llama polinomio de Maclaurin y a la serie de Taylor a menudo
se le conoce como serie de Maclaurin.
El t√©rmino error de truncamientoHQHOSROLQRPLRGH7D\ORUVHUH√ÄHUHDOHUURULPSOLFDGRDOXWLOL]DUXQDVXPDWUXQFDGDR√ÄQLWDSDUDDSUR[LPDUODVXPDGHXQDVHULHLQ√ÄQLWD
Si f(x = cos x y x0 = 0. Determine
a)

el segundo polinomio de Taylor para f alrededor de x0; y

b)

el tercer polinomio de Taylor para f alrededor de x0.

Soluci√≥n

Puesto que f ‚àà C ‚àû (R), el teorema de Taylor puede aplicarse a cualquiera n ‚â• 0.

Adem√°s,

f (x) = ‚àí sen x, f (x) = ‚àí cos x, f (x) = sen x, y

f (4) (x) = cos x,

Por lo tanto
f (0) = 1, f (0) = 0, f (0) = ‚àí1,
a)

y

f (0) = 0.

Para n = 2 y x0 = 0, obtenemos
cos x = f (0) + f (0)x +

f (0) 2
f (Œæ(x)) 3
x +
x
2!
3!

1
1
= 1 ‚àí x 2 + x 3 sen Œæ(x),
2
6
donde Œæ(x)HVDOJ~QQ~PHUR SRUORJHQHUDOGHVFRQRFLGR HQWUH\x &RQVXOWHOD√ÄJXUD
Figura 1.9
y

1
p
22
2

2p

y 5 cos x
p
2
2
p

1
2

y 5 P2(x) 5 1 2 2 x 2

x

10

CAP√çTULO 1

Preliminares matem√°ticos y an√°lisis de error

Cuando x = 0.01, esto se convierte en

1
1
10‚àí6
cos 0.01 = 1 ‚àí (0.01)2 + (0.01)3 sen Œæ(0.01) = 0.99995 +
sen Œæ(0.01).
2
6
6
Por lo tanto, la aproximaci√≥n para cos 0.01 provista por el polinomio de Taylor es 0.99995.
El error de truncamiento, o t√©rmino restante, relacionado con esta aproximaci√≥n es

10‚àí6
sen Œæ(0.01) = 0.16 √ó 10‚àí6 sen Œæ(0.01),
6
donde la barra sobre el 6 en 0 .16 VHXWLOL]DSDUDLQGLFDUTXHHVWHGtJLWRVHUHSLWHLQGH√ÄQLGDmente. A pesar de que no existe una forma de determinar sen Œæ(0.01), sabemos que todos los
valores del seno se encuentran en el intervalo [‚àí1, 1], por lo que el error que se presenta si
utilizamos la aproximaci√≥n 0.99995 para el valor de cos 0.01 est√° limitado por

| cos(0.01) ‚àí 0.99995| = 0.16 √ó 10‚àí6 | sen Œæ(0.01)| ‚â§ 0.16 √ó 10‚àí6 .
Por lo tanto, la aproximaci√≥n 0.99995 corresponde por lo menos a los primeros cinco d√≠gitos
de cos 0.01 y

0.9999483 < 0.99995 ‚àí 1.6 √ó 10‚àí6 ‚â§ cos 0.01
‚â§ 0.99995 + 1.6 √ó 10‚àí6 < 0.9999517.
El l√≠mite del error es mucho m√°s grande que el error real. Esto se debe, en parte, al escaso l√≠mite que usamos para | sen Œæ(x)|. En el ejercicio 27 se muestra que para todos los valores
de x, tenemos | sen x| ‚â§ |x|. Puesto que 0 ‚â§ Œæ < 0.01, podr√≠amos haber usado el hecho de
que | sen Œæ(x)| ‚â§ 0.01 en la f√≥rmula de error, lo cual produce el l√≠mite 0.16 √ó 10‚àí8 .
b) Puesto que f (0) = 0, el tercer polinomio de Taylor con el t√©rmino restante alrededor de x0 = 0 es

1
1
cos x = 1 ‚àí x 2 + x 4 cos ŒæÃÉ (x),
2
24
donde 0 < ŒæÃÉ (x) < 0.01. El polinomio de aproximaci√≥n sigue siendo el mismo y la aproximaci√≥n sigue siendo 0.99995, pero ahora tenemos mayor precisi√≥n. Puesto que | cos ŒæÃÉ (x)| ‚â§ 1
para todas las x, obtenemos

1 4
1
x cos ŒæÃÉ (x) ‚â§
(0.01)4 (1) ‚âà 4.2 √ó 10‚àí10 .
24
24
por lo tanto

| cos 0.01 ‚àí 0.99995| ‚â§ 4.2 √ó 10‚àí10 ,
y

0.99994999958 = 0.99995 ‚àí 4.2 √ó 10‚àí10
‚â§ cos 0.01 ‚â§ 0.99995 + 4.2 √ó 10‚àí10 = 0.99995000042.
(OHMHPSORLOXVWUDORVGRVREMHWLYRVGHODQiOLVLVQXPpULFR
i)

Encuentre una aproximaci√≥n a la soluci√≥n de un problema determinado.

ii)

Determine un l√≠mite o cota para la precisi√≥n de la aproximaci√≥n.

/RVSROLQRPLRVGH7D\ORUHQDPEDVSDUWHVSURSRUFLRQDQODPLVPDUHVSXHVWDSDUDL SHURHO
WHUFHURSURYHHXQDUHVSXHVWDPXFKRPHMRUSDUDLL TXHHOVHJXQGR7DPELpQSRGHPRVXWLOL]DU
estos polinomios para obtener aproximaciones de las integrales.

1.2 Errores de redondeo y aritm√©tica computacional

Ilustraci√≥n

11

Podemos utilizar el tercer polinomio de Taylor y su t√©rmino restante encontrado en el ejem0.1
SORSDUDDSUR[LPDU 0 cos x d x. Tenemos
0.1

0.1

cos x d x =

0

0

1
1 ‚àí x2
2

dx +

0.1
1
1
+
= x ‚àí x3
6
24
0

1
1
= 0.1 ‚àí (0.1)3 +
6
24

1
24

0.1

0.1

x 4 cos ŒæÃÉ (x) d x

0

x 4 cos ŒæÃÉ (x) d x

0
0.1

x 4 cos ŒæÃÉ (x) d x.

0

Por lo tanto,
0.1
0

1
cos x d x ‚âà 0.1 ‚àí (0.1)3 = 0.09983.
6

Un l√≠mite o cota para el error en esta aproximaci√≥n se determina a partir de la integral del
t√©rmino restante de Taylor y el hecho de que | cos ŒæÃÉ (x)| ‚â§ 1 para todas las x:

1
24

0.1

x 4 cos ŒæÃÉ (x) d x ‚â§

0

‚â§

1
24
1
24

0.1

x 4 | cos ŒæÃÉ (x)| d x

0
0.1
0

x4 dx =

(0.1)5
= 8.3 √ó 10‚àí8 .
120

El valor verdadero de esta integral es
0.1
0

0.1

cos x d x = sen x

= sen 0.1 ‚âà 0.099833416647,
0

por lo que el error real para esta aproximaci√≥n es 8.√ó 10‚àí8, que se encuentra dentro
del l√≠mite del error.
La secci√≥n Conjunto de ejercicios 1.1 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

1.2 Errores de redondeo y aritm√©tica computacional
La aritm√©tica realizada con una calculadora o computadora es diferente a la aritm√©tica que
se imparte en los cursos
‚àö de √°lgebra y c√°lculo. Podr√≠a esperarse que declaraciones como
2 +2 = 4, 4¬∑8 = 32, y ( 3)2 = 3 siempre sean verdaderas; sin embargo con la aritm√©tica
computacional, esperamos
resultados exactos para 2 + 2 = 4 y 4 ¬∑ 8 = 32, pero no obtendre‚àö
mos exactamente ( 3)2 = 3. Para comprender por qu√© esto es verdadero, debemos explorar
HOPXQGRGHODDULWPpWLFDGHGtJLWRV√ÄQLWRV
(QQXHVWURPXQGRPDWHPiWLFRWUDGLFLRQDOSHUPLWLPRVQ~PHURVFRQXQDFDQWLGDGLQ√ÄQL‚àö
ta de d√≠gitos. La aritm√©tica que usamos en este mundo GH√ÄQH 3 como el √∫nico n√∫mero poVLWLYRTXHFXDQGRVHPXOWLSOLFDSRUVtPLVPRSURGXFHHOHQWHUR1RREVWDQWHHQHOPXQGR
computacional, cada n√∫mero representable s√≥ORWLHQHXQQ~PHUR√ÄMR\√ÄQLWRGHGtJLWRV(VWR
VLJQL√ÄFDTXHSRUHMHPSORV√≥lo los n√∫meros
racionales, e incluso no todos ellos, se pueden
‚àö
representar de forma exacta. Ya que 3 no es racional, se proporciona una representaci√≥n
DSUR[LPDGDFX\RFXDGUDGRQRVHUiH[DFWDPHQWHDSHVDUGHTXHHVSUREDEOHTXHHVWpVX√ÄFLHQWHPHQWHFHUFDGHSDUDVHUDFHSWDEOHHQODPD\RUtDGHODVVLWXDFLRQHV(QWRQFHVHQPXchos casos, esta aritm√©tica mec√°nica es satisfactoria y pasa sin importancia o preocupaci√≥n,
pero algunas veces surgen problemas debido a su discrepancia.

12

CAP√çTULO 1

Preliminares matem√°ticos y an√°lisis de error

El error debido al redondeo
deber√≠a esperarse siempre que se
realizan c√°lculos con n√∫meros
que no son potencias de 2.
Mantener este error bajo control
es en extremo importante cuando
el n√∫mero de c√°lculos es grande.

El error que se produce cuando se utiliza una calculadora o computadora para realizar
c√°lculos con n√∫meros reales recibe el nombre de error de redondeo. Se presenta porque la
DULWPpWLFDUHDOL]DGDHQXQDPiTXLQDLQFOX\HQ~PHURVFRQXQVRORQ~PHUR√ÄQLWRGHGtJLWRV
y esto da como resultado c√°lculos realizados √∫nicamente con representaciones aproximadas
de los n√∫meros reales. En una computadora, s√≥lo un subconjunto relativamente peque√±o del
sistema de n√∫meros reales se usa para la representaci√≥n de todos los n√∫meros reales. Este
subconjunto s√≥lo contiene n√∫meros racionales, tanto positivos como negativos, y almacena
la parte fraccionaria, junto con una parte exponencial.

N√∫meros de m√°quina binarios
En 1985, el Instituto para Ingenieros El√©ctricos y Electr√≥nicos (IEEE; Institute for ElectriFDODQG(OHFWURQLF(QJLQHHUV SXEOLFyXQUHSRUWHOODPDGRBinary Floating Point Arithmetic
6WDQGDUG ¬≤ (VWiQGDU SDUD OD DULWPpWLFD ELQDULD GH SXQWR √ÅRWDQWH . En 2008 se
public√≥ una versi√≥n actualizada con el nombre de IEEE 754-2008; la cual proporciona esWiQGDUHV SDUD Q~PHURV GH SXQWR √ÅRWDQWH GHFLPDOHV \ ELQDULRV IRUPDWRV SDUD LQWHUFDPELR
de datos, algoritmos para redondear operaciones aritm√©ticas y manejo de excepciones. Se
HVSHFL√ÄFDFXiOHVVRQORVIRUPDWRVSDUDODVSUHFLVLRQHVLQGLYLGXDOHVGREOHV\DPSOLDGDV\HQ
JHQHUDOWRGRVORVIDEULFDQWHVGHPLFURFRPSXWDGRUDVTXHXWLOL]DQKDUGZDUHGHSXQWR√ÅRWDQWH
siguen estos est√°ndares.
8QDUHSUHVHQWDFLyQGHELWV GtJLWRELQDULR VHXVDSDUDXQQ~PHURUHDO(OSULPHUELW
es un indicador de signo, denominado s. A √©ste le sigue un exponente de 11 bits, c, llamado
caracter√≠stica, y una fracci√≥n binaria de 52 bits, f, llamada mantisa. La base para el exponente es 2.
Puesto que los 52 d√≠gitos binarios corresponden con d√≠gitos decimales entre 16 y 17,
podemos asumir que un n√∫mero representado en este sistema tiene por lo menos 16 d√≠gitos
decimales de precisi√≥n. El exponente de 11 d√≠gitos binarios provee un rango de 0 a 211 ‚àí 1
=6LQHPEDUJRXVDUVyORHQWHURVSRVLWLYRVSDUDHOH[SRQHQWHQRSHUPLWLUtDXQDUHpresentaci√≥n adecuada de los n√∫meros con magnitud peque√±a. Para garantizar que estos
Q~PHURVVRQLJXDOPHQWHUHSUHVHQWDEOHVVHUHVWDDODFDUDFWHUtVWLFDSRUORTXHHOUDQJR
del exponente en realidad se encuentra entre ‚àí\
Para ahorrar almacenamiento y proporcionar una representaci√≥n √∫nica para cada n√∫mero
GHSXQWR√ÅRWDQWHVHLPSRQHXQDQRUPDOL]DFLyQ3RUPHGLRGHHVWHVLVWHPDREWHQHPRVXQ
Q~PHURGHSXQWR√ÅRWDQWHGHODIRUPD

(‚àí1)s 2c‚àí1023 (1 + f ).
Ilustraci√≥n

Considere el n√∫mero de m√°quina
0 10000000011 1011100100010000000000000000000000000000000000000000.
El bit m√°s a la izquierda es s = 0, lo cual indica que es un n√∫mero positivo. Los siguientes
11 bits, 10000000011, proveen la caracter√≠stica y son equivalentes al n√∫mero decimal

c = 1 ¬∑ 210 + 0 ¬∑ 29 + ¬∑ ¬∑ ¬∑ + 0 ¬∑ 22 + 1 ¬∑ 21 + 1 ¬∑ 20 = 1024 + 2 + 1 = 1027.
La parte exponencial del n√∫mero es, por lo tanto, 21027‚àí1023 = 24/RVELWV√ÄQDOHVHVSHFL√ÄFDQTXHODPDQWLVDHV

f =1¬∑

1
2

1

+1¬∑

1
2

3

+1¬∑

1
2

4

+1¬∑

1
2

5

+1¬∑

1
2

8

+1¬∑

1
2

12

.

1.2 Errores de redondeo y aritm√©tica computacional

13

Como secuencia, este n√∫mero de m√°quina representa precisamente el n√∫mero decimal

(‚àí1)s 2c‚àí1023 (1 + f ) = (‚àí1)0 ¬∑ 21027‚àí1023 1 +

1 1
1
1
1
1
+ +
+
+
+
2 8 16 32 256 4096

= 27.56640625.
Sin embargo, el siguiente n√∫mero de m√°quina m√°s peque√±o es
0 10000000011 1011100100001111111111111111111111111111111111111111,
el siguiente n√∫mero de m√°quina m√°s grande es
0 10000000011 1011100100010000000000000000000000000000000000000001.
(VWRVLJQL√ÄFDTXHQXHVWURQ~PHURGHPiTXLQDRULJLQDOQRV√≥ORUHSUHVHQWDVLQR
WDPELpQODPLWDGGHORVQ~PHURVUHDOHVTXHVHHQFXHQWUDQHQWUH\HOVLJXLHQWH
Q~PHURGHPiTXLQDPiVSHTXHxRDVtFRPRODPLWDGGHORVQ~PHURVHQWUH\
el siguiente n√∫mero de m√°quina m√°s grande. Para ser preciso, representa cualquier n√∫mero
>
 
El n√∫mero positivo normalizado m√°s peque√±o que se puede representar tiene s = 0, c = 1,
y f = 0 y es equivalente

2‚àí1022 ¬∑ (1 + 0) ‚âà 0.22251 √ó 10‚àí307 ,
y el m√°s grande tiene s = 0, c =\f = 1 ‚àí 2‚àí52 y es equivalente a

21023 ¬∑ (2 ‚àí 2‚àí52 ) ‚âà 0.17977 √ó 10309 .
Los n√∫meros que se presentan en los c√°lculos que tienen una magnitud menor que

2‚àí1022 ¬∑ (1 + 0)
resultan en un subdesbordamiento\HQJHQHUDOVHFRQ√ÄJXUDQHQFHUR/RVQ~PHURVVXSHriores a

21023 ¬∑ (2 ‚àí 2‚àí52 )
resultan en desbordamiento y, com√∫nmente, causan que los c√°lculos se detengan (a menos
TXH HO SURJUDPD KD\D VLGR GLVHxDGR SDUD GHWHFWDU HVWDV SUHVHQFLDV  2EVHUYH TXH H[LVWHQ
dos representaciones para el n√∫mero cero: un 0 positivo cuando s = 0, c = 0 y f = 0, y un 0
negativo cuando s = 1, c = 0 y f = 0.

N√∫meros de m√°quina decimales
(OXVRGHGtJLWRVELQDULRVWLHQGHDRFXOWDUODVGL√ÄFXOWDGHVFRPSXWDFLRQDOHVTXHVHSUHVHQWDQ
FXDQGRVHXVDXQDFROHFFLyQ√ÄQLWDGHQ~PHURVGHPiTXLQDSDUDUHSUHVHQWDUWRGRVORVQ~PHros reales. Para examinar estos problemas, utilizaremos n√∫meros decimales m√°s familiares
HQOXJDUGHXQDUHSUHVHQWDFLyQELQDULD(QHVSHFt√ÄFRVXSRQHPRVTXHORVQ~PHURVPiTXLQD
VHUHSUHVHQWDQHQIRUPDWRQRUPDOL]DGRGHSXQWR√ÅRWDQWHdecimal

¬±0.d1 d2 . . . dk √ó 10n ,

1 ‚â§ d1 ‚â§ 9,

y

0 ‚â§ di ‚â§ 9,

14

CAP√çTULO 1

Preliminares matem√°ticos y an√°lisis de error

para cada i = 2, . . . , k. Los n√∫meros de esta forma reciben el nombre de n√∫meros de
m√°quina decimales de d√≠gito k.
Cualquier n√∫mero real positivo dentro del rango num√©rico de la m√°quina puede ser
normalizado a la forma

y = 0.d1 d2 . . . dk dk+1 dk+2 . . . √ó 10n .
El error que resulta de
reemplazar un n√∫mero con
HVWDIRUPDGHSXQWR√ÅRWDQWH
se llama error de redondeo,
independientemente de si se
usa el m√©todo de redondeo o de
corte.

/DIRUPDGHSXQWR√ÅRWDQWHGH\TXHVHGHQRWDfl(y , se obtiene al terminar la mantisa de y
en los d√≠gitos decimales de k. Existen dos maneras comunes para realizar esta terminaci√≥n.
Un m√©todo, llamado de corte, es simplemente cortar los d√≠gitos dk+1 dk+2 . . . Esto produce
ODIRUPDGHSXQWR√ÅRWDQWH

f l(y) = 0.d1 d2 . . . dk √ó 10n .
El otro m√©todo, llamado redondeo, suma 5 √ó 10n‚àí(k+1) a y y entonces corta el resultado
para obtener un n√∫mero con la forma

f l(y) = 0.Œ¥1 Œ¥2 . . . Œ¥k √ó 10n .
Para redondear, cuando dk+1 ‚â• 5, sumamos 1 a dk para obtener fl(y HVGHFLUredondeamos
hacia arriba. Cuando dk+1 < 5, simplemente cortamos todo, excepto los primeros d√≠gitos
k; es decir, redondeamos hacia abajo. Si redondeamos hacia abajo, entonces Œ¥i = di, para
cada i = 1, 2, . . . , k. Sin embargo, si redondeamos hacia arriba, los d√≠gitos (e incluso el
H[SRQHQWH SXHGHQFDPELDU
Ejemplo 1

'HWHUPLQHORVYDORUHVD GHFRUWH\E GHUHGRQGHRGHFLQFRGtJLWRVGHOQ~PHURLUUDFLRQDOœÄ.
Soluci√≥n El n√∫mero œÄWLHQHXQDH[SDQVLyQGHFLPDOLQ√ÄQLWDGHODIRUPDœÄ =....
Escrito en una forma decimal normalizada, tenemos

œÄ = 0.314159265 . . . √ó 101 .
En general, el error relativo es
una mejor medici√≥n de precisi√≥n
que el error absoluto porque
considera el tama√±o del n√∫mero
que se va a aproximar.

a) (OIRUPDWRGHSXQWR√ÅRWDQWHGHœÄ usando el recorte de cinco d√≠gitos es

f l(œÄ ) = 0.31415 √ó 101 = 3.1415.
b) El sexto d√≠gito de la expansi√≥n decimal de œÄ es un 9, por lo que el formato de punto
√ÅRWDQWHGHœÄ con redondeo de cinco d√≠gitos es

f l(œÄ ) = (0.31415 + 0.00001) √ó 101 = 3.1416.
/DVLJXLHQWHGH√ÄQLFLyQGHVFULEHWUHVPpWRGRVSDUDPHGLUHUURUHVGHDSUR[LPDFLyQ
DeÔ¨Ånici√≥n 1.15

Suponga que p ‚àó es una aproximaci√≥n a p. El error real es p ‚àí p ‚àó, el error absoluto es
| p ‚àí p‚àó |
| p ‚àí p ‚àó |, y el error relativo es
, siempre y cuando p = 0.
| p|
Considere los errores real, absoluto y relativo al representar p con p ‚àó en el siguiente ejemplo.

Ejemplo 2

Determine los errores real, absoluto y relativo al aproximar p con p ‚àó cuando

a)

p = 0.3000 √ó 101 y p ‚àó = 0.3100 √ó 101 ;

b)

p = 0.3000 √ó 10‚àí3 y p ‚àó = 0.3100 √ó 10‚àí3 ;

c)

p = 0.3000 √ó 104 y p ‚àó = 0.3100 √ó 104 .

1.2 Errores de redondeo y aritm√©tica computacional
Soluci√≥n

a)
b)
A menudo no podemos encontrar
un valor preciso para el error
verdadero en una aproximaci√≥n.
Por el contrario, encontramos
una cota para el error, lo cual nos
proporciona un error del ‚Äúpeor
caso‚Äù.

c)

15

Para p =√ó 101 y p ‚àó =√ó 101, el error real es <0.1, el error absoluto
es 0.1 y el error relativo es 0.3333 √ó 10‚àí1.
Para p =√ó 10√Ø y p ‚àó =√ó 10√Ø, el error real es <0.1 √ó 10√Ø, el error
absoluto es 0.1 √ó 10√Ø y el error relativo es 0.3333 √ó 10‚àí1.
Para p =√ó 10 y p ‚àó =√óHOHUURUUHDOHV√Ø√ó 10, el error absoluto es 0.1 √ó 10 y, de nuevo, el error relativo es 0.3333 √ó 10‚àí1 .

Este ejemplo muestra que el mismo error relativo, 0.3333 √ó 10‚àí1, se presenta para errores
absolutos ampliamente variables. Como una medida de precisi√≥n, el error absoluto puede ser
HQJDxRVR\HOHUURUUHODWLYRPiVVLJQL√ÄFDWLYRGHELGRDTXHHVWHHUURUFRQVLGHUDHOWDPDxR
del valor.
Un l√≠mite de error es un n√∫mero no negativo mayor que el error absoluto. Algunas veces se obtiene con los m√©todos de c√°lculo para encontrar el valor absoluto m√°ximo de una
IXQFLyQ(VSHUDPRVHQFRQWUDUHOOtPLWHVXSHULRUPiVSHTXHxRSRVLEOHSDUDHOHUURUD√ÄQGH
obtener un estimado del error real que es lo m√°s preciso posible.
/D VLJXLHQWH GH√ÄQLFLyQ XVD HO HUURU UHODWLYR SDUD SURSRUFLRQDU XQD PHGLGD GH GtJLWRV
VLJQL√ÄFDWLYRVGHSUHFLVLyQSDUDXQDDSUR[LPDFLyQ

DeÔ¨Ånici√≥n 1.16
A menudo, el t√©rmino d√≠gitos
VLJQL√ÄFDWLYRV se usa para
describir vagamente el n√∫mero
de d√≠gitos decimales que parecen
VHUH[DFWRV/DGH√ÄQLFLyQHVPiV
precisa y provee un concepto
continuo.

Tabla 1.1

Se dice que el n√∫mero p ‚àó se aproxima a p para t GtJLWRVVLJQL√ÄFDWLYRV RFLIUDV VLt es el
entero no negativo m√°s grande para el que

| p ‚àí p‚àó |
‚â§ 5 √ó 10‚àít .
| p|
/DWDEODLOXVWUDODQDWXUDOH]DFRQWLQXDGHORVGtJLWRVVLJQL√ÄFDWLYRVDOHQXPHUDUSDUD
los diferentes valores de p, el l√≠mite superior m√≠nimo de | p ‚àí p ‚àó |, denominado m√°x. | p ‚àí p ‚àó |,
cuando p ‚àó concuerda con pHQFXDWURGtJLWRVVLJQL√ÄFDWLYRV

p

0.1

0.5

100

1000

5000

9990

10000

m√°x | p ‚àí p ‚àó |

0.00005

0.00025

0.05

0.5

2.5

4.995

5.

Al regresar a la representaci√≥n de los n√∫meros de m√°quina, observamos que la represenWDFLyQGHSXQWR√ÅRWDQWHf l(y SDUDHOQ~PHURy tiene el error relativo

y ‚àí f l(y)
.
y
Si se usan k d√≠gitos decimales y corte para la representaci√≥n de m√°quina de

y = 0.d1 d2 . . . dk dk+1 . . . √ó 10n ,
entonces

y ‚àí f l(y)
0.d1 d2 . . . dk dk+1 . . . √ó 10n ‚àí 0.d1 d2 . . . dk √ó 10n
=
y
0.d1 d2 . . . √ó 10n
=

0.dk+1 dk+2 . . .
0.dk+1 dk+2 . . . √ó 10n‚àík
=
√ó 10‚àík .
n
0.d1 d2 . . . √ó 10
0.d1 d2 . . .

16

CAP√çTULO 1

Preliminares matem√°ticos y an√°lisis de error

Puesto que d1 = 0, el valor m√≠nimo del denominador es 0.1. El numerador se limita en la
parte superior mediante 1. Como consecuencia,

y ‚àí f l(y)
1
√ó 10‚àík = 10‚àík+1 .
‚â§
y
0.1
De igual forma, un l√≠mite para el error relativo al utilizar aritm√©tica de redondeo de d√≠gitos k
es 0.5 √ó 10‚àík+1 &RQVXOWHHOHMHUFLFLR
Observe que los l√≠mites para el error relativo mediante aritm√©tica de d√≠gitos k son independientes del n√∫mero que se va a representar. Este resultado se debe a la forma en la que se
distribuyen los n√∫meros de m√°quina a lo largo de la recta real. Debido al formato exponencial de la caracter√≠stica, el mismo n√∫mero de los n√∫meros de m√°quina decimales se usa para
representar cada uno de los intervalos [0.1, 1], [1, 10] y [10, 100]. De hecho, dentro de los
l√≠mites de la m√°quina, el n√∫mero de los n√∫meros de m√°quina decimales en [10 n , 10n+1 ] es
constante para todos los enteros n.

Aritm√©tica de d√≠gitos Ô¨Ånitos
Adem√°s de la representaci√≥n inexacta de n√∫meros, la aritm√©tica que se efect√∫a en una computadora no es exacta. La aritm√©tica implica manipular d√≠gitos binarios mediante diferentes
operaciones de cambio, o l√≥gicas. Puesto que la mec√°nica real de estas operaciones no es
pertinente para esta presentaci√≥n, debemos idear una aproximaci√≥n propia para aritm√©tica
computacional. A pesar de que nuestra aritm√©tica no proporcionar√° el panorama exacto, es
VX√ÄFLHQWHSDUDH[SOLFDUORVSUREOHPDVTXHVHSUHVHQWDQ 3DUDXQDH[SOLFDFLyQGHODVPDQLSXlaciones realmente incluidas, se insta al lector a consultar textos de ciencias computacionales
con una orientaci√≥n m√°s t√©cnica, como [Ma], Computer System Architecture [Arquitectura
de sistemas computacionales
3LHQVHTXHODVUHSUHVHQWDFLRQHVGHSXQWR√ÅRWDQWH√Å(x y fl(y est√°n dadas para los n√∫meros reales x y y y que los s√≠mbolos ‚äï, , ‚äó, y .. representan operaciones de m√°quina
de suma, resta, multiplicaci√≥n y divisi√≥n, respectivamente. Supondremos una aritm√©tica de
GtJLWRV√ÄQLWRVSURYLVWDSRU

x ‚äï y = f l( f l(x) + f l(y)),
x

x ‚äó y = f l( f l(x) √ó f l(y)),
x .. y = f l( f l(x) √∑ f l(y)).

y = f l( f l(x) ‚àí f l(y)),

(VWDDULWPpWLFDFRUUHVSRQGHDUHDOL]DUDULWPpWLFDH[DFWDHQODVUHSUHVHQWDFLRQHVGHSXQWR√ÅRtante de x y y \GHVSXpVFRQYHUWLUHOUHVXOWDGRH[DFWRDVXUHSUHVHQWDFLyQGHSXQWR√ÅRWDQWH
GHGtJLWRV√ÄQLWRV
Ejemplo 3

Suponga que x = 57 y y = 13. Utilice el corte de cinco d√≠gitos para calcular x + y, x ‚àí y, x √ó y,
y x √∑ y.
Soluci√≥n

Observe que

x=

5
= 0.714285 y
7

y=

1
= 0.3
3

implica que los valores de corte de cinco d√≠gitos de x y y son

f l(x) = 0.71428 √ó 100

y

f l(y) = 0.33333 √ó 100 .

Por lo tanto,

x ‚äï y = f l( f l(x) + f l(y)) = f l 0.71428 √ó 100 + 0.33333 √ó 100
= f l 1.04761 √ó 100 = 0.10476 √ó 101 .

1.2 Errores de redondeo y aritm√©tica computacional

17

El valor verdadero es x + y = 57 + 13 = 22
21, por lo que tenemos

Error absoluto =

22
‚àí 0.10476 √ó 101 = 0.190 √ó 10‚àí4
21

Error relativo =

0.190 √ó 10‚àí4
= 0.182 √ó 10‚àí4 .
22/21

y

La tabla 1.2 enumera los valores de √©ste y otros c√°lculos.

Tabla 1.2

Operaci√≥n

Resultado

Valor real

Error absoluto

Error relativo

x‚äïy
x y
x‚äóy
x .. y

0.10476 √ó 101
0.38095 √ó 100
0.23809 √ó 100
0.21428 √ó 101

22/21
8/21
5/21
15/7

0.190 √ó 10‚àí4
0.238 √ó 10‚àí5
0.524 √ó 10‚àí5
0.571 √ó 10‚àí4

0.182 √ó 10‚àí4
0.625 √ó 10‚àí5
0.220 √ó 10‚àí4
0.267 √ó 10‚àí4

(OHUURUUHODWLYRPi[LPRSDUDODVRSHUDFLRQHVHQHOHMHPSORHV 0.267 √ó 10‚àí4, por lo
TXHODDULWPpWLFDSURGXFHUHVXOWDGRVVDWLVIDFWRULRVGHFLQFRGtJLWRVeVWHQRHVHOFDVRHQHO
siguiente ejemplo.
Ejemplo 4

Suponga que adem√°s de x = 57 y y = 13 tenemos

u = 0.714251,

v = 98765.9,

y

w = 0.111111 √ó 10‚àí4 ,

de tal forma que

f l(u) = 0.71425 √ó 100 ,

f l(v) = 0.98765 √ó 105 ,

Determine los valores de corte de cinco d√≠gitos de x

y

u, (x

f l(w) = 0.11111 √ó 10‚àí4 .
u) .. w, (x

u) ‚äó v, y u ‚äï v.

Soluci√≥n

Estos n√∫meros fueron seleccionados para ilustrar algunos problemas que pueden
VXUJLUFRQODDULWPpWLFDGHGtJLWRV√ÄQLWRV3XHVWRTXHx y u son casi iguales, su diferencia es
peque√±a. El error absoluto para x u es

|(x ‚àí u) ‚àí (x

u)| = |(x ‚àí u) ‚àí ( f l( f l(x) ‚àí f l(u)))|
=

5
‚àí 0.714251 ‚àí f l 0.71428 √ó 100 ‚àí 0.71425 √ó 100
7

= 0.347143 √ó 10‚àí4 ‚àí f l 0.00003 √ó 100

= 0.47143 √ó 10‚àí5 .

Esta aproximaci√≥n tiene un error absoluto, pero un error relativo grande

0.47143 √ó 10‚àí5
‚â§ 0.136.
0.347143 √ó 10‚àí4
La divisi√≥n subsiguiente entre el n√∫mero peque√±o w o la multiplicaci√≥n por el n√∫mero grande v PDJQL√ÄFD HO HUURU DEVROXWR VLQ PRGL√ÄFDU HO HUURU UHODWLYR /D VXPD GH ORV Q~PHURV
grande y peque√±o u y v produce un error absoluto grande, pero no un error relativo grande.
(VWRVFiOFXORVVHPXHVWUDQHQODWDEOD

18

CAP√çTULO 1

Preliminares matem√°ticos y an√°lisis de error

Tabla 1.3

Operaci√≥n

Resultado

Valor real

Error absoluto

Error relativo

x u
(x u) .. w
(x u) ‚äó v
u‚äïv

0.30000 √ó 10‚àí4
0.27000 √ó 101
0.29629 √ó 101
0.98765 √ó 105

0.34714 √ó 10‚àí4
0.31242 √ó 101
0.34285 √ó 101
0.98766 √ó 105

0.471 √ó 10‚àí5
0.424
0.465
0.161 √ó 101

0.136
0.136
0.136
0.163 √ó 10‚àí4

Uno de los c√°lculos m√°s comunes que producen errores implica la cancelaci√≥n de d√≠giWRVVLJQL√ÄFDWLYRVGHELGRDODUHVWDGHQ~PHURVFDVLLJXDOHV6XSRQJDTXHGRVQ~PHURVFDVL
iguales x y y, con x > y, tienen las representaciones de d√≠gitos k

f l(x) = 0.d1 d2 . . . d p Œ± p+1 Œ± p+2 . . . Œ±k √ó 10n
y

f l(y) = 0.d1 d2 . . . d p Œ≤ p+1 Œ≤ p+2 . . . Œ≤k √ó 10n .
(OIRUPDWRGHSXQWR√ÅRWDQWHGHx ‚àí y es

f l( f l(x) ‚àí f l(y)) = 0.œÉ p+1 œÉ p+2 . . . œÉk √ó 10n‚àí p ,
donde

0.œÉ p+1 œÉ p+2 . . . œÉk = 0.Œ± p+1 Œ± p+2 . . . Œ±k ‚àí 0.Œ≤ p+1 Œ≤ p+2 . . . Œ≤k .
(O Q~PHUR GH SXQWR √ÅRWDQWH TXH VH XVD SDUD UHSUHVHQWDUx ‚àí y tiene por lo menos k ‚àí p
GtJLWRVVLJQL√ÄFDWLYRV6LQHPEDUJRHQPXFKRVGLVSRVLWLYRVGHFiOFXORDx ‚àí y se le asignar√°n
k d√≠gitos, con la √∫ltima p igual a cero o asignada de manera aleatoria. Cualquier otro c√°lculo
relacionado con x ‚àí y conserva el problema de tener solamente k ‚àí p GtJLWRVVLJQL√ÄFDWLYRV
puesto que una cadena de c√°lculos no es m√°s precisa que su parte m√°s d√©bil.
6LXQDUHSUHVHQWDFLyQRXQFiOFXORGHGtJLWRV√ÄQLWRVSUHVHQWDXQHUURURWUDDPSOLDFLyQ
del error ocurre al dividir entre un n√∫mero de menor magnitud (o, de manera equivalente, al
PXOWLSOLFDUSRUXQQ~PHURGHPD\RUPDJQLWXG 6XSRQJDSRUHMHPSORTXHHOQ~PHURz tiene
XQDDSUR[LPDFLyQGHGtJLWRV√ÄQLWRVz + Œ¥, en donde el error Œ¥ se introduce por representaci√≥n
o por c√°lculo previo. Ahora divida entre Œµ = 10‚àín, en donde n > 0. Entonces

z
‚âà fl
Œµ

f l(z)
f l(Œµ)

= (z + Œ¥) √ó 10n .

El error absoluto en esta aproximaci√≥n, |Œ¥| √ó 10n, es el error absoluto original, |Œ¥|, multiplicado por el factor 10n.
Ejemplo 5

Si p = 0.\q = 0.8VHDULWPpWLFDGHFXDWURGtJLWRVSDUDDSUR[LPDUp ‚àí q y determine los errores absoluto y relativo mediante a) redondeo y b) corte.
Soluci√≥n El valor exacto de r = p ‚àí q es r = 0.00016.

a)

Suponga que se realiza la resta con aritm√©tica de redondeo de cuatro d√≠gitos. Al
redondear p y q a cuatro d√≠gitos obtenemos p ‚àó = 0.5462 y q ‚àó = 0.5460, respectivamente, y r ‚àó = p ‚àó ‚àí q ‚àó = 0.0002 es la aproximaci√≥n de cuatro d√≠gitos para r.
Puesto que

|0.00016 ‚àí 0.0002|
|r ‚àí r ‚àó |
=
= 0.25,
|r |
|0.00016|

1.2 Errores de redondeo y aritm√©tica computacional

b)

19

HOUHVXOWDGRVyORWLHQHXQGtJLWRVLJQL√ÄFDWLYRPLHQWUDV p ‚àó y q ‚àó sean precisos para
FXDWUR\FLQFRGtJLWRVVLJQL√ÄFDWLYRVUHVSHFWLYDPHQWH
Si se usa el corte para obtener los cuatro d√≠gitos, la aproximaci√≥n de cuatro d√≠gitos
para p, q, y r son p ‚àó = 0.5461, q ‚àó = 0.5460, y r ‚àó = p ‚àó ‚àí q ‚àó = 0.0001. Esto nos
da

|r ‚àí r ‚àó |
|0.00016 ‚àí 0.0001|
=
= 0.375,
|r |
|0.00016|
ORTXHWDPELpQUHVXOWDHQXQVRORGtJLWRVLJQL√ÄFDWLYRGHSUHFLVLyQ
A menudo, la p√©rdida de precisi√≥n debido al error de redondeo se puede evitar al
reformular los c√°lculos, como se ilustra en el siguiente ejemplo.
Ilustraci√≥n

La f√≥rmula cuadr√°tica establece que las ra√≠ces de ax 2 + bx + c = 0, cuando a = 0, son
‚àö
‚àö
‚àíb + b2 ‚àí 4ac
‚àíb ‚àí b2 ‚àí 4ac
y x2 =
.
x1 =

2a
2a
Considere esta f√≥rmula aplicada a la ecuaci√≥n x2 + 62.10x + 1 = 0, cuyas ra√≠ces son aproximadamente

x1 = ‚àí0.01610723 y
Las ra√≠ces x1 y x2 de una
ecuaci√≥n cuadr√°tica general est√°n
UHODFLRQDGDVFRQORVFRH√ÄFLHQWHV
por el hecho de que
x1 + x2 = ‚àí

b
a

y

x1 x2 =

Usaremos otra vez la aritm√©tica de redondeo de cuatro d√≠gitos en los c√°lculos para determinar la ra√≠z. En esta ecuaci√≥n, b2HVPXFKRPiVJUDQGHTXHac, por lo que el numerador en el
c√°lculo para x1 implica la resta de n√∫meros casi iguales. Ya que

c
.
a

eVWHHVXQFDVRHVSHFLDOGH
las f√≥rmulas de Vi√®ta para los
FRH√ÄFLHQWHVGHORVSROLQRPLRV

x2 = ‚àí62.08390.

b2 ‚àí 4ac =

(62.10)2 ‚àí (4.000)(1.000)(1.000)
‚àö
‚àö
= 3856. ‚àí 4.000 = 3852. = 62.06,

tenemos

f l(x1 ) =

‚àí62.10 + 62.06
‚àí0.04000
=
= ‚àí0.02000,
2.000
2.000

XQDDSUR[LPDFLyQGH√ÄFLHQWHDx1 = ‚àí 0.01611, con un error relativo grande

| ‚àí 0.01611 + 0.02000|
‚âà 2.4 √ó 10‚àí1 .
| ‚àí 0.01611|
‚àö Por otro lado, el c√°lculo para x2 implica la suma de los n√∫meros casi iguales ‚àíb y
‚àí b2 ‚àí 4ac. Esto no presenta problemas debido a que
f l(x2 ) =

‚àí124.2
‚àí62.10 ‚àí 62.06
=
= ‚àí62.10
2.000
2.000

tiene un error relativo peque√±o

| ‚àí 62.08 + 62.10|
‚âà 3.2 √ó 10‚àí4 .
| ‚àí 62.08|
Para obtener una aproximaci√≥n por redondeo de cuatro d√≠gitos para x1PRGL√ÄFDPRVHO
formato de la f√≥rmula cuadr√°tica al racionalizar el numerador:

x1 =

‚àíb +

‚àö

b2 ‚àí 4ac
2a

‚àíb ‚àí
‚àíb ‚àí

‚àö
‚àö

b2 ‚àí 4ac
b2 ‚àí 4ac

=

b2 ‚àí (b2 ‚àí 4ac)
‚àö
,
2a(‚àíb ‚àí b2 ‚àí 4ac)

20

CAP√çTULO 1

Preliminares matem√°ticos y an√°lisis de error

ODFXDOVHVLPSOL√ÄFDHQXQDIyUPXODFXDGUiWLFDDOWHUQD

x1 =

‚àí2c
‚àö
.
b + b2 ‚àí 4ac



3RUPHGLRGHODHFXDFLyQ  REWHQHPRV

f l(x1 ) =

‚àí2.000
‚àí2.000
=
= ‚àí0.01610,
62.10 + 62.06
124.2

que tiene el error relativo peque√±o 6.2 √ó 10‚àí4 .
La t√©cnica de racionalizaci√≥n tambi√©n puede aplicarse para proporcionar la siguiente
f√≥rmula cuadr√°tica alterna para x2:

x2 =

‚àí2c
‚àö
.
b ‚àí b2 ‚àí 4ac



eVWHHVHOIRUPDWRTXHVHXVDVLb es un n√∫mero negativo. En la ilustraci√≥n, sin embargo, el
uso err√≥neo de esta f√≥rmula para x2 no s√≥lo resultar√≠a en la resta de n√∫meros casi iguales,
sino tambi√©n en la divisi√≥n entre el resultado peque√±o de esta resta. La falta de precisi√≥n que
produce esta combinaci√≥n,

f l(x2 ) =

‚àí2.000
‚àí2.000
‚àí2c
‚àö
=
=
= ‚àí50.00,
2
62.10
‚àí
62.06
0.04000
b ‚àí b ‚àí 4ac

tiene el error relativo grande 1.9 √ó 10‚àí1 .
‚Ä¢ La lecci√≥n: ¬°Piense antes de calcular!

Aritm√©tica anidada
La p√©rdida de precisi√≥n debido a un error de redondeo tambi√©n se puede reducir al reacomodar los c√°lculos, como se muestra en el siguiente ejemplo.
Ejemplo 6

Eval√∫e f (x) = x 3 ‚àí 6.1x 2 + 3.2x + 1.5 en x =FRQDULWPpWLFDGHWUHVGtJLWRV
Soluci√≥n /DWDEODSURYHHORVUHVXOWDGRVLQWHUPHGLRVGHORVFiOFXORV

Tabla 1.4
Exacto
Tres d√≠gitos (corte)
Tres d√≠gitos (redondeo)

x

x2

x3

6.1x 2

3.2x

4.71
4.71
4.71

22.1841
22.1
22.2

104.487111
104.
105.

135.32301
134.
135.

15.072
15.0
15.1

Para ilustrar los c√°lculos, observemos los que participan para encontrar x usando la
aritm√©tica de redondeo de tres d√≠gitos. Primero encontramos

x 2 = 4.712 = 22.1841 que se redondea a 22.2.
A continuaci√≥n, usamos este valor de x2 para encontrar

x 3 = x 2 ¬∑ x = 22.2 ¬∑ 4.71 = 104.562

que se redondea a 105.

Adem√°s,

6.1x 2 = 6.1(22.2) = 135.42

TXHVHUHGRQGHDD

1.2 Errores de redondeo y aritm√©tica computacional

21

y

3.2x = 3.2(4.71) = 15.072

que se redondea a 15.1.

El resultado exacto de la evaluaci√≥n es
Exacto: f (4.71) = 104.487111 ‚àí 135.32301 + 15.072 + 1.5 = ‚àí14.263899.
&RQODDULWPpWLFDGHGtJLWRV√ÄQLWRVODIRUPDHQODTXHVXPDPRVORVUHVXOWDGRVSXHGHDIHFWDU
HOUHVXOWDGR√ÄQDO6XSRQJDTXHORKDFHPRVGHL]TXLHUGDDGHUHFKD(QWRQFHVSDUDODDULWPptica de corte tenemos
7UHVGtJLWRV FRUWH  f (4.71) = ((104. ‚àí 134.) + 15.0) + 1.5 = ‚àí13.5,
y para la aritm√©tica de redondeo tenemos
7UHVGtJLWRV UHGRQGHR  f (4.71) = ((105. ‚àí 135.) + 15.1) + 1.5 = ‚àí13.4.
8VWHGGHEHYHUL√ÄFDUGHPDQHUDFXLGDGRVDHVWRVUHVXOWDGRVSDUDDVHJXUDUVHGHTXHVXQRFLyQ
GHDULWPpWLFDGHGtJLWRV√ÄQLWRVHVFRUUHFWD 2EVHUYHTXHORVYDORUHVGHFRUWHGHWUHVGtJLWRV
VyORUHWLHQHQORVWUHVGtJLWRVSULQFLSDOHVVLQLQFOXLUUHGRQGHR\GL√ÄHUHQVLJQL√ÄFDWLYDPHQWH
de los valores de redondeo de tres d√≠gitos.
Los errores relativos para los m√©todos de tres d√≠gitos son

Corte:

Ilustraci√≥n
5HFXHUGHTXHHOFRUWH RUHGRQGHR 
se realiza despu√©s del c√°lculo.

‚àí14.263899 + 13.5
‚âà 0.05, y redondeo:
‚àí14.263899

‚àí14.263899 + 13.4
‚âà 0.06.
‚àí14.263899

Como enfoque alternativo, el polinomio f(x HQHOHMHPSORVHSXHGHUHHVFULELUGHIRUPD
anidada como

f (x) = x 3 ‚àí 6.1x 2 + 3.2x + 1.5 = ((x ‚àí 6.1)x + 3.2)x + 1.5.
Ahora, la aritm√©tica de corte de tres d√≠gitos produce

f (4.71) =((4.71 ‚àí 6.1)4.71 + 3.2)4.71 + 1.5 = ((‚àí1.39)(4.71) + 3.2)4.71 + 1.5
=(‚àí6.54 + 3.2)4.71 + 1.5 = (‚àí3.34)4.71 + 1.5 = ‚àí15.7 + 1.5 = ‚àí14.2.
De manera similar, ahora obtenemos una respuesta de redondeo de tres d√≠gitos de ‚àí/RV
nuevos errores relativos son
7UHVGtJLWRV FRUWH 

‚àí14.263899 + 14.2
‚âà 0.0045;
‚àí14.263899

7UHVGtJLWRV UHGRQGHR 

‚àí14.263899 + 14.3
‚âà 0.0025.
‚àí14.263899

El anidado ha reducido el error relativo para la aproximaci√≥n de corte a menos de 10% del
valor obtenido al inicio. Para la aproximaci√≥n de redondeo, la mejora ha sido todav√≠a m√°s
dr√°stica; el error, en este caso, se ha reducido m√°s de 95 por ciento.
Los polinomios siempre deber√≠an expresarse en forma anidada antes de realizar una
evaluaci√≥n porque esta forma minimiza el n√∫mero de c√°lculos aritm√©ticos. La disminuci√≥n
del error en la ilustraci√≥n se debe a la reducci√≥n de los c√°lculos de cuatro multiplicaciones y
tres sumas a dos multiplicaciones y tres sumas. Una forma de disminuir el error de redondeo
es reducir el n√∫mero de c√°lculos.
La secci√≥n Conjunto de ejercicios 1.2 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

22

CAP√çTULO 1

Preliminares matem√°ticos y an√°lisis de error

1.3 Algoritmos y convergencia

El uso de algoritmos es tan
antiguo como las matem√°ticas
formales pero el nombre se
deriva del matem√°tico √°rabe
Muhammad ibn-Ms√¢ al.KZDUkUL]Pv F¬≤ /D
traducci√≥n latina de sus trabajos
comenz√≥ con las palabras ‚ÄúDixit
$OJRULVPL¬µTXHVLJQL√ÄFDQ
¬¥DO.KZDUkUL]Pv dice‚Äù.

A lo largo del texto examinaremos procedimientos de aproximaci√≥n, llamados algoritmos,
los cuales incluyen secuencias de c√°lculos. Un algoritmo es un procedimiento que describe,
GHPDQHUDLQHTXtYRFDXQDVHFXHQFLD√ÄQLWDGHSDVRVTXHVHGHVDUUROODU√°n en un orden espeFt√ÄFR(OREMHWRGHODOJRULWPRHVLPSOHPHQWDUXQSURFHGLPLHQWRSDUDUHVROYHUXQSUREOHPD
o aproximar una soluci√≥n para el problema.
Nosotros usamos un pseudoc√≥digo para describir los algoritmos. Este pseudoc√≥digo describe la forma de la entrada que se proporcionar√° y la forma de la salida deseada. No todos los
procedimientos proporcionan una salida satisfactoria para una entrada seleccionada de manera
arbitraria. Como consecuencia, se incluye una t√©cnica de detenci√≥n independiente de la t√©cQLFDQXPpULFDHQFDGDDOJRULWPRSDUDHYLWDUFLFORVLQ√ÄQLWRV
En los algoritmos se usan dos s√≠mbolos de puntuaci√≥n:
¬á 8QSXQWR  LQGLFDHO√ÄQDOGHXQSDVR
¬á 3XQWR\FRPD  VHSDUDQODVWDUHDVGHQWURGHXQSDVR
La sangr√≠a se usa para indicar que los grupos de declaraciones se tratar√°n como una sola
entidad.
Las t√©cnicas de ciclado en los algoritmos tambi√©n son controladas por contador, como

Para

i = 1, 2, . . . , n

tome

xi = a + i ¬∑ h

O controladas por condici√≥n, como
Mientras i < N UHDOLFHORVSDVRV¬≤
Para permitir una ejecuci√≥n condicional, usamos las construcciones est√°ndar
Si7 entonces

o

Si7 entonces
si no

Los pasos en los algoritmos siguen las reglas de la construcci√≥n del programa estructurado. Se han ordenado de tal forma que no deber√≠a ser dif√≠cil traducir el pseudoc√≥digo en
FXDOTXLHUOHQJXDMHGHSURJUDPDFLyQDGHFXDGRSDUDDSOLFDFLRQHVFLHQWt√ÄFDV
/RVDOJRULWPRVHVWiQPH]FODGRVOLEUHPHQWHFRQFRPHQWDULRVeVWRVVHHVFULEHQHQLWilicas y se encuentran entre par√©ntesis para distinguirlos de las declaraciones algor√≠tmicas.
127$ &XDQGR HV GLItFLO GHWHUPLQDU HO √ÄQDO GH FLHUWRV SDVRV DQLGDGRV XWLOL]DPRV XQ
FRPHQWDULRFRPR √ÄQGHOSDVR DODGHUHFKDRGHEDMRGHODGHFODUDFLyQGH√ÄQDOL]DFLyQ
Consulte, por ejemplo, el comentario en el paso 5, en el ejemplo 1.
N

Ilustraci√≥n

xi , dado N y los n√∫meros
El siguiente algoritmo calcula x1 + x2 + ¬∑ ¬∑ ¬∑ + x N =
i=1
x1, x2,   , xN
ENTRADA

N , x1 , x2 , . . . , xn .

SALIDA SUM =

N
i=1 x i .

Paso 1 Tome SUM = 0.

(Inicialice el acumulador. )

Paso 2 Para i = 1, 2, . . . , N hacer
Tome SUM = SUM + xi . (A√±adir el siguiente t√©rmino. )
Paso 3 SALIDA (SUM);
PARE.

1.3 Algoritmos y convergencia

Ejemplo 1

23

El n-√©simo polinomio de Taylor para f(x = ln x ampliado alrededor de x0 = 1 es
N

PN (x) =

(‚àí1)i+1
(x ‚àí 1)i ,
i
i=1

y el valor de ln 1.SDUDORVRFKROXJDUHVGHFLPDOHVHV&RQVWUX\DXQDOJRULWPR
para determinar el valor m√≠nimo de N requerido para

| ln 1.5 ‚àí PN (1.5)| < 10‚àí5
sin utilizar el t√©rmino restante del polinomio de Taylor.
‚àû
A partir del c√°lculo sabemos que si n=1 an es una serie alterna con l√≠mite A cuN
an
yos t√©rminos disminuyen en magnitud, entonces A y la n-√©sima suma parcial A N = n=1
GL√ÄHUHQSRUPHQRVHQODPDJQLWXGGHOWpUPLQR(N + 1); es decir,

Soluci√≥n

|A ‚àí A N | ‚â§ |a N +1 |.
El siguiente algoritmo utiliza este hecho.

ENTRADA valor x, tolerancia TOL, n√∫mero m√°ximo de iteraciones M.
SALIDA grado N del polinomio o un mensaje de falla.
Paso 1 Sea N = 1;
y = x ‚àí 1;
SUM = 0;
POWER = y;
TERM = y;
SIGN = ‚àí1. (Se utiliza para implementar la alternancia de los signos.)
Paso 2 Mientras N ‚â§ M haga los pasos 3‚Äì5.
Paso 3 Determine SIGN = ‚àíSIGN; (Alterne los signos. )
SUM = SUM + SIGN ¬∑ TERM; (Acumule los t√©rminos.)
POWER = POWER ¬∑ y;
TERM = POWER/(N + 1). (Calcule el siguiente t√©rmino. )
Paso 4 Si |TERM | < TOL entonces (Prueba para la precisi√≥n.)
SALIDA (N );
PARE. (El procedimiento fue exitoso. )
Paso 5 Determinar N = N + 1.

(Preparar la siguiente iteraci√≥n. (Fin del paso 2))

Paso 6 SALIDA (‚ÄòEl m√©todo fall√≥‚Äô); (El procedimiento no fue exitoso.)
PARE.
La entrada para nuestro problema es x = 1.5, TOL = 10‚àí5 y tal vez M = 15. Esta elecci√≥n
de M provee un l√≠mite o una cota superior para el n√∫mero de c√°lculos que queremos realizar,
al reconocer que el algoritmo probablemente va a fallar si se excede este l√≠mite. La salida es
un valor para N o el mensaje de fracaso que depende de la precisi√≥n del dispositivo computacional.

Algoritmos de caracterizaci√≥n
Consideraremos diversos problemas de aproximaci√≥n a lo largo del texto y en cada caso
QHFHVLWDPRVGHWHUPLQDUPpWRGRVGHDSUR[LPDFLyQTXHSURGXFHQUHVXOWDGRVSUHFLVRV√ÄDEOHV
para una amplia clase de problemas. Debido a las diferentes formas de derivar los m√©todos
GHDSUR[LPDFLyQUHTXHULPRVXQDYDULHGDGGHFRQGLFLRQHVSDUDFODVL√ÄFDUVXSUHFLVLyQ1R
todas estas condiciones son apropiadas para cualquier problema en particular.

24

CAP√çTULO 1

Preliminares matem√°ticos y an√°lisis de error

La palabra estable tiene la misma
ra√≠z que las palabras posici√≥n
y est√°ndar. En matem√°ticas, el
t√©rmino estable aplicado a un
problema indica que un peque√±o
cambio en los datos o las
condiciones iniciales no resultan
en un cambio dr√°stico en la
soluci√≥n del problema.

DeÔ¨Ånici√≥n 1.17

Un criterio que impondremos en un algoritmo, siempre que sea posible, es que los peque√±os cambios en los datos iniciales producen, de forma proporcional, peque√±os cambios
HQ ORV UHVXOWDGRV √ÄQDOHV 8Q DOJRULWPR TXH VDWLVIDFH HVWD SURSLHGDG UHFLEH HO QRPEUH GH
estable; de lo contrario, es inestable. Algunos algoritmos son estables s√≥lo para ciertas elecciones de datos iniciales y reciben el nombre de estables condicionalmente&ODVL√ÄFDUHPRV
las propiedades de estabilidad de los algoritmos siempre que sea posible.
Para considerar m√°s el tema del crecimiento del error de redondeo y su conexi√≥n con
la estabilidad del algoritmo, suponga que se presenta un error con una magnitud E0 > 0 en
alguna etapa en los c√°lculos y que la magnitud del error despu√©s de n operaciones subsiguientes se denota con En. En la pr√°ctica, los dos casos que surgen con mayor frecuencia se
GH√ÄQHQDFRQWLQXDFLyQ
Suponga que E0 > 0 denota un error que se presenta en alguna etapa en los c√°lculos y En
representa la magnitud del error despu√©s de n operaciones subsiguientes.
‚Ä¢ Si E n ‚âà C nE 0, donde C es una constante independiente de n, entonces se dice que el crecimiento del error es lineal.
‚Ä¢ Si E n ‚âà C n E 0, para algunas C > 1, entonces el crecimiento del error recibe el nombre de
exponencial.
Normalmente, el crecimiento lineal del error es inevitable, y cuando C y E0 son peque√±as, en general, los resultados son aceptables. El crecimiento exponencial del error deber√≠a
evitarse porque el t√©rmino Cn se vuelve grande incluso para los valores relativamente peque√±os de n. Esto conduce a imprecisiones inaceptables, independientemente del tama√±o de
E0. Como consecuencia, mientras un algoritmo que presenta crecimiento lineal del error es
estable, un algoritmo que presenta crecimiento exponencial del error es inestable. (Consulte
OD√ÄJXUD

Figura 1.10
En

Crecimiento exponencial del error inestable
E n 5 C nE 0

Crecimiento lineal del error estable
E n 5 CnE 0
E0
1

Ilustraci√≥n

2

3

4

5

6

7

8

pn = c1

1
3

n

Para cualquier constante c1 y c2,
n

+ c 2 3n ,



1.3 Algoritmos y convergencia

25

es una soluci√≥n a la ecuaci√≥n recursiva

pn =

10
pn‚àí1 ‚àí pn‚àí2 ,
3

para n = 2, 3, . . . .

Se puede observar que

10
10
pn‚àí1 ‚àí pn‚àí2 =
c1
3
3
= c1

1
3

= c1

1
3

1
3

n‚àí1

n‚àí2

+ c2 3n‚àí1 ‚àí c1

n‚àí2

1
3

+ c2 3n‚àí2

10 1
10
¬∑ ‚àí 1 + c2 3n‚àí2
¬∑3‚àí1
3 3
3

n‚àí2

1
9

+ c2 3n‚àí2 (9) = c1

1
3

n

+ c2 3n = pn .

Suponga que tenemos p0 = 1 y p1 = 138VDQGRHVWRVYDORUHV\ODHFXDFLyQ  SRGHPRV
1 n
determinar valores √∫nicos para las constantes c1 = 1 y c2 = 0. Por lo tanto, pn = 3 para
todas las n.
Si se utiliza aritm√©tica de redondeo de cinco d√≠gitos para calcular los t√©rminos de la
sucesi√≥n determinada por esta ecuaci√≥n, entonces pÃÇ0 = 1.0000 y pÃÇ1 = 0.33333, lo cual reTXLHUHODPRGL√ÄFDFLyQGHODVFRQVWDQWHVSDUDcÃÇ1 = 1.0000 y cÃÇ2 = ‚àí0.12500 √ó 10‚àí5. As√≠, la
sucesi√≥n generada { pÃÇn }‚àû
n=0 est√° dada por

pÃÇn = 1.0000

1
3

n

‚àí 0.12500 √ó 10‚àí5 (3)n ,

que tiene un error de redondeo,

pn ‚àí pÃÇn = 0.12500 √ó 10‚àí5 (3n ).
Este procedimiento es inestable ya que el error aumenta exponencialmente con n, lo cual se
UH√ÅHMDHQODVLPSUHFLVLRQHVH[WUHPDVGHVSXpVGHORVSULPHURVWpUPLQRVFRPRVHPXHVWUDHQ
la tabla 1.5.

Tabla 1.5

n

pÃÇn calculada

pn corregida

Error relativo

0
1
2
3
4
5
6
7
8

0.10000 √ó 101
0.33333 √ó 100
0.11110 √ó 100
0.37000 √ó 10‚àí1
0.12230 √ó 10‚àí1
0.37660 √ó 10‚àí2
0.32300 √ó 10‚àí3
‚àí0.26893 √ó 10‚àí2
‚àí0.92872 √ó 10‚àí2

0.10000 √ó 101
0.33333 √ó 100
0.11111 √ó 100
0.37037 √ó 10‚àí1
0.12346 √ó 10‚àí1
0.41152 √ó 10‚àí2
0.13717 √ó 10‚àí2
0.45725 √ó 10‚àí3
0.15242 √ó 10‚àí3

9 √ó 10‚àí5
1 √ó 10‚àí3
9 √ó 10‚àí3
8 √ó 10‚àí2
8 √ó 10‚àí1
7 √ó 100
6 √ó 101

Ahora considere esta ecuaci√≥n recursiva:

pn = 2 pn‚àí1 ‚àí pn‚àí2 , para n = 2, 3, . . . .
Tiene la soluci√≥n pn = c1 + c2 n para cualquier constante c1 y c2 porque

2 pn‚àí1 ‚àí pn‚àí2 = 2(c1 + c2 (n ‚àí 1)) ‚àí (c1 + c2 (n ‚àí 2))
= c1 (2 ‚àí 1) + c2 (2n ‚àí 2 ‚àí n + 2) = c1 + c2 n = pn .

26

CAP√çTULO 1

Preliminares matem√°ticos y an√°lisis de error

Si tenemos p0 = 1 y p1 = 13, entonces las constantes en esta ecuaci√≥n se determinan
exclusivamente como c1 = 1 y c2 = ‚àí 23. Esto implica que pn = 1 ‚àí 23 n.
Si se utiliza aritm√©tica de redondeo de cinco d√≠gitos para calcular los t√©rminos de la
sucesi√≥n provista por esta ecuaci√≥n, entonces pÃÇ0 = 1.0000 y pÃÇ1 = 0.33333. Como
consecuencia, las constantes de redondeo de cinco d√≠gitos son cÃÇ1 = 1.0000 y cÃÇ2 = ‚àí0.66667.
Por lo tanto,

pÃÇn = 1.0000 ‚àí 0.66667n,
que tiene un error de redondeo

pn ‚àí pÃÇn =

0.66667 ‚àí

2
3

n.

Este procedimiento es estable porque el error aumenta linealmente con nORFXDOVHUH√ÅHMD
en las aproximaciones que se muestran en la tabla 1.6.

Tabla 1.6

n

pÃÇn calculada

pn corregida

Error relativo

0
1
2
3
4
5
6
7
8

0.10000 √ó 101
0.33333 √ó 100
‚àí0.33330 √ó 100
‚àí0.10000 √ó 101
‚àí0.16667 √ó 101
‚àí0.23334 √ó 101
‚àí0.30000 √ó 101
‚àí0.36667 √ó 101
‚àí0.43334 √ó 101

0.10000 √ó 101
0.33333 √ó 100
‚àí0.33333 √ó 100
‚àí0.10000 √ó 101
‚àí0.16667 √ó 101
‚àí0.23333 √ó 101
‚àí0.30000 √ó 101
‚àí0.36667 √ó 101
‚àí0.43333 √ó 101

9 √ó 10‚àí5
0
0
4 √ó 10‚àí5
0
0
2 √ó 10‚àí5

Los efectos del error de redondeo se pueden reducir con la aritm√©tica de d√≠gitos de orden
superior, como la opci√≥n de precisi√≥n doble o m√∫ltiple disponible en muchas computadoras.
Las desventajas de utilizar la aritm√©tica de precisi√≥n doble son que requiere m√°s tiempo de
c√°lculo y el crecimiento del error de redondeo no se elimina por completo.
Un enfoque para calcular el error de redondeo es usar la aritm√©tica de intervalo (es decir,
UHWHQHUORVYDORUHVPiVJUDQGH\PiVSHTXHxRSRVLEOHV GHHVWDIRUPDDO√ÄQDOREWHQHPRV
un intervalo que contiene el valor verdadero. Por desgracia, podr√≠a ser necesario un intervalo
peque√±o para la implementaci√≥n razonable.

Tasas de convergencia
Puesto que con frecuencia se utilizan t√©cnicas iterativas relacionadas con sucesiones, esta
secci√≥n concluye con un an√°lisis breve sobre la terminolog√≠a que se usa para describir la
rapidez con que ocurre la convergencia. En general, nos gustar√≠a que la t√©cnica converja tan
UiSLGRFRPRVHDSRVLEOH/DVLJXLHQWHGH√ÄQLFLyQVHXVDSDUDFRPSDUDUODVWDVDVGHFRQYHUgencia de las sucesiones.
DeÔ¨Ånici√≥n 1.18

‚àû
Suponga que {Œ≤n }‚àû
n=1 es una sucesi√≥n conocida que converge a cero y {Œ±n }n=1 converge a un
n√∫mero Œ±. Si existe una constante positiva K con

|Œ±n ‚àí Œ±| ‚â§ K |Œ≤n |,

para una n grande,

entonces decimos que {Œ±n }‚àû
n=1 converge a Œ± con una rapidez, u orden de convergencia
O(Œ≤n ). (Esta expresi√≥n se lee ‚ÄúO de Œ≤n¬µ 6HLQGLFDDOHVFULELUŒ±n = Œ± + O(Œ≤n ).

1.3 Algoritmos y convergencia

27

$SHVDUGHTXHODGH√ÄQLFLyQSHUPLWHFRPSDUDU{Œ±n }‚àû
n=1 con una sucesi√≥n arbitraria
{Œ≤n }‚àû
n=1, en casi todas las situaciones usamos

Œ≤n =

1
,
np

para alg√∫n n√∫mero p > 0. En general, nos interesa el valor m√°s grande de p con
Œ±n = Œ± + O(1/n p ).
Ejemplo 2

Suponga que, para n ‚â• 1,

n+1
n2

Œ±n =

Œ±ÃÇn =

y

n+3
.
n3

aunque l√≠m n‚Üí‚àû Œ±n = 0 y l√≠m n‚Üí‚àû Œ±ÃÇn = 0, la sucesi√≥n {Œ±ÃÇn } converge a este l√≠mite mucho
m√°s r√°pido que la sucesi√≥n {Œ±n }. Al usar la aritm√©tica de redondeo de cinco d√≠gitos, tenemos
los valores que se muestran en la tabla 1.7. Determine la rapidez de convergencia para estas
dos sucesiones.

Tabla 1.7
Existen muchas otras formas
de describir el crecimiento de
las sucesiones y las funciones,
algunas requieren l√≠mites tanto
por encima como por debajo
de la sucesi√≥n o funci√≥n que
se considera. Cualquier buen
libro que analiza algoritmos, por
ejemplo, [CLRS], incluir√≠a esta
informaci√≥n.

n

1

2

3

4

5

6

7

Œ±n
Œ±ÃÇn

2.00000
4.00000

0.75000
0.62500

0.44444
0.22222

0.31250
0.10938

0.24000
0.064000

0.19444
0.041667

0.16327
0.029155

Soluci√≥n 'H√ÄQDODVVXFHVLRQHVŒ≤n = 1/n y Œ≤ÃÇn = 1/n 2 . Entonces

|Œ±n ‚àí 0| =

n+1
n+n
1
‚â§
= 2 ¬∑ = 2Œ≤n
n2
n2
n

y

|Œ±ÃÇn ‚àí 0| =

n+3
n + 3n
1
‚â§
= 4 ¬∑ 2 = 4Œ≤ÃÇn .
n3
n3
n

Por lo tanto, la rapidez de convergencia de {Œ±n } cero es similar a la convergencia de {1/n}
a cero, mientras {Œ±ÃÇn } converge a cero con una rapidez similar para la sucesi√≥n que converge
m√°s r√°pido {1/n 2 }. Expresamos esto al escribir

Œ±n = 0 + O

1
n

y

Œ±ÃÇn = 0 + O

1
n2

.

Tambi√©n usamos la notaci√≥n O (O grande SDUDGHVFULELUODUDSLGH]FRQODTXHFRQYHUgen las funciones.
DeÔ¨Ånici√≥n 1.19

Suponga que l√≠m h‚Üí0 G(h) = 0 y l√≠m h‚Üí0 F(h) = L. Si existe una constante positiva K con

|F(h) ‚àí L| ‚â§ K |G(h)|,

para h VX√ÄFLHQWHPHQWHSHTXHxD

entonces escribimos F(h) = L + O(G(h)).
En general, las funciones que utilizamos para comparar tienen la forma de G(h) = h p,
donde p > 0. Nos interesa el valor m√°s grande de p, para el que F(h) = L + O(h p ).

28

CAP√çTULO 1

Preliminares matem√°ticos y an√°lisis de error

Ejemplo 3

1
Use el tercer polinomio de Taylor alrededor de h = 0 para mostrar que cos h + h 2 = 1 +
2
O(h 4 ).
Soluci√≥n (QHOHMHPSORE GHODVHFFLyQYLPRVTXHHVWHSROLQRPLRHV

1
1
cos h = 1 ‚àí h 2 + h 4 cos ŒæÃÉ (h),
2
24
para alg√∫n n√∫mero ŒæÃÉ (h) entre cero y h. Esto implica que

1
1
cos h + h 2 = 1 + h 4 cos ŒæÃÉ (h).
2
24
Por lo tanto,

1
cos h + h 2
2

‚àí1 =

1 4
1
cos ŒæÃÉ (h) h 4 ‚â§
h ,
24
24

de modo que h ‚Üí 0, cos h + 12 h 2 converge a este l√≠mite, 1, tan r√°pido como h converge a
0. Es decir,
1
cos h + h 2 = 1 + O(h 4 ).
2
La secci√≥n Conjunto de ejercicios 1.3 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

1.4 Software num√©rico
Los paquetes de software computacional para aproximar las soluciones num√©ricas a los problemas en muchas formas. En nuestro sitio web para el libro
https://sites.google.com/site/numericalanalysis1burden/

proporcionamos programas escritos en C, FORTRAN, Maple, Mathematica, MATLAB y
3DVFDODVtFRPRDSSOHWVGH-$9$eVWRVVHSXHGHQXWLOL]DUSDUDUHVROYHUORVSUREOHPDVSURvistos en los ejemplos y ejercicios, y aportan resultados satisfactorios para muchos de los
problemas que usted quiz√° necesite resolver. Sin embargo, son lo que llamamos programas
de prop√≥sito especial. Nosotros usamos este t√©rmino para distinguir estos programas de
aquellos disponibles en las librer√≠as de subrutinas matem√°ticas est√°ndar. Los programas en
estos paquetes recibir√°n el nombre de prop√≥sito general.
/RVSURJUDPDVHQORVSDTXHWHVGHVRIWZDUHGHSURSyVLWRJHQHUDOGL√ÄHUHQHVVXVLQWHQFLRnes de los algoritmos y programas proporcionados en este libro. Los paquetes de software de
prop√≥sito general consideran formas para reducir los errores debido al redondeo de m√°quina,
el subdesbordamiento y el desbordamiento. Tambi√©n describen el rango de entrada que conGXFLUiDORVUHVXOWDGRVFRQFLHUWDSUHFLVLyQHVSHFt√ÄFDeVWDVVRQFDUDFWHUtVWLFDVGHSHQGLHQWHV
de la m√°quina, por lo que los paquetes de software de prop√≥sito general utilizan par√°metros
TXHGHVFULEHQODVFDUDFWHUtVWLFDVGHSXQWR√ÅRWDQWHGHODPiTXLQDTXHVHXVDSDUDORVFiOFXORV
Ilustraci√≥n

Para ilustrar algunas diferencias entre los programas incluidos en un paquete de prop√≥sito
general y un programa que nosotros proporcionar√≠amos en este libro, consideremos un algoritmo que calcula la norma euclidiana de un vector n dimensional x 5 (x1, x2,7 , xn t. A
PHQXGRHVWDQRUPDVHUHTXLHUHGHQWURGHORVSURJUDPDVPiVJUDQGHV\VHGH√ÄQHPHGLDQWH
1/2

n

xi2

||x||2 =
i=1

.

1.4

Software num√©rico

29

La norma da una medida de la distancia del vector x y el vector 0. Por ejemplo, el vector
x 5  t tiene
‚àö
||x||2 = [22 + 12 + 32 + (‚àí2)2 + (‚àí1)2 ]1/2 = 19,

‚àö
por lo que su distancia a partir de 0 = (0, 0, 0, 0, 0)t es 19 ‚âà 4.36.
Un algoritmo del tipo que presentar√≠amos para este problema se proporciona aqu√≠. No
incluye par√°metros dependientes de m√°quina y no ofrece garant√≠as de precisi√≥n, pero aportar√° resultados precisos ‚Äúla mayor parte del tiempo‚Äù.
ENTRADA n, x1 , x2 , . . . , xn .
SALIDA NORM .
Paso 1 Haga SUM = 0.
Paso 2 Para i = 1, 2, . . . , n determine SUM = SUM + xi2 .
Paso 3 Determine NORM = SUM1/2 .
Paso 4 SALIDA (NORM );
PARE.
Un programa con base en nuestro algoritmo es f√°cil de escribir y comprender. Sin embarJRHOSURJUDPDQRVHUtDVX√ÄFLHQWHPHQWHSUHFLVRGHELGRDGLIHUHQWHVUD]RQHV3RUHMHPSOROD
magnitud de algunos n√∫meros podr√≠a ser demasiado grande o peque√±a para representarse con
SUHFLVLyQHQHOVLVWHPDGHSXQWR√ÅRWDQWHGHODFRPSXWDGRUD$GHPiVHVWHRUGHQSDUDUHDOLzar los c√°lculos quiz√° no produzca los resultados m√°s precisos, o que la rutina para obtener
la ra√≠z cuadrada podr√≠a no ser la mejor disponible para el problema. Los dise√±adores del
algoritmo consideran asuntos de este tipo al escribir programas para software de prop√≥sito
general. A menudo, estos programas contienen subprogramas para resolver problemas m√°s
amplios, por lo que deben incluir controles que nosotros no necesitaremos.

Algoritmos de prop√≥sito general
Ahora consideremos un algoritmo para un programa de software de prop√≥sito general para
calcular la norma euclidiana. Primero, es posible que a pesar de que un componente xi del
vector se encuentre dentro del rango de la m√°quina, el cuadrado del componente no lo est√©.
Esto se puede presentar cuando alguna |xi| es tan peque√±a que x2i causa subdesbordamiento o
cuando alguna |xi| es tan grande que x2i causa desbordamiento.
Tambi√©n es posible que todos estos t√©rminos se encuentren dentro del rango de la m√°quina, pero que ocurra desbordamiento a partir de la suma de un cuadrado de uno de los
t√©rminos para la suma calculada previamente.
Los criterios de precisi√≥n dependen de la m√°quina en la que se realizan los c√°lculos, por
lo que los par√°metros dependientes de la m√°quina se incorporan en el algoritmo. Suponga
que trabajamos en una computadora hipot√©tica con base 10, la cual tiene t ‚â• 4 d√≠gitos de
precisi√≥n, un exponente m√≠nimo em√≠n y un exponente m√°ximo em√°x. Entonces el conjunto
GHQ~PHURVGHSXQWR√ÅRWDQWHHQHVWDPiTXLQDFRQVLVWHHQ\ORVQ~PHURVGHODIRUPD

x = f ¬∑ 10e ,

donde

f = ¬±( f 1 10‚àí1 + f 2 10‚àí2 + ¬∑ ¬∑ ¬∑ + f t 10‚àít ),

donde 1 ‚â§ f 1 ‚â§ 9 y 0 ‚â§ f i ‚â§ 9, para cada i 5 2,   , t, y donde em√≠n ‚â§ e ‚â§ em√°x. Estas
restricciones implican que el n√∫mero positivo m√°s peque√±o representado en la m√°quina es
œÉ = 10em√≠n‚àí1, por lo que cualquier n√∫mero calculado x con |x| < œÉ causa subdesbordamiento y que x sea 0. El n√∫mero positivo m√°s grande es Œª = (1 ‚àí 10‚àít )10em√°x, y cualquier
n√∫mero calculado x con |x| > Œª causa desbordamiento. Cuando se presenta subdesbordaPLHQWRHOSURJUDPDFRQWLQXDUiDPHQXGRVLQXQDSpUGLGDVLJQL√ÄFDWLYDGHSUHFLVLyQ6LVH
presenta desbordamiento, el programa fallar√°.

30

CAP√çTULO 1

Preliminares matem√°ticos y an√°lisis de error

(ODOJRULWPRVXSRQHTXHODVFDUDFWHUtVWLFDVGHSXQWR√ÅRWDQWHGHODPiTXLQDVHGHVFULEHQ
a trav√©s de los par√°metros N, s, S, y y Y. El n√∫mero m√°ximo de entradas que se pueden sumar
con por lo menos t GtJLWRVGHSUHFLVLyQHVWiSURYLVWRSRUN. Esto implica que el algoritmo
proceder√° a encontrar la norma de un vector x 5 (x1, x2,7 , xn t s√≥lo si n ‚â§ N . Para resolver
HOSUREOHPDGHVXEGHVERUGDPLHQWRGHVERUGDPLHQWRORVQ~PHURVGHSXQWR√ÅRWDQWHGLVWLQWRV
a cero se dividen en tres grupos:
‚Ä¢ n√∫meros de magnitud peque√±a x, aquellos que satisfacen 0 < |x| < y;
‚Ä¢ n√∫meros de magnitud media x, donde y ‚â§ |x| < Y ;
‚Ä¢ n√∫meros de magnitud grande x, donde Y ‚â§ |x|.
Los par√°metros y y Y VH VHOHFFLRQDQ FRQ HO √ÄQ GH HYLWDU HO SUREOHPD GH VXEGHVERUdamiento-desbordamiento al sumar y elevar al cuadrado los n√∫meros de magnitud media.
Elevar al cuadrado los n√∫meros de magnitud peque√±a puede causar subdesbordamiento, por
lo que se utiliza un factor de escala S mucho mayor a 1 con el resultado (sx 2 que evita el
subdesbordamiento incluso cuando x2 no lo hace. Sumar y elevar al cuadrado los n√∫meros
que tienen una magnitud grande puede causar desbordamiento. Por lo que, en este caso, se
utiliza un factor de escala positivo s mucho menor a 1 para garantizar que (sx 2 no cause
desbordamiento al calcularlo o incluirlo en una suma, a pesar de que x2 lo har√≠a.
Para evitar escalamiento innecesario, y y Y se seleccionan de tal forma que el rango de
n√∫meros de magnitud media sea tan largo como sea posible. El siguiente algoritmo es una moGL√ÄFDFLyQGHOTXHVHGHVFULEHHQ>%URZ:@SeVWHLQFOX\HXQSURFHGLPLHQWRSDUDVXPDU
los componentes escalados del vector, que son de magnitud peque√±a hasta encontrar un componente de magnitud media. Entonces se elimina la escala de la suma previa y contin√∫a al elevar al cuadrado y sumar los n√∫meros peque√±os y medianos hasta encontrar un componente con
una magnitud grande. Una vez que el componente con magnitud grande aparece, el algoritmo
escala la suma anterior y procede a escalar, elevar al cuadrado y sumar los n√∫meros restantes.
El algoritmo supone que, en la transici√≥n desde n√∫meros peque√±os a medianos, los n√∫meros peque√±os no escalados son despreciables, al compararlos con n√∫meros medianos. De
igual forma, en la transici√≥n desde n√∫meros medianos a grandes, los n√∫meros medianos no
escalados son despreciables, al compararlos con n√∫meros grandes. Por lo tanto, las selecciones de los par√°metros de escalamiento se deben realizar de tal forma que se igualen a 0 s√≥lo
cuando son verdaderamente despreciables. Las relaciones comunes entre las caracter√≠sticas
de m√°quina, como se describen en t, œÉ , Œª, em√≠n y em√°x y los par√°metros del algoritmo N, s,
S, y y Y se determinan despu√©s del algoritmo.
El algoritmo usa tres indicadores para se√±alar las diferentes etapas en el proceso de
VXPD(VWRVLQGLFDGRUHVVRQYDORUHVLQLFLDOHVGHWHUPLQDGRVHQHOSDVRGHODOJRULWPR)/$*
%$1'(5$ HVKDVWDHQFRQWUDUXQFRPSRQHQWHPHGLDQRRJUDQGHHQWRQFHVVHFRQYLHUWH
HQ)/$* %$1'(5$ HVPLHQWUDVVHVXPDQQ~PHURVSHTXHxRVFDPELDDFXDQGR
se encuentra un n√∫mero mediano por primera vez, y regresa a 0 cuando se encuentra un n√∫PHURJUDQGH,QLFLDOPHQWH)/$* %$1'(5$ HV\FDPELDDFXDQGRVHHQFXHQWUDXQ
Q~PHURJUDQGHSRUSULPHUDYH](OSDVRWDPELpQLQWURGXFHHOLQGLFDGRU'21( +(&+2 
que es 0 hasta que se terminan los c√°lculos y, entonces, regresa a 1.

ENTRADA N , s, S, y, Y, Œª, n, x1 , x2 , . . . , xn .
SALIDA NORM o un mensaje de error apropiado.
Paso 1 Si n ‚â§ 0 entonces SALIDA (‚ÄòEl entero n debe ser positivo.‚Äô)
PARE.
Paso 2 Si n ‚â• N entonces SALIDA (‚ÄòEl entero n es demasiado grande.‚Äô)
PARE.

1.4

Software num√©rico

31

Paso 3 Determine SUM = 0;
FLAG1 = 1; (Se suman los n√∫meros peque√±os.)
FLAG2 = 0;
FLAG3 = 0;
DONE = 0;
i = 1.
Paso 4 Mientras ( i ‚â§ n y FLAG1 = 1) haga el paso 5.
Paso 5 Si |xi | < y entonces determine SUM = SUM +(Sxi )2 ;
i =i +1
tambi√©n determine FLAG1 = 0. (Se encuentra un n√∫mero no peque√±o.)
Paso 6 Si i > n entonces determine NORM = (SUM)1/2 /S;
DONE = 1
tambi√©n determine SUM = (SUM/S)/S; (Escalamiento de n√∫meros grandes.)
FLAG2 = 1.
Paso 7 Mientras ( i ‚â§ n y FLAG2 = 1) haga el paso 8. (Se suman los n√∫meros medianos.)
Paso 8 Si |xi | < Y entonces determine SUM = SUM + xi2 ;
i =i +1
tambi√©n determine FLAG2 = 0. (Se encuentra un n√∫mero no grande.)
Paso 9 Si DONE = 0 entonces
si i > n entonces determine NORM = (SUM)1/2 ;
DONE = 1
tambi√©n determine SUM = ((SUM)s)s; (Escalamiento de n√∫meros grandes.)
FLAG3 = 1.
Paso 10 Mientras i ‚â§ n y FLAG3 = 1) haga el paso 11.
Paso 11 Determine SUM = SUM +(sxi )2 ;
i = i + 1.

(Sume los n√∫meros grandes.)

Paso 12 Si DONE = 0 entonces
si SUM 1/2 < Œªs entonces determine NORM = (SUM)1/2 /s;
DONE = 1
tambi√©n determine SUM = Œª. (La norma es demasiado grande.)
Paso 13 Si DONE = 1 entonces SALIDA (‚ÄòNorma es‚Äô, NORM)
tambi√©n SALIDA (‚ÄòNorma ‚â•‚Äô, NORM, 'ocurri√≥ sobreflujo').
Paso 14 PARE.

Las relaciones entre las caracter√≠sticas de m√°quina t∆±Œª, em√≠n y em√°x y los par√°metros
del algoritmo N, s, S, y y Y VHVHOHFFLRQDURQHQ>%URZ:@SFRPR

N = 10e N,

donde e N
(t ‚àí 2)/2;

(t ‚àí 2)/2 ,

s = 10es,

donde es

(em√°x + e N )/2

eS

La primera computadora port√°til
fue la Osborne I, producida en
1981, a pesar de que era mucho
m√°s grande y pesada de lo que
podr√≠amos pensar como port√°til.

El entero m√°s grande menor o igual a

S = 10 ,

donde e S
(1 ‚àí em√≠n)/2 ,
que (1 ‚àí em√≠n)/2;

y = 10e y,

donde e y

(em√≠n + t ‚àí 2)/2

eY

donde eY

(em√°x ‚àí e N )/2 .

Y = 10 ,

El entero m√°s peque√±o mayor o igual

32

CAP√çTULO 1

Preliminares matem√°ticos y an√°lisis de error

El sistema FORTRAN (FORmula
75$1VODWRU IXHHOOHQJXDMH
GHSURJUDPDFLyQFLHQWt√ÄFDGH
prop√≥sito general original.
Sigue utiliz√°ndose ampliamente
en situaciones que requieren
FiOFXORVFLHQWt√ÄFRVLQWHQVLYRV
(OSUR\HFWR(,63$&.IXHHO
primer paquete de software
num√©rico a gran escala en estar
disponible para dominio p√∫blico
y lider√≥ el camino para que
muchos paquetes lo siguieran.

La ingenier√≠a de software se
estableci√≥ como disciplina de
laboratorio durante las d√©cadas
GH\(,63$&.VH
desarroll√≥ en Argonne Labs
\/,13$&.SRFRGHVSXpV$
principios de la d√©cada de 1980,
Argonne fue reconocido en el
√°mbito internacional como l√≠der
mundial en c√°lculos simb√≥licos y
num√©ricos.

En 1970, IMSL se convirti√≥ en
ODSULPHUDOLEUHUtDFLHQWt√ÄFDD
gran escala para computadoras
centrales. Ya que en esa √©poca,
exist√≠an librer√≠as para sistemas
computacionales que iban
desde supercomputadoras hasta
computadoras personales.

/D√ÄDELOLGDGFRQVWUXLGDHQHVWHDOJRULWPRKDLQFUHPHQWDGRDPSOLDPHQWHODFRPSOHMLGDGHQ
comparaci√≥n con el algoritmo provisto antes en esta secci√≥n. En la mayor√≠a de los casos,
los algoritmos de prop√≥sito especial y general proporcionan resultados id√©nticos. La ventaja
del algoritmo de prop√≥sito general es que proporciona seguridad para sus resultados.
Existen muchas formas de software num√©rico de prop√≥sito general disponibles en el √°mbito comercial y en el dominio p√∫blico. La mayor parte de los primeros se escribi√≥ para las
computadoras centrales, y una buena referencia es Sources and Development of Mathematical
Software (Fuentes y desarrollo de software matem√°tico), editado por Wayne Cowell [Co].
$KRUDTXHODVFRPSXWDGRUDVSHUVRQDOHVVRQVX√ÄFLHQWHPHQWHSRGHURVDVH[LVWHVRIWZDUH
num√©rico est√°ndar para ellas. La mayor√≠a de este software num√©rico se escribe en FORTRAN, a pesar de que algunos est√°n escritos en C, C++ y FORTRAN90.
Los procedimientos ALGOL se presentaron para c√°lculos de matrices en 1971 en [WR].
Despu√©s, se desarroll√≥ un paquete de subrutinas FORTRAN con base principalmente en los
SURFHGLPLHQWRV$/*2/ GHQWUR GH ODV UXWLQDV (,63$&. (VWDV UXWLQDV VH GRFXPHQWDQ HQ
los manuales publicados por Springer-Verlag como parte de sus Lecture Notes (Notas de
clase) en la serie Computer Science (Ciencias computacionales) [Sm, B] y [Gar]. Las subrutinas FORTRAN se utilizan para calcular los valores propios y los vectores propios para una
variedad de diferentes tipos de matrices.
/,13$&.HVXQSDTXHWHGHVXEUXWLQDV)2575$1SDUDDQDOL]DU\UHVROYHUVLVWHPDVGH
ecuaciones lineales y resolver problemas de m√≠nimos cuadrados lineales. La documentaci√≥n
HQ HVWH SDTXHWH VH HQFXHQWUD HQ >'%06@ 8QD LQWURGXFFLyQ SDVR D SDVR SDUD /,13$&.
(,63$&.\%/$6 %DVLF/LQHDU$OJHEUD6XESURJUDPV6XESURJUDPDVGH√âOJHEUD/LQHDO
%iVLFD VHSURSRUFLRQDHQ>&9@
(OSDTXHWH/$3$&.GLVSRQLEOHSRUSULPHUDYH]HQHVXQDOLEUHUtDGHODVVXEUXWLQDV)2575$1TXHVXVWLWX\HQD/,13$&.\(,63$&.DOLQWHJUDUHVWRVGRVFRQMXQWRVGHDOJRULWPRVHQXQSDTXHWHXQL√ÄFDGR\DFWXDOL]DGR(OVRIWZDUHVHKDUHHVWUXFWXUDGRSDUDORJUDU
PD\RUH√ÄFLHQFLDHQSURFHVDGRUHVGHYHFWRUHV\RWURVPXOWLSURFHVDGRUHVGHDOWRGHVHPSHxR
\PHPRULDFRPSDUWLGD/$3$&.VHH[SDQGHHQSURIXQGLGDG\DPSOLWXGHQODYHUVLyQ
disponible en FORTRAN, FORTRAN90, C, C++ y JAVA. C y JAVA s√≥lo est√°n disponibles
FRPRLQWHUIDFHVGHLGLRPDRWUDGXFFLRQHVGHODVOLEUHUtDV)2575$1GH/$3$&.(OSDTXHWH%/$6QRIRUPDSDUWHGH/$3$&.SHURHOFyGLJRSDUD%/$6VHGLVWULEX\HFRQ/$3$&.
2WURVSDTXHWHVSDUDUHVROYHUWLSRVHVSHFt√ÄFRVGHSUREOHPDVHVWiQGLVSRQLEOHVHQHOGRminio p√∫blico. Como alternativa para netlib, puede utilizar Xnetlib para buscar en la base de
datos y recuperar software. Encuentre m√°s informaci√≥n en el art√≠culo Software Distribution
Using Netlib de Dongarra Roman y Wade [DRW].
(VWRVSDTXHWHVGHVRIWZDUHVRQPX\H√ÄFLHQWHVSUHFLVRV\FRQ√ÄDEOHV6HSUXHEDQGHPDnera meticulosa y la documentaci√≥n est√° disponible f√°cilmente. A pesar de que los paquetes
son port√°tiles, es una buena idea investigar la dependencia de la m√°quina y leer la documentaci√≥n con todo detalle. Los programas prueban casi todas las contingencias especiales que
SRGUtDQUHVXOWDUHQHUURUHVRIDOODV$O√ÄQDOGHFDGDFDStWXORDQDOL]DUHPRVDOJXQRVGHORV
paquetes de prop√≥sito general adecuados.
Los paquetes comercialmente disponibles tambi√©n representan los m√©todos num√©ricos
de vanguardia. A menudo, su contenido est√° basado en los paquetes de dominio p√∫blico, pero
incluyen m√©todos en bibliotecas para casi cualquier tipo de problemas.
Las IMSL (International Mathematical and Statistical Libraries; Bibliotecas Estad√≠sticas
\0DWHPiWLFDV,QWHUQDFLRQDOHV HVWiQIRUPDGDVSRUELEOLRWHFDV0$7+67$7\6)81SDUD
matem√°ticas num√©ricas, estad√≠sticas y funciones especiales, respectivamente. Estas bibliotecas contienen m√°s de 900 subrutinas originalmente disponibles en FOR-TRAN 77 y ahora
disponibles en C, FORTRAN90 y JAVA. Estas subrutinas resuelven los problemas de an√°lisis
num√©ricos m√°s comunes. Las librer√≠as est√°n comercialmente disponibles en Visual Numerics.
Los paquetes se entregan en formato compilado con documentaci√≥n amplia. Existe un
programa de ejemplo para cada rutina, as√≠ como informaci√≥n de referencia de fondo. IMSL
contiene m√©todos para sistemas lineales, an√°lisis de sistemas propios, interpolaci√≥n y aproximaci√≥n, integraci√≥n y diferenciaci√≥n, ecuaciones diferenciales, transformadas, ecuaciones
QROLQHDOHVRSWLPL]DFLyQ\RSHUDFLRQHVEiVLFDVPDWUL]YHFWRU/DELEOLRWHFDWDPELpQLQFOX\H
amplias rutinas estad√≠sticas.

1.4

El Numerical Algorithms Group
1$* VHIXQGyHQ5HLQR
Unido en 1971 y desarroll√≥ la
primera biblioteca de software
matem√°tico. Actualmente
tiene m√°s de 10 000 usuarios
a nivel mundial y contiene
m√°s de 1000 funciones
matem√°ticas y estad√≠sticas
que van desde software de
simulaci√≥n estad√≠stica, simb√≥lica,
visualizaci√≥n y num√©rica hasta
compiladores y herramientas de
desarrollo de aplicaciones.

Originalmente, MATLAB se
escribi√≥ para proporcionar
acceso al software de matriz
desarrollado en proyectos
/,13$&.\(,63$&./D
primera versi√≥n se escribi√≥ a
√ÄQDOHVGHODGpFDGDGHSDUD
utilizarse en cursos de teor√≠a de
matriz, √°lgebra lineal y an√°lisis
num√©rico. Actualmente, existen
m√°s de 500 000 usuarios de
MATLAB en m√°s de 100 pa√≠ses.
Las rutinas NAG son
compatibles con Maple, desde la
versi√≥n 9.0.

Software num√©rico

33

(O1XPHULFDO$OJRULWKPV*URXS 1$**UXSRGH$OJRULWPRV1XPpULFRV KDH[LVWLGR
en Reino Unido desde 1970. NAG ofrece m√°s de 1000 subrutinas en una biblioteca FOR75$1DSUR[LPDGDPHQWHVXEUXWLQDVHQXQDELEOLRWHFD&PiVGHVXEUXWLQDVHQ
una biblioteca FORTRAN 90 y una biblioteca MPI FORTRAN para m√°quinas paralelas y
agrupaciones de estaciones de trabajo o computadoras personales. Una introducci√≥n √∫til para
las rutinas NAG es [Ph]. La biblioteca NAG contiene rutinas para realizar la mayor parte
de las tareas de an√°lisis num√©rico est√°ndar de manera similar a la de IMSL. Tambi√©n incluye
DOJXQDVUXWLQDVHVWDGtVWLFDV\XQFRQMXQWRGHUXWLQDVJUi√ÄFDV
/RVSDTXHWHV,06/\1$*HVWiQGLVHxDGRVSDUDORVPDWHPiWLFRVFLHQWt√ÄFRVRLQJHQLHros que desean llamar subrutinas de alta calidad de C, Java o FORTRAN desde dentro de un
programa. La documentaci√≥n disponible con los paquetes comerciales ilustra el programa
activador com√∫n, requerido para utilizar las rutinas de la librer√≠a. Los siguientes tres paquetes de software son ambientes aut√≥nomos. Cuando se activan, los usuarios introducen comandos para hacer que el paquete resuelva un problema. Sin embargo, cada paquete permite
programar dentro del lenguaje de comando.
MATLAB es una matriz de laboratorio que, originalmente, era un programa Fortran
publicado por Cleve Moler [Mo] en la d√©cada de 1980. El laboratorio est√° basado princiSDOPHQWH HQ ODV VXEUXWLQDV (,63$&. \ /,13$&. D SHVDU GH TXH VH KDQ LQWHJUDGR IXQciones, como sistemas no lineales, integraci√≥n num√©rica, splines c√∫bicos, ajuste de curvas,
optimizaci√≥n, ecuaciones diferenciales QRUPDOHV \ KHUUDPLHQWDV JUi√ÄFDV $FWXDOPHQWH
MATLAB est√° escrito en C y lenguaje ensamblador y la versi√≥n para PC de este paquete
requiere un coprocesador num√©rico. La estructura b√°sica es realizar operaciones de matriz,
como encontrar los valores propios de una matriz introducida desde la l√≠nea de comando o
GHVGH XQ DUFKLYR H[WHUQR D WUDYpV GH OODPDGDV D IXQFLRQHV eVWH HV XQ VLVWHPD DXWyQRPR
poderoso que es especialmente √∫til para instrucci√≥n en un curso de √°lgebra lineal aplicada.
El segundo paquete es GAUSS, un sistema matem√°tico y estad√≠stico producido por Lee
( (GLHIVRQ \ 6DPXHO ' -RQHV HQ  (VWi FRGL√ÄFDGR SULQFLSDOPHQWH HQ OHQJXDMH HQVDPEODGRU\EDVDGR(,63$&.\/,13$&.$OLJXDOTXHHQHOFDVRGH0$7/$%H[LVWH
LQWHJUDFLyQGLIHUHQFLDFLyQVLVWHPDVQROLQHDOHVWUDQVIRUPDGDVUiSLGDVGH)RXULHU\JUi√ÄFDV
GAUSS se orienta menos hacia la ense√±anza de √°lgebra lineal y m√°s hacia el an√°lisis estad√≠stico de datos. Este paquete tambi√©n utiliza un coprocesador num√©rico, cuando existe uno.
El tercer paquete es Maple, un sistema de √°lgebra computacional desarrollado en 1980
0HGLDQWHHO6\PEROLF&RPSXWDWLRQDO*URXS *UXSR&RPSXWDFLRQDO6LPEyOLFR HQOD8QLversity of Waterloo. El dise√±o para el sistema original Maple se presenta en el art√≠culo de
%:&KDU.2*HGGHV:0*HQWOHPHQ\*+*RQQHW>&***@
Maple, escrito en C, tiene la capacidad de manipular informaci√≥n de manera simb√≥lica. Esta manipulaci√≥n simb√≥lica permite al usuario obtener respuestas exactas, en lugar de
valores num√©ricos. Maple puede proporcionar respuestas exactas a problemas matem√°ticos
como integrales, ecuaciones diferenciales y sistemas lineales. Contiene una estructura de
programaci√≥n y permite texto, as√≠ como comandos, para guardarlos en sus archivos de hojas
de trabajo. Estas hojas de trabajo se pueden cargar a Maple y ejecutar los comandos.
El igualmente popular Mathematica, liberado en 1988, es similar a Maple.
([LVWHQQXPHURVRVSDTXHWHVTXHVHSXHGHQFODVL√ÄFDUFRPRSDTXHWHVGHVXSHUFDOFXODGRUD
para la PC. Sin embargo, √©stos no deber√≠an confundirse con el software de prop√≥sito general
que se ha mencionado aqu√≠. Si est√° interesado en uno de estos paquetes, deber√≠a leer Supercalculators on the PC (Supercalculadoras en la PC) de B. Simon y R. M. Wilson [SW].
Informaci√≥n adicional sobre el software y las bibliotecas de software se puede encontrar
HQORVOLEURVGH&RG\\:DLWH>&:@\GH.RFNOHU>.R@\HODUWtFXORGHGH'RQJDUUD
\:DONHU>':@0iVLQIRUPDFLyQVREUHFiOFXORGHSXQWR√ÅRWDQWHVHSXHGHHQFRQWUDUHQHO
libro de Chaitini-Chatelin y Frayse [CF] y el art√≠culo de Goldberg [Go].
Los libros que abordan la aplicaci√≥n de t√©cnicas num√©ricas sobre computadoras paralelas incluyen los de Schendell [Sche], Phillips and Freeman [PF] y Golub y Ortega [GO].

Las secciones Pregunta de an√°lisis, Conceptos clave y Revisi√≥n del cap√≠tulo est√°n disponibles en l√≠nea. Encuentre la ruta de acceso en las p√°ginas preliminares.

CAP√çTULO

Soluciones de las ecuaciones en una variable

Introducci√≥n
A menudo, el crecimiento de una poblaci√≥n se puede modelar sobre periodos breves al asumir que aumenta de manera continua con el tiempo a una tasa proporcional al n√∫mero actual
en ese momento. Suponga que N(t) denota el n√∫mero en la poblaci√≥n en el tiempo t y Œª
denota la tasa constante de natalidad. Entonces, dicha poblaci√≥n satisface la ecuaci√≥n diferencial

d N (t)
= ŒªN (t),
dt
cuya soluci√≥n es N (t) = N0 eŒªt donde N0 denota la poblaci√≥n inicial.
N()

3000
Poblaci√≥n (miles)

2

2000

N() 5 1000e 1

435 
(e 2 1)


1564
1435
1000

√çndice de natalidad

1



Este modelo exponencial s√≥lo es v√°lido cuando la poblaci√≥n est√° aislada, sin inmigraci√≥n. Si se permite inmigraci√≥n a una tasa constante v, entonces la ecuaci√≥n diferencial se
convierte en
d N (t)
= ŒªN (t) + v,
dt
cuya soluci√≥n es

N (t) = N0 eŒªt +

v Œªt
(e ‚àí 1).
Œª
35

36

CAP√çTULO 2

Soluciones de las ecuaciones en una variable

Suponga que, en un inicio, cierta poblaci√≥n contiene N(0) 5 1 000 000 individuos, que
435 000 individuos inmigran a la comunidad durante el primer a√±o y que existen N(1) 5
LQGLYLGXRVDO√ÄQDOGHODxR3DUDGHWHUPLQDUODQDWDOLGDGGHHVWDSREODFLyQQHFHVLtamos encontrar l en la ecuaci√≥n

1 564 000 = 1 000 000eŒª +

435 000 Œª
(e ‚àí 1).
Œª

En esta ecuaci√≥n no es posible resolver de manera expl√≠cita para Œª, pero los m√©todos
num√©ricos que se analizan en este cap√≠tulo se pueden utilizar para aproximar las soluciones de las ecuaciones de este tipo con una precisi√≥n arbitrariamente alta. Las soluci√≥n a
este problema particular se considera en el ejercicio 22 de la secci√≥n 2.3.

2.1 El m√©todo de bisecci√≥n
En este cap√≠tulo consideramos uno de los problemas b√°sicos de la aproximaci√≥n num√©rica, el
problema de la b√∫squeda de la ra√≠z. Este proceso implica encontrar una ra√≠z, o soluci√≥n,
para una ecuaci√≥n de la forma f (x) 5 0, para una funci√≥n f dada. Una ra√≠z de esta ecuaci√≥n
tambi√©n recibe el nombre de cero de la funci√≥n f.
El problema de encontrar una aproximaci√≥n para la ra√≠z de una ecuaci√≥n se puede rastrear por lo menos al a√±o 1 700 a.C. Una tabla cuneiforme en la Colecci√≥n Babil√≥nica de Yale
que data del periodo provee
‚àö un n√∫mero sexagesimal (base 60) equivalente a 1.414222 como
una aproximaci√≥n para 2, un resultado que es preciso dentro de 1025. Esta aproximaci√≥n
se puede encontrar al aplicar una t√©cnica descrita en el ejercicio 19 de la secci√≥n 2.2.

T√©cnica de bisecci√≥n
En las ciencias computacionales,
al proceso que consiste en dividir
a la mitad un conjunto de manera
continua para buscar la soluci√≥n
de un problema, como lo hace
el m√©todo de bisecci√≥n, se le
conoce como procedimiento de
b√∫squeda binaria.

La primera t√©cnica, basada en el teorema de valor intermedio, recibe el nombre de bisecci√≥n,
o m√©todo de b√∫squeda binaria.
Suponga que fHVXQDIXQFLyQFRQWLQXDGH√ÄQLGDGHQWURGHOLQWHUYDOR>a, b] con f (a) y f(b)
de signos opuestos. El teorema de valor intermedio implica que existe un n√∫mero p en (a,
b) con f ( p) 5 0. A pesar de que el procedimiento operar√° cuando haya m√°s de una ra√≠z en el
intervalo (a, b), para simplicidad, nosotros asumimos que la ra√≠z en este intervalo es √∫nica.
El m√©todo realiza repetidamente una reducci√≥n a la mitad (o bisecci√≥n) de los subintervalos
GH>a, b] y, en cada paso, localizar la mitad que contiene p.
3DUDFRPHQ]DUVHDa1 5 a y b1 5 b y sea p1HVHOSXQWRPHGLRGH>a, b], es decir,

p1 = a 1 +

b1 ‚àí a 1
a 1 + b1
=
.
2
2

‚Ä¢ Si f (p1) 5 0, entonces p 5 p1 y terminamos.
‚Ä¢ Si f ( p1 ) = 0, entonces f(p1) tiene el mismo signo que ya sea f(a1) o f(b1).
Si f (p1) y f(a1) tienen el mismo signo, p ‚àà (p1, b1). Sea a2 5 p1 y b2 5 b1.
Si f (p1) y f(a1) tienen signos opuestos, p ‚àà (a1, p1). Sea a2 5 a1 y b2 5 p1.
(QWRQFHVYROYHPRVDDSOLFDUHOSURFHVRDOLQWHUYDOR>a2, b2]. Esto produce el m√©todo descrito
HQHODOJRULWPR FRQVXOWHOD√ÄJXUD 

2.1 El m√©todo de bisecci√≥n

Figura 2.1

37

y
f (b)
y 5 f (x)
p3

f ( p1)
a 5 a1

pp

p2

1

b 5 b1

p1

b1

x

f (p2)
f (a)
a1
a2

p2
a3

ALGORITMO

2.1

b2
p3

b3

Bisecci√≥n
3DUDHQFRQWUDUXQDVROXFLyQDf(x) 5 0 dada la funci√≥n continua determinada f en el intervalo
>a, b], donde f (a) y f(b) tienen signos opuestos:

ENTRADA
SALIDA

puntos finales a, b; tolerancia TOL; n√∫mero m√°ximo de iteraciones N0 .

soluci√≥n aproximada p o mensaje de falla.

Paso 1 Sea i = 1;
FA = f (a).
Paso 2 Mientras i ‚â§ N0 haga los pasos 3‚Äì6.
Paso 3 Sea p = a + (b ‚àí a)/2; (Calcule p i .)
FP = f ( p).
Paso 4 Si FP = 0 o (b ‚àí a)/2 < TOL entonces
SALIDA (p); (Procedimiento completado exitosamente.)
PARE.
Paso 5 Sea i = i + 1.
Paso 6 Si FA ¬∑ FP > 0 entonces determine a = p; (Calcule a i , bi .)
FA = FP
tambi√©n determine b = p. (F A no cambia.)
Paso 7 SALIDA (‚ÄòEl m√©todo fracas√≥ despu√©s de N0 iteraciones, N0 =‚Äô, N0 );
(El procedimiento no fue exitoso.)
PARE.
Se pueden aplicar otros procedimientos de parada en el paso 4 del algoritmo 2.1 o en
FXDOTXLHUWpFQLFDLWHUDWLYDHQHVWHFDStWXOR3RUHMHPSORSRGHPRVVHOHFFLRQDUXQDWROHUDQFLD
‚àà > 0 y generar p1, 7, pN hasta que se cumpla una de las siguientes condiciones:

38

CAP√çTULO 2

Soluciones de las ecuaciones en una variable

| p N ‚àí p N ‚àí1 |

(2.1)

| p N ‚àí p N ‚àí1 |
| pN |

p N = 0,

o

| f ( p N )|

(2.2)
(2.3)

3RUGHVJUDFLDVHSXHGHQSUHVHQWDUGL√ÄFXOWDGHVDOXWLOL]DUFXDOTXLHUDGHHVWRVFULWHULRV
GHSDUDGD3RUHMHPSORH[LVWHQVXFHVLRQHV { pn }‚àû
n=0 con la propiedad de que las diferencias
pn ‚àí pn‚àí1 convergen a cero mientras la sucesi√≥n misma diverge (consulte el ejercicio 19).
Tambi√©n es posible que f(pn) est√© cerca de cero mientras pnGL√ÄHUHVLJQL√ÄFDWLYDPHQWHGHp
(consulte el ejercicio 20). Sin conocimiento adicional sobre f o p, la desigualdad (2.2) es el
PHMRUFULWHULRGHSDUDGDTXHVHSXHGHDSOLFDUSRUTXHYHUL√ÄFDHOHUURUUHODWLYR
$OXWLOL]DUXQDFRPSXWDGRUDSDUDJHQHUDUDSUR[LPDFLRQHVHVEXHQRSUDFWLFDUODFRQ√Äguraci√≥n de un l√≠mite superior sobre el n√∫mero de iteraciones. Esto elimina la posibilidad
GHHQWUDUHQXQFLFORLQ√ÄQLWRXQDVLWXDFLyQTXHSXHGHVXUJLUFXDQGRODVXFHVLyQGLYHUJH \
WDPELpQFXDQGRHOSURJUDPDVHFRGL√ÄFDGHPDQHUDLQFRUUHFWD (VRVHUHDOL]yHQHOSDVR
del algoritmo 2.1, donde se estableci√≥ el l√≠mite N0 y se concluy√≥ el procedimiento si i > N0.
2EVHUYHTXHSDUDLQLFLDUHODOJRULWPRGHELVHFFLyQVHGHEHHQFRQWUDUXQLQWHUYDOR>a, b]
con f(a) ? f(b) < 0. En cada paso, la longitud del intervalo conocida por contener un cero de
fVHUHGXFHHQXQIDFWRUGHSRUORWDQWRHVYHQWDMRVRHOHJLUHOLQWHUYDORLQLFLDO>a, b] tan
SHTXHxRFRPRVHDSRVLEOH3RUHMHPSORVLf(x) 5 2x3 2 x3 1 x 2 1, tenemos

f (‚àí4) ¬∑ f (4) < 0

y

f (0) ¬∑ f (1) < 0,

SRUORTXHHODOJRULWPRGHELVHFFLyQVHSXHGHXVDUHQ>2@RHQ>@$OLQLFLDUHODOJRULWPRGHELVHFFLyQHQ>@HQOXJDUGH>24, 4] reduciremos en 3 el n√∫mero de iteraciones
UHTXHULGRSDUDDOFDQ]DUXQDSUHFLVLyQHVSHFt√ÄFD
El siguiente ejemplo ilustra el algoritmo de bisecci√≥n. La iteraci√≥n en este ejemplo termina cuando una cota para el error relativo es menor a 0.0001. Esto se garantiza al tener

| p ‚àí pn |
< 10‚àí4 .
m√≠n{|an |, |bn |}
Ejemplo 1

Muestre que f (x) = x 3 + 4x 2 ‚àí 10 = 0 WLHQHXQDUDt]HQ>@\XWLOLFHHOPpWRGRGHELsecci√≥n para determinar una aproximaci√≥n para la ra√≠z que sea precisa por lo menos dentro
de 10‚Äì4.
Soluci√≥n 3XHVWRTXH f (1) = ‚àí5 y f (2) = 14, el teorema de valor intermedio 1.11 ga-

UDQWL]DTXHHVWDIXQFLyQFRQWLQXDWHQJDXQDUDt]HQ>@
3DUDODSULPHUDLWHUDFLyQGHOPpWRGRGHELVHFFLyQXVDPRVHOKHFKRGHTXHHQHOSXQWR
PHGLRGH>@WHQHPRVf(1.5) 5 2.375 > 0. Esto indica que deber√≠amos seleccionar el inWHUYDOR>@SDUDQXHVWUDVHJXQGDLWHUDFLyQ$FRQWLQXDFLyQHQFRQWUDPRVTXHf(1.25) 5
2SRUORTXHQXHVWURLQWHUYDORVHYXHOYH>@FX\RSXQWRPHGLRHV6L
continuamos de esta forma, obtenemos los valores en la tabla 2.1.
Despu√©s de 13 iteraciones, p13 5 1.365112305 se aproxima a la ra√≠z p con un error

| p ‚àí p13 | < |b14 ‚àí a14 | = |1.365234375 ‚àí 1.365112305| = 0.000122070.
Ya que |a14 | < | p|, tenemos
|b14 ‚àí a14 |
| p ‚àí p13 |
<
‚â§ 9.0 √ó 10‚àí5 ,
| p|
|a14 |

2.1 El m√©todo de bisecci√≥n

Tabla 2.1

39

n

an

bn

pn

f ( pn )

1
2
3
4
5
6
7
8
9
10
11
12
13

1.0
1.0
1.25
1.25
1.3125
1.34375
1.359375
1.359375
1.36328125
1.36328125
1.364257813
1.364746094
1.364990235

2.0
1.5
1.5
1.375
1.375
1.375
1.375
1.3671875
1.3671875
1.365234375
1.365234375
1.365234375
1.365234375

1.5
1.25
1.375
1.3125
1.34375
1.359375
1.3671875
1.36328125
1.365234375
1.364257813
1.364746094
1.364990235
1.365112305

2.375
‚àí1.79687
0.16211
‚àí0.84839
‚àí0.35098
‚àí0.09641
0.03236
‚àí0.03215
0.000072
‚àí0.01605
‚àí0.00799
‚àí0.00396
‚àí0.00194

de modo que la aproximaci√≥n es correcta por lo menos dentro de 1024. El valor correcto de p
para nueve lugares decimales es p 5 1. 365230013. Observe que p9 est√° m√°s cerca de p que es
ODDSUR[LPDFLyQ√ÄQDOp13. Se podr√≠a sospechar que esto es verdad porque | f ( p9 )| < | f ( p13 )|,
pero no podemos asegurarlo a menos que conozcamos la respuesta verdadera.
El m√©todo de bisecci√≥n, a pesar de que est√° conceptualmente claro, tiene desventajas
VLJQL√ÄFDWLYDV6XYHORFLGDGGHFRQYHUJHQFLDHVPiVOHQWD HVGHFLU N se puede volver bastante grande antes de que | p ‚àí p N |VHDVX√ÄFLHQWHPHQWHSHTXHxD \VHSRGUtDGHVFDUWDULQDGYHUtidamente una buena aproximaci√≥n intermedia. Sin embargo, el m√©todo tiene la importante
propiedad de que siempre converge a una soluci√≥n y por esta raz√≥n con frecuencia se utiliza
FRPRLQLFLDGRUSDUDORVPpWRGRVPiVH√ÄFLHQWHVTXHYHUHPRVPiVDGHODQWHHQHVWHFDStWXOR
Teorema 2.1

Suponga que f ‚àà C[a, b] y f (a) ¬∑ f (b) < 0. El m√©todo de bisecci√≥n genera una sucesi√≥n
{ pn }‚àû
n=1 que se aproxima a cero p de f con

| pn ‚àí p| ‚â§

b‚àía
,
2n

cuando

n ‚â• 1.

Demostraci√≥n 3DUDFDGDn $ 1, se tiene

bn ‚àí an =

1
(b ‚àí a)
2n‚àí1

y

p ‚àà (an , bn ).

Ya que pn = 12 (an + bn ) para toda n¬ïVHVLJXHTXH

| pn ‚àí p| ‚â§

1
b‚àía
(bn ‚àí an ) =
.
2
2n

porque
1
| pn ‚àí p| ‚â§ (b ‚àí a) n ,
2
la sucesi√≥n { pn }‚àû
n=1 converge en p con una raz√≥n de convergencia O

pn = p + O

1
2n

1
2n

; es decir

.

Es importante se√±alar que el teorema 2.1 s√≥lo provee una cota para el error de aproximaFLyQ\TXHpVWDSRGUtDVHUEDVWDQWHFRQVHUYDGRUD3RUHMHPSORFXDQGRVHDSOLFDDOSUREOHPD

40

CAP√çTULO 2

Soluciones de las ecuaciones en una variable

en el ejemplo 1 s√≥lo garantiza que

| p ‚àí p9 | ‚â§

2‚àí1
‚âà 2 √ó 10‚àí3 ,
29

pero el error real es mucho menor:

| p ‚àí p9 | = |1.365230013 ‚àí 1.365234375| ‚âà 4.4 √ó 10‚àí6 .
Ejemplo 2

Determine el n√∫mero de iteraciones necesarias para resolver f (x) = x 3 + 4x 2 ‚àí 10 = 0
con precisi√≥n de 1023 mediante a1 5 1 y b1 5 2.
Soluci√≥n

Usaremos logaritmos para encontrar un entero N que satisface

| p N ‚àí p| ‚â§ 2‚àíN (b ‚àí a) = 2‚àíN < 10‚àí3 .
Los logaritmos para cualquier base bastar√≠an, pero usaremos logaritmos base 10 porque la
tolerancia se determina como una potencia de 10. Ya que 22N < 1023 implica que log10 22N
< log10 1023 5 23, tenemos

‚àíN log10 2 < ‚àí3

y

N>

3
‚âà 9.96.
log10 2

3RUORWDQWRVHUHTXLHUHQLWHUDFLRQHVSDUDXQDDSUR[LPDFLyQSUHFLVDGHQWURGH23.
La tabla 2.1 muestra que el valor de p9 5 1.365234375 es exacto dentro de 1024. De
nuevo, es importante considerar que el an√°lisis de error s√≥lo proporciona una cota para el n√∫mero de iteraciones. En muchos casos, la cota es mucho mayor que el n√∫mero real requerido.
La cota para el n√∫mero de iteraciones en el m√©todo de bisecci√≥n supone que los c√°lculos
VH UHDOL]DQ FRQ DULWPpWLFD GH GtJLWRV LQ√ÄQLWRV$O LPSOHPHQWDU HO PpWRGR HQ XQD FRPSX
WDGRUDQHFHVLWDPRVFRQVLGHUDUORVHIHFWRVGHOHUURUGHUHGRQGHR3RUHMHPSORHOFiOFXORGHO
SXQWRPHGLRGHOLQWHUYDOR>an, bn] se deber√≠a encontrar a partir de la ecuaci√≥n

pn = a n +

La palabra en lat√≠n signum
VLJQL√ÄFD¬¥VHxDO¬µR¬¥VLJQR¬µ3RU
lo que la funci√≥n signum retorna
de forma bastante natural el
signo de un n√∫mero (a menos que
el n√∫mero sea cero).

bn ‚àí a n
2

en lugar de pn =

a n + bn
.
2

La primera ecuaci√≥n a√±ade una peque√±a correcci√≥n, (bn 2 an)/2, al valor conocido an. Cuando bn 2 an est√° cerca de la precisi√≥n m√°xima de la m√°quina, esta correcci√≥n podr√≠a ser un
HUURUSHURHOHUURUQRDIHFWDUtDVLJQL√ÄFDWLYDPHQWHHOYDORUFDOFXODGRGHpn. Sin embargo, es
posible que (bn 5 an)/2 regrese a un punto medio que no se encuentra dentro del intervalo
>an, bn].
&RPRREVHUYDFLyQ√ÄQDOSDUDGHWHUPLQDUHOVXELQWHUYDORGH>an, bn] contiene una ra√≠z de
f, es mejor utilizar la funci√≥n signumTXHVHGH√ÄQHFRPR
‚éß
‚é™
‚é®‚àí1, si x < 0,
sgn(x) = 0,
si x = 0,
‚é™
‚é©
1,
si x > 0.
Utilizar

sgn ( f (an )) sgn ( f ( pn )) < 0

en lugar de

f (an ) f ( pn ) < 0

nos da el mismo resultado, pero evita la posibilidad de desbordamiento o subdesbordamiento
en la multiplicaci√≥n de f(an) y f(pn).
La secci√≥n Conjunto de ejercicios 2.1 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

2.2 Iteraci√≥n de punto Ô¨Åjo

41

2.2 Iteraci√≥n de punto Ô¨Åjo
Un SXQWR√ÄMR para una funci√≥n es un n√∫mero en el que el valor de la funci√≥n no cambia
cuando se aplica la funci√≥n.

DeÔ¨Ånici√≥n 2.2

/RVUHVXOWDGRVGHSXQWR√ÄMR
ocurren en muchas √°reas de
las matem√°ticas y son una
herramienta principal de los
economistas para proporcionar
resultados concernientes a los
equilibrios. Aunque la idea
detr√°s de la t√©cnica es antigua, la
terminolog√≠a fue usada primero
por el matem√°tico holand√©s L.
E. J. Brouwer (1882-1962) a
principios de 1900.

El n√∫mero p es un SXQWR√ÄMR para una funci√≥n dada g si g(p) 5 p.

En esta secci√≥n, consideramos el problema de encontrar soluciones para los problemas
GHSXQWR√ÄMR\ODFRQH[LyQHQWUHORVSUREOHPDVGHSXQWR√ÄMR\DTXHOORVSDUDHQFRQWUDUODUDt]
TXHTXHUHPRVUHVROYHU/RVSUREOHPDVSDUDHQFRQWUDUODUDt]\ORVGHSXQWR√ÄMRVRQFODVHV
equivalentes en el siguiente sentido:
‚Ä¢ Dado un problema para encontrar la ra√≠z f(p) 5 0, SRGHPRVGH√ÄQLUODVIXQFLRQHVg con un
SXQWR√ÄMRHQp en diferentes formas, por ejemplo,

g(x) = x ‚àí f (x) o

g(x) = x + 3 f (x).

¬á 3RURWUDSDUWHVLODIXQFLyQg WLHQHXQSXQWR√ÄMRHQpHQWRQFHVODIXQFLyQGH√ÄQLGDSRU

f (x) = x ‚àí g(x)
tiene un cero en p.
A pesar de que los problemas que queremos resolver se encuentran en la forma para
HQFRQWUDUODUDt]ODIRUPDGHSXQWR√ÄMRHVPiVIiFLOGHDQDOL]DU\FLHUWDVHOHFFLRQHVGHSXQWR
√ÄMRFRQGXFHQDWpFQLFDVPX\SRGHURVDVSDUDHQFRQWUDUODUDt]
3ULPHURQHFHVLWDPRVHVWDUFyPRGRVFRQHVWHQXHYRWLSRGHSUREOHPD\GHFLGLUFXiQGR
XQDIXQFLyQWLHQHXQSXQWR√ÄMR\FyPRVHSXHGHQDSUR[LPDUORVSXQWRV√ÄMRVGHQWURGHXQD
SUHFLVLyQHVSHFL√ÄFDGD

Ejemplo 1

'HWHUPLQHFXDOTXLHUSXQWR√ÄMRGHODIXQFLyQg(x) = x 2 ‚àí 2.
Soluci√≥n 8QSXQWR√ÄMRp para g tiene la propiedad de que

p = g( p) = p 2 ‚àí 2,

lo cual implica que 0 = p 2 ‚àí p ‚àí 2 = ( p + 1)( p ‚àí 2).

8QSXQWR√ÄMRSDUDgRFXUUHSUHFLVDPHQWHFXDQGRODJUi√ÄFDGHy 5 g(x LQWHUVHFDODJUi√ÄFD
de y 5 x, por lo que gWLHQHGRVSXQWRV√ÄMRVXQRHQp 5 21 y el otro en p 5 2. √âstos se
PXHVWUDQHQOD√ÄJXUD

42

CAP√çTULO 2

Soluciones de las ecuaciones en una variable

Figura 2.2
y
5

y 5 x2 2 2

4
y5x

3
2
1
23 22

3 x

2

23

(OVLJXLHQWHWHRUHPDSURSRUFLRQDVX√ÄFLHQWHVFRQGLFLRQHVSDUDODH[LVWHQFLD\XQLFLGDG
GHXQSXQWR√ÄMR

Teorema 2.3

i)

Si g ‚àà C>a, b] y g(x) ‚àà >a, b] para todas x ‚àà >a, b], entonces g tiene por lo menos un
SXQWR√ÄMRHQ>a, b].

ii)

Si, adem√°s, g9(x) existe en (a, b) y hay una constante positiva k < 1 con

|g (x)| ‚â§ k,

para todas las x ‚àà (a, b),

HQWRQFHVH[LVWHH[DFWDPHQWHXQSXQWR√ÄMRHQ>a, b]. 9pDVHOD√ÄJXUD

Figura 2.3
y
y5x

b

p 5 g(p)
y 5 g(x)
a
a

p

b

x

Demostraci√≥n

i)

Si g(a) 5 a o g(b) 5 b, entonces gWLHQHXQSXQWR√ÄMRHQXQH[WUHPR'HORFRQWUDULR
entonces g(a) > a y g(b) < b. La funci√≥n h(x) 5 g(x) 2 xHVFRQWLQXDHQ>a, b], con

h(a) = g(a) ‚àí a > 0

y h(b) = g(b) ‚àí b < 0.

2.2 Iteraci√≥n de punto Ô¨Åjo

43

El teorema de valor intermedio implica que existe p ‚àà (a, b) para la cual h(p) 5 0.
Este n√∫mero pHVXQSXQWR√ÄMRSDUDg porque

0 = h( p) = g( p) ‚àí p
ii)

implica que

g( p) = p.

Suponga, adem√°s, que |g (x)| ‚â§ k < 1 y que p y q VRQSXQWRV√ÄMRVHQ>a, b]. Si
p = q, entonces el teorema de valor medio implica que existe un n√∫mero j entre p
y q \SRUORWDQWRHQ>a, b] con

g( p) ‚àí g(q)
= g (Œæ ).
p‚àíq
3RUORWDQWR

| p ‚àí q| = |g( p) ‚àí g(q)| = |g (Œæ )|| p ‚àí q| ‚â§ k| p ‚àí q| < | p ‚àí q|,
lo cual es una contradicci√≥n. Esta contradicci√≥n debe provenir de la √∫nica suposici√≥n p = q3RUORWDQWRp 5 q\HOSXQWR√ÄMRHQ>a, b] es √∫nico.
Ejemplo 2

Muestre que g(x) = (x 2 ‚àí 1)/3 WLHQHXQSXQWR√ÄMR~QLFRHQHOLQWHUYDOR>21, 1].
Soluci√≥n Los valores m√°ximo y m√≠nimo de g(x) para x HQ >21, 1] deben ocurrir ya sea
cuando xHVXQH[WUHPRGHOLQWHUYDORRFXDQGRODGHULYDGDHV3XHVWRTXH g (x) = 2x/3,
la funci√≥n g es continua y g9(x H[LVWHHQ>21, 1]. Los valores m√°ximo y m√≠nimo de g(x) se
presentan en x 5 21, x 5 0 o x 53HURg(21) 5 0, g(1) 5 0 y g(0) 5 21/3, por lo que un
m√°ximo absoluto para g(x HQ>¬≤@VHSUHVHQWDHQ x 5 21 y x 5 1 y un m√≠nimo absoluto
en x 5 0.
Adem√°s,

|g (x)| =

2
2x
‚â§ ,
3
3

para todas las x ‚àà (‚àí1, 1).

De este modo gVDWLVIDFHWRGDVODVKLSyWHVLVGHOWHRUHPD\WLHQHXQSXQWR√ÄMR~QLFRHQ
>21, 1].
3DUDODIXQFLyQHQHOHMHPSORHO~QLFRSXQWR√ÄMRpHQHOLQWHUYDOR>21, 1] se puede
determinar de manera algebraica. Si

p = g( p) =

p2 ‚àí 1
,
3

entonces p 2 ‚àí 3 p ‚àí 1 = 0,

ORFXDOSRUODIyUPXODFXDGUiWLFDLPSOLFDFRPRVHPXHVWUDHQODJUi√ÄFDL]TXLHUGDHQOD
√ÄJXUDTXH

p=

‚àö
1
(3 ‚àí 13).
2

‚àö
Observe que gWDPELpQWLHQHXQSXQWR√ÄMR~QLFR p = 12 (3+ 13)SDUDHOLQWHUYDOR>@
Sin embargo, g(4) = 5 y g (4) = 83 > 1, por lo que g no satisface la hip√≥tesis del teorema
HQ>@(VWRGHPXHVWUDTXHODKLSyWHVLVGHOWHRUHPDHVVX√ÄFLHQWHSHURQRQHFHVDULD
SDUDJDUDQWL]DUODXQLFLGDGGHOSXQWR√ÄMR &RQVXOWHODJUi√ÄFDGHODGHUHFKDHQOD√ÄJXUD

44

CAP√çTULO 2

Soluciones de las ecuaciones en una variable

Figura 2.4

y

y
4

y5

x2 2 1
3

4

x2 2 1
3

3

3
y5x

2

y5x

2
1

1

1

2

3

4

x

1

2

3

4

x

21

21

Ejemplo 3

y5

0XHVWUHTXHHOWHRUHPDQRJDUDQWL]DXQSXQWR√ÄMR~QLFRGHg(x) 5 32x en el intervalo
>@DXQTXHH[LVWHXQSXQWR√ÄMR~QLFRHQHVWHLQWHUYDOR
g9(x) 5 232x ln 3 <  HQ > @ OD IXQFLyQ g es estrictamente decreciente en
>@3RUORTXH
Soluci√≥n

g(1) =

1
‚â§ g(x) ‚â§ 1 = g(0),
3

para

0 ‚â§ x ‚â§ 1.

3RUORWDQWRSDUDx ‚àà>@WHQHPRVg(x) ‚àà>@/DSULPHUDSDUWHGHOWHRUHPDJDUDQWL]D
TXHH[LVWHSRUORPHQRVXQSXQWR√ÄMRHQ>@
Sin embargo,

g (0) = ‚àí ln 3 = ‚àí1.098612289,
por lo que |g (x)| ‚â§ 1 en (0, 1), y el teorema 2.3 no se puede usar para determinar la uniFLGDG3HURg VLHPSUHGHFUHFH\HVFODURDSDUWLUGHOD√ÄJXUDTXHHOSXQWR√ÄMRGHEHVHU
√∫nico.

Figura 2.5
y
y5x
1

y 5 32x

1

x

2.2 Iteraci√≥n de punto Ô¨Åjo

45

Iteraci√≥n de punto Ô¨Åjo
1R SRGHPRV GHWHUPLQDU H[SOtFLWDPHQWH HO SXQWR √ÄMR HQ HO HMHPSOR  SRUTXH QR WHQHPRV
otra forma de resolver la ecuaci√≥n p = g( p) = 3‚àí p. Sin embargo, podemos determinar las
DSUR[LPDFLRQHVSDUDHVWHSXQWR√ÄMRFRQFXDOTXLHUJUDGRHVSHFt√ÄFRGHSUHFLVLyQ$KRUDFRQsideraremos la forma de hacerlo.
3DUDDSUR[LPDUHOSXQWR√ÄMRGHXQDIXQFLyQg, elegimos una aproximaci√≥n inicial p0 y
generamos la sucesi√≥n { pn }‚àû
n=0 al permitir pn = g( pn‚àí1 ), para cada n ‚â• 1. Si la sucesi√≥n
converge a p y g es continua, entonces

p = l√≠m pn = l√≠m g( pn‚àí1 ) = g
n‚Üí‚àû

l√≠m pn‚àí1 = g( p),

n‚Üí‚àû

n‚Üí‚àû

y se obtiene una soluci√≥n para x 5 g(x). Esta t√©cnica recibe el nombre de SXQWR√ÄMR o iteraci√≥n funcional(OSURFHGLPLHQWRVHLOXVWUDHQOD√ÄJXUD\VHGHWDOODHQHODOJRULWPR

Figura 2.6
y

p2 5 g( p1)
p3 5 g( p2)
p1 5 g( p0)

y

y5x
(p1, p2)

( p2, p2)

( p1, p1)

(p2, p3)

p3 5 g(p2)
p2 5 g(p1)
p1 5 g(p0)

( p0, p1)

y5x
y 5 g(x)

(p2, p2)
(p0, p1)

(p1, p1)

y 5 g(x)

p 1 p3 p2 p0

p0

x

a)

ALGORITMO

2.2

p1

p2

x

b)

Iteraci√≥n de punto Ô¨Åjo
Encontrar una soluci√≥n de p 5 g(p) dada una aproximaci√≥n inicial p0:

ENTRADA

aproximaci√≥n inicial p0 ; tolerancia TOL; n√∫mero m√°ximo de iteraciones N0 .

SALIDA aproximada p o mensaje de falla.
Paso 1 Determine i = 1.
Paso 2 Mientras i ‚â§ N0 haga los pasos 3‚Äì6.
Paso 3 Determine p = g( p0 ).

(Calcule p i .)

Paso 4 Si | p ‚àí p0 | < TOL entonces
SALIDA ( p); (El procedimiento fue exitoso.)
PARE.
Paso 5 Determine i = i + 1.
Paso 6 Determine p0 = p.

(Actualizar p0 .)

46

CAP√çTULO 2

Soluciones de las ecuaciones en una variable

Paso 7 SALIDA (‚ÄòEl m√©todo fall√≥ despu√©s de N0 iteraciones, N0 =‚Äô, N0 );
(El procedimiento no fue exitoso.)
PARE.
Lo siguiente ilustra algunas caracter√≠sticas de la iteraci√≥n funcional.
Ilustraci√≥n

La ecuaci√≥n x 3 + 4x 2 ‚àí 10 = 0 WLHQHXQDUDt]~QLFDHQ>@([LVWHQPXFKDVIRUPDVGH
FDPELDUODHFXDFLyQSDUDODIRUPDGHSXQWR√ÄMRx 5 g(x) mediante una simple manipulaci√≥n
DOJHEUDLFD3RUHMHPSORSDUDREWHQHUODIXQFLyQg descrita en la parte c), podemos manipular
la ecuaci√≥n x 3 + 4x 2 ‚àí 10 = 0 como sigue:

4x 2 = 10 ‚àí x 3 ,

por lo que x 2 =

1
(10 ‚àí x 3 ),
4

y

1
x = ¬± (10 ‚àí x 3 )1/2 .
2

3DUDREWHQHUXQDVROXFLyQSRVLWLYDVHVHOHFFLRQDg3(x). No es importante derivar las funcioQHVTXHVHPXHVWUDQDTXtSHURGHEHPRVYHUL√ÄFDUTXHHOSXQWR√ÄMRGHFDGDXQDHVHQUHDOLGDG
una soluci√≥n para la ecuaci√≥n original, x 3 + 4x 2 ‚àí 10 = 0.

a)

x = g1 (x) = x ‚àí x 3 ‚àí 4x 2 + 10

c)

x = g3 (x) =

e)

x = g5 (x) = x ‚àí

1
(10 ‚àí x 3 )1/2
2

b)

x = g2 (x) =

10
‚àí 4x
x

d)

x = g4 (x) =

10
4+x

1/2

1/2

x 3 + 4x 2 ‚àí 10
3x 2 + 8x

Con p0 5 1.5,ODWDEODHQXPHUDORVUHVXOWDGRVGHODLWHUDFLyQGHSXQWR√ÄMRSDUDODVFLQFR
selecciones de g.

Tabla 2.2

n

(a)

(b)

(c)

(d)

(e)

0
1
2
3
4
5
6
7
8
9
10
15
20
25
30

1.5
‚àí0.875
6.732
‚àí469.7
1.03 √ó 108

1.5
0.8165
2.9969
(‚àí8.65)1/2

1.5
1.286953768
1.402540804
1.345458374
1.375170253
1.360094193
1.367846968
1.363887004
1.365916734
1.364878217
1.365410062
1.365223680
1.365230236
1.365230006
1.365230013

1.5
1.348399725
1.367376372
1.364957015
1.365264748
1.365225594
1.365230576
1.365229942
1.365230022
1.365230012
1.365230014
1.365230013

1.5
1.373333333
1.365262015
1.365230014
1.365230013

La ra√≠z real es 1.365230013, como se observ√≥ en el ejemplo 1 de la secci√≥n 2.1. Al
comparar los resultados con el algoritmo de bisecci√≥n provisto en ese ejemplo, se puede
observar que se han obtenido excelentes resultados para las opciones d) y e) (el m√©todo de
bisecci√≥n requiere 27 iteraciones para esta precisi√≥n). Es interesante observar que la opci√≥n
D HUDGLYHUJHQWH\TXHODRSFLyQE VHYROYLyLQGH√ÄQLGDSRUTXHLQYROXFUDODUDt]FXDGUDGD
de un n√∫mero negativo.

2.2 Iteraci√≥n de punto Ô¨Åjo

47

$XQTXHODVGLIHUHQWHVIXQFLRQHVTXHKHPRVSURSRUFLRQDGRVRQSUREOHPDVGHSXQWR√ÄMR
para el mismo problema de encontrar la ra√≠z, var√≠an ampliamente como t√©cnicas para aproximar la soluci√≥n a este √∫ltimo. Su objetivo es ilustrar lo que se debe contestar:
¬á 3UHJXQWD¬¢&yPRSRGHPRVHQFRQWUDUXQSUREOHPDGHSXQWR√ÄMRTXHSURGXFHXQDVXFHVLyQ
HQODTXHOD√ÄDELOLGDG\ODUDSLGH]FRQYHUJHQDODVROXFLyQGHOSUREOHPDGHHQFRQWUDUOD
ra√≠z determinada?
El siguiente teorema y su corolario nos proporcionan algunas claves respecto a los procedimientos que deber√≠amos seguir y, tal vez m√°s importante, algunos que deber√≠amos rechazar.
Teorema 2.4

(Teorema de punto Ô¨Åjo)
Sea g ‚àà C>a, b] tal que g(x) ‚àà >a, b] para todas las xHQ>a, b]. Suponga, adem√°s, que existe
g9 en (a, b) y que existe una constante 0 < k < 1 con

|g (x)| ‚â§ k,

x ‚àà (a, b).

para todas

Entonces, para cualquier n√∫mero p0HQ>DE@ODVXFHVLyQGH√ÄQLGDSRU

pn = g( pn‚àí1 ),

n ‚â• 1,

FRQYHUJHDO~QLFRSXQWR√ÄMRpHQ>a, b].
Demostraci√≥n El teorema 2.3 implica que existe un √∫nico punto pHQ>a, b] con g(p) 5 p.
Ya que gPDSHD>a, b] en s√≠ mismo, la sucesi√≥n { pn }‚àû
n=0VHGH√ÄQHSDUDWRGDVODV n ‚â• 0, y
pn ‚àà [a, b] para todas las n. Al utilizar el hecho de que |g (x)| ‚â§ k y el teorema de valor
medio 1.8, tenemos, para cada n,

| pn ‚àí p| = |g( pn‚àí1 ) ‚àí g( p)| = |g (Œæn )|| pn‚àí1 ‚àí p| ‚â§ k| pn‚àí1 ‚àí p|,
donde Œæn ‚àà (a, b). Al aplicar esta desigualdad de manera inductiva obtenemos

| pn ‚àí p| ‚â§ k| pn‚àí1 ‚àí p| ‚â§ k 2 | pn‚àí2 ‚àí p| ‚â§ ¬∑ ¬∑ ¬∑ ‚â§ k n | p0 ‚àí p|.

(2.4)

Ya que 0 < k < 1, tenemos l√≠mn‚Üí‚àû k n = 0 y

l√≠m | pn ‚àí p| ‚â§ l√≠m k n | p0 ‚àí p| = 0.

n‚Üí‚àû

n‚Üí‚àû

3RUORWDQWR{ pn }‚àû
n=0 converge a p.
Corolario 2.5

Si g satisface las hip√≥tesis del teorema 2.4, entonces las cotas del error relacionado con el uso
de pn para aproximar p, est√°n dadas por por

| pn ‚àí p| ‚â§ k n m√°x{ p0 ‚àí a, b ‚àí p0 }

(2.5)

y
| pn ‚àí p| ‚â§

kn
| p1 ‚àí p0 |,
1‚àík

para toda n ‚â• 1.

Demostraci√≥n 3XHVWRTXHp ‚àà >a, b] la primera cota se sigue la desigualdad (2.4):

| pn ‚àí p| ‚â§ k n | p0 ‚àí p| ‚â§ k n m√°x{ p0 ‚àí a, b ‚àí p0 }.
3DUDn ‚â• 1, el procedimiento que se usa en la prueba del teorema 2.4 implica que

| pn+1 ‚àí pn | = |g( pn ) ‚àí g( pn‚àí1 )| ‚â§ k| pn ‚àí pn‚àí1 | ‚â§ ¬∑ ¬∑ ¬∑ ‚â§ k n | p1 ‚àí p0 |.

(2.6)

48

CAP√çTULO 2

Soluciones de las ecuaciones en una variable

3RUORWDQWRSDUDm > n ‚â• 1,

| pm ‚àí pn | = | pm ‚àí pm‚àí1 + pm‚àí1 ‚àí ¬∑ ¬∑ ¬∑ + pn+1 ‚àí pn |
‚â§ | pm ‚àí pm‚àí1 | + | pm‚àí1 ‚àí pm‚àí2 | + ¬∑ ¬∑ ¬∑ + | pn+1 ‚àí pn |
‚â§ k m‚àí1 | p1 ‚àí p0 | + k m‚àí2 | p1 ‚àí p0 | + ¬∑ ¬∑ ¬∑ + k n | p1 ‚àí p0 |
= k n | p1 ‚àí p0 | 1 + k + k 2 + ¬∑ ¬∑ ¬∑ + k m‚àín‚àí1 .
Mediante el teorema 2.3, l√≠m m‚Üí‚àû pm = p, por lo que

| p ‚àí pn | = l√≠m | pm ‚àí pn | ‚â§ l√≠m k n | p1 ‚àí p0 |
m‚Üí‚àû

m‚àín‚àí1

m‚Üí‚àû

k i ‚â§ k n | p1 ‚àí p0 |

i=0

‚àû

ki .

i=0

‚àû
3HUR i=0 k i es una serie geom√©trica con radio k y 0 < k < 1. Esta sucesi√≥n converge a
1 (1 2 k), lo que nos da la segunda cota:

| p ‚àí pn | ‚â§

kn
| p1 ‚àí p0 |.
1‚àík

Ambas desigualdades en el corolario relacionan la raz√≥n en la que { pn }‚àû
n=0 converge a la
cota k de la primera derivada. La raz√≥n de convergencia depende del factor kn. Mientras m√°s
peque√±o sea el valor de k, m√°s r√°pido converger√°. Sin embargo, la convergencia ser√≠a muy
lenta si k est√° cercana a 1.
Ilustraci√≥n

5HFRQVLGHUHPRVORVGLIHUHQWHVHVTXHPDVGHSXQWR√ÄMRGHVFULWRVHQODLOXVWUDFLyQDQWHULRUHQ
YLVWDGHOWHRUHPDGHSXQWR√ÄMR\VXFRURODULR
a)

3DUDg1(x) 5 x 2 x3 2 4x2 1 10, tenemos g1(1) 5 6 y g1(2) 5 212, por lo que g1
QRPDSHD>@HQVtPLVPR$GHPiVg91(x) 5 1 2 3x2 ‚Äì 8x, por lo que |g1 (x)| > 1
para todas las xHQ>@$SHVDUGHTXHHOWHRUHPDQRJDUDQWL]DTXHHOPpWRGR
debe fallar para esta selecci√≥n de g, no existe una raz√≥n para esperar convergencia.

b)

Con g2 (x) = [(10/x) ‚àí 4x]1/2, podemos ver que g2 QR WUD]D XQ PDSD > @ HQ
‚àû
>@\ODVXFHVLyQ{ pn }n=0QRHVWiGH√ÄQLGDFXDQGRp0 5 1.5. Adem√°s, no existe un
intervalo que contiene p ‚âà 1.365 tal que |g2 (x)| < 1 puesto que |g2 ( p)| ‚âà 3.4.
No existe una raz√≥n para esperar que este m√©todo converger√°.

c)

3DUDODIXQFLyQg3 (x) = 12 (10 ‚àí x 3 )1/2 tenemos

3
g3 (x) = ‚àí x 2 (10 ‚àí x 3 )‚àí1/2 < 0
4

en [1, 2],

por lo que g3 HVHVWULFWDPHQWHGHFUHFLHQWHHQ>@6LQHPEDUJR |g3 (2)| ‚âà 2.12,
por lo que la condici√≥n |g3 (x)| ‚â§ k < 1 IDOODHQ>@8QDQiOLVLVPiVFHUFDQRGH
la sucesi√≥n { pn }‚àû
n=0 comenzando con p0 5 PXHVWUDTXHHVVX√ÄFLHQWHFRQVLGHUDU
HOLQWHUYDOR>@HQOXJDUGH>@(QHVWHLQWHUYDORVLJXHVLHQGRYHUGDGTXH
g93(x) < 0 y g3 es estrictamente decreciente, pero, adem√°s,

1 < 1.28 ‚âà g3 (1.5) ‚â§ g3 (x) ‚â§ g3 (1) = 1.5,
para todas las x ‚àà > @ (VWR PXHVWUD TXH g3 PDSHD HO LQWHUYDOR> @ HQ Vt
mismo. Tambi√©n es verdad que |g3 (x)| ‚â§ |g3 (1.5)| ‚âà 0.66 en este intervalo, por
ORTXHHOWHRUHPDFRQ√ÄUPDODFRQYHUJHQFLDTXH\DFRQRFHPRV

2.3

d)

M√©todo de Newton y sus extensiones

49

3DUDg4 (x) = (10/(4 + x))1/2, tenemos

|g4 (x)| = ‚àö

‚àí5
10(4 + x)3/2

5
< 0.15,
‚â§‚àö
10(5)3/2

para todas las x ‚àà [1, 2].

La magnitud de la cota de g94(x) es mucho menor que la magnitud de la cota (encontrada en c)) g93(x), lo cual explica la convergencia m√°s r√°pida usando g4.
e)

/DVXFHVLyQGH√ÄQLGDSRU

g5 (x) = x ‚àí

x 3 + 4x 2 ‚àí 10
3x 2 + 8x

converge mucho m√°s r√°pido que nuestras otras elecciones. En las siguientes secciones, observaremos de d√≥nde proviene esta elecci√≥n y porqu√© es tan efectiva.
A partir de lo que hemos observado, la
¬á 3UHJXQWD¬¢&yPRSRGHPRVHQFRQWUDUXQSUREOHPDGHSXQWR√ÄMRTXHSURGXFHXQDVXFHVLyQ
TXHFRQYHUJHGHPDQHUDFRQ√ÄDEOH\UiSLGDHQXQDVROXFLyQSDUDXQSUREOHPDGDGRGH
encontrar la ra√≠z?
podr√≠amos tener la
¬á 5HVSXHVWDPDQLSXODUHOSUREOHPDGHHQFRQWUDUODUDt]HQXQSUREOHPDGHSXQWR√ÄMRTXH
VDWLVIDJDODVFRQGLFLRQHVGHOWHRUHPDGHSXQWR√ÄMR\KDFHUODGHULYDGDWDQSHTXHxD
FRPRVHDSRVLEOHFHUFDGHOSXQWR√ÄMR
En las siguientes secciones examinaremos esto con mayor detalle.
La secci√≥n Conjunto de ejercicios 2.2 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

2.3 M√©todo de Newton y sus extensiones
Isaac Newton (1641‚Äì1727)
IXHXQRGHORVFLHQWt√ÄFRVPiV
brillantes de todos los tiempos.
(O√ÄQDOGHOVLJORXVII fue un
periodo vibrante para la ciencia
y las matem√°ticas, y el trabajo
de Newton toc√≥ casi todos los
aspectos de esta √∫ltima ciencia.
Se present√≥ su m√©todo de
resoluci√≥n para encontrar la ra√≠z
de la ecuaci√≥n y 3 ‚àí 2y ‚àí 5 = 0.
A pesar de que demostr√≥ el
m√©todo s√≥lo para polinomios,
es claro que conoc√≠a sus
aplicaciones m√°s amplias.

El m√©todo de Newton (o de Newton-Raphson) es uno de los m√©todos num√©ricos m√°s poderosos y reconocidos para resolver un problema de encontrar la ra√≠z. Existen muchas formas
de presentar el m√©todo de Newton.

M√©todo de Newton
Si s√≥OR TXHUHPRV XQ DOJRULWPR SRGHPRV FRQVLGHUDU OD WpFQLFD GH PDQHUD JUi√ÄFD FRPR
a menudo se hace en c√°lculo. Otra posibilidad es derivar el m√©todo de Newton como una
t√©cnica para obtener convergencia m√°s r√°pida de lo que ofrecen otros tipos de iteraci√≥n
funcional, como hacemos en la secci√≥n 2.4. Una tercera forma para presentar el m√©todo de
Newton, que se analiza a continuaci√≥n, est√° basada en los polinomios de Taylor. Ah√≠ observaremos que esta forma particular no s√≥lo produce el m√©todo, sino tambi√©n una cota para el
error de aproximaci√≥n.
Suponga que f ‚àà C2>a, b]. Si p0 ‚àà >a, b] es una aproximaci√≥n para p, de tal forma que
f ( p0 ) = 0 y | p ‚àí p0 |HV¬¥SHTXHxR¬µ&RQVLGHUHTXHHOSULPHUSROLQRPLRGH7D\ORUSDUDf(x)
expandido alrededor de p0 y evaluado en x 5 p:

f ( p) = f ( p0 ) + ( p ‚àí p0 ) f ( p0 ) +

( p ‚àí p0 )2
f (Œæ( p)),
2

50

CAP√çTULO 2

Soluciones de las ecuaciones en una variable

donde j(p) se encuentra entre p y p03XHVWRTXHf(p) 5 0, esta ecuaci√≥n nos da

0 = f ( p0 ) + ( p ‚àí p 0 ) f ( p 0 ) +

( p ‚àí p0 )2
f (Œæ( p)).
2

El m√©todo de Newton se deriva al suponer que como | p ‚àí p0 | es peque√±o, el t√©rmino
relacionado con (p 2 p0)2 es mucho m√°s peque√±o, entonces
Joseph Raphson (1648‚Äì1715)
proporcion√≥ una descripci√≥n
del m√©todo atribuido a Isaac
Newton en 1690 y reconoci√≥
a Newton como la fuente
del descubrimiento. Ni
Newton ni Raphson utilizaron
expl√≠citamente la derivada en
su descripci√≥n ya que ambos
s√≥lo consideraron polinomios.
Otros matem√°ticos, en especial
James Gregory (1636‚Äì1675),
estaban conscientes del proceso
subyacente en esa √©poca o antes.

0 ‚âà f ( p0 ) + ( p ‚àí p0 ) f ( p0 ).
Al resolver para p obtenemos

f ( p0 )
‚â° p1 .
f ( p0 )

p ‚âà p0 ‚àí

Esto constituye la base para el m√©todo de Newton, que empieza con una aproximaci√≥n
inicial p0 y genera la sucesi√≥n { pn }‚àû
n=0, mediante

pn = pn‚àí1 ‚àí

f ( pn‚àí1 )
,
f ( pn‚àí1 )

para n ‚â• 1.

(2.7)

/D√ÄJXUDLOXVWUDFyPRVHREWLHQHQODVDSUR[LPDFLRQHVXVDQGRWDQJHQWHVVXFHVLYDV
(Adem√°s observe el ejercicio 31.) Al empezar con la aproximaci√≥n inicial p0, la aproximaci√≥n p1 es la intersecci√≥n con el eje xGHODUHFWDWDQJHQWHDODJUi√ÄFDGHf en (p0, f(p0)). La
aproximaci√≥n p2 es la intersecci√≥n con el eje xGHODUHFWDWDQJHQWHDODJUi√ÄFDf en (p1, f (p1))
y as√≠ sucesivamente. El algoritmo 2.3 implementa este procedimiento.

Figura 2.7
y
Pendiente f 9(p1)

y 5 f (x)

(p1, f (p1))

p0

p

Pendiente f 9(p0)
p2

p1

x

(p0, f (p0))

ALGORITMO

2.3

M√©todo de Newton
3DUDHQFRQWUDUXQDVROXFLyQDf(x) 5 0 dada una aproximaci√≥n inicial p0:

ENTRADA aproximaci√≥n inicial p0 tolerancia TOL; n√∫mero m√°ximo de iteraciones N 0
SALIDA

soluci√≥n aproximada p o mensaje de falla.

Paso 1 Determine i = 1.

2.3

M√©todo de Newton y sus extensiones

51

Paso 2 Mientras i ‚â§ N0 haga los pasos 3‚Äì6.
Paso 3 Determine p = p0 ‚àí f ( p0 )/ f ( p0 ).

(Calcule p i .)

Paso 4 Si | p ‚àí p0 | < TOL entonces
SALIDA (p); (El procedimiento fue exitoso.)
PARE.
Paso 5 Determine i = i + 1.
Paso 6 Determine p0 = p. (Actualizce p 0 .)
Paso 7 SALIDA (‚ÄòEl m√©todo fall√≥ despu√©s de N0 iteraciones, N0 =‚Äô, N0 );
(El procedimiento no fue exitoso.)
PARE.

Las desigualdades de la t√©cnica de parada determinadas con el m√©todo de bisecci√≥n son
aplicables al m√©todo de Newton. Es decir, seleccione una tolerancia Œµ > 0 y construya p1,
7, pN hasta que

| p N ‚àí p N ‚àí1 | < Œµ,
| p N ‚àí p N ‚àí1 |
< Œµ,
| pN |

(2.8)

p N = 0,

(2.9)

o
| f ( p N )| < Œµ.

(2.10)

Una forma de la desigualdad (2.8) se usa en el paso 4 del algoritmo 2.3. Observe que ninguna de las desigualdades (2.8), (2.9) o (2.10) dan informaci√≥n precisa sobre el error real
| p N ‚àí p|. (Consulte los ejercicios 19 y 20 en la secci√≥n 2.1).
El m√©todo de Newton es una t√©cnica de iteraci√≥n funcional con pn = g( pn‚àí1 ), para la
que

g( pn‚àí1 ) = pn‚àí1 ‚àí

f ( pn‚àí1 )
,
f ( pn‚àí1 )

para n ‚â• 1.

(2.11)

De hecho, esta es la t√©cnica de iteraci√≥n funcional que se us√≥ para proporcionar la convergencia r√°pida que vimos en la columna e) de la tabla 2.2 en la secci√≥n 2.2.
A partir de la ecuaci√≥n (2.7) es claro que el m√©todo de Newton no se puede continuar si
f ( pn‚àí1 ) = 0 para alguna n.'HKHFKRREVHUYDUHPRVTXHHOPpWRGRHVPiVH√ÄFD]FXDQGRf 9
est√° acotada lejos de cero y cerca de p.
Ejemplo 1

Considere la funci√≥n f(x) 5 cos x 2 x 5 0. Aproxime una ra√≠z de f usando a) el m√©todo de
SXQWR√ÄMR\b) el m√©todo de Newton.
Soluci√≥n

a) Una soluci√≥n para este problema de encontrar la ra√≠z tambi√©n es una soluci√≥n
SDUDHOSUREOHPDGHSXQWR√ÄMRx 5 cos x\ODJUi√ÄFDHQOD√ÄJXUDLPSOLFDTXHXQVROR
SXQWR√ÄMRpVHHQFXHQWUDHQ>p/2].

52

CAP√çTULO 2

Soluciones de las ecuaciones en una variable

Figura 2.8
y
y5x
Observe que la variable en la
funci√≥n trigonom√©trica est√°
medida en radianes, no grados.
√âste siempre ser√° el caso a
PHQRVTXHVHHVSHFL√ÄTXHOR
contrario.

1
y 5 cos x

1

Tabla 2.3
n

pn

0
1
2
3
4
5
6
7

0.7853981635
0.7071067810
0.7602445972
0.7246674808
0.7487198858
0.7325608446
0.7434642113
0.7361282565

x

/DWDEODPXHVWUDORVUHVXOWDGRVGHODLWHUDFLyQGHSXQWR√ÄMRFRQp0 5 p/4. Lo mejor
que podr√≠amos concluir a partir de estos resultados es que p ‚âà 0.74.
b)3DUDDSOLFDUHOPpWRGRGH1HZWRQDHVWHSUREOHPDQHFHVLWDPRVf9(x) 5 2sen x 2 1.
Al iniciar de nuevo en p0 5 p/4, tenemos

p1 = p0 ‚àí

p0
f ( p0 )

œÄ
cos(œÄ/4) ‚àí œÄ/4
‚àí
4
‚àí sen(œÄ/4) ‚àí 1
‚àö
2/2 ‚àí œÄ/4
œÄ
= ‚àí ‚àö
4
‚àí 2/2 ‚àí 1
=

= 0.7395361337
p2 = p1 ‚àí

cos( p1 ) ‚àí p1
‚àí sen( p1 ) ‚àí 1

= 0.7390851781
Nosotros generamos continuamente la sucesi√≥n mediante

Tabla 2.4
M√©todo de Newton
n

pn

0
1
2
3
4

0.7853981635
0.7395361337
0.7390851781
0.7390851332
0.7390851332

pn = pn‚àí1 ‚àí

f ( pn‚àí1 )
cos pn‚àí1 ‚àí pn‚àí1
= pn‚àí1 ‚àí
.
f ( pn‚àí1 )
‚àí sen pn‚àí1 ‚àí 1

Esto nos da las aproximaciones en la tabla 2.4. Se obtiene una excelente aproximaci√≥n con
n 5 3. Debido a la cercan√≠a de p3 y p4, podr√≠amos esperar razonablemente que este resultado
sea preciso con los decimales enumerados.

Convergencia con el m√©todo de Newton
El ejemplo 1 muestra que el m√©todo de Newton puede dar aproximaciones en extremo preFLVDVFRQPX\SRFDVLWHUDFLRQHV3DUDHVHHMHPSORV√≥lo se necesit√≥ una iteraci√≥n del m√©todo
GH1HZWRQSDUDGDUXQDSUHFLVLyQPD\RUTXHODVVLHWHLWHUDFLRQHVGHOPpWRGRGHSXQWR√ÄMR
Ahora es momento de examinar el m√©todo de Newton para descubrir porqu√© es tan eficaz.
La derivaci√≥n del m√©todo de Newton por medio de la serie de Taylor al inicio de la secci√≥n se√±ala la importancia de una aproximaci√≥n inicial precisa. La suposici√≥n crucial es que
el t√©rmino relacionado con (p 2 p0)2 es, en comparaci√≥n con | p ‚àí p0 |, tan peque√±o que se
puede eliminar. Claramente esto ser√° falso a menos que p0 sea una buena aproximaci√≥n para
p. Si p0QRHVWiVX√ÄFLHQWHPHQWHFHUFDGHODUDt]UHDOH[LVWHQSRFDVUD]RQHVSDUDVRVSHFKDU
que el m√©todo de Newton converger√° en la ra√≠z. Sin embargo, en algunos casos, incluso las
malas aproximaciones iniciales producir√°n convergencia. (Los ejercicios 15 y 16 ilustran
algunas de las posibilidades.)

2.3

M√©todo de Newton y sus extensiones

53

El siguiente teorema de convergencia para el m√©todo de Newton ilustra la importancia
te√≥rica de la selecci√≥n de p0.
Teorema 2.6

Sea f ‚àà C2>a, b]. Si p ‚àà (a, b) es tal que f ( p) = 0 y f ( p) = 0, entonces existe una d . 0
tal que el m√©todo de Newton genera una sucesi√≥n { pn }‚àû
n=1 que converge a p para cualquier
aproximaci√≥n inicial p0 ‚àà >p 2 d, p 1 d].
Demostraci√≥n La prueba se basa en el an√°lisis del m√©todo de Newton como un esquema de

iteraci√≥n funcional pn 5 g(pn‚Äì1) para n ‚â• 1, con

g(x) = x ‚àí

f (x)
.
f (x)

Sea kXQQ~PHURHQWUH  3ULPHURHQFRQWUDPRVXQLQWHUYDOR>p 2 d, p 1 d] que g mapea
en s√≠ mismo y para el cual |g (x)| ‚â§ k para todas las x ‚àà (p 2 d, p 1 d).
Ya que f 9 es continua y f ( p) = 0, la parte a) del ejercicio 30 en la secci√≥n 1.1 implica
que existe una d1 > 0, tal que f (x) = 0 para x ‚àà >p 2 d1, p 1 d1] ‚äÜ >a, b@3RUORWDQWR
gHVWiGH√ÄQLGD\HVFRQWLQXDHQ>p 2 d1, p 1 d1]. Adem√°s,

g (x) = 1 ‚àí

f (x) f (x) ‚àí f (x) f (x)
f (x) f (x)
=
,
[ f (x)]2
[ f (x)]2

para x ‚àà >p 2 d1, p 1 d1] y, puesto que f ‚àà C2>a, b], tenemos g ‚àà C1>p 2 d1, p 1 d1].
3RUKLSyWHVLVf (p) 5 0 por lo que

g ( p) =

f ( p) f ( p)
= 0.
[ f ( p)]2

3XHVWRTXHg9 es continua y 0 < k < 1, la parte b) del ejercicio 30 en la secci√≥n 1.1 implica
que existe d, con 0 < d < d1, para el cual

|g (x)| ‚â§ k,

para todas las x ‚àà [ p ‚àí Œ¥, p + Œ¥].

Falta probar que g PDSHD >p 2 d, p 1 d@ HQ >p 2 d, p 1 d]. Si x ‚àà >p 2 d, p 1
d], el teorema de valor medio implica que para alg√∫n n√∫mero j entre x y p, |g(x) ‚àí g( p)|
= |g (Œæ )||x ‚àí p|.3RUORWDQWR

|g(x) ‚àí p| = |g(x) ‚àí g( p)| = |g (Œæ )||x ‚àí p| ‚â§ k|x ‚àí p| < |x ‚àí p|.
3XHVWRTXHx ‚àà >p 2 d, p 1 d], se sigue que |x ‚àí p| < Œ¥ y que |g(x) ‚àí p| < Œ¥. 3RUORWDQWR
gPDSHD>p 2 d, p 1 d] en >p 2 d, p 1 d].
$KRUDVHVDWLVIDFHQWRGDVODVKLSyWHVLVGHOWHRUHPDGHSXQWR√ÄMRSRUORTXHODVXcesi√≥n { pn }‚àû
n=1GH√ÄQLGDSRU

pn = g( pn‚àí1 ) = pn‚àí1 ‚àí

f ( pn‚àí1 )
,
f ( pn‚àí1 )

para n ‚â• 1,

converge a p para cualquier p0 ‚àà >p 2 d, p 1 d].
El teorema 2.6 establece que, de acuerdo con suposiciones razonables, el m√©todo de
1HZWRQFRQYHUJHVLHPSUHTXHVHVHOHFFLRQHXQDDSUR[LPDFLyQLQLFLDOVX√ÄFLHQWHPHQWHH[DFta. Tambi√©n implica que la constante k que acota la derivada de g y, por consiguiente, indica
que la velocidad de convergencia del m√©todo disminuye a 0, conforme el procedimiento
contin√∫a. Este resultado es importante para la teor√≠a del m√©todo de Newton, pero casi nunca
se aplica en la pr√°ctica porque no nos dice c√≥mo determinar d.
En una aplicaci√≥n pr√°ctica, se selecciona una aproximaci√≥n inicial y se generan aproximaciones sucesivas con el m√©todo de Newton. Ya sea que √©stos converjan r√°pidamente a la
ra√≠z o ser√° claro que la convergencia es poco probable.

54

CAP√çTULO 2

Soluciones de las ecuaciones en una variable

El m√©todo de la secante
El m√©todo de Newton es una t√©cnica en extremo poderosa, pero tiene una debilidad importante: la necesidad de conocer el valor de la derivada de f en cada aproximaci√≥n. Con frecuencia, f 9(x) es mucho m√°s dif√≠cil y necesita m√°s operaciones aritm√©ticas para calcular f (x).
3DUDHYLWDUHOSUREOHPDGHODHYDOXDFLyQGHODGHULYDGDHQHOPpWRGRGH1HZWRQSUHVHQWDPRVXQDOLJHUDYDULDFLyQ3RUGH√ÄQLFLyQ

f ( pn‚àí1 ) = l√≠m

x‚Üí pn‚àí1

f (x) ‚àí f ( pn‚àí1 )
.
x ‚àí pn‚àí1

Si pn‚Äì2 est√° cerca de pn‚Äì1, entonces

f ( pn‚àí1 ) ‚âà

f ( pn‚àí2 ) ‚àí f ( pn‚àí1 )
f ( pn‚àí1 ) ‚àí f ( pn‚àí2 )
=
.
pn‚àí2 ‚àí pn‚àí1
pn‚àí1 ‚àí pn‚àí2

Usando esta aproximaci√≥n para f 9(pn‚Äì1) en la f√≥rmula de Newton obtenemos
La palabra secante se deriva de
la palabra en lat√≠n secan, que
VLJQL√ÄFD¬¥FRUWDU¬µ(OPpWRGRGH
la secante usa una l√≠nea secante,
que une dos puntos que cortan la
curva, para aproximar una ra√≠z.

pn = pn‚àí1 ‚àí

f ( pn‚àí1 )( pn‚àí1 ‚àí pn‚àí2 )
.
f ( pn‚àí1 ) ‚àí f ( pn‚àí2 )

(2.12)

Esta t√©cnica recibe el nombre de m√©todo de la secante y se presenta en el algoritmo 2.4
FRQVXOWHOD√ÄJXUD (PSH]DQGRFRQGRVDSUR[LPDFLRQHVLQLFLDOHVp0 y p1, la aproximaci√≥n p2 es la intersecci√≥n en x de la recta que une los puntos (p0, f(p0)) y (p1, f(p1)). La aproximaci√≥n p3 es la intersecci√≥n en x de la recta que une los puntos (p1, f(p1)) y (p2, f(p2)) y as√≠
sucesivamente. Observe que s√≥lo se necesita una evaluaci√≥n de la funci√≥n por cada paso para
el m√©todo de la secante despu√©s de haber determinado p2. En contraste, cada paso del m√©todo
de Newton requiere una evaluaci√≥n tanto de la funci√≥n como de su derivada.

Figura 2.9
y

y 5 f (x)

p0

ALGORITMO

2.4

p3
p2

p
p4

p1

x

M√©todo de la secante
3DUDHQFRQWUDUXQDVROXFLyQSDUDf(x) = 0 dadas las aproximaciones iniciales p0 y p1:

ENTRADA

aproximaciones iniciales p0 , p1 tolerancia TOL; n√∫mero m√°ximo
de iteraciones N0 .

SALIDA soluci√≥n aproximada p o mensaje de falla.
Paso 1 Determine i = 2;
q0 = f ( p0 );
q1 = f ( p1 ).

2.3

M√©todo de Newton y sus extensiones

55

Paso 2 Mientras i ‚â§ N0 haga los pasos 3‚Äì6.
Paso 3 Determine p = p1 ‚àí q1 ( p1 ‚àí p0 )/(q1 ‚àí q0 ).

(Calcule p i .)

Paso 4 Si | p ‚àí p1 | < TOL entonces
SALIDA (p); (El procedimiento fue exitoso.)
PARE.
Paso 5 Determine i = i + 1.
Paso 6 Determine p0 = p1 ; (Actualice
q0 = q1 ;
p1 = p;
q1 = f ( p).

p 0 , q0 , p1 , q1 .)

Paso 7 SALIDA (‚ÄòEl m√©todo fall√≥ despu√©s de N0 iteraciones, N0 =‚Äô, N0 );
(El procedimiento no fue exitoso.)
PARE.

El siguiente ejemplo incluye el problema que se consider√≥ en el ejemplo 1, donde utilizamos el m√©todo de Newton con p0 5 œÄ/4.
Ejemplo 2

Use el m√©todo de la secante para encontrar una soluci√≥n para x = cos x y compare las aproximaciones con las determinadas en el ejemplo 1, el cual aplica el m√©todo de Newton.
Soluci√≥n (Q HO HMHPSOR  FRPSDUDPRV OD LWHUDFLyQ GHO SXQWR √ÄMR \ HO PpWRGR GH 1HZton con la aproximaci√≥n inicial p0 5 œÄ/4. 3DUD HO P√©todo de la secante, necesitamos dos
aproximaciones iniciales.

Suponga que usamos p0 5 0.5 y p1 5 p/4:

p 2 = p1 ‚àí
=

( p1 ‚àí p0 )(cos p1 ‚àí p1 )
(cos p1 ‚àí p1 ) ‚àí (cos p2 ‚àí p2 )

œÄ
(œÄ/4 ‚àí 0.5)(cos(œÄ/4) ‚àí œÄ/4)
‚àí
4
(cos(œÄ/4) ‚àí œÄ/4) ‚àí (cos 0.5 ‚àí 0.5)

= 0.7363841388.

Tabla 2.5
Secante
n

pn

0
1
2
3
4
5

0.5
0.7853981635
0.7363841388
0.7390581392
0.7390851493
0.7390851332

n

Newton
pn

0
1
2
3
4

0.7853981635
0.7395361337
0.7390851781
0.7390851332
0.7390851332

Las aproximaciones sucesivas se generan con la f√≥rmula

pn = pn‚àí1 ‚àí

( pn‚àí1 ‚àí pn‚àí2 )(cos pn‚àí1 ‚àí pn‚àí1 )
,
(cos pn‚àí1 ‚àí pn‚àí1 ) ‚àí (cos pn‚àí2 ‚àí pn‚àí2 )

para n ‚â• 2.

Esto proporciona los resultados en la tabla 2.5. Observamos que a pesar de que la f√≥rmula
para p2 parece indicar un c√°lculo repetido, una vez que se calcula f(p0) y f(p1), no se calculan
de nuevo.
Al comparar los resultados en la tabla 2.5 a partir del m√©todo de la secante y el m√©todo
de Newton, observamos que la aproximaci√≥n p5 del m√©todo de la secante es precisa hasta la
d√©cima cifra decimal, mientras que con el m√©todo de Newton se obtuvo esta precisi√≥n por
p33DUDHVWHHMHPSORODFRQYHUJHQFLDGHOP√©todo de la secante es mucho m√°s r√°pida que la
iteraci√≥n funcional, pero ligeramente m√°s lenta que el m√©todo de Newton. √âste es, en general, el caso. (Consulte el ejercicio 14 de la secci√≥n 2.4.)
El m√©todo de Newton o el m√©todo de la VHFDQWHFRQIUHFXHQFLDVHXVDQSDUDUH√ÄQDUXQD
respuesta obtenida con otra t√©cnica, como el m√©todo de bisecci√≥n, ya que estos m√©todos requieren una primera aproximaci√≥n adecuada pero, en general, proveen convergencia r√°pida.

56

CAP√çTULO 2

Soluciones de las ecuaciones en una variable

El m√©todo de posici√≥n falsa
Cada par sucesivo de aproximaciones en el m√©todo de bisecci√≥n agrupa una ra√≠z p de la ecuaci√≥n; es decir, para cada entero positivo n, se encuentra una ra√≠z entre an y bn. Esto implica
que, para cada n, las iteraciones del m√©todo de bisecci√≥n satisfacen

| pn ‚àí p| <

El t√©rmino Regula Falsi,
OLWHUDOPHQWH¬¥UHJODIDOVD¬µR
¬¥SRVLFLyQIDOVD¬µVHUH√ÄHUHD
una t√©cnica en la que se usan
resultados que se sabe son falsos,
SHURGHDOJ~QPRGRHVSHFt√ÄFR
para obtener convergencia
a un resultado verdadero.
Los problemas de posici√≥n
falsa se pueden encontrar en
el papiro Rhind, que data de
aproximadamente 1650 a.C.

1
|an ‚àí bn |,
2

lo cual provee una cota del error f√°cilmente calculable para las aproximaciones.
La agrupaci√≥n de ra√≠ces no est√° garantizada para el m√©todo de Newton ni para el
m√©todo de la secante. En el ejemplo 1, el m√©todo de Newton se aplic√≥ a f(x) 5 cos x 2 x y
se encontr√≥ que la ra√≠z aproximada es 0.7390851332. La tabla 25 muestra que esta ra√≠z no
se agrupa mediante p0 y p1 o p1 y p2. Las aproximaciones del m√©todo de la secante para este
problema tambi√©n se determinan en la tabla 2.5. En este caso, las aproximaciones iniciales
p0 y p1 agrupan la ra√≠z, pero el par de aproximaciones p3 y p4 fallan al hacerlo.
El m√©todo de posici√≥n falsa (tambi√©n llamado Regula Falsi) genera aproximaciones
de la misma manera que el m√©todo de la secante, pero incluye una prueba para garantizar que
la ra√≠z siempre se agrupa entre iteraciones sucesivas. A pesar de que no es un m√©todo que por
lo general recomendamos, ilustra c√≥mo se puede integrar la agrupaci√≥n.
En primer lugar, seleccionamos las aproximaciones iniciales p0 y p1 con f(p0) ? f(p1) < 0.
La aproximaci√≥n p2 se selecciona de la misma forma que en el m√©todo de la secante como la
intersecci√≥n en x de la recta que une (p0, f(p0)) y (p1, f(p1 3DUDGHFLGLUFXiOOtQHDVHFDQWH
se usa para calcular p3, considere f (p2) ? f(p1) o, m√°s correctamente, sgn f(p2) ? sgn f (p1).
‚Ä¢ Si sgn f(p2) ? sgn f(p1) < 0, entonces p1 y p2 agrupan una ra√≠z. Seleccione p3 como la intersecci√≥n en x de la recta que une (p1, f(p1)) y (p2, f(p2)).
‚Ä¢ Si no, seleccionamos p3 como la intersecci√≥n en x de la recta que une (p0, f(p0)) y (p2,
f(p2)) y, a continuaci√≥n intercambia los √≠ndices en p0 y p1.
De manera similar, una vez que se encuentra p3, el signo de f (p3) ? f(p2) determina si usamos
p2 y p3 o p3 y p1 para calcular p4. En el √∫ltimo caso, se vuelve a etiquetar p2 y p1. Reetiquetar
garantiza que la ra√≠z se agrupa entre iteraciones sucesivas. El proceso, como se describe en
HODOJRULWPR\OD√ÄJXUDPXHVWUDFyPRODVLWHUDFLRQHVSXHGHQGLIHULUGHODVGHOP√©todo de la secante. En esta ilustraci√≥n, las primeras tres aproximaciones son iguales, pero la
FXDUWDGL√ÄHUH

Figura 2.10
M√©todo de posici√≥n falsa

M√©todo de la secante
y

y 5 f (x)

p2
p0

y

y 5 f (x)

p3

p2
p4

p1 x

p0

p3
p4

p1 x

2.3

M√©todo de Newton y sus extensiones

57

Posici√≥n falsa

ALGORITMO

2.5

3DUDHQFRQWUDUXQDVROXFLyQSDUDf (x) 5 0 dada la funci√≥n fFRQWLQXDHQHOLQWHUYDOR>p0, p1]
donde f(p0) y f(p1) tienen signos opuestos:

ENTRADA

aproximaciones iniciales p0 , p1 tolerancia TOL; n√∫mero m√°ximo de
iteraciones N0 .

SALIDA soluci√≥n aproximada p o mensaje de falla.
Paso 1 Determine i = 2;
q0 = f ( p0 );
q1 = f ( p1 ).
Paso 2 Mientras i ‚â§ N0 haga los pasos 3‚Äì7.
Paso 3 Determine p = p1 ‚àí q1 ( p1 ‚àí p0 )/(q1 ‚àí q0 ).

(Calcule p i .)

Paso 4 Si | p ‚àí p1 | < TOL entonces
SALIDA (p); (El procedimiento fue exitoso.)
PARE.
Paso 5 Determine i = i + 1;
q = f ( p).
Paso 6 Si q ¬∑ q1 < 0 entonces determine p0 = p1 ;
q0 = q1 .
Paso 7 Determine p1 = p;
q1 = q.
Paso 8 SALIDA ('El m√©todo fall√≥ despu√©s de N0 iteraciones, N0 =‚Äô, N0 );
(El procedimiento no fue exitoso.)
PARE.

Ejemplo 3

Use el m√©todo de posici√≥n falsa para encontrar una soluci√≥n a x 5 cos x y compare las apro[LPDFLRQHVFRQDTXHOODVGHWHUPLQDGDVHQHOHMHPSORTXHDSOLFDQODLWHUDFLyQGHOSXQWR√ÄMR
y el m√©todo de Newton, y con aquellas encontradas en el ejemplo 2, que aplica el m√©todo
de la secante.
Soluci√≥n 3DUD HIHFWXDU XQD FRPSDUDFLyQ UD]RQDEOH XVDUHPRV ODV PLVPDV DSUR[LPDFLRQHV
iniciales que en el m√©todo de la secante; es decir p0 5 0.5 y p1 5 œÄ/4. La tabla 2.6 muestra los
resultados del m√©todo de posici√≥n falsa aplicado a f(x) 5 cos x 2 x junto con los obtenidos
mediante los m√©todos de la secante y de Newton. Observe que las aproximaciones de posici√≥n
falsa y de la secante concuerdan a trav√©s de p3 y que el m√©todo de posici√≥n falsa requiere una
iteraci√≥n adicional para obtener la misma precisi√≥n que el m√©todo de la secante.

Tabla 2.6
n
0
1
2
3
4
5
6

Posici√≥n falsa
pn
0.5
0.7853981635
0.7363841388
0.7390581392
0.7390848638
0.7390851305
0.7390851332

Secante
pn
0.5
0.7853981635
0.7363841388
0.7390581392
0.7390851493
0.7390851332

Newton
pn
0.7853981635
0.7395361337
0.7390851781
0.7390851332
0.7390851332

58

CAP√çTULO 2

Soluciones de las ecuaciones en una variable

Com√∫nmente, la seguridad adicional del m√©todo de posici√≥n falsa requiere m√°s c√°lculos
TXHHOPpWRGRGHODVHFDQWHGHODPLVPDIRUPDHQTXHODVLPSOL√ÄFDFLyQSURSRUFLRQDGDSRUHO
m√©todo de la secante sobre el m√©todo de Newton se realiza a expensas de iteraciones adicionales. M√°s ejemplos de las caracter√≠sticas positivas y negativas de estos m√©todos se pueden
observar al trabajar en los ejercicios 13 y 14.
La secci√≥n Conjunto de ejercicios 2.3 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

2.4 An√°lisis de error para m√©todos iterativos
En esta secci√≥n investigamos el orden de convergencia de esquemas de iteraci√≥n funcional
y, con el prop√≥sito de obtener convergencia r√°pida, redescubrimos el m√©todo de Newton.
Tambi√©n consideramos formas para acelerar la convergencia del m√©todo de Newton en cirFXQVWDQFLDV HVSHFLDOHV 3ULPHUR VLQ HPEDUJR QHFHVLWDPRV XQ QXHYR SURFHGLPLHQWR SDUD
medir qu√© tan r√°pido converge una sucesi√≥n.

Orden de convergencia
DeÔ¨Ånici√≥n 2.7

Suponga que { pn }‚àû
n=0 es una sucesi√≥n que converge a p, con pn = p para todas las n. Si
existen constantes positivas Œª y Œ± con

l√≠m

n‚Üí‚àû

| pn+1 ‚àí p|
= Œª,
| pn ‚àí p|Œ±

Entonces { pn }‚àû
n=0 converge a p de orden a, con constante de error asint√≥tica l.
Se dice que una t√©cnica iterativa de la forma pn = g( pn‚àí1 ) es de orden Œ± si la sucesi√≥n
{ pn }‚àû
n=0 converge a la soluci√≥n p 5 g(p) de orden Œ±.
En general, una sucesi√≥n con un alto orden converge m√°s r√°pidamente que una sucesi√≥n
con un orden m√°s bajo. La constante asint√≥tica afecta la velocidad de convergencia pero no
el grado del orden. Se presta atenci√≥n especial a dos casos:
i)

Si Œ± 5 1 (y Œª < 1), la sucesi√≥n es linealmente convergente.

ii)

Si Œ± 5 2, la sucesi√≥n es cuadr√°ticamente convergente.

La siguiente ilustraci√≥n compara una linealmente convergente con una que es cuadr√°ticamente convergente. Y se muestra por qu√© tratamos de encontrar m√©todos que producen
sucesiones convergentes de orden superior.
Ilustraci√≥n

Suponga que { pn }‚àû
n=0 es linealmente convergente en 0 con

l√≠m

n‚Üí‚àû

| pn+1 |
= 0.5
| pn |

Y que { pÃÉn }‚àû
n=0 es cuadr√°ticamente convergente a 0 con la misma constante de error asint√≥tico,

| pÃÉn+1 |
= 0.5.
n‚Üí‚àû | pÃÉn |2
l√≠m

3RUVLPSOLFLGDGDVXPLPRVTXHSDUDFDGDn, tenemos

| pn+1 |
‚âà 0.5
| pn |

y

| pÃÉn+1 |
‚âà 0.5.
| pÃÉn |2

2.4

An√°lisis de error para m√©todos iterativos

59

3DUDHOHVTXHPDOLQHDOPHQWHFRQYHUJHQWHHVWRVLJQL√ÄFDTXH

| pn ‚àí 0| = | pn | ‚âà 0.5| pn‚àí1 | ‚âà (0.5)2 | pn‚àí2 | ‚âà ¬∑ ¬∑ ¬∑ ‚âà (0.5)n | p0 |,
mientras el procedimiento cuadr√°ticamente convergente es

| pÃÉn ‚àí 0| = | pÃÉn | ‚âà 0.5| pÃÉn‚àí1 |2 ‚âà (0.5)[0.5| pÃÉn‚àí2 |2 ]2 = (0.5)3 | pÃÉn‚àí2 |4
‚âà (0.5)3 [(0.5)| pÃÉn‚àí3 |2 ]4 = (0.5)7 | pÃÉn‚àí3 |8
n

n

‚âà ¬∑ ¬∑ ¬∑ ‚âà (0.5)2 ‚àí1 | pÃÉ0 |2 .
La tabla 2.7 ilustra la velocidad relativa de convergencia de las sucesiones en 0 si
| p0 | = | pÃÉ0 | = 1.

Tabla 2.7

Sucesi√≥n de convergencia
lineal { pn }‚àû
n=0
n
(0.5)n

Sucesi√≥n de convergencia
cuadr√°tica { pÃÉn }‚àû
n=0
n
(0.5)2 ‚àí1

5.0000 √ó 10‚àí1
2.5000 √ó 10‚àí1
1.2500 √ó 10‚àí1
6.2500 √ó 10‚àí2
3.1250 √ó 10‚àí2
1.5625 √ó 10‚àí2
7.8125 √ó 10‚àí3

5.0000 √ó 10‚àí1
1.2500 √ó 10‚àí1
7.8125 √ó 10‚àí3
3.0518 √ó 10‚àí5
4.6566 √ó 10‚àí10
1.0842 √ó 10‚àí19
5.8775 √ó 10‚àí39

1
2
3
4
5
6
7

La sucesi√≥n cuadr√°ticamente convergente se encuentra dentro de 10238 de 0 mediante el
s√©ptimo t√©rmino. Se necesitan por lo menos 126 t√©rminos para garantizar esta precisi√≥n para
la sucesi√≥n linealmente convergente.
Se espera que las sucesiones cuadr√°ticamente convergentes converjan mucho m√°s r√°pido que las que s√≥lo convergen linealmente, pero el siguiente resultado implica que una
WpFQLFDGHSXQWR√ÄMRDUELWUDULDTXHJHQHUDVHFXHQFLDVFRQYHUJHQWHVV√≥lo lo hace linealmente.
Teorema 2.8

Sea g ‚àà>a, b] tal que g(x) ‚àà>a, b] para todas las x ‚àà>a, b]. Suponga adem√°s que g9 es continua en (a, b) y que existe una constante positiva k , 1 con

|g (x)| ‚â§ k,

para toda x ‚àà (a, b).

Si g ( p) = 0, entonces para cualquier n√∫mero p0 = p en [a, b], la sucesi√≥n

pn = g( pn‚àí1 ),

para n ‚â• 1,

Converge s√≥OROLQHDOPHQWHSDUDHO~QLFRSXQWR√ÄMRp HQ>a, b].
Demostraci√≥n 6DEHPRVTXHDSDUWLUGHOWHRUHPDGHSXQWR√ÄMRHQODVHFFLyQODVXFHVLyQ

converge a p3XHVWRTXHH[LVWHg9 en (a, b), podemos aplicar el teorema del valor medio para
g para demostrar que para cualquier n,

pn+1 ‚àí p = g( pn ) ‚àí g( p) = g (Œæn )( pn ‚àí p),
‚àû
donde jn est√° entre pn y p. Ya que { pn }‚àû
n=0 converge a p, tambi√©n tenemos que {Œæn }n=0
converge a p3XHVWRTXHg0 es continua en (a, b), tenemos

l√≠m g (Œæn ) = g ( p).

n‚Üí‚àû

60

CAP√çTULO 2

Soluciones de las ecuaciones en una variable

3RUORWDQWR

l√≠m

n‚Üí‚àû

pn+1 ‚àí p
= l√≠m g (Œæn ) = g ( p)
n‚Üí‚àû
pn ‚àí p

y

l√≠m

n‚Üí‚àû

| pn+1 ‚àí p|
= |g ( p)|.
| pn ‚àí p|

De este modo, si g ( p) = 0, ODLWHUDFLyQGHSXQWR√ÄMRPXHVWUDFRQYHUJHQFLDOLQHDOFRQHUURU
asint√≥tico constante |g ( p)|.
El teorema 2.8 implica que la convergencia de orden superior para los m√©todos de punto
√ÄMRGHODIRUPDg(p) 5 p s√≥lo se puede presentar cuando g9(p) 5 0. El siguiente resultado
describe condiciones adicionales que garantizan la convergencia cuadr√°tica que buscamos.
Teorema 2.9

Sea p una soluci√≥n de la ecuaci√≥n x 5 g(x). Suponga que g9(p) 5 0 y que g0 es continua con
|g (x)| < M en un intervalo abierto I que contiene a p. Entonces existe d > 0 tal que para p0
‚àà>p 2 d, p 1 d],ODVXFHVLyQGH√ÄQLGDSRUpn = g(pn21), cuando n ‚â• 1, converge, por lo menos
cuadr√°ticamente a p$GHPiVFRQYDORUHVVX√ÄFLHQWHPHQWHJUDQGHVGH n,

| pn+1 ‚àí p| <

M
| pn ‚àí p|2 .
2

Demostraci√≥n Seleccione k en (0, 1) y d >WDOTXHHQHOLQWHUYDOR>p 2 d, p 1 d], contenido en I, tenemos |g (x)| ‚â§ k y g FRQWLQXD3XHVWRTXH|g (x)| ‚â§ k < 1, el argumento
utilizado en la prueba del teorema 2.6 en la secci√≥n 2.3 muestra que los t√©rminos de la sucesi√≥n { pn }‚àû
n=0HVWiQFRQWHQLGRVHQ>p 2 d, p 1 d]. Al expandir g(x) en un polinomio lineal de
Taylor, para x ‚àà>p 2 d, p 1 d] obtenemos

g(x) = g( p) + g ( p)(x ‚àí p) +

g (Œæ )
(x ‚àí p)2 ,
2

donde j se encuentra entre x y p. Las hip√≥tesis g(p) 5 p y g9(p) 5 0 implican que

g(x) = p +

g (Œæ )
(x ‚àí p)2 .
2

pn+1 = g( pn ) = p +

g (Œæn )
( pn ‚àí p)2 ,
2

En especial, cuando x 5 pn,

con jn entre pn y p3RUORWDQWR

pn+1 ‚àí p =

g (Œæn )
( pn ‚àí p)2 .
2

3XHVWRTXH|g (x)| ‚â§ k < 1 HQ>p 2 d, p 1 d] y gPDSHD>p 2 d, p 1 d] en s√≠ mismo, por el
WHRUHPDGHSXQWR√ÄMRVHVLJXHTXH{ pn }‚àû
n=0 converge a p3HURFRPRjn se encuentra entre p
y pn para cada n, entonces {Œæn }‚àû
tambi√©n
convergen a p y
n=0

l√≠m

n‚Üí‚àû

| pn+1 ‚àí p|
|g ( p)|
.
=
| pn ‚àí p|2
2

Este resultado implica que la sucesi√≥n { pn }‚àû
n=0 es cuadr√°ticamente convergente si g ( p) = 0
y de convergencia de orden superior si g0(p) 5 0.
3XHVWRTXHg0 es continua y est√° estrictamente acotada por MHQHOLQWHUYDOR>p 2 d, p 1 d],
HVWRWDPELpQLPSOLFDTXHSDUDORVYDORUHVVX√ÄFLHQWHPHQWHJUDQGHVGHn,

| pn+1 ‚àí p| <

M
| pn ‚àí p|2 .
2

2.4

An√°lisis de error para m√©todos iterativos

61

/RVWHRUHPDV\QRVLQGLFDQTXHQXHVWUDE~VTXHGDGHPpWRGRVGHSXQWR√ÄMRTXH
convergen cuadr√°ticamente deber√≠an se√±alar hacia funciones cuyas derivadas son cero en el
SXQWR√ÄMR(VGHFLU
¬á 3DUDXQPpWRGRGHSXQWR√ÄMRTXHFRQYHUJHFXDGUiWLFDPHQWHQHFHVLWDPRVWHQHUWDQWRg(p)
5 p como g9(p) 5 0.
/DIRUPDPiVIiFLOGHFRQVWUXLUXQSUREOHPDGHSXQWR√ÄMRUHODFLRQDGRFRQHOSUREOHPD
de encontrar la ra√≠z f(x) 5 0 es sumar o restar un m√∫ltiplo de f (x) a partir de x. Considere la
sucesi√≥n

pn = g( pn‚àí1 ),

para n ‚â• 1,

para g en la forma de

g(x) = x ‚àí œÜ(x) f (x),
donde f es una funci√≥n diferenciable que se seleccionar√° m√°s adelante.
3DUD TXH HO SURFHGLPLHQWR LWHUDWLYR GHULYDGR GH g sea convergente cuadr√°ticamente,
necesitamos tener g9(p) 5 0 cuando f(p) 5 0. Ya que

g (x) = 1 ‚àí œÜ (x) f (x) ‚àí f (x)œÜ(x)
y f(p) 5 0, tenemos

g ( p) = 1 ‚àí œÜ ( p) f ( p) ‚àí f ( p)œÜ( p) = 1 ‚àí œÜ ( p) ¬∑ 0 ‚àí f ( p)œÜ( p) = 1 ‚àí f ( p)œÜ( p),
y g9(p) 5 0 si y s√≥lo si f(p) 5 1/f9(p).
Si hacemos f(x) 5 1/f 9(x) entonces garantizamos que el procedimiento converge cuadr√°ticamente

pn = g( pn‚àí1 ) = pn‚àí1 ‚àí

f ( pn‚àí1 )
.
f ( pn‚àí1 )

(VWRSRUVXSXHVWRHVVLPSOHPHQWHHOPpWRGRGH1HZWRQ3RUORWDQWR
‚Ä¢ Si f(p) 5 0 y f ( p) = 0, HQWRQFHVSDUDORVYDORUHVVX√ÄFLHQWHPHQWHFHUFDQRVDp, el m√©todo de Newton converger√° por lo menos cuadr√°ticamente.

Ra√≠ces m√∫ltiples
En el an√°lisis anterior se efectu√≥ una restricci√≥n en la que f ( p) = 0, donde p es la soluci√≥n
de f (x) 5 0. En especial, el m√©todo de Newton y el m√©todo de la secante, en general, dan
problemas si f 9(p) 5 0 cuando f(p) 53DUDH[DPLQDUHVWDVGL√ÄFXOWDGHVPiVGHWDOODGDPHQWHGDPRVODVLJXLHQWHGH√ÄQLFLyQ
DeÔ¨Ånici√≥n 2.10

Una soluci√≥n p de f(x) 5 0 es un cero de multiplicidad m de f si para x = p, podemos
escribir f (x) = (x ‚àí p)m q(x), donde l√≠mx‚Üí p q(x) = 0.

3DUDSROLQRPLRVp es un cero de
multiplicidad m de f, si
f (x) = (x ‚àí p)m q(x), donde
q( p) 0.

En esencia, q(x) representa la porci√≥n de f(x) que no contribuye con el cero de f. El
VLJXLHQWHUHVXOWDGRSURSRUFLRQDORVPHGLRVSDUDLGHQWL√ÄFDUIiFLOPHQWHORVFHURVsimples de
una funci√≥n, aquellos que tienen multiplicidad uno.

62

CAP√çTULO 2

Soluciones de las ecuaciones en una variable

Teorema 2.11

La funci√≥n f ‚àà C1>a, b] tiene un cero simple en p en (a, b) si y s√≥lo si f(p) 5 0, pero f ( p) = 0.
Si f tiene un cero simple en p, entonces f ( p) = 0 y f (x) = (x ‚àí p)q(x),
donde l√≠mx‚Üí p q(x) = 0. Ya que f ‚àà C 1 [a, b],

Demostraci√≥n

f ( p) = l√≠m f (x) = l√≠m [q(x) + (x ‚àí p)q (x)] = l√≠m q(x) = 0.
x‚Üí p

x‚Üí p

x‚Üí p

3RU RWUD SDUWH VL f ( p) = 0 pero f ( p) = 0, represente a f como polinomio de Taylor de
grado cero alrededor de p. Entonces

f (x) = f ( p) + f (Œæ(x))(x ‚àí p) = (x ‚àí p) f (Œæ(x)),
donde j(x) est√° entre x y p. Ya que f ‚ààC1>a, b].

l√≠m f (Œæ(x)) = f

x‚Üí p

l√≠m Œæ(x)

x‚Üí p

= f ( p) = 0.

Haciendo q 5 f 9 o j obtenemos f (x) 5 (x 2 p) q (x), donde l√≠m x‚Üí p q(x) = 0. 3RUORWDQWR
f tiene un cero simple en p.
La siguiente generalizaci√≥n del teorema 2.11 se considera en el ejercicio 12.
Teorema 2.12

La funci√≥n f ‚àà Cm>a, b] tiene un cero de multiplicidad m en p en (a, b) si y s√≥lo si

0 = f ( p) = f ( p) = f ( p) = ¬∑ ¬∑ ¬∑ = f (m‚àí1) ( p),

pero f (m) ( p) = 0.

El resultado en el teorema 2.12 implica que existe un intervalo alrededor de p donde el
m√©todo de Newton converge cuadr√°ticamente a p para cualquier aproximaci√≥n inicial p0 5 p,
siempre que p sea un cero simple. El siguiente ejemplo muestra que la convergencia cuadr√°tica podr√≠a no presentarse si el cero no es simple.
Ejemplo 1

Sea f (x) 5 ex 2 x 2 1. a) Muestre que f tiene un cero de multiplicidad 2 en x 5 0. b) Muestre
que el m√©todo de Newton con p0 5 1 converge para este cero, pero no de manera cuadr√°tica.
Soluci√≥n

a) Tenemos

f (x) = e x ‚àí x ‚àí 1,

f (x) = e x ‚àí 1,

y

f (x) = e x ,

f (0) = e0 ‚àí 1 = 0,

y

f (0) = e0 = 1.

por lo que
f (0) = e0 ‚àí 0 ‚àí 1 = 0,

El teorema 2.12 implica que f tiene un cero de multiplicidad 2 en x 5 0.
b) Los primeros dos t√©rminos generados por el m√©todo de Newton aplicado a f con p0 5 1
son

p1 = p0 ‚àí

f ( p0 )
e‚àí2
=1‚àí
‚âà 0.58198
f ( p0 )
e‚àí1

p2 = p 1 ‚àí

f ( p1 )
0.20760
‚âà 0.58198 ‚àí
‚âà 0.31906.
f ( p1 )
0.78957

y

Los primeros ocho t√©rminos de la sucesi√≥n generada por el m√©todo de Newton se muestran
en la tabla 2.8. La sucesi√≥n es claramente convergente a 0, pero no cuadr√°ticamente. La gr√°√ÄFDGHfVHPXHVWUDHQOD√ÄJXUD

2.4

63

An√°lisis de error para m√©todos iterativos

Figura 2.11
f (x)

Tabla 2.8
1

n

pn

0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

1.0
0.58198
0.31906
0.16800
0.08635
0.04380
0.02206
0.01107
0.005545
2.7750 √ó 10‚àí3
1.3881 √ó 10‚àí3
6.9411 √ó 10‚àí4
3.4703 √ó 10‚àí4
1.7416 √ó 10‚àí4
8.8041 √ó 10‚àí5
4.2610 √ó 10‚àí5
1.9142 √ó 10‚àí6

(1, e 2 2)

e22
(21, e21)

e21
f (x) 5 e x 2 x 2 1

21

1

x

Un m√©todo para manejar el problema de ra√≠ces m√∫ltiples de una funci√≥n fHVGH√ÄQLU

Œº(x) =

f (x)
.
f (x)

Si p es un cero de f de multiplicidad m con f (x) 5 (x 2 p) m q(x), entonces

Œº(x) =

(x ‚àí p)m q(x)
m(x ‚àí p)m‚àí1 q(x) + (x ‚àí p)m q (x)

= (x ‚àí p)

q(x)
mq(x) + (x ‚àí p)q (x)

tambi√©n tiene un cero en p. Sin embargo, q( p) = 0, por lo que

q( p)
1
=
= 0,
mq( p) + ( p ‚àí p)q ( p)
m
y p es un cero simple de m(x). Entonces, el m√©todo de Newton se puede aplicar a m(x) para
proporcionar

g(x) = x ‚àí

Œº(x)
f (x)/ f (x)
=x‚àí
,
2
Œº (x)
{[ f (x)] ‚àí [ f (x)][ f (x)]}/[ f (x)]2

/RFXDOVHVLPSOL√ÄFDHQ

g(x) = x ‚àí

f (x) f (x)
.
[ f (x)]2 ‚àí f (x) f (x)

(2.13)

Si g tiene las condiciones de continuidad requeridas, la iteraci√≥n funcional aplicada a
g ser√° convergente cuadr√°ticamente, sin importar la multiplicidad del cero de f. En teor√≠a
el √∫nico inconveniente de este m√©todo es el c√°lculo adicional de f 0(x) y el procedimiento
m√°s laborioso que consiste en calcular las iteraciones. Sin embargo, en la pr√°ctica las ra√≠ces
m√∫ltiples pueden causar graves problemas de redondeo debido a que el denominador de la
ecuaci√≥n (2.13) consiste en la diferencia de dos n√∫meros, ambos cerca de 0.
Ejemplo 2

En el ejemplo 1 se mostr√≥ que f(x) 5 ex 2 x 2 1 tiene un cero de multiplicidad 2 en x 5 0 y
que el m√©todo de Newton con p0 5 1 converge en este cero, pero no de manera cuadr√°tica.
0XHVWUHTXHODPRGL√ÄFDFLyQGHOPpWRGRGH1HZWRQFRPRVHLQGLFDHQODHFXDFLyQ  
mejora la tasa de convergencia.

64

CAP√çTULO 2

Soluciones de las ecuaciones en una variable
Soluci√≥n (OPpWRGRPRGL√ÄFDGRGH1HZWRQGD

Tabla 2.9
n

pn

1
2
3
4
5

‚àí2.3421061 √ó 10‚àí1
‚àí8.4582788 √ó 10‚àí3
‚àí1.1889524 √ó 10‚àí5
‚àí6.8638230 √ó 10‚àí6
‚àí2.8085217 √ó 10‚àí7

p 1 = p0 ‚àí

f ( p0 ) f ( p 0 )
(e ‚àí 2)(e ‚àí 1)
=1‚àí
‚âà ‚àí2.3421061 √ó 10‚àí1 .
f ( p0 )2 ‚àí f ( p0 ) f ( p0 )
(e ‚àí 1)2 ‚àí( e ‚àí 2)e

Esto est√° considerablemente m√°s cerca de 0 que el primer t√©rmino al usar el m√©todo de Newton,
que era 0.58918. La tabla 2.9 enumera las primeras cinco aproximaciones para el doble cero
en x 5 0. Los resultados se obtuvieron a partir de un sistema con 10 d√≠gitos de precisi√≥n. La
falta relativa de mejora en las √∫ltimas dos entradas se debe al hecho de que al usar este sistePDWDQWRHOQXPHUDGRUFRPRHOGHQRPLQDGRUVHDSUR[LPDQD3RUFRQVLJXLHQWHH[LVWHXQD
SpUGLGDGHGtJLWRVVLJQL√ÄFDWLYRVGHSUHFLVLyQFRQIRUPHODVDSUR[LPDFLRQHVVHDFHUFDQD
/R VLJXLHQWH LOXVWUD TXH HO PpWRGR PRGL√ÄFDGR GH 1HZWRQ FRQYHUJH FXDGUiWLFDPHQWH
incluso en el caso de un cero simple.

Ilustraci√≥n

En la secci√≥n 2.2, encontramos que un cero de f(x) 5 x3 1 4x2 2 10 5 0 es p 5 1. 36523001.
Aqu√≠ compararemos la convergencia de un cero simple usando tanto el m√©todo de Newton
FRPRHOPpWRGRPRGL√ÄFDGRGH1HZWRQGDGRHQODHFXDFLyQ  6HD

i)

pn = pn‚àí1 ‚àí

3
2
+ 4 pn‚àí1
‚àí 10
pn‚àí1
,
2
3 pn‚àí1 + 8 pn‚àí1

a partir del m√©todo de Newton,

\DSDUWLUGHOPpWRGRPRGL√ÄFDGRGH1HZWRQSURYLVWRSRUODHFXDFLyQ  

ii)

pn = pn‚àí1 ‚àí

3
2
2
+ 4 pn‚àí1
‚àí 10)(3 pn‚àí1
+ 8 pn‚àí1 )
( pn‚àí1
.
2
3
2
2
(3 pn‚àí1 + 8 pn‚àí1 ) ‚àí ( pn‚àí1 + 4 pn‚àí1 ‚àí 10)(6 pn‚àí1 + 8)

Con p0 5 1.5, tenemos
M√©todo de Newton

p1 = 1.37333333,

p2 = 1.36526201,

y

p3 = 1.36523001.

p2 = 1.36519585,

y

p3 = 1.36523001.

0pWRGRPRGL√ÄFDGRGH1HZWRQ

p1 = 1.35689898,

Ambos m√©todos son r√°pidamente convergentes al cero real, el cual es dado por ambos m√©todos como p3. Sin embargo, observe que, en el caso de un cero simple, el m√©todo original de
Newton requiere considerablemente menos c√°lculos.
La secci√≥n Conjunto de ejercicios 2.4 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

2.5 Convergencia acelerada
El teorema 2.8 indica que es raro tener el lujo de la convergencia cuadr√°tica. Ahora consideramos una t√©cnica llamada m√©todo D2 de Aitken, que se puede utilizar para acelerar la
convergencia de una sucesi√≥n que es linealmente convergente, independientemente de su
origen o aplicaci√≥n.

M√©todo D2 de Aitken
Suponga { pn }‚àû
n=0 es una sucesi√≥n linealmente convergente con l√≠mite p 3DUD PRWLYDU OD
‚àû
construcci√≥n de una sucesi√≥n { pÃÇn }‚àû
n=0 que converge m√°s r√°pidamente a p que { pn }n=0

2.5
Alexander Aitken (1895‚Äì1967)
us√≥ esta t√©cnica en 1926 para
acelerar la tasa de convergencia
de una serie en un art√≠culo sobre
HFXDFLRQHVDOJHEUDLFDV>$L@
Este proceso es similar al que
us√≥ mucho antes el matem√°tico
japon√©s Takakazu Seki Kowa
(1642‚Äì1708).

65

Convergencia acelerada

suponga que los signos de pn ‚àí p, pn+1 ‚àí p, y pn+2 ‚àí p concuerdan y que nHVVX√ÄFLHQWHmente grande para que

pn+1 ‚àí p
pn+2 ‚àí p
‚âà
.
pn ‚àí p
pn+1 ‚àí p
Entonces
( pn+1 ‚àí p)2 ‚âà ( pn+2 ‚àí p)( pn ‚àí p),
por lo que
2
pn+1
‚àí 2 pn+1 p + p 2 ‚âà pn+2 pn ‚àí ( pn + pn+2 ) p + p 2

y
2
( pn+2 + pn ‚àí 2 pn+1 ) p ‚âà pn+2 pn ‚àí pn+1
.

Al resolver p obtenemos
p‚âà

2
pn+2 pn ‚àí pn+1
.
pn+2 ‚àí 2 pn+1 + pn

Al sumar y restar los t√©rminos pn2 y 2 pn pn+1 en el numerador y agrupar los t√©rminos adecuadamente obtenemos

p‚âà

2
pn pn+2 ‚àí 2 pn pn+1 + pn2 ‚àí pn+1
+ 2 pn pn+1 ‚àí pn2
pn+2 ‚àí 2 pn+1 + pn

=

2
‚àí 2 pn pn+1 + pn2 )
pn ( pn+2 ‚àí 2 pn+1 + pn ) ‚àí ( pn+1
pn+2 ‚àí 2 pn+1 + pn

= pn ‚àí

( pn+1 ‚àí pn )2
.
pn+2 ‚àí 2 pn+1 + pn

El m√©todo D2 de Aitken VHEDVDHQODVXSRVLFLyQGHTXHODVXFHVLyQGH√ÄQLGDSRU{ pÃÇn }‚àû
n=0.

pÃÇn = pn ‚àí

( pn+1 ‚àí pn )2
,
pn+2 ‚àí 2 pn+1 + pn

(2.14)

‚àû

converge m√°s r√°pidamente a p que la sucesi√≥n original { pn }n=0 .
Ejemplo 1

Tabla 2.10
n

pn

pÃÇn

1
2
3
4
5
6
7

0.54030
0.87758
0.94496
0.96891
0.98007
0.98614
0.98981

0.96178
0.98213
0.98979
0.99342
0.99541

DeÔ¨Ånici√≥n 2.13

La sucesi√≥n { pn }‚àû
n=1 , donde pn = cos(1/n), converge linealmente a p 5 1. Determine los
primeros cinco t√©rminos de la sucesi√≥n provista por el m√©todo 2 de Aitken.
Soluci√≥n &RQHO√ÄQGHGHWHUPLQDUHOWpUPLQR pÃÇn de la sucesi√≥n con el m√©todo

2

de Aitken,
necesitamos tener los t√©rminos pn , pn+1 , y pn+2GHODVXFHVLyQRULJLQDO3RUORWDQWRSDUD
determinar pÃÇ5, necesitamos los primeros siete t√©rminos de {pn}. Estos se muestran en la tabla
‚àû
2.10. Ciertamente, parece que { pÃÇn }‚àû
n=1 converge m√°s r√°pido a p 5 1 que { pn }n=1 .
La notaci√≥n

UHODFLRQDGDFRQHVWDWpFQLFDWLHQHVXRULJHQHQODVLJXLHQWHGH√ÄQLFLyQ

3DUDXQDVXFHVLyQ{ pn }‚àû
n=0 determinada, la diferencia hacia adelante
pn¬µ VHGH√ÄQHPHGLDQWH

pn = pn+1 ‚àí pn ,

para n ‚â• 0.

pn TXHVHOHH¬¥GHOWD

66

CAP√çTULO 2

Soluciones de las ecuaciones en una variable

Las potencias superiores del operador
k

VHGH√ÄQHQGHPDQHUDUHFXUVLYDFRQ
k‚àí1

pn =

pn ),

para k ‚â• 2.

/DGH√ÄQLFLyQLPSOLFDTXH
2

pn =

pn+1 ‚àí pn ) =

pn+1 ‚àí

pn = ( pn+2 ‚àí pn+1 ) ‚àí ( pn+1 ‚àí pn ).

3RU OR TXH 2 pn = pn+2 ‚àí 2 pn+1 + pn , y la f√≥rmula para pÃÇn determinada en la ecuaci√≥n
(2.14) se puede escribir como

pn )2
,
2p
n

pÃÇn = pn ‚àí

para n ‚â• 0.

(2.15)

En este punto de nuestro an√°lisis del m√©todo 2 de Aitken, hemos establecido que la
‚àû
sucesi√≥n { pÃÇn }‚àû
n=0 converge a p m√°s r√°pidamente que la sucesi√≥n original { pn }n=0, pero no
KHPRVGLFKRORTXHVLJQL√ÄFDHOWpUPLQRFRQYHUJHQFLD¬¥PiVUiSLGD¬µ(OWHRUHPDH[SOLFD
\MXVWL√ÄFDHVWDWHUPLQRORJtD/DSUXHEDGHHVWHWHRUHPDVHFRQVLGHUDHQHOHMHUFLFLR
Teorema 2.14

Suponga que { pn }‚àû
n=0, es una sucesi√≥n que converge linealmente en el l√≠mite p y que

l√≠m

n‚Üí‚àû

pn+1 ‚àí p
< 1.
pn ‚àí p

‚àû
(QWRQFHVODVHFXHQFLD∆ã2 de Aitken { pÃÇn }‚àû
n=0 converge a p m√°s r√°pido que { pn }n=0 en el sentido en que

pÃÇn ‚àí p
= 0.
n‚Üí‚àû pn ‚àí p
l√≠m

M√©todo de Steffensen
Johan Frederik Steffensen (1873‚Äì
1961) escribi√≥ un prestigioso
libro titulado Interpolation en
1927.

$ODSOLFDUXQDPRGL√ÄFDFLyQGHO m√©todo 2 de Aitken a una sucesi√≥n linealmente convergenWHREWHQLGDDSDUWLUGHODLWHUDFLyQGHSXQWR√ÄMRSRGHPRVDFHOHUDUODFRQYHUJHQFLDDFXDGUiWLFD(VWHSURFHGLPLHQWRUHFLEHHOQRPEUHGHPpWRGRGH6WHIIHQVHQ\GL√ÄHUHOLJHUDPHQWHGH
la aplicaci√≥n del m√©todo 2 de Aitken directamente para la sucesi√≥n de iteraci√≥n de punto
√ÄMROLQHDOPHQWHFRQYHUJHQWH(OPpWRGR 2 de Aitken construye los t√©rminos en el orden:

p0 ,

p1 = g( p0 ),

p3 = g( p2 ),

p2 = g( p1 ),

pÃÇ1 = {

2

pÃÇ0 = {

2

}( p0 ),

}( p1 ), . . . ,

donde { 2} indica que se usa la ecuaci√≥n (2.15). El m√©todo de Steffensen construye los
primeros cuatro t√©rminos p0, p1, p2 y pÃÇ0. Sin embargo, en este paso suponemos que pÃÇ0 es una
mejor aproximaci√≥n a p que es p2\DSOLFDPRVODLWHUDFLyQGHSXQWR√ÄMRD pÃÇ0 en lugar de a p2.
Con esta notaci√≥n, la sucesi√≥n generada es

p0(0) ,

p1(0) = g( p0(0) ),

p2(0) = g( p1(0) ),

p0(1) = {

2

}( p0(0) ),

p1(1) = g( p0(1) ), . . . .

Cada tercer t√©rmino de la sucesi√≥n de Steffensen se genera con la ecuaci√≥n (2.15); los
otros usan la iteraci√≥n de punto fijo en el t√©rmino previo. El proceso se describe en el
algoritmo 2.6.

2.5

ALGORITMO

2.6

Convergencia acelerada

67

M√©todo de Steffensen
3DUDHQFRQWUDUXQDVROXFLyQSDUDp 5 g(p) dada una aproximaci√≥n p0:

aproximaci√≥n inicial p0 tolerancia TOL; n√∫mero m√°ximo de
iteraciones N0 .

ENTRADA
SALIDA

aproxime la soluci√≥n p o mensaje de falla.

Paso 1 Determine i = 1.
Paso 2 Mientras i ‚â§ N0 haga los pasos 3‚Äì6.
Paso 3 Determine p1 = g( p0 );
p2 = g( p1 );

(Calcule p 1(i‚àí1) .)
(Calcule p 2(i‚àí1) .)

p = p0 ‚àí ( p1 ‚àí p0 )2 /( p2 ‚àí 2 p1 + p0 ). (Calcule p 0(i) .)
Paso 4 Si | p ‚àí p0 | < TOL entonces
SALIDA (p); (Procedimiento completado exitosamente.)
PARE.
Paso 5 Determine i = i + 1.
Paso 6 Determine p0 = p. (Actualice p0 .)
Paso 7 SALIDA (‚ÄòEl m√©todo fall√≥ despu√©s de N0 iteraciones, N0 =‚Äô, N0 );
(Procedimiento no completado exitosamente.)
PARE.
Observe que 2 pn podr√≠a ser 0, que introducir√° un 0 en el denominador de la siguiente
(n‚àí1)
iteraci√≥n. Si esto pasa, terminamos la sucesi√≥n y seleccionamos p2
como la mejor aproximaci√≥n.
Ilustraci√≥n

3DUDUHVROYHUx3 1 4x2 2 10 5 0 con el m√©todo de Steffensen, si x3 1 4x2 5 10, divida entre
x 1 4 y resuelva para x(VWHSURFHGLPLHQWRFUHDHOPpWRGRGHSXQWR√ÄMR

g(x) =

10
x +4

1/2

.

&RQVLGHUDPRVHOPpWRGRGHSXQWR√ÄMRHQODWDEODFROXPQDd) de la secci√≥n 2.2.
Al aplicar el procedimiento de Steffensen con p0 5 1.5 obtenemos los valores en la ta(2)
bla 2.11. La iteraci√≥n p0 = 1.365230013 es exacta hasta el noveno lugar decimal. En este
ejemplo, el m√©todo de Steffensen ofrece casi la misma precisi√≥n que el m√©todo de Newton
DSOLFDGRDHVWHSROLQRPLR(VWRVUHVXOWDGRVVHSXHGHQREVHUYDUHQODLOXVWUDFLyQDO√ÄQDOGH
la secci√≥n 2.4.

Tabla 2.11

p0(k)

k
0

p0(0)

1

p0(1) = p0(0) ‚àí

2

p0(2) = p0(1) ‚àí

( p1(0) ‚àí p0(0) )2
(0)
p2 ‚àí 2 p1(0) + p0(0)

p1(k)

p2(k)

p1(0) = g( p0(0) )

p2(0) = g( p1(0) )

p1(1) = g( p0(1) )

p2(1) = g( p1(1) )

1.348399725
1.365225534

1.367376372
1.365230583

( p1(1) ‚àí p0(1) )2
p2(1) ‚àí 2 p2(1) + p0(1)

Lo cual produce la siguiente tabla
0
1
2

1.5
1.365265224
1.365230013

68

CAP√çTULO 2

Soluciones de las ecuaciones en una variable

A partir de la ilustraci√≥n, parece que el m√©todo de Steffensen provee convergencia cuadr√°tica sin evaluar una derivada y el teorema 2.14 establece que √©ste es el caso. La demostraFLyQGHHVWHWHRUHPDVHSXHGHHQFRQWUDUHQ>+H@SS¬≤R>,.@SS¬≤
Teorema 2.15

Suponga que x 5 g(x) tiene la soluci√≥n p con g ( p) = 1. Si existe una d > 0 tal que g ‚àà C 3
>p 2 d, p 1 d], entonces el m√©todo de Steffensen proporciona convergencia cuadr√°tica para
cualquier p0 ‚àà>p 2 d, p 1 d].
La secci√≥n Conjunto de ejercicios 2.5 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

2.6 Ceros de polinomios y m√©todo de M√ºller
Un polinomio de grado n tiene la forma

P(x) = an x n + an‚àí1 x n‚àí1 + ¬∑ ¬∑ ¬∑ + a1 x + a0 ,
donde las ai, llamadas FRH√ÄFLHQWHV de P, son constantes y an = 0. La funci√≥n cero P(x) 5 0
para todos los valores de x se considera un polinomio, pero no tiene un grado asignado.

Polinomios algebraicos
Teorema 2.16

(Teorema fundamental de √°lgebra)
Si P(x) es un polinomio de grado n ‚â•FRQFRH√ÄFLHQWHVUHDOHVRFRPSOHMRVHQWRQFHVP(x)
5 0 tiene por lo menos una ra√≠z (posiblemente compleja).
A pesar de que el teorema fundamental de √°lgebra es b√°sico para cualquier estudio de
las funciones elementales, la prueba usual requiere t√©cnicas a partir del estudio de la teor√≠a
GHIXQFLyQFRPSOHMD(OOHFWRUSXHGHFRQVXOWDU>6D6@SSDUDODFXOPLQDFLyQGHXQGHsarrollo sistem√°tico de los temas necesarios para mejorar este teorema.

Ejemplo 1

Determine todos los ceros del polinomio P(x) 5 x3 2 5x2 1 17x 2 13.
Soluci√≥n (VPX\IiFLOYHUL√ÄFDUTXHP(1) 5 1 2 5 1 17 2 13 5 0, por lo que x 5 1 es un
cero de P y (x 2 1) es un factor del polinomio. Al dividir P(x) entre x 2 1 obtenemos

P(x) = (x ‚àí 1)(x 2 ‚àí 4x + 13).
Carl Friedrich Gauss (1777‚Äì
1855), uno de los m√°s grandes
matem√°ticos de todos los
tiempos, demostr√≥ el teorema
fundamental de √°lgebra en
su tesis doctoral y lo public√≥
HQ3XEOLFyGLIHUHQWHV
demostraciones de este resultado
a lo largo de su vida, en 1815,
1816 y hasta en 1848. El
resultado fue establecido, sin
demostraci√≥n, por Albert Girard
(1595‚Äì1632), y Jean d‚ÄôAlembert
(1717‚Äì1783), Euler y Lagrange
aportaron pruebas parciales.

3DUDGHWHUPLQDUORVFHURVGHx2 2 4x 1 13, usamos la f√≥rmula cuadr√°tica en su forma est√°ndar, la cual da los ceros complejos
‚àö
4 ¬± ‚àí36
‚àí(‚àí4) ¬± (‚àí4)2 ‚àí 4(1)(13)
=
= 2 ¬± 3i.
2(1)
2
3RUORWDQWRHOSROLQRPLRGHWHUFHUJUDGRP(x) tiene tres ceros, x1 5 1, x2 5 2 2 3i y x2 5
2 1 3i.
En el ejemplo anterior, encontramos que el polinomio de tercer grado tiene tres ceros
diferentes. Una consecuencia importante del teorema fundamental de √°lgebra es el siguiente
corolario. Establece que √©ste siempre es el caso con la condici√≥n de que cuando los ceros no
son distintos, contamos el n√∫mero de ceros de acuerdo con sus multiplicidades.

2.6

Corolario 2.17

Ceros de polinomios y m√©todo de M√ºller

69

Si P(x) es un polinomio de grado n ‚â•FRQFRH√ÄFLHQWHVUHDOHVRFRPSOHMRVHQWRQFHVH[LVWHQ
constantes √∫nicas x1, x2, 7, xk, posiblemente complejas y enteros positivos √∫nicos m1, m2,
k
7, mk, tal que i=1 m i = n y

P(x) = an (x ‚àí x1 )m 1 (x ‚àí x2 )m 2 ¬∑ ¬∑ ¬∑ (x ‚àí xk )m k .
Mediante el corolario 2.17, la colecci√≥n de ceros de un polinomio es √∫nica y, si cada
cero xi se cuenta tantas veces como su multiplicidad mi, un polinomio de grado n tiene exactamente n ceros.
El siguiente corolario del teorema fundamental de √°lgebra se usa con frecuencia en esta
secci√≥n y en cap√≠tulos posteriores.
Corolario 2.18

Sean P(x) y Q(x) polinomios de grado a lo m√°s n. Si x1, x2, 7, xk, con k > n, son n√∫meros distintos con P(xi) 5 Q(xi) para i 5 1, 2, 7, k, entonces P(x) 5 Q(x) para todos los valores de x.
Este resultado implica que para mostrar que dos polinomios de grado menor o igual que
n son iguales, s√≥lo necesitamos mostrar que concuerdan en n 1 1 valores. Esto se usar√° con
mucha frecuencia, especialmente en los cap√≠tulos 3 y 8.

William Horner (1786‚Äì1837) era
un ni√±o prodigio que se convirti√≥
en maestro de una escuela
en Bristol a los 18 a√±os. El
m√©todo de Horner para resolver
ecuaciones algebraicas se public√≥
en 1819 en las Philosophical
Transactions of the Royal Society
7UDQVDFFLRQHV√ÄORVy√ÄFDVGHOD
Real Sociedad).

Teorema 2.19

M√©todo de Horner
Al usar el m√©todo de Newton para localizar los ceros aproximados de un polinomio P(x),
necesitamos evaluar P(x) y P9(x HQYDORUHVHVSHFt√ÄFRV3XHVWRTXHWDQWRP(x) como P9(x)
VRQSROLQRPLRVODH√ÄFLHQFLDFRPSXWDFLRQDOUHTXLHUHTXHODHYDOXDFLyQGHHVWDVIXQFLRQHVVH
realice de la manera anidada que se analiza en la secci√≥n 1.2. El m√©todo de Horner incorpora
esta t√©cnica anidada y como consecuencia, s√≥lo requiere n multiplicaciones y n sumas para
evaluar un polinomio de en√©simo grado.
(M√©todo de Horner)
Sea

P(x) = an x n + an‚àí1 x n‚àí1 + ¬∑ ¬∑ ¬∑ + a1 x + a0 .
DeÔ¨Åna bn = an y
bk = ak + bk+1 x0 ,

para k = n ‚àí 1, n ‚àí 2, . . . , 1, 0.

Entonces b0 = P(x0 ). Adem√°s, si
Q(x) = bn x n‚àí1 + bn‚àí1 x n‚àí2 + ¬∑ ¬∑ ¬∑ + b2 x + b1 ,
Entonces
P(x) = (x ‚àí x0 )Q(x) + b0 .
3DROR5XI√ÄQL ¬≤ 
describi√≥ un m√©todo similar que
lo hizo merecedor de la medalla
de oro de la Italian Mathematical
Society for Science (Sociedad
Matem√°tica Italiana para la
&LHQFLD 1L5XI√ÄQLQL+RUQHU
fueron los primeros en descubrir
este m√©todo; ya se conoc√≠a en
China 500 a√±os antes.

Demostraci√≥n 3RUODGH√ÄQLFLyQGHQ(x),

(x ‚àí x0 )Q(x) + b0 = (x ‚àí x0 )(bn x n‚àí1 + ¬∑ ¬∑ ¬∑ + b2 x + b1 ) + b0
= (bn x n + bn‚àí1 x n‚àí1 + ¬∑ ¬∑ ¬∑ + b2 x 2 + b1 x)
‚àí (bn x0 x n‚àí1 + ¬∑ ¬∑ ¬∑ + b2 x0 x + b1 x0 ) + b0
= bn x n + (bn‚àí1 ‚àí bn x0 )x n‚àí1 + ¬∑ ¬∑ ¬∑ + (b1 ‚àí b2 x0 )x + (b0 ‚àí b1 x0 ).
Por la hip√≥tesis, bn = an y bk ‚àí bk+1 x0 = ak , por lo que
(x ‚àí x0 )Q(x) + b0 = P(x)

y

b0 = P(x0 ).

70

CAP√çTULO 2

Soluciones de las ecuaciones en una variable

Ejemplo 2

Use el m√©todo de Horner para evaluar P(x) 5 2x4 2 3x2 1 3x 2 4 en x0 5 22.
Soluci√≥n Cuando usamos el c√°lculo manual en el m√©todo de Horner, primero construimos
una tabla que sugiere el nombre de la GLYLVLyQVLQWpWLFD, que a menudo se aplica a esta t√©cnica.
3DUDHVWHSUREOHPDODWDEODDSDUHFHDFRQWLQXDFLyQ

x0 = ‚àí2

/DSDODEUD¬¥VLQWpWLFR¬µWLHQHVXV
ra√≠ces en diferentes lenguajes.
En ingl√©s est√°ndar, por lo general
provee un sentido de algo
TXHHV¬¥IDOVR¬µR¬¥VXVWLWXLGR¬µ
Sin embargo, en matem√°ticas
toma la forma de algo que
HVWi¬¥DJUXSDGR¬µ/DJHRPHWUtD
sint√©tica trata las formas como
un todo en lugar de c√≥mo objetos
individuales, que es como se
usa en geometr√≠a anal√≠tica. En la
divisi√≥n sint√©tica de polinomios,
las diferentes potencias de las
variables no se proporcionan
de modo expl√≠cito, pero se
mantienen juntas.

Coeficiente
de x 4
a4 = 2

Coeficiente
de x 3
a3 = 0
b4 x0 = ‚àí4

Coeficiente
de x 2
a2 = ‚àí3
b3 x 0 = 8

Coeficiente
de x
a1 = 3
b2 x0 = ‚àí10

T√©rmino
constante
a0 = ‚àí4
b1 x0 = 14

b4 = 2

b3 = ‚àí4

b2 = 5

b1 = ‚àí7

b0 = 10

Por lo que,
P(x) = (x + 2)(2x 3 ‚àí 4x 2 + 5x ‚àí 7) + 10.
Una ventaja adicional del uso del procedimiento de Horner (o de divisi√≥n sint√©tica) es
que, ya que

P(x) = (x ‚àí x0 )Q(x) + b0 ,
donde
Q(x) = bn x n‚àí1 + bn‚àí1 x n‚àí2 + ¬∑ ¬∑ ¬∑ + b2 x + b1 ,
al diferenciar respecto a x obtenemos
P (x) = Q(x) + (x ‚àí x0 )Q (x)

P (x0 ) = Q(x0 ).

y

(2.16)

Cuando el m√©todo de Newton-Raphson se usa para encontrar un cero aproximado de un
polinomio, P(x) y P9(x) se puede evaluar de la misma forma.
Ejemplo 3

Encuentre una aproximaci√≥n al cero de

P(x) = 2x 4 ‚àí 3x 2 + 3x ‚àí 4,
usando el m√©todo de Newton con x0 5 22 y la divisi√≥n sint√©tica para evaluar P(xn) y P9(xn)
para cada iteraci√≥n xn.
Soluci√≥n
mediante

Con x0 5 22 como aproximaci√≥n inicial, obtuvimos P(22) en el ejemplo 1

x0 = ‚àí2

2

0
‚àí4

‚àí3
8

3
‚àí10

‚àí4
14

2

‚àí4

5

‚àí7

10

= P(‚àí2).

Usando el teorema 2.19 y la ecuaci√≥n (2.16),
Q(x) = 2x 3 ‚àí 4x 2 + 5x ‚àí 7

y

P (‚àí2) = Q(‚àí2),

por lo que P (‚àí2) se puede encontrar al evaluar Q(‚àí2) en forma similar:
x0 = ‚àí2

2

‚àí4
‚àí4

5
16

‚àí7
‚àí42

2

‚àí8

21

‚àí49

= Q(‚àí2) = P (‚àí2)

2.6

Ceros de polinomios y m√©todo de M√ºller

71

y
10
P(x0 )
P(x0 )
= x0 ‚àí
= ‚àí2 ‚àí
‚âà ‚àí1.796.
P (x0 )
Q(x0 )
‚àí49

x1 = x0 ‚àí

Al repetir el procedimiento para encontrar x2 obtenemos
‚àí1.796

2

0
‚àí3.592

‚àí3
6.451

3
‚àí6.197

‚àí4
5.742

2

‚àí3.592
‚àí3.592

3.451
12.902

‚àí3.197
‚àí29.368

1.742

= P(x1 )

2

‚àí7.184

16.353

‚àí32.565

= Q(x1 )

= P (x1 ).

Por lo que, P(‚àí1.796) = 1.742, P (‚àí1.796) = Q(‚àí1.796) = ‚àí32.565, y
x2 = ‚àí1.796 ‚àí

1.742
‚âà ‚àí1.7425.
‚àí32.565

De manera similar, x3 5 21.73897, y un cero real con cinco cifras decimales es 21.73896.
Observe que el polinomio Q(x) depende de la aproximaci√≥n que se usa y cambia de una
iteraci√≥n a otra.
El algoritmo 2.7 calcula P(x0) y P9(x0) usando el m√©todo de Horner.

ALGORITMO

M√©todo de Horner

2.7

3DUDHYDOXDUHOSROLQRPLR

P(x) = an x n + an‚àí1 x n‚àí1 + ¬∑ ¬∑ ¬∑ + a1 x + a0 = (x ‚àí x0 )Q(x) + b0
Y su derivada en x0:

ENTRADA grado n; coeficientes a0 , a1 , . . . , an ; x0 .
SALIDA

y = P(x0 ); z = P (x0 ).

Paso 1 Determine y = an ; (Calcule b n para P.)
z = an . (Calcule b n‚àí1 para Q.)
Paso 2 Para j = n ‚àí 1, n ‚àí 2, . . . , 1
determine y = x0 y + a j ; (Calcule b j para P.)
z = x0 z + y. (Calcule b j‚àí1 para Q.)
Paso 3 Determine y = x0 y + a0 . (Calcule b0 para P.)
Paso 4 SALIDA (y, z);
PARE.
Si la en√©sima iteraci√≥n, xN, en el m√©todo de Newton es un cero aproximado para P,
entonces

P(x) = (x ‚àí x N )Q(x) + b0 = (x ‚àí x N )Q(x) + P(x N ) ‚âà (x ‚àí x N )Q(x).
3RUORTXHx 2 xN es un factor aproximado de P(x). Suponiendo que xÃÇ1 = x N sea el cero
aproximado de P y Q 1 (x) ‚â° Q(x) sea el factor aproximado obtenemos

P(x) ‚âà (x ‚àí xÃÇ1 )Q 1 (x).
3RGHPRVHQFRQWUDUXQVHJXQGRFHURDSUR[LPDGRGHP al aplicar el m√©todo de Newton para
Q1(x).

72

CAP√çTULO 2

Soluciones de las ecuaciones en una variable

Si P(x) es un polinomio de en√©simo grado con n ceros reales, este procedimiento aplicaGRUHSHWLGDPHQWHDO√ÄQDOUHVXOWDUiHQ n 2 2) ceros aproximados de P y un factor cuadr√°tico
aproximado Q n‚àí2 (x). En esta etapa, Q n‚àí2 (x) = 0 puede resolverse con la f√≥rmula cuadr√°tica para encontrar por lo menos dos ceros aproximados de P. A pesar de que este m√©todo se
puede usar para encontrar todos los ceros aproximados, depende del uso repetido de aproximaciones y puede conducir a resultados imprecisos.
El procedimiento que se ha descrito recientemente recibe el nombre de GH√ÅDFLyQ. La
GL√ÄFXOWDGGHODSUHFLVLyQFRQGH√ÅDFLyQVHGHEHDOKHFKRGHTXHFXDQGRREWHQHPRVORVFHURV
aproximados de P(x), el m√©todo de Newton se usa en el polinomio reducido Qk(x), es decir,
el polinomio que tiene la propiedad de que

P(x) ‚âà (x ‚àí xÃÇ1 )(x ‚àí xÃÇ2 ) ¬∑ ¬∑ ¬∑ (x ‚àí xÃÇk )Q k (x).

Un cero aproximado xÃÇk+1 de Q k por lo general no se aproximar√° a la ra√≠z de P(x) 5 0, como
lo hace una ra√≠z de la ecuaci√≥n reducida Qk(x) 5 0 y la imprecisi√≥n aumenta conforme lo
hace k8QDIRUPDGHHOLPLQDUHVWDGL√ÄFXOWDGHVXVDUODVHFXDFLRQHVUHGXFLGDVSDUDHQFRQWUDU
las aproximaciones xÃÇ2 , xÃÇ3 , . . . , xÃÇk para los ceros de P y, a continuaci√≥n, mejorar estas aproximaciones al aplicar el m√©todo de Newton al polinomio original P(x).

Ceros complejos: m√©todo de M√ºller
Un problema con la aplicaci√≥n de los m√©todos de la secante, de posici√≥n falsa o de Newton a
los polinomios, es la posibilidad de que el polinomio tenga ra√≠ces complejas incluso cuando
WRGRVORVFRH√ÄFLHQWHVVRQQ~PHURVUHDOHV6LODDSUR[LPDFLyQLQLFLDOHVXQQ~PHURUHDOWRGDV
las aproximaciones subsiguientes tambi√©n ser√°n n√∫meros reales. Una forma de superar esta
GL√ÄFXOWDGHVFRPHQ]DUFRQXQDDSUR[LPDFLyQLQLFLDOFRPSOHMD\UHDOL]DUWRGRVORVFiOFXORV
con aritm√©tica compleja. Un enfoque alterno tiene sus bases en el siguiente teorema.

Teorema 2.20

El m√©todo de M√ºller es similar
al m√©todo de la secante. Sin
embargo, mientras en el m√©todo
de la secante se usa una recta que
pasa por dos puntos en la curva
para aproximar la ra√≠z, en el
m√©todo de M√ºller se utiliza una
par√°bola a lo largo de tres puntos
en la curva para la aproximaci√≥n.

Si z 5 a 1 bi es un cero complejo de multiplicidad m del polinomio P(x FRQFRH√ÄFLHQWHV
reales, entonces z = a ‚àí bi tambi√©n es un cero de multiplicidad m del polinomio P(x) y
(x2 2 2ax 1 a2 1 b2)m es un factor de P(x).

Una divisi√≥n sint√©tica mediante polinomios cuadr√°ticos se puede concebir para factorizar aproximadamente el polinomio, de modo que un t√©rmino ser√° un polinomio cuadr√°tico
cuyas ra√≠ces complejas son aproximaciones a las ra√≠ces del polinomio original. Esta t√©cnica
VHGHVFULELyFRQFLHUWRGHWDOOHHQQXHVWUDVHJXQGDHGLFLyQ>%)5@(QOXJDUGHSURFHGHUFRQ
HVWDVOtQHDVDKRUDFRQVLGHUDUHPRVXQPpWRGRTXH'(0¬ÅOOHU>0X@SUHVHQWySULPHUR(VWD
t√©cnica se puede usar para cualquier problema de encontrar la ra√≠z, pero es especialmente √∫til
para aproximar las ra√≠ces de los polinomios.
El m√©todo de la secante comienza con dos aproximaciones iniciales p0 y p1 y determina la siguiente aproximaci√≥n p2, como la intersecci√≥n del eje x con la recta que pasa por
(p0, f(p0)) y (p1, f(p1  &RQVXOWH OD √ÄJXUD D   (Q HO PpWRGR GH 0¬ÅOOHU VH XVDQ WUHV
aproximaciones iniciales, p0, p1 y p2, y determina la siguiente aproximaci√≥n p3 al considerar
la intersecci√≥n del eje x con la par√°bola a trav√©s de (p0, f( p0)), (p1, f(p1)) y ( p2, f( p2)). (ConVXOWHOD√ÄJXUDE 

2.6

73

Ceros de polinomios y m√©todo de M√ºller

Figura 2.12
y

y

p0

p1

f

p2

x

p0

a)

p1

p2

p3

x
f

b)

La derivaci√≥n del m√©todo de M√ºller comienza al considerar el polinomio cuadr√°tico

P(x) = a(x ‚àí p2 )2 + b(x ‚àí p2 ) + c
que pasa a trav√©s de ( p0 , f ( p0 )), ( p1 , f ( p1 )), y ( p2 , f ( p2 )). Las constantes a, b y c se pueden determinar a partir de las condiciones

f ( p0 ) = a( p0 ‚àí p2 )2 + b( p0 ‚àí p2 ) + c,

(2.17)

f ( p1 ) = a( p1 ‚àí p2 )2 + b( p1 ‚àí p2 ) + c,

(2.18)

f ( p 2 ) = a ¬∑ 02 + b ¬∑ 0 + c = c

(2.19)

y

para ser
c = f ( p2 ),

(2.20)

b=

( p0 ‚àí p2 )2 [ f ( p1 ) ‚àí f ( p2 )] ‚àí ( p1 ‚àí p2 )2 [ f ( p0 ) ‚àí f ( p2 )]
,
( p0 ‚àí p2 )( p1 ‚àí p2 )( p0 ‚àí p1 )

(2.21)

a=

( p1 ‚àí p2 )[ f ( p0 ) ‚àí f ( p2 )] ‚àí ( p0 ‚àí p2 )[ f ( p1 ) ‚àí f ( p2 )]
.
( p0 ‚àí p2 )( p1 ‚àí p2 )( p0 ‚àí p1 )

(2.22)

y

3DUDGHWHUPLQDUp3, un cero de P, aplicamos la f√≥rmula cuadr√°tica para P(x) 5 0. Sin embargo, debido a los problemas de error de redondeo causados por la resta de los n√∫meros
casi iguales, aplicamos la f√≥rmula en la manera prescrita en las ecuaciones (1.2) y (1.3) de
la secci√≥n 1.2:

p3 ‚àí p2 =

‚àí2c
‚àö
.
b ¬± b2 ‚àí 4ac

Esta f√≥rmula proporciona dos posibilidades para p3, dependiendo del signo que precede al
t√©rmino radical. En el m√©todo de M√ºller, el signo se selecciona de acuerdo con el signo de b.

74

CAP√çTULO 2

Soluciones de las ecuaciones en una variable

Elegido de esta forma, el denominador ser√° el m√°s grande en magnitud y resultar√° en p3
siendo seleccionado como el cero m√°s cercano de P para p23RUORWDQWR

p 3 = p2 ‚àí

2c
‚àö
,
b + sgn(b) b2 ‚àí 4ac

donde a, b y c se obtienen de las ecuaciones (2.20) a (2.22).
Una vez que se determina p3, el procedimiento se reinicializa usando p1, p2 y p3 en lugar de p0, p1 y p2 para determinar la siguiente aproximaci√≥n, p4. El m√©todo contin√∫a
hasta
‚àö
obtener una conclusi√≥n satisfactoria. En cada paso, el m√©todo implica el radical b2 ‚àí 4ac,
por lo que el m√©todo proporciona ra√≠ces complejas aproximadas cuando b2 2 4ac < 0.
El algoritmo 2.8 implementa este procedimiento.

ALGORITMO

2.8

M√©todo de M√ºller
3DUDHQFRQWUDUXQDVROXFLyQSDUDf(x) 5 0 dadas las tres aproximaciones p0, p1 y p2:

ENTRADA p0 , p1 , p2 tolerancia TOL; n√∫mero m√°ximo de iteraciones N0 .
SALIDA soluci√≥n aproximada p o mensaje de falla.
Paso 1 Determine h 1 = p1 ‚àí p0 ;
h 2 = p2 ‚àí p 1 ;
Œ¥1 = ( f ( p1 ) ‚àí f ( p0 ))/ h 1 ;
Œ¥2 = ( f ( p2 ) ‚àí f ( p1 ))/ h 2 ;
d = (Œ¥2 ‚àí Œ¥1 )/(h 2 + h 1 );
i = 3.
Paso 2 Mientras i ‚â§ N0 haga los pasos 3‚Äì7.
Paso 3 b = Œ¥2 + h 2 d;
D = (b2 ‚àí 4 f ( p2 )d)1/2 . (Nota: puede requerir aritm√©tica compleja. )
Paso 4 Si |b ‚àí D| < |b + D| entonces determine E = b + D
tambi√©n determine E = b ‚àí D.
Paso 5 Determine h = ‚àí2 f ( p2 )/E;
p = p2 + h.
Paso 6 Si |h| < TOL entonces
SALIDA (p); (El procedimiento fue exitoso.)
PARE.
Paso 7 Determine p0 = p1 ; (Prepare la siguiente iteraci√≥n.)
p 1 = p2 ;
p2 = p;
h 1 = p1 ‚àí p 0 ;
h 2 = p2 ‚àí p 1 ;
Œ¥1 = ( f ( p1 ) ‚àí f ( p0 ))/ h 1 ;
Œ¥2 = ( f ( p2 ) ‚àí f ( p1 ))/ h 2 ;
d = (Œ¥2 ‚àí Œ¥1 )/(h 2 + h 1 );
i = i + 1.
Paso 8 SALIDA (‚ÄòEl m√©todo fall√≥ despu√©s de N0 iteraciones, N0 =‚Äô, N0 );
(El procedimiento no fue exitoso.)
PARE.

2.6

Ilustraci√≥n

Ceros de polinomios y m√©todo de M√ºller

75

Considere el polinomio f(x) 5 x4 2 3x3 1 x2 1 x 1FX\DJUi√ÄFDVHPXHVWUDHQOD√ÄJXUD
2.13.

Figura 2.13

y
3
2

2

y 5 x4 2 3x3 1 x 1 x 1 1

1

21

1

2

3

x

21

Tres conjuntos de tres puntos iniciales se usar√°n con el algoritmo 2.8 y TOL 5 1025 para
aproximar los ceros de f. El primer conjunto usar√° p0 5 0.5, p1 5 20.5 y p2 5 0. La par√°bola
que pasa a trav√©s de estos puntos tiene ra√≠ces complejas porque no interseca el eje x. La tabla
2.12 provee aproximaciones para los ceros complejos correspondientes de f.

Tabla 2.12

p0 = 0.5, p1 = ‚àí0.5, p2 = 0
pi
f ( pi )

i
3
4
5
6
7
8
9

‚àí0.100000 + 0.888819i
‚àí0.492146 + 0.447031i
‚àí0.352226 + 0.484132i
‚àí0.340229 + 0.443036i
‚àí0.339095 + 0.446656i
‚àí0.339093 + 0.446630i
‚àí0.339093 + 0.446630i

‚àí0.01120000 + 3.014875548i
‚àí0.1691201 ‚àí 0.7367331502i
‚àí0.1786004 + 0.0181872213i
0.01197670 ‚àí 0.0105562185i
‚àí0.0010550 + 0.000387261i
0.000000 + 0.000000i
0.000000 + 0.000000i

La tabla 2.13 nos da aproximaciones para los dos ceros reales de f. El m√°s peque√±o de
√©stos usa p0 5 0.5, p1 5 1.0 y p2 5 1.5, y la ra√≠z m√°s grande se aproxima cuando p0 5 1.5,
p1 5 2.0 y p2 5 2.5.

Tabla 2.13

p0 = 0.5, p1 = 1.0,
i
pi
3
4
5
6

1.40637
1.38878
1.38939
1.38939

p2 = 1.5
f ( pi )
‚àí0.04851
0.00174
0.00000
0.00000

p0 = 1.5, p1 = 2.0,
i
pi
3
4
5
6
7

2.24733
2.28652
2.28878
2.28880
2.28879

p2 = 2.5
f ( pi )
‚àí0.24507
‚àí0.01446
‚àí0.00012
0.00000
0.00000

Los valores en las tablas son aproximaciones precisas para los lugares enumerados.

76

CAP√çTULO 2

Soluciones de las ecuaciones en una variable

La ilustraci√≥n muestra que el m√©todo de M√ºller puede aproximar las ra√≠ces de los polinomios con una variedad de valores iniciales. De hecho, el m√©todo de M√ºller por lo general
converge con la ra√≠z de un polinomio para cualquier selecci√≥n de aproximaci√≥n inicial, a
pesar de que se pueden construir problemas para los cuales la convergencia no se presentaUi3RUHMHPSORVXSRQJDTXHSDUDDOJXQDVi tenemos f ( pi ) = f ( pi+1 ) = f ( pi+2 ) = 0.
Entonces, la ecuaci√≥n cuadr√°tica se reduce a una funci√≥n constante diferente de cero y nunca
interseca el eje x. Sin embargo, esto no es normalmente el caso, y los paquetes de software
de prop√≥sito general que usan el m√©todo de M√ºller s√≥lo requieren una aproximaci√≥n inicial
por ra√≠z e incluso proporcionar√°n esta aproximaci√≥n como una opci√≥n.
La secci√≥n Conjunto de ejercicios 2.6 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

2.7 Software num√©rico y revisi√≥n del cap√≠tulo
'DGDXQDIXQFLyQHVSHFt√ÄFDf\XQDWROHUDQFLDXQSURJUDPDH√ÄFLHQWHGHEHUtDSURGXFLUXQD
aproximaci√≥n para una o m√°s soluciones de f(x) 5 0, cada una tiene un error absoluto o
relativo dentro de la tolerancia y los resultados deber√≠an generarse en una cantidad razonable
de tiempo. Si el programa no puede realizar esta tarea, por lo menos deber√≠a proporcionar
explicaciones l√≥gicas de porqu√© no se consigui√≥ el √©xito y una indicaci√≥n sobre c√≥mo remediar la causa de la falla.
,06/ WLHQH VXEUXWLQDV TXH LPSOHPHQWDQ HO PpWRGR GH 0¬ÅOOHU FRQ GH√ÅDFLyQ (Q HVWH
SDTXHWHWDPELpQVHLQFOX\HXQDUXWLQDGHELGDD53%UHQWHQODTXHVHXVDXQDFRPELQDFLyQ
de interpolaci√≥n lineal, una interpolaci√≥n cuadr√°tica inversa similar al m√©todo de M√ºller y el
m√©todo de bisecci√≥n. El m√©todo de Laguerre tambi√©n se usa para encontrar los ceros de un
polinomio real. Otra rutina para encontrar los ceros de los polinomios reales usa el m√©todo
de Jenkins‚ÄìTraub, que tambi√©n sirve para encontrar los ceros de un polinomio complejo.
La biblioteca NAG tiene una subrutina que usa una combinaci√≥n del m√©todo de bisecci√≥n, la interpolaci√≥n lineal y la extrapolaci√≥n para aproximar un cero real de una funci√≥n en
un intervalo determinado. NAG tambi√©n provee subrutinas para aproximar todos los ceros
de un polinomio real o complejo, respectivamente. Ambas subrutinas usan un m√©todo de
/DJXHUUHPRGL√ÄFDGR
La biblioteca netlib contiene una subrutina que usa una combinaci√≥n de los m√©todos
de bisecci√≥n y de secante desarrollada por T. J. Dekker para aproximar un cero real de la
IXQFLyQHQHOLQWHUYDOR5HTXLHUHHVSHFL√ÄFDUXQLQWHUYDORTXHFRQWLHQHXQDUDt]\UHJUHVDXQ
LQWHUYDORFRQXQDQFKRTXHVHHQFXHQWUDGHQWURGHXQDWROHUDQFLDHVSHFt√ÄFD2WUDVXEUXWLQD
usa una combinaci√≥n del m√©todo de bisecci√≥n, la interpolaci√≥n y la extrapolaci√≥n para encontrar un cero real de la funci√≥n en el intervalo.
Observe que a pesar de la diversidad de los m√©todos, los paquetes escritos de manera
profesional est√°n basados principalmente en m√©todos y principios que se analizan en este
cap√≠tulo. Usted deber√≠a ser capaz de utilizar estos paquetes al leer los manuales adjuntos para
FRPSUHQGHUPHMRUORVSDUiPHWURV\ODVHVSHFL√ÄFDFLRQHVGHORVUHVXOWDGRVREWHQLGRV
Existen tres libros que consideramos cl√°sicos para la soluci√≥n de ecuaciones no lineaOHVORVGH7UDXE>7U@GH2VWURZVNL>2V@\GH+RXVHKROGHU>+R@$GHPiVHOOLEURGH%UHQW
>%UH@VLUYLyFRPREDVHSDUDPXFKRVGHORVPpWRGRVSDUDHQFRQWUDUODUDt]TXHVHXVDQHQOD
actualidad.
Las secciones Preguntas de an√°lisis, Conceptos clave y Revisi√≥n del cap√≠tulo est√°n disponibles en l√≠nea. Encuentre la ruta de acceso en las p√°ginas preliminares.

CAP√çTULO

Interpolaci√≥n y aproximaci√≥n polinomial

Introducci√≥n
Se realiza un censo de la poblaci√≥n de Estados Unidos cada 10 a√±os. La siguiente tabla
muestra la poblaci√≥n, en miles de personas, desde 1960 hasta 2010, y los datos tambi√©n se
UHSUHVHQWDQHQOD√ÄJXUD
A√±o

1960

1970

1980

1990

2000

2010

Poblaci√≥n
(en miles)

179 323

203 302

226542

249 633

281 422

308 746

P(t)
3 3 10 8

2 3 10 8
Poblaci√≥n

3

1 3 10 8

1960 1970 1980 1990 2000 2010
A√±o

t

Al revisar estos datos, podr√≠amos preguntar si se podr√≠an usar para efectuar un c√°lculo
razonable de la poblaci√≥n, digamos, en 1975 o incluso en el a√±o 2020. Las predicciones de
este tipo pueden obtenerse por medio de una funci√≥n que se ajuste a los datos proporcionados. Este proceso recibe el nombre de interpolaci√≥n y es el tema de este cap√≠tulo. Este
problema de poblaci√≥n se considera a lo largo del cap√≠tulo y en los ejercicios 19 de la secci√≥n
3.1, 17 de la secci√≥n 3.3 y 24 de la secci√≥n 3.5.

77

78

CAP√çTULO 3

Interpolaci√≥n y aproximaci√≥n polinomial

3.1 Interpolaci√≥n y el polinomio de Lagrange
Una de las clases m√°s √∫tiles y conocidas de funciones que mapean el conjunto de n√∫meros
reales en s√≠ mismo son los polinomios algebraicos, el conjunto de funciones de la forma

Pn (x) = an x n + an‚àí1 x n‚àí1 + ¬∑ ¬∑ ¬∑ + a1 x + a0 ,
donde n es un entero positivo y a0, 7, an son constantes reales. Una raz√≥n de su importancia
es que se aproximan de manera uniforme a las funciones continuas. Con esto queremos decir
TXHGDGDXQDIXQFLyQGH√ÄQLGD\FRQWLQXDVREUHXQLQWHUYDORFHUUDGR\DFRWDGRH[LVWHXQ
polinomio que est√° tan ‚Äúcerca‚Äù de la funci√≥n dada como se desee. Este resultado se expresa
FRQSUHFLVLyQHQHOWHRUHPDGHDSUR[LPDFLyQGH:HLHUVWUDVV FRQVXOWHOD√ÄJXUD 

Figura 3.1
y

y 5 f(x) 1
y 5 P (x)
y 5 f(x)
y 5 f(x) 2

a

Teorema 3.1

b

(Teorema de aproximaci√≥n de Weierstrass)
Suponga que fHVWiGH√ÄQLGD\HVFRQWLQXDHQ>a, b]. Para cada
P (x), con la propiedad de que

A menudo, se hace referencia a
Karl Weierstrass (1815‚Äì1897)
como el padre del an√°lisis
moderno debido a su insistencia
sobre el rigor en la demostraci√≥n
de resultados matem√°ticos. Fue
fundamental para el desarrollo
de pruebas de convergencia de
series y para determinar formas
GHGH√ÄQLUULJXURVDPHQWHORV
n√∫meros irracionales. Fue el
primero en demostrar que una
funci√≥n podr√≠a ser continua en
todas partes, pero diferenciable
en ninguna parte, un resultado
que escandaliz√≥ a algunos de sus
contempor√°neos.

x

| f (x) ‚àí P(x)|

. 0, existe un polinomio

para todas las x en [a, b].

La prueba de este teorema se puede encontrar en la mayor√≠a de los textos b√°sicos sobre
DQiOLVLVUHDO FRQVXOWHSRUHMHPSOR>%DUW@SS¬≤ 
Otra raz√≥n importante para considerar la clase de polinomios en la aproximaci√≥n de
IXQFLRQHVHVTXHODGHULYDGD\ODLQWHJUDOLQGH√ÄQLGDGHXQSROLQRPLRVRQIiFLOHVGHGHWHUPLnar y tambi√©n son polinomios. Por esta raz√≥n, a menudo se usan polinomios para aproximar
funciones continuas.
Los polinomios de Taylor se presentaron en la secci√≥n 1.1, donde se describieron como
uno de los componentes b√°sicos del an√°lisis num√©rico. Debido a su importancia, se podr√≠a
esperar que la aproximaci√≥n polinomial usar√° estas funciones en gran medida; sin embargo,
√©ste no es el caso. Los polinomios de Taylor concuerdan tanto como es posible con una
IXQFLyQGDGDHQXQSXQWRHVSHFt√ÄFRSHURFRQFHQWUDQVXSUHFLVLyQFHUFDGHHVHSXQWR8Q
buen polinomio de aproximaci√≥n debe dar precisi√≥n relativa sobre un intervalo completo y,
en general, los polinomios de Taylor no lo hacen. Por ejemplo, suponga que calculamos los

3.1 Interpolaci√≥n y el polinomio de Lagrange

79

primeros seis polinomios de Taylor alrededor de x0 5 0 para f (x) 5 e x. Ya que las derivadas
de f (x) son todas ex, que evaluadas en x0 5 0 dan 1, los polinomios de Taylor son
Se public√≥ muy poco del trabajo
de Weierstrass durante su vida;
no obstante, sus conferencias,
en especial sobre la teor√≠a de las
IXQFLRQHVLQ√ÅX\HURQGHPDQHUD
VLJQL√ÄFDWLYDHQXQDJHQHUDFLyQ
completa de estudiantes.

P0 (x) = 1,

P1 (x) = 1 + x,

P4 (x) = 1 + x +

P2 (x) = 1 + x +

x3
x4
x2
+
+ ,
2
6
24

x2
,
2

P3 (x) = 1 + x +

P5 (x) = 1 + x +

y

x3
x2
+ ,
2
6

x3
x4
x5
x2
+
+
+
.
2
6
24 120

/DVJUi√ÄFDVGHORVSROLQRPLRVVHPXHVWUDQHQOD√ÄJXUD REVHUYHTXHLQFOXVRSDUD
los polinomios de grado m√°s alto, el error empeora progresivamente conforme nos alejamos
de cero).

Figura 3.2
y
20

y 5 P5(x)

y 5 ex

y 5 P4(x)

15

y 5 P3(x)
10

y 5 P2(x)

5

y 5 P1(x)
y 5 P0(x)

21

2

1

x

3

Aunque se obtienen mejores aproximaciones para f (x) 5 e x si se usan polinomios de
Taylor, esto no es verdad para todas las funciones. Considere, como un ejemplo extremo,
usar la expansi√≥n en polinomios de Taylor de diferentes grados para f (x) 5 1/ x alrededor de
x0 5 1 para aproximar f (3) 5 1/3. Puesto que

f (x) = x ‚àí1 , f (x) = ‚àíx ‚àí2 , f (x) = (‚àí1)2 2 ¬∑ x ‚àí3 ,
y, en general,

f (k) (x) = (‚àí1)k k!x ‚àík‚àí1 ,
los polinomios de Taylor son
n

Pn (x) =

n

f (k) (1)
(x ‚àí 1)k =
(‚àí1)k (x ‚àí 1)k .
k!
k=0
k=0

Para aproximar f (3) 5 1/3 mediante P n (3) para valores cada vez mayores de n, obtenemos
los valores en la tabla 3.1 (¬°un terrible fracaso!). Cuando aproximamos f (3) 5 1/3 mediante
P n (3) y para valores m√°s grandes de n, la aproximaci√≥n se vuelve cada vez m√°s imprecisa.

Tabla 3.1

n

0

1

2

3

4

5

6

7

Pn (3)

1

‚àí1

3

‚àí5

11

‚àí21

43

‚àí85

80

CAP√çTULO 3

Interpolaci√≥n y aproximaci√≥n polinomial

Para los polinomios de Taylor, toda la informaci√≥n que se usa en la aproximaci√≥n se
concentra en el √∫nico n√∫mero x0, por lo que, en general, √©stos dar√°n aproximaciones imprecisas conforme nos alejamos de x0. Esto limita la aproximaci√≥n de polinomios de Taylor a
situaciones en las que las aproximaciones s√≥lo se necesitan en n√∫meros cercanos a x0. Para
SURSyVLWRVFRPSXWDFLRQDOHVRUGLQDULRVHVPiVH√ÄFLHQWHXVDUPpWRGRVTXHLQFOX\DQLQIRUmaci√≥n en varios puntos. Consideramos esto en el resto del cap√≠tulo. El uso principal de los
polinomios de Taylor en el an√°lisis num√©rico no tiene prop√≥sitos de aproximaci√≥n, sino la
derivaci√≥n de t√©cnicas num√©ricas y el c√°lculo de errores.

Polinomios de interpolaci√≥n de Lagrange
El problema de determinar un polinomio de grado uno que pasa por diferentes puntos (x0, y0)
y (x1, y1) es igual al de aproximar una funci√≥n f para la que f(x0 ) 5 y0 y f (x1 ) 5 y1 por medio
de un polinomio de primer grado que se interpola, o que coincida con los valores de f en
los puntos determinados. El uso de estos polinomios para aproximaci√≥n dentro del intervalo
GHWHUPLQDGRPHGLDQWHSXQWRV√ÄQDOHVUHFLEHHOQRPEUHGHinterpolaci√≥n.
'H√ÄQDODVIXQFLRQHV

L 0 (x) =

x ‚àí x1
x0 ‚àí x1

y L 1 (x) =

x ‚àí x0
.
x1 ‚àí x0

El polinomio de interpolaci√≥n de Lagrange lineal a trav√©s de (x0, y0) y (x1, y1) es

P(x) = L 0 (x) f (x0 ) + L 1 (x) f (x1 ) =

x ‚àí x1
x ‚àí x0
f (x0 ) +
f (x1 ).
x0 ‚àí x1
x1 ‚àí x0

Observe que

L 0 (x0 ) = 1,

L 0 (x1 ) = 0,

L 1 (x0 ) = 0,

y L 1 (x1 ) = 1,

lo cual implica que

P(x0 ) = 1 ¬∑ f (x0 ) + 0 ¬∑ f (x1 ) = f (x0 ) = y0
y

P(x1 ) = 0 ¬∑ f (x0 ) + 1 ¬∑ f (x1 ) = f (x1 ) = y1 .
Por lo que P es el √∫nico polinomio de grado a lo m√°s 1 que pasa por (x0, y0) y (x1, y1).
Ejemplo 1

Determine el polinomio de interpolaci√≥n de Lagrange que pasa por los puntos (2, 4) y (5, 1).
Soluci√≥n en este caso, tenemos

L 0 (x) =

1
x ‚àí5
= ‚àí (x ‚àí 5)
2‚àí5
3

y

L 1 (x) =

1
x ‚àí2
= (x ‚àí 2),
5‚àí2
3

por lo que
1
1
4
20 1
2
P(x) = ‚àí (x ‚àí 5) ¬∑ 4 + (x ‚àí 2) ¬∑ 1 = ‚àí x +
+ x ‚àí = ‚àíx + 6.
3
3
3
3
3
3
/DJUi√ÄFDGHy 5 P(x VHPXHVWUDHQOD√ÄJXUD

3.1 Interpolaci√≥n y el polinomio de Lagrange

81

Figura 3.3
y
(2,4)

4
3
2

y 5 P(x) = 2x 1 6

1

1

2

3

4

(5,1)

5

x

Para generalizar el concepto de interpolaci√≥n lineal, considere la construcci√≥n de un
polinomio de grado n que pasa a trav√©s de n 1 1 puntos

(x0 , f (x0 )), (x1 , f (x1 )), . . . , (xn , f (xn )).
9pDVHOD√ÄJXUD

Figura 3.4
y

y 5 f (x)
y 5 P(x)

x0

x1

x2

xn

x

En este caso, primero construimos, para cada k 5 0, 1, 7, n, una funci√≥n L n,k (x) con la
propiedad de que Ln,k (xi ) = 0 cuando i = k y L n,k (x k) 5 1. Para satisfacer Ln,k (xi ) = 0 para
cada i = k se requiere que el numerador de L n,k (x) contenga el t√©rmino

(x ‚àí x0 )(x ‚àí x1 ) ¬∑ ¬∑ ¬∑ (x ‚àí xk‚àí1 )(x ‚àí xk+1 ) ¬∑ ¬∑ ¬∑ (x ‚àí xn ).
Para satisfacer L n,k (x k) 5 1, el denominador de L n,k (x) debe ser el mismo t√©rmino, pero
evaluado en x 5 xk. Por lo tanto,

L n,k (x) =

(x ‚àí x0 ) ¬∑ ¬∑ ¬∑ (x ‚àí xk‚àí1 )(x ‚àí xk+1 ) ¬∑ ¬∑ ¬∑ (x ‚àí xn )
.
(xk ‚àí x0 ) ¬∑ ¬∑ ¬∑ (xk ‚àí xk‚àí1 )(xk ‚àí xk+1 ) ¬∑ ¬∑ ¬∑ (xk ‚àí xn )

8QERVTXHMRGHODJUi√ÄFDGHXQDL n,k (cuando n HVSDU VHPXHVWUDHQOD√ÄJXUD

82

CAP√çTULO 3

Interpolaci√≥n y aproximaci√≥n polinomial

Figura 3.5
L n,k(x)

1

x0

x1

...

x k21

xk

x k11

...

x n21

xn

x

El polinomio de interpolaci√≥n se describe f√°cilmente una vez que se conoce la forma
L n,k . Este polinomio, llamado en√©simo polinomio de interpolaci√≥n de Lagrange,VHGH√ÄQH
en el siguiente teorema.
Teorema 3.2
La f√≥rmula de interpolaci√≥n
nombrada por Joseph Louis
Lagrange (1736‚Äì1813)
probablemente era conocida
por Newton alrededor de 1675,
pero al parecer fue publicada por
primera vez en 1779 por Edward
Waring (1736‚Äì1798). Lagrange
escribi√≥ mucho sobre el tema de
interpolaci√≥n y su trabajo tuvo
XQDLQ√ÅXHQFLDVLJQL√ÄFDWLYDVREUH
los matem√°ticos posteriores. √âl
public√≥ este resultado en 1795.

Si x0, x1, 7, x n son n 1 1 n√∫meros distintos y f es una funci√≥n cuyos valores est√°n determinados en estos n√∫meros, entonces existe un √∫nico polinomio P(x) de grado a lo sumo n con

f (xk ) = P(xk ),
Este polinomio est√° determinado por

n

P(x) = f (x0 )L n,0 (x) + ¬∑ ¬∑ ¬∑ + f (xn )L n,n (x) =

f (xk )L n,k (x),

(3.1)

k=0

donde, para cada k = 0, 1, . . . , n,
L n,k (x) =

(x ‚àí x0 )(x ‚àí x1 ) ¬∑ ¬∑ ¬∑ (x ‚àí xk‚àí1 )(x ‚àí xk+1 ) ¬∑ ¬∑ ¬∑ (x ‚àí xn )
(xk ‚àí x0 )(xk ‚àí x1 ) ¬∑ ¬∑ ¬∑ (xk ‚àí xk‚àí1 )(xk ‚àí xk+1 ) ¬∑ ¬∑ ¬∑ (xk ‚àí xn )

(3.2)

n

El s√≠mbolo se usa para escribir
productos de manera compacta y
es similar al s√≠mbolo , que se
utiliza para escribir sumas. Por
ejemplo
3
i=0 ai = a1 ‚àó a2 ‚àó a3 .

para cada k = 0, 1, . . . , n.

=

(x ‚àí xi )
.
(xk ‚àí xi )
i=0

i =k

Escribiremos Ln,k (x) simplemente como L k (x) cuando no haya confusi√≥n en cuanto a
su grado.

Ejemplo 2

a)

Use los n√∫meros (llamados nodos)
x0 5 2, x1 5 2.75 y x2 5 4 para encontrar el polinomio de interpolaci√≥n de Lagrange
de segundo grado para f (x) 5 1/x.

b)

Use este polinomio para aproximar f(3) 5 1/3.

Soluci√≥n a)3ULPHURGHWHUPLQDPRVORVFRH√ÄFLHQWHVSROLQyPLFRVL 0 (x), L 1 (x) y L 2 (x). En
forma anidada, estos son

L 0 (x) =

2
(x ‚àí 2.75)(x ‚àí 4)
= (x ‚àí 2.75)(x ‚àí 4),
(2 ‚àí 2.75)(2 ‚àí 4)
3

L 1 (x) =

16
(x ‚àí 2)(x ‚àí 4)
= ‚àí (x ‚àí 2)(x ‚àí 4),
(2.75 ‚àí 2)(2.75 ‚àí 4)
15

L 2 (x) =

2
(x ‚àí 2)(x ‚àí 2.75)
= (x ‚àí 2)(x ‚àí 2.75).
(4 ‚àí 2)(4 ‚àí 2.75)
5

y

3.1 Interpolaci√≥n y el polinomio de Lagrange

83

Adem√°s, f (x0 ) = f (2) = 1/2, f (x1 ) = f (2.75) = 4/11, y f (x2 ) = f (4) = 1/4, por lo
que
2

P(x) =

f (xk )L k (x)
k=0

1
64
1
(x ‚àí 2.75)(x ‚àí 4) ‚àí
(x ‚àí 2)(x ‚àí 4) + (x ‚àí 2)(x ‚àí 2.75)
3
165
10
49
1 2 35
x ‚àí x+ .
=
22
88
44
=

b) Una aproximaci√≥n para f (3) = 1/3 (v√©ase la figura 3.6) es
f (3) ‚âà P(3) =

105 49
29
9
‚àí
+
=
‚âà 0.32955.
22
88
44
88

Recuerde que en la secci√≥n de apertura de este cap√≠tulo (consulte la tabla 3.1), encontramos
que ninguna expansi√≥n en polinomios de Taylor alrededor de x0 5 1 se puede usar para
aproximar razonablemente f(x) 5 1/x en x 5 3.
Figura 3.6
y
4
3
2

y 5 f (x)

1
y 5 P(x)
1

2

3

4

5

x

El siguiente paso es calcular un residuo o cota para el error involucrado en la aproximaci√≥n de una funci√≥n mediante un polinomio de interpolaci√≥n.
Teorema 3.3

Existen otras formas de expresar
el t√©rmino de error para el
polinomio de Lagrange, pero
√©sta puede ser la forma m√°s
√∫til y la que concuerda m√°s
estrechamente con la forma de
error del polinomio est√°ndar
de Taylor.

Suponga x0 , x1 , . . . , xnVRQQ~PHURVGLVWLQWRVHQHOLQWHUYDOR>a, b] y f ‚àà C n+1 [a, b]. Entonces, para cada x en >a, b], existe un n√∫mero Œæ(x) (generalmente no conocido) entre m√≠n {x0,
x1, 7, xn} y m√°x{x0, x1, 7, xn} y, por lo tanto, en (a, b), con

f (x) = P(x) +

f (n+1) (Œæ(x))
(x ‚àí x0 )(x ‚àí x1 ) ¬∑ ¬∑ ¬∑ (x ‚àí xn ),
(n + 1)!

(3.3)

donde P(x) es el polinomio de interpolaci√≥n determinado en la ecuaci√≥n (3.1).
Demostraci√≥n Primero observe que si x 5 x k para cualquier k 5 0, 1, 7, n, entonces f(xk ) 5
P(x k ) y al elegir Œæ (x k )de manera arbitraria en (a, b) se obtiene la ecuaci√≥n (3.3).

84

CAP√çTULO 3

Interpolaci√≥n y aproximaci√≥n polinomial

Si x = xk , para todas las k = 0, 1, . . . , nGH√ÄQDODIXQFLyQg para tHQ>a, b] mediante

g(t) = f (t) ‚àí P(t) ‚àí [ f (x) ‚àí P(x)]

(t ‚àí x0 )(t ‚àí x1 ) ¬∑ ¬∑ ¬∑ (t ‚àí xn )
(x ‚àí x0 )(x ‚àí x1 ) ¬∑ ¬∑ ¬∑ (x ‚àí xn )
n

= f (t) ‚àí P(t) ‚àí [ f (x) ‚àí P(x)]

(t ‚àí xi )
.
(x
‚àí xi )
i=0

Puesto que f ‚àà C n+1 [a, b], y P ‚àà C ‚àû [a, b], se sigue que g ‚àà C n+1 [a, b]. Para t = xk ,
tenemos
n

g(xk ) = f (xk ) ‚àí P(xk ) ‚àí [ f (x) ‚àí P(x)]

(xk ‚àí xi )
= 0 ‚àí [ f (x) ‚àí P(x)] ¬∑ 0 = 0.
(x ‚àí xi )
i=0

Adem√°s,
n

g(x) = f (x) ‚àí P(x) ‚àí [ f (x) ‚àí P(x)]

(x ‚àí xi )
= f (x) ‚àí P(x) ‚àí [ f (x) ‚àí P(x)] = 0.
(x ‚àí xi )
i=0

Por lo tanto, g ‚àà C n+1 [a, b], y g se anula en los n 1 2 n√∫meros distintos x, x0 , x1 , . . . , xn.
Por el teorema generalizado de Rolle 1.10, existe un n√∫mero Œæ en (a, b) para el que
g (n+1) (Œæ ) = 0.Por lo que,

0 = g (n + 1) (Œæ ) = f (n+1) (Œæ ) ‚àí P (n+1) (Œæ ) ‚àí [ f (x) ‚àí P(x)]

d n+1
dt n+1

n

(t ‚àí xi )
(x ‚àí xi )
i=0

. (3.4)
t=Œæ

Sin embargo, P(x) es un polinomio de grado a lo sumo n, por lo que la derivada (n 1 1),
n
P (n+1) (x), es cero. Adem√°s i=0 [(t ‚àí xi )/(x ‚àí xi )] es un polinomio de grado (n 1 1), por
lo que
n

(t ‚àí xi )
=
(x ‚àí xi )
i=0

1
n
i=0 (x ‚àí x i )

t n+1 + (t√©rminos de menor grado en t),

y
n

d n+1
(t ‚àí xi )
=
n+1
dt
(x ‚àí xi )
i=0

(n + 1)!

.
n
i=0 (x ‚àí x i )

Ahora, la ecuaci√≥n (3.4) se convierte en
0 = f (n+1) (Œæ ) ‚àí 0 ‚àí [ f (x) ‚àí P(x)]

(n + 1)!

,
n
i=0 (x ‚àí x i )

y, despu√©s de resolver f (x), tenemos
n

f (x) = P(x) +

f (n+1) (Œæ )
(x ‚àí xi ).
(n + 1)! i=0

La f√≥rmula de error en el teorema 3.3 es un resultado te√≥rico importante porque los
polinomios de Lagrange se usan ampliamente para deducir la diferenciaci√≥n num√©rica y
los m√©todos de integraci√≥n. Las cotas de error para estas t√©cnicas se obtienen a partir de la
f√≥rmula del error de Lagrange.
Observe que la forma del error para el polinomio de Lagrange es bastante similar a la
del polinomio de Taylor. El en√©simo polinomio de Taylor alrededor de x0 concentra toda
la informaci√≥n conocida en x0 y tiene un t√©rmino de error de la forma

f (n+1) (Œæ(x))
(x ‚àí x0 )n+1 .
(n + 1)!

3.1 Interpolaci√≥n y el polinomio de Lagrange

85

El polinomio de Lagrange de grado n utiliza informaci√≥n en los distintos n√∫meros x0, x1,
  , xn y, en lugar de (x 2 x0)n su f√≥rmula de error utiliza el producto de los n 1 1 t√©rminos
(x ‚àí x0 ), (x ‚àí x1 ), . . . , (x ‚àí xn ):

f (n+1) (Œæ(x))
(x ‚àí x0 )(x ‚àí x1 ) ¬∑ ¬∑ ¬∑ (x ‚àí xn ).
(n + 1)!
Ejemplo 3

En el ejemplo 2 encontramos el segundo polinomio de Lagrange para f(x) 5 1/x HQ>@
usando los nodos x0 5 2, x1 5 2.75 y x2 = 4. Determine la forma del error para este polinomio
y el error m√°ximo cuando el polinomio se usa para aproximar f (x) para x ‚àà >@
Como f (x) = x ‚àí1, tenemos

Soluci√≥n

f (x) = ‚àíx ‚àí2 ,

f (x) = 2x ‚àí3 ,

y

f (x) = ‚àí6x ‚àí4 .

En consecuencia, el segundo polinomio de Lagrange tiene el error de la forma

f (Œæ(x))
(x‚àí x0 )(x‚àí x1 )(x‚àí x2 ) = ‚àí (Œæ(x))‚àí4 (x‚àí 2)(x‚àí 2.75)(x‚àí 4), para Œæ(x)en(2, 4).
3!
El valor m√°ximo de (Œæ(x))‚àí4 en el intervalo es 2‚àí4 = 1/16. Ahora necesitamos determinar el
valor m√°ximo en este intervalo del valor absoluto del polinomio

g(x) = (x ‚àí 2)(x ‚àí 2.75)(x ‚àí 4) = x 3 ‚àí

35 2 49
x + x ‚àí 22.
4
2

Como
Dx

x3 ‚àí

35 2 49
x + x ‚àí 22
4
2

= 3x 2 ‚àí

49
1
35
x+
= (3x ‚àí 7)(2x ‚àí 7),
2
2
2

los puntos cr√≠ticos se presentan en
x=

7
, con g
3

7
3

=

25
,
108

y

x=

7
, con g
2

7
2

=‚àí

9
.
16

Por lo tanto, el error m√°ximo es
1
9
9
f (Œæ(x))
‚àí
=
|(x ‚àí x0 )(x ‚àí x1 )(x ‚àí x2 )| ‚â§
‚âà 0.03515625.
3!
16
16
256
El siguiente ejemplo ilustra c√≥mo se puede usar la f√≥rmula del error para preparar una
tabla de datos que garantizar√° un error de interpolaci√≥n dentro de una cota establecida.
Ejemplo 4

Suponga que se va a preparar una tabla para la funci√≥n f (x) = e x , para x en [0, 1]. Imagine
que el n√∫mero de lugares decimales proporcionado por entrada es d $ 8 y que h, el tama√±o
del paso es la diferencia entre valores adyacentes x. ¬øQu√© tama√±o de paso h garantizar√° que
la interpolaci√≥n lineal proporcione un error absoluto a lo m√°ximo de 10‚Äì6 para todas las x
HQ>@"
Soluci√≥n

Sean x0, x1, 7 los n√∫meros en los que se eval√∫a f y xHVWiHQ>@\VXSRQJDTXHj
satisface xj # x # x j 11. La ecuaci√≥n (3.3) implica que el error en la interpolaci√≥n lineal es

| f (x) ‚àí P(x)| =

f (2) (Œæ )
| f (2) (Œæ )|
(x ‚àí x j )(x ‚àí x j+1 ) =
|(x ‚àí x j )||(x ‚àí x j+1 )|.
2!
2

Como el tama√±o del paso es h, entonces x j = j h, x j+1 = ( j + 1)h, y
| f (x) ‚àí P(x)| ‚â§

| f (2) (Œæ )|
|(x ‚àí j h)(x ‚àí ( j + 1)h)|.
2!

86

CAP√çTULO 3

Interpolaci√≥n y aproximaci√≥n polinomial

Por lo tanto,

m√°xŒæ ‚àà[0,1] eŒæ
m√°x |(x ‚àí j h)(x ‚àí ( j + 1)h)|
x j ‚â§x‚â§x j+1
2
e
‚â§
m√°x |(x ‚àí j h)(x ‚àí ( j + 1)h)|.
2 x j ‚â§x‚â§x j+1

| f (x) ‚àí P(x)| ‚â§

Considere la funci√≥n g(x) = (x ‚àí j h)(x ‚àí ( j + 1)h), para j h ‚â§ x ‚â§ ( j + 1)h. Luego
g (x) = (x ‚àí ( j + 1)h) + (x ‚àí j h) = 2 x ‚àí j h ‚àí

h
2

,

el √∫nico punto cr√≠tico para g se encuentra en x = j h + h/2, con g( j h + h/2) = (h/2)2
= h 2 /4.
Puesto que g( j h) = 0 y g(( j + 1)h) = 0, el valor m√°ximo de |g (x)| en [j h, ( j + 1)h]
se debe presentar en el punto cr√≠tico, lo cual implica que (v√©ase el ejercicio 21)

| f (x) ‚àí P(x)| ‚â§

e
e h2
eh 2
m√°x |g(x)| ‚â§ ¬∑
=
.
2 x j ‚â§x‚â§x j+1
2 4
8

Por consiguiente, para garantizar que el error en la interpolaci√≥n lineal est√° acotado por
1026HVVX√ÄFLHQWHHOHJLUh de tal forma que

eh 2
‚â§ 10‚àí6 .
8

Esto implica que

h < 1.72 √ó 10‚àí3 .

Puesto que n 5 (1 2 0)/h debe ser un entero, una selecci√≥n razonable para el tama√±o del
paso es h 5 0.001.
La secci√≥n Conjunto de ejercicios 3.1 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

3.2 Aproximaci√≥n de datos y m√©todo de Neville
En la secci√≥n anterior encontramos una representaci√≥n expl√≠cita para los polinomios de Lagrange y su error cuando se aproxima una funci√≥n sobre un intervalo. El uso frecuente de
estos polinomios implica la interpolaci√≥n de datos tabulados. En este caso, una representaci√≥n expl√≠cita del polinomio podr√≠a no ser necesaria, s√≥lo los valores del polinomio en
SXQWRVHVSHFt√ÄFRV(QHVWDVLWXDFLyQVHUtDSRVLEOHTXHODIXQFLyQVXE\DFHQWHDORVGDWRVQR
se conozca, por lo que la forma expl√≠cita del error no se puede usar. Ahora, ilustraremos una
aplicaci√≥n pr√°ctica de interpolaci√≥n en dicha situaci√≥n.
Ilustraci√≥n

Tabla 3.2
x

f (x)

1.0
1.3
1.6
1.9
2.2

0.7651977
0.6200860
0.4554022
0.2818186
0.1103623

La tabla 3.2 lista los valores de una funci√≥n f en diferentes puntos. Las aproximaciones para
f (1.5) obtenidas con distintos polinomios de Lagrange que usan estos datos se comparar√°
para probar y determinar la precisi√≥n de la aproximaci√≥n.
El polinomio lineal m√°s apropiado usa x0 5 1.3 y x1 5 1.6 porque 1.5 se encuentra entre 1.3
y 1.6. El valor del polinomio de interpolaci√≥n en 1.5 es

P1 (1.5) =
=

(1.5 ‚àí 1.6)
(1.5 ‚àí 1.3)
f (1.3) +
f (1.6)
(1.3 ‚àí 1.6)
(1.6 ‚àí 1.3)
(1.5 ‚àí 1.3)
(1.5 ‚àí 1.6)
(0.6200860) +
(0.4554022) = 0.5102968.
(1.3 ‚àí 1.6)
(1.6 ‚àí 1.3)

3.2

Aproximaci√≥n de datos y m√©todo de Neville

87

Es posible usar razonablemente dos polinomios de grado dos, uno con x0 5 1.3, x1 5 1.6 y
x2 5 1.9, lo cual nos da

P2 (1.5) =

(1.5 ‚àí 1.6)(1.5 ‚àí 1.9)
(1.5 ‚àí 1.3)(1.5 ‚àí 1.9)
(0.6200860) +
(0.4554022)
(1.3 ‚àí 1.6)(1.3 ‚àí 1.9)
(1.6 ‚àí 1.3)(1.6 ‚àí 1.9)
+

(1.5 ‚àí 1.3)(1.5 ‚àí 1.6)
(0.2818186) = 0.5112857,
(1.9 ‚àí 1.3)(1.9 ‚àí 1.6)

y uno con x0 5 1.0, x1 5 1.3 y x2 5 1.6, lo cual nos da PÃÇ2 (1.5) = 0.5124715.
En el caso de tercer grado, tambi√©n hay dos opciones razonables para el polinomio, una
con x0 5 1.3, x1 5 1.6, x2 5 1.9 y x3 5 2.2, lo cual nos da P3(1.5) 5 0.5118302. La segunda
aproximaci√≥n de tercer grado se obtiene con x0 51.0, x1 5 1.3, x2 5 1.6 y x3 5 1.9, lo cual
nos da PÃÇ3 (1.5) = 0.5118127.
El polinomio de Lagrange de cuarto grado usa todas las entradas en la tabla. Con
x0 5 1.0, x1 5 1.3, x2 = 1.6, x3 5 1.9 y x4 5 2.2, la aproximaci√≥n es P4(1.5) = 0.5118200.
Puesto que P3(1.5), PÃÇ3 (1.5) y P4(1.5) concuerdan con una exactitud de 2 3 1025 unidades, esperamos este grado de precisi√≥n para estas aproximaciones. Tambi√©n esperamos que
P4(1.5) sea la aproximaci√≥n m√°s precisa ya que usa la mayor parte de los datos proporcionados.
/DIXQFLyQTXHHVWDPRVDSUR[LPDQGRHVHQUHDOLGDGODIXQFLyQGH%HVVHOGHSULPHUD
clase de orden cero, cuyo valor en 1.5 se conoce como 0.5118277. Por lo tanto, las verdaderas precisiones de las aproximaciones son las siguientes:

|P1 (1.5) ‚àí f (1.5)| ‚âà 1.53 √ó 10‚àí3 ,
|P2 (1.5) ‚àí f (1.5)| ‚âà 5.42 √ó 10‚àí4 ,
| PÃÇ2 (1.5) ‚àí f (1.5)| ‚âà 6.44 √ó 10‚àí4 ,
|P3 (1.5) ‚àí f (1.5)| ‚âà 2.5 √ó 10‚àí6 ,
| PÃÇ3 (1.5) ‚àí f (1.5)| ‚âà 1.50 √ó 10‚àí5 ,
|P4 (1.5) ‚àí f (1.5)| ‚âà 7.7 √ó 10‚àí6 .
Aunque P3(1.5) es la aproximaci√≥n m√°s precisa, si no conocemos el valor real de f (1.5),
aceptar√≠amos P4(1.5) como la mejor aproximaci√≥n ya que incluye la mayor cantidad de datos sobre la funci√≥n. El t√©rmino del error de Lagrange derivado del teorema 3.3 no se puede
aplicar aqu√≠ porque no conocemos la cuarta derivada de f. Por desgracia, este casi siempre
es el caso.

M√©todo de Neville
8QDGL√ÄFXOWDGSUiFWLFDFRQODLQWHUSRODFLyQGH/DJUDQJHHVTXHHOWpUPLQRGHOHUURUHVGLItFLO
de aplicar, por lo que el grado del polinomio que se necesita para la precisi√≥n deseada en
general se desconoce hasta que se realizan los c√°lculos. Una pr√°ctica com√∫n es calcular los
resultados dados a partir de diferentes polinomios hasta que se obtiene el acuerdo apropiado, como se hizo en la ilustraci√≥n anterior. Sin embargo, el trabajo efectuado al calcular la
aproximaci√≥n con el segundo polinomio no disminuye el trabajo necesario para calcular
la tercera aproximaci√≥n, ni la cuarta aproximaci√≥n es f√°cil de obtener una vez que se conoce la
tercera aproximaci√≥n y as√≠ sucesivamente. Ahora, derivaremos estos polinomios de aproximaci√≥n de una manera que use los c√°lculos previos para una mayor ventaja.
DeÔ¨Ånici√≥n 3.4

Sea fXQDIXQFLyQGH√ÄQLGDHQx0 , x1 , x2 , . . . , xn y suponga que m 1 , m 2 , . . . , m k son k enteros
diferentes, con 0 ‚â§ m i ‚â§ n para cada i. El polinomio de Lagrange que concuerda con f (x)
en los puntos k xm 1 , xm 2 , . . . , xm k se denota Pm 1 ,m 2 ,... ,m k (x).

88

CAP√çTULO 3

Interpolaci√≥n y aproximaci√≥n polinomial

Ejemplo 1

Suponga que x0 5 1, x1 5 2, x2 5 3, x3 5 4, x4 5 6 y f(x) = ex. Determine el polinomio de
interpolaci√≥n que se denota P1,2,4(x) y use este polinomio para aproximar f(5).
Soluci√≥n √âste es el polinomio de Lagrange que concuerda con f(x) en x1 5 2, x2 5 3 y
x4 5 6. Por lo tanto,

P1,2,4 (x) =

(x ‚àí 3)(x ‚àí 6) 2 (x ‚àí 2)(x ‚àí 6) 3 (x ‚àí 2)(x ‚àí 3) 6
e +
e +
e .
(2 ‚àí 3)(2 ‚àí 6)
(3 ‚àí 2)(3 ‚àí 6)
(6 ‚àí 2)(6 ‚àí 3)

por lo que,
f (5) ‚âà P(5) =

(5 ‚àí 3)(5 ‚àí 6) 2 (5 ‚àí 2)(5 ‚àí 6) 3 (5 ‚àí 2)(5 ‚àí 3) 6
e +
e +
e
(2 ‚àí 3)(2 ‚àí 6)
(3 ‚àí 2)(3 ‚àí 6)
(6 ‚àí 2)(6 ‚àí 3)

1
1
= ‚àí e2 + e3 + e6 ‚âà 218.105.
2
2
El siguiente resultado describe un m√©todo para generar de forma recursiva las aproximaciones del polinomio de Lagrange.
Teorema 3.5

Sea f GH√ÄQLGDHQx0, x1, 7, xk y sean xj y xi dos n√∫meros distintos en este conjunto. Entonces

P(x) =

(x ‚àí x j )P0,1,... , j‚àí1, j+1,... ,k (x) ‚àí (x ‚àí xi )P0,1,... ,i‚àí1,i+1,... ,k (x)
(xi ‚àí x j )

es el k-√©simo polinomio de Lagrange que interpola f en los puntos k 1 1 x0, x1, 7, xk.
Para la facilidad de la notaci√≥n, sea Q ‚â° P0,1,... ,i‚àí1,i+1,... ,k y QÃÇ ‚â° P0,1,... ,
Puesto que Q(x) y QÃÇ(x) son polinomios de grado k 2 1 o menos, P(x) es de
grado m√°ximo k.
Primero, observe que QÃÇ(xi ) = f (xi ) implica que
Demostraci√≥n
j‚àí1, j+1,... ,k .

P(xi ) =

(xi ‚àí x j ) QÃÇ(xi ) ‚àí (xi ‚àí xi )Q(xi )
(xi ‚àí x j )
f (xi ) = f (xi ).
=
xi ‚àí x j
(xi ‚àí x j )

Similarmente, como Q(x j ) = f (x j ), tenemos que P(x j ) = f (x j ).
Adem√°s, si 0 ‚â§ r ‚â§ k y r no es i ni j, entonces Q(xr ) = QÃÇ(xr ) = f (xr ). Por lo tanto,

P(xr ) =

(xr ‚àí x j ) QÃÇ(xr ) ‚àí (xr ‚àí xi )Q(xr )
(xi ‚àí x j )
f (xr ) = f (xr ).
=
xi ‚àí x j
(xi ‚àí x j )

3HURSRUGH√ÄQLFLyQP0,1,... ,k (x) es el √∫nico polinomio de grado m√°ximo k que concuerda con
f en x0 , x1 , . . . , xk . Por lo tanto P ‚â° P0,1,... ,k .
El teorema 3.5 implica que los polinomios de interpolaci√≥n pueden generarse de manera
recursiva. Por ejemplo, tenemos

P0,1 =

1
[(x ‚àí x0 )P1 + (x ‚àí x1 )P0 ],
x1 ‚àí x0

P0,1,2 =

1
[(x ‚àí x0 )P1,2 + (x ‚àí x2 )P0,1 ],
x2 ‚àí x0

P1,2 =

1
[(x ‚àí x1 )P2 + (x ‚àí x2 )P1 ],
x2 ‚àí x1

y as√≠ sucesivamente. Estos se generan de la manera que se muestra en la tabla 3.3, donde
FDGD√ÄODVHFRPSOHWDDQWHVGHTXHODV√ÄODVVXFHVLYDVFRPLHQFHQ

3.2

Tabla 3.3

Eric Harold Neville (1889‚Äì1961)
DSRUWyHVWDPRGL√ÄFDFLyQGH
la f√≥rmula de Lagrange en un
DUWtFXORSXEOLFDGRHQ>1@

x0
x1
x2
x3
x4

P0
P1
P2
P3
P4

P0,1
P1,2
P2,3
P3,4

P0,1,2
P1,2,3
P2,3,4

P0,1,2,3
P1,2,3,4

Aproximaci√≥n de datos y m√©todo de Neville

89

P0,1,2,3,4

El procedimiento que usa el resultado del teorema 3.5 para generar recursivamente las
aproximaciones de polinomios de interpolaci√≥n recibe el nombre de m√©todo de Neville. La
notaci√≥n P que se usa en la tabla 3.3 es pesada debido al n√∫mero de sub√≠ndices que se utilizan
para representar las entradas. Observe, sin embargo, que mientras se construye un arreglo,
s√≥lo se necesitan dos sub√≠ndices. El procedimiento hacia abajo en la tabla corresponde al
uso consecutivo de los puntos xi con una i m√°s grande, y el procedimiento hacia la derecha
corresponde al incremento del grado del polinomio de interpolaci√≥n. Puesto que los puntos
aparecen de manera consecutiva en cada entrada, necesitamos describir s√≥lo un punto de
inicio y el n√∫mero de puntos adicionales que se usan en la construcci√≥n de la aproximaci√≥n.
Para evitar los m√∫ltiples √≠ndices, dejamos que Qi,j (x) para 0 ‚â§ j ‚â§ i, denote el polinomio
de interpolaci√≥n de grado j en los n√∫meros (j + 1) xi‚àí j , xi‚àí j+1 , . . . , xi‚àí1 , xi ; es decir

Q i, j = Pi‚àí j,i‚àí j+1,... ,i‚àí1,i .
Usando esta notaci√≥n obtenemos el arreglo de notaci√≥n Q en la tabla 3.4.

Tabla 3.4

Ejemplo 2

Tabla 3.5
x

f (x)

1.0
1.3
1.6
1.9
2.2

0.7651977
0.6200860
0.4554022
0.2818186
0.1103623

x0
x1
x2
x3
x4

P0 = Q 0,0
P1 = Q 1,0
P2 = Q 2,0
P3 = Q 3,0
P4 = Q 4,0

P0,1 = Q 1,1
P1,2 = Q 2,1
P2,3 = Q 3,1
P3,4 = Q 4,1

P0,1,2 = Q 2,2
P1,2,3 = Q 3,2
P2,3,4 = Q 4,2

P0,1,2,3 = Q 3,3
P1,2,3,4 = Q 4,3

P0,1,2,3,4 = Q 4,4

Los valores de diferentes polinomios de interpolaci√≥n en x 5 1.5 se obtuvieron en la ilustraci√≥n al inicio de esta secci√≥n usando los datos que se muestran en la tabla 3.5. Aplique el
m√©todo de Neville a los datos mediante la construcci√≥n de una tabla recursiva de la forma
que se observa en la tabla 3.4.
Soluci√≥n

Sea x0 5 1.0, x1 5 1.3, x2 5 1.6, x3 5 1.9 y x4 = 2.2, entonces Q0,0 5 f (1.0), Q1,0
5 f(1.3), Q2,0 5 f(1.6), Q3,0 5 f(1.9) y Q4,0 5 f(2.2). Estos son los cinco polinomios de grado
cero (constantes) que aproximan f(1.5) y son iguales a los datos que se proporcionan en la
tabla 3.5.
Al calcular la aproximaci√≥n de primer grado Q1,1 (1.5) obtenemos

Q 1,1 (1.5) =

(x ‚àí x0 )Q 1,0 ‚àí (x ‚àí x1 )Q 0,0
x1 ‚àí x0

(1.5 ‚àí 1.0)Q 1,0 ‚àí (1.5 ‚àí 1.3)Q 0,0
1.3 ‚àí 1.0
0.5(0.6200860) ‚àí 0.2(0.7651977)
= 0.5233449.
=
0.3
=

De igual forma,
Q 2,1 (1.5) =

(1.5 ‚àí 1.3)(0.4554022) ‚àí (1.5 ‚àí 1.6)(0.6200860)
= 0.5102968,
1.6 ‚àí 1.3

Q 3,1 (1.5) = 0.5132634,

y

Q 4,1 (1.5) = 0.5104270.

90

CAP√çTULO 3

Interpolaci√≥n y aproximaci√≥n polinomial

Se espera que la mejor aproximaci√≥n lineal sea Q2,1 porque 1.5 se encuentra entre
x1 5 1.3 y x2 5 1.6.
De manera similar, las aproximaciones usando polinomios de grado superior est√°n dadas por

Q 2,2 (1.5) =

(1.5 ‚àí 1.0)(0.5102968) ‚àí (1.5 ‚àí 1.6)(0.5233449)
= 0.5124715,
1.6 ‚àí 1.0

Q 3,2 (1.5) = 0.5112857,

Q 4,2 (1.5) = 0.5137361.

y

Las aproximaciones de grado superior se generan de una manera similar y se muestran
en la tabla 3.6.

Tabla 3.6

1.0
1.3
1.6
1.9
2.2

0.7651977
0.6200860
0.4554022
0.2818186
0.1103623

0.5233449
0.5102968
0.5132634
0.5104270

0.5124715
0.5112857
0.5137361

0.5118127
0.5118302

0.5118200

Si la √∫ltima aproximaci√≥n Q4,4 QRIXHVX√ÄFLHQWHPHQWHSUHFLVDVHUtDSRVLEOHVHOHFFLRQDU
otro nodo x5\DxDGLURWUD√ÄODDODWDEOD

x5

Q 5,0

Q 5,1

Q 5,2

Q 5,3

Q 5,4

Q 5,5 .

Entonces Q4,4, Q5,4 y Q5,5 podr√≠an compararse para determinar la precisi√≥n posterior.
/DIXQFLyQHQHOHMHPSORHVODIXQFLyQGH%HVVHOGHSULPHUDFODVHGHRUGHQFHURFX\R
valor en 2.5 es 2\ODVLJXLHQWH√ÄODGHDSUR[LPDFLRQHVSDUDf(1.5) es

2.5

‚àí 0.0483838

0.4807699

0.5301984

0.5119070

0.5118430

0.5118277.

La √∫ltima nueva entrada, 0.5118277, es correcta para siete lugares decimales.
Ejemplo 3

Tabla 3.7
i

xi

ln xi

0
1
2

2.0
2.2
2.3

0.6931
0.7885
0.8329

La tabla 3.7 lista los valores de f(x) 5 ln x precisos para los lugares dados. Use el m√©todo
de Neville y la aritm√©tica de redondeo de cuatro d√≠gitos para aproximar f (2.1) 5 ln 2.1 al
completar la tabla de Neville.
Soluci√≥n Puesto que x 2 x0 5 0.1, x 2 x1 5 20.1 y x 2 x2 5 20.2, tenemos Q0,0 5 0.6931,

Q1,0 5 0.7885 y Q2,0 5 0.8329,

Q 1,1 =

0.1482
1
= 0.7410
[(0.1)0.7885 ‚àí (‚àí0.1)0.6931] =
0.2
0.2

Q 2,1 =

1
0.07441
= 0.7441.
[(‚àí0.1)0.8329 ‚àí (‚àí0.2)0.7885] =
0.1
0.1

y

La aproximaci√≥n final que podemos obtener a partir de estos datos es
Q 2,1 =

1
0.2276
= 0.7420.
[(0.1)0.7441 ‚àí (‚àí0.2)0.7410] =
0.3
0.3

Estos valores se muestran en la tabla 3.8.

Tabla 3.8

i

xi

x ‚àí xi

Q i0

Q i1

Q i2

0
1
2

2.0
2.2
2.3

0.1
‚àí0.1
‚àí0.2

0.6931
0.7885
0.8329

0.7410
0.7441

0.7420

3.3

Diferencias divididas

91

En el ejemplo anterior, tenemos f(2.1) 5 ln 2.1 5 0.7419 para cuatro lugares decimales,
por lo que el error absoluto es

| f (2.1) ‚àí P2 (2.1)| = |0.7419 ‚àí 0.7420| = 10‚àí4 .
Sin embargo, f (x) = 1/x, f (x) = ‚àí1/x 2 , y f (x) = 2/x 3, por lo que la f√≥rmula de error
de Lagrange (3.3) en el teorema 3.3 nos da la cota del de error

| f (2.1) ‚àí P2 (2.1)| =
=

f (Œæ(2.1))
(x ‚àí x 0 )(x ‚àí x1 )(x ‚àí x2 )
3!
1
0.002
(0.1)(‚àí0.1)(‚àí0.2) ‚â§
= 8.3 √ó 10‚àí5 .
3
3(2)3
3 (Œæ(2.1))

Observe que el error real, 1024, excede la cota del error, 8.3 √ó 10‚àí5. Esta aparente conWUDGLFFLyQHVXQDFRQVHFXHQFLDGHORVFiOFXORVGHGtJLWRV√ÄQLWRV1RVRWURVXVDPRVODDULWm√©tica de redondeo de cuatro d√≠gitos, y la f√≥rmula del error de Lagrange (3.3) supone la
DULWPpWLFDGHGtJLWRVLQ√ÄQLWRV(VWRFDXVyTXHQXHVWURVHUURUHVUHDOHVH[FHGLHUDQHOFiOFXOR
de error te√≥rico.
‚Ä¢ Recuerde: No puede esperar mayor precisi√≥n de la proporcionada por la aritm√©tica.
(ODOJRULWPRFRQVWUX\HSRU√ÄODVODVHQWUDGDVHQHOPpWRGRGH1HYLOOH
ALGORITMO

3.1

Interpolaci√≥n iterada de Neville
Para evaluar el polinomio de interpolaci√≥n P en los diferentes n√∫meros n 1 1, x0, 7, xn en
el n√∫mero x para la funci√≥n f :

ENTRADA n√∫meros x, x0 , x1 , . . . , xn ; valores f (x0 ), f (x1 ), . . . , f (xn ) como la primera
columna Q 0,0 , Q 1,0 , . . . , Q n,0 de Q.
SALIDA

la tabla Q con P(x) = Q n,n .

Paso 1 Para i = 1, 2, . . . , n
para j = 1, 2, . . . , i
haga Q i, j =

(x ‚àí xi‚àí j )Q i, j‚àí1 ‚àí (x ‚àí xi )Q i‚àí1, j‚àí1
.
xi ‚àí xi‚àí j

Paso 2 SALIDA (Q);
PARE.
La secci√≥n Conjunto de ejercicios 3.2 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

3.3 Diferencias divididas
La interpolaci√≥n iterada se us√≥ en la secci√≥n previa para generar sucesivamente aproximaFLRQHVSROLQRPLDOHVGHJUDGRVXSHULRUHQXQSXQWRHVSHFt√ÄFR/RVPpWRGRVGHGLIHUHQFLD
dividida que se presentan en esta secci√≥n se usan para generar sucesivamente los polinomios
en s√≠ mismos.

Diferencias divididas
Suponga que Pn(x) es el en√©simo polinomio de interpolaci√≥n que concuerda con la funci√≥n f
en los diferentes n√∫meros x0, x1, 7, xn. A pesar de que este polinomio es √∫nico, existen re-

92

CAP√çTULO 3

Interpolaci√≥n y aproximaci√≥n polinomial

presentaciones algebraicas que son √∫tiles en ciertas situaciones. Las diferencias divididas de
f respecto a x0, x1, 7, xn se usan para expresar Pn(x) en la forma

Pn (x) = a0 + a1 (x ‚àí x0 ) + a2 (x ‚àí x0 )(x ‚àí x1 ) + ¬∑ ¬∑ ¬∑ + an (x ‚àí x0 ) ¬∑ ¬∑ ¬∑ (x ‚àí xn‚àí1 ), (3.5)
para constantes apropiadas a0, a1, 7, an. Para determinar la primera de estas constantes, a0,
observe que si Pn(x) se escribe en la forma de la ecuaci√≥n (3.5), entonces evaluando Pn(x) en
x0 queda s√≥lo el t√©rmino constante a0; es decir,

a0 = Pn (x0 ) = f (x0 ).
Como en muchas √°reas, Isaac
Newton es prominente en
el estudio de ecuaciones de
diferencia. Desarroll√≥ f√≥rmulas
de interpolaci√≥n desde 1675,
usando su notaci√≥n en tablas
de diferencias. Adopt√≥ un
enfoque muy general hacia las
f√≥rmulas de diferencias, por lo
que los ejemplos expl√≠citos que
produjo, incluyendo las f√≥rmulas
de Lagrange, a menudo son
conocidas con otros nombres.

Similarmente, cuando P(x) se eval√∫a en x1, los √∫nicos t√©rminos diferentes de cero en la
evaluaci√≥n de Pn(x1) son los t√©rminos constante y lineal,

f (x0 ) + a1 (x1 ‚àí x0 ) = Pn (x1 ) = f (x1 );
por lo que
a1 =

f (x1 ) ‚àí f (x0 )
.
x1 ‚àí x0

(3.6)

Ahora presentaremos la notaci√≥n de diferencias divididas, que se relaciona con la notaci√≥n 2 de Aitkens que se us√≥ en la secci√≥n 2.5. La cero√©sima diferencia dividida de la
funci√≥n f respecto a xi, denotada f >xi], es simplemente el valor de f en xi:

f [xi ] = f (xi ).

(3.7)

/DVGLIHUHQFLDVGLYLGLGDVUHVWDQWHVVHGH√ÄQHQGHPDQHUDUHFXUVLYDODprimera diferencia
dividida de f respecto a xi y xi+1 se denota f [xi , xi+1 ] \VHGH√ÄQHFRPR

f [xi , xi+1 ] =

f [xi+1 ] ‚àí f [xi ]
.
xi+1 ‚àí xi

(3.8)

La segunda diferencia dividida, f [xi , xi+1 , xi+2 ], VHGH√ÄQHFRPR

f [xi , xi+1 , xi+2 ] =

f [xi+1 , xi+2 ] ‚àí f [xi , xi+1 ]
.
xi+2 ‚àí xi

De igual forma, despu√©s de que las (k 21) -√©simas diferencias divididas,

f [xi , xi+1 , xi+2 , . . . , xi+k‚àí1 ] y

f [xi+1 , xi+2 , . . . , xi+k‚àí1 , xi+k ],

se han determinado, la k-√©sima diferencia dividida relativa a xi , xi+1 , xi+2 , . . . , xi+k es

f [xi , xi+1 , . . . , xi+k‚àí1 , xi+k ] =

f [xi+1 , xi+2 , . . . , xi+k ] ‚àí f [xi , xi+1 , . . . , xi+k‚àí1 ]
.
xi+k ‚àí xi

(3.9)

El proceso termina con la √∫nica en√©sima diferencia dividida,

f [x0 , x1 , . . . , xn ] =

f [x1 , x2 , . . . , xn ] ‚àí f [x0 , x1 , . . . , xn‚àí1 ]
.
xn ‚àí x0

Debido a la ecuaci√≥n (3.6), podemos escribir a1 = f [x0 , x1 ], justo cuando a0 se puede expresar como a0 = f (x0 ) = f [x0 ]. Por lo tanto, el polinomio de interpolaci√≥n en la ecuaci√≥n
(3.5) es

Pn (x) = f [x0 ] + f [x0 , x1 ](x ‚àí x0 ) + a2 (x ‚àí x0 )(x ‚àí x1 )
+ ¬∑ ¬∑ ¬∑ + an (x ‚àí x0 )(x ‚àí x1 ) ¬∑ ¬∑ ¬∑ (x ‚àí x n‚àí1 ).

3.3

93

Diferencias divididas

Como se puede esperar a partir de la evaluaci√≥n de a0 y a1, las constantes requeridas son

ak = f [x0 , x1 , x2 , . . . , xk ],
para cada k 5 0, 1, 7, n. Por lo que Pn (x), se puede reescribir en una forma llamada diferencias divididas de Newton:
n

Pn (x) = f [x0 ] +

f [x0 , x1 , . . . , xk ](x ‚àí x0 ) ¬∑ ¬∑ ¬∑ (x ‚àí xk‚àí1 ).

(3.10)

k=1

El valor de f >x0, x1, 7, xk] es independiente del orden de los n√∫meros x0, x1, 7, xk, como se
muestra en el ejercicio 23.
La generaci√≥n de las diferencias divididas se describe en la tabla 3.9. A partir de estos
datos, tambi√©n se pueden determinar dos cuartas y una quinta diferencia.

Tabla 3.9
x

f (x)

x0

f [x0 ]

Primeras
diferencias divididas

f [x0 , x1 ] =
x1

f [x0 , x1 , x2 ] =

f [x1 , x2 , x3 ] =

f [x2 , x3 , x4 ] =
f [x4 ] ‚àí f [x3 ]
x4 ‚àí x3

f [x4 ]

f [x3 , x4 , x5 ] =
f [x4 , x5 ] =

x5

f [x3 ] ‚àí f [x2 ]
x3 ‚àí x2

f [x3 ]
f [x3 , x4 ] =

x4

f [x2 ] ‚àí f [x1 ]
x2 ‚àí x1

f [x2 ]
f [x2 , x3 ] =

x3

f [x1 ] ‚àí f [x0 ]
x1 ‚àí x0

f [x1 ]
f [x1 , x2 ] =

x2

f [x5 ]

ALGORITMO

3.2

Segundas
diferencias divididas

f [x5 ] ‚àí f [x4 ]
x5 ‚àí x4

Terceras
diferencias divididas

f [x1 , x2 ] ‚àí f [x0 , x1 ]
x2 ‚àí x0
f [x0 , x1 , x2 , x3 ] =

f [x1 , x2 , x3 ] ‚àí f [x0 , x1 , x2 ]
x3 ‚àí x0

f [x1 , x2 , x3 , x4 ] =

f [x2 , x3 , x4 ] ‚àí f [x1 , x2 , x3 ]
x4 ‚àí x1

f [x2 , x3 , x4 , x5 ] =

f [x3 , x4 , x5 ] ‚àí f [x2 , x3 , x4 ]
x5 ‚àí x2

f [x2 , x3 ] ‚àí f [x1 , x2 ]
x3 ‚àí x1
f [x3 , x4 ] ‚àí f [x2 , x3 ]
x4 ‚àí x2
f [x4 , x5 ] ‚àí f [x3 , x4 ]
x5 ‚àí x3

F√≥rmula de las diferencias divididas de Newton
3DUDREWHQHUORVFRH√ÄFLHQWHVGHODVGLIHUHQFLDVGLYLGLGDVGHOSROLQRPLRGHLQWHUSRODFLyQP en
los (n 1 1) n√∫meros distintos x0, x1, 7, xn para la funci√≥n f :

ENTRADA
SALIDA

los n√∫meros x0 , x1 , . . . , xn ; valores f (x0 ), f (x1 ), . . . , f (xn ) conforme
F0,0 , F1,0 , . . . , Fn,0 .
los n√∫meros F0,0 , F1,1 , . . . , Fn,n donde
n

Pn (x) = F0,0 +

i‚àí1

Fi,i
i=1

Paso 1 Para i = 1, 2, . . . , n
Para j = 1, 2, . . . , i
Fi, j‚àí1 ‚àí Fi‚àí1, j‚àí1
haga Fi, j =
.
xi ‚àí xi‚àí j

(x ‚àí x j ). (Fi,i is f [x0 , x1 , . . . , xi ].)
j=0

(Fi, j = f [xi‚àí j , . . . , xi ].)

Paso 2 SALIDA (F0,0 , F1,1 , . . . , Fn,n );
PARE.
/D IRUPD GH OD VDOLGD HQ HO DOJRULWPR  VH SXHGH PRGL√ÄFDU SDUD SURGXFLU WRGDV ODV
diferencias divididas, como se muestra en el ejemplo 1.

94

CAP√çTULO 3

Interpolaci√≥n y aproximaci√≥n polinomial

Ejemplo 1

Tabla 3.10
x

f (x)

1.0
1.3
1.6
1.9
2.2

0.7651977
0.6200860
0.4554022
0.2818186
0.1103623

Tabla 3.11

Complete la tabla de diferencias divididas para los datos utilizados en el ejemplo 1 de la
secci√≥n 3.2 y reproducidos en la tabla 3.10, y construya el polinomio de interpolaci√≥n que
usa todos estos datos.
Soluci√≥n

La primera diferencia dividida relacionada con x0 y x1 es

f [x1 ] ‚àí f [x0 ]
0.6200860 ‚àí 0.7651977
= ‚àí0.4837057.
=
x1 ‚àí x0
1.3 ‚àí 1.0

f [x0 , x1 ] =

Las restantes primeras diferencias divididas se calculan de la misma forma y se muestran en
la cuarta columna en la tabla 3.11
i

xi

f [xi ]

f [xi‚àí1 , xi ]

0

1.0

0.7651977

1

1.3

0.6200860

‚àí0.4837057
‚àí0.5489460
2

1.6

0.4554022
‚àí0.5786120

3

1.9

0.2818186

f [xi‚àí2 , xi‚àí1 , xi ]

f [xi‚àí3 , . . . , xi ]

f [xi‚àí4 , . . . , xi ]

‚àí0.1087339
0.0658784
‚àí0.0494433

0.0018251
0.0680685

0.0118183
‚àí0.5715210

4

2.2

0.1103623

La segunda diferencia divida relacionada con x0, x1 y x2 es

f [x0 , x1 , x2 ] =

f [x1 , x2 ] ‚àí f [x0 , x1 ]
‚àí0.5489460 ‚àí (‚àí0.4837057)
= ‚àí0.1087339.
=
x2 ‚àí x0
1.6 ‚àí 1.0

Las restantes segundas diferencias divididas se muestran en la quinta columna de la tabla
3.11. La tercera diferencia dividida relacionada con x0, x1, x2 y x3 y la cuarta diferencia dividida relacionada con todos los puntos de datos son, respectivamente,

f [x0 , x1 , x2 , x3 ] =

f [x1 , x2 , x3 ] ‚àí f [x0 , x1 , x2 ]
‚àí0.0494433 ‚àí (‚àí0.1087339)
=
x3 ‚àí x0
1.9 ‚àí 1.0

= 0.0658784,
y
f [x0 , x1 , x2 , x3 , x4 ] =

f [x1 , x2 , x3 , x4 ] ‚àí f [x0 , x1 , x2 , x3 ]
0.0680685 ‚àí 0.0658784
=
x4 ‚àí x0
2.2 ‚àí 1.0

= 0.0018251.
Todas las entradas se dan en la tabla 3.11.
/RVFRH√ÄFLHQWHVGHODIRUPDGHGLIHUHQFLDVGLYLGLGDVKDFLDDGHODQWHGH1HZWRQGHOSRlinomio interpolante se encuentran a lo largo de la diagonal en la tabla. Este polinomio es

P4 (x) = 0.7651977 ‚àí 0.4837057(x ‚àí 1.0) ‚àí 0.1087339(x ‚àí 1.0)(x ‚àí 1.3)
+ 0.0658784(x ‚àí 1.0)(x ‚àí 1.3)(x ‚àí 1.6)
+ 0.0018251(x ‚àí 1.0)(x ‚àí 1.3)(x ‚àí 1.6)(x ‚àí 1.9).
Observe que el valor P4(1.5) 5 0.5118200 concuerda con el resultado en la tabla 3.6 para el
ejemplo 2 de la secci√≥n 3.2, ya que los polinomios son los mismos.

3.3

Diferencias divididas

95

El teorema de valor medio 1.8 aplicado a la ecuaci√≥n (3.8) cuando i 5 0,

f [x0 , x1 ] =

f (x1 ) ‚àí f (x0 )
,
x1 ‚àí x0

implica que cuando existe f , f [x0 , x1 ] = f (Œæ ) para alg√∫n n√∫mero Œæ entre x0 y x1. El
siguiente teorema generaliza este resultado.
Teorema 3.6

Suponga que f ‚àà C n [a, b] y x0 , x1 , . . . , xnVRQQ~PHURVGLVWLQWRVHQ>a, b]. Entonces existe
un n√∫mero Œæ en (a, b) con

f (n) (Œæ )
.
n!

f [x0 , x1 , . . . , xn ] =
Demostraci√≥n Sea

g(x) = f (x) ‚àí Pn (x).
Puesto que f (xi ) = Pn (xi ) para cada i = 0, 1, . . . , n, la funci√≥n g tiene n + 1 ceros distinWRVHQ>a, b]. El teorema generalizado de Rolle 1.10 implica que existe un n√∫mero Œæ en (a, b)
con g (n) (Œæ ) = 0, por lo que
0 = f (n) (Œæ ) ‚àí Pn(n) (Œæ ).
Puesto que Pn (x) es un polinomio de grado n cuyo coeficiente principal es f [x0 , x1 , . . . , xn ],
Pn(n) (x) = n! f [x0 , x1 , . . . , xn ],
para todos los valores de x. En consecuencia,
f (n) (Œæ )
.
n!

f [x0 , x1 , . . . , xn ] =

/DIyUPXODGHODVGLIHUHQFLDVGLYLGLGDVGH1HZWRQVHSXHGHH[SUHVDUHQIRUPDVLPSOL√Äcada cuando los nodos se ordenan de manera consecutiva con igual espaciado. En este caso,
introducimos la notaci√≥n h = xi+1 ‚àí xi , para cada i = 0, 1, . . . , n ‚àí 1 y sea x = x0 + sh .
Entonces la diferencia x 2 xi es x 2 xi 5 (s 2 i)h. Por lo que la ecuaci√≥n (3.10) se convierte en

Pn (x) = Pn (x0 + sh) = f [x0 ] + sh f [x0 , x1 ] + s(s ‚àí 1)h 2 f [x0 , x1 , x2 ]
+ ¬∑ ¬∑ ¬∑ + s(s ‚àí 1) ¬∑ ¬∑ ¬∑ (s ‚àí n + 1)h n f [x0 , x1 , . . . , xn ]
n

s(s ‚àí 1) ¬∑ ¬∑ ¬∑ (s ‚àí k + 1)h k f [x0 , x1 , . . . , xk ].

= f [x0 ] +
k=1

Usando la notaci√≥n de coeficiente binomial,
s
k

=

s(s ‚àí 1) ¬∑ ¬∑ ¬∑ (s ‚àí k + 1)
,
k!

podemos expresar Pn (x) de manera compacta como
n

Pn (x) = Pn (x0 + sh) = f [x0 ] +
k=1

s
k!h k f [x0 , xi , . . . , xk ].
k

(3.11)

96

CAP√çTULO 3

Interpolaci√≥n y aproximaci√≥n polinomial

Diferencias hacia adelante
La f√≥rmula de diferencias hacia adelante de Newton se construye al usar la notaci√≥n de
diferencias hacia adelante que se present√≥ en el m√©todo 2 de Aitken. Con esta notaci√≥n,

f [x0 , x1 ] =

f (x1 ) ‚àí f (x0 )
1
1
= ( f (x1 ) ‚àí f (x0 )) =
x1 ‚àí x0
h
h

f [x0 , x1 , x2 ] =

1
2h

f (x1 ) ‚àí
h

f [x0 , x1 , . . . , xk ] =

1
k!h k

k

f (x0 )

=

1
2h 2

2

f (x0 )

f (x0 ),

y, en general,
f (x0 ).

Puesto que f [x0 ] = f (x0 ), la ecuaci√≥n (3.11) tiene la siguiente forma.

F√≥rmula de diferencias hacia adelante de Newton
n

Pn (x) = f (x0 ) +
k=1

s
k

k

f (x0 )

(3.12)

Diferencias hacia atr√°s
Si los nodos de interpolaci√≥n se reordenan desde el √∫ltimo hasta el primero como
xn , xn‚àí1 , . . . , x0 podemos escribir la f√≥rmula de interpolaci√≥n como

Pn (x) = f [xn ] + f [xn , xn‚àí1 ](x ‚àí xn ) + f [xn , xn‚àí1 , xn‚àí2 ](x ‚àí xn )(x ‚àí xn‚àí1 )
+ ¬∑ ¬∑ ¬∑ + f [xn , . . . , x0 ](x ‚àí xn )(x ‚àí xn‚àí1 ) ¬∑ ¬∑ ¬∑ (x ‚àí x1 ).
Si, adem√°s, los nodos tienen el mismo espaciado con x = xn +sh y x = xi +(s +n ‚àíi)h,
entonces

Pn (x) = Pn (xn + sh)
= f [xn ] + sh f [xn , xn‚àí1 ] + s(s + 1)h 2 f [xn , xn‚àí1 , xn‚àí2 ] + ¬∑ ¬∑ ¬∑
+ s(s + 1) ¬∑ ¬∑ ¬∑ (s + n ‚àí 1)h n f [xn , . . . , x0 ].
Esto se usa para deducir una f√≥rmula com√∫nmente aplicada conocida como f√≥rmula
de diferencias hacia atr√°s de Newton. Para analizar esta f√≥rmula, necesitamos la siguiente
GH√ÄQLFLyQ
DeÔ¨Ånici√≥n 3.7

Dada la sucesi√≥n { pn }‚àû
n=0GH√ÄQDODGLIHUHQFLDKDFLDDWUiV‚àá pn (lea nabla pn) con

‚àá pn = pn ‚àí pn‚àí1 ,

para n ‚â• 1.

Las potencias superiores se definen de manera recursiva con
‚àá k pn = ‚àá(‚àá k‚àí1 pn ),

para k ‚â• 2.

La definici√≥n 3.7 implica que
f [xn , xn‚àí1 ] =

1
‚àá f (xn ),
h

f [xn , xn‚àí1 , xn‚àí2 ] =

1 2
‚àá f (xn ),
2h 2

y, en general,
f [xn , xn‚àí1 , . . . , xn‚àík ] =

1
‚àá k f (xn ).
k!h k

3.3

97

Diferencias divididas

Por consiguiente,
Pn (x) = f [xn ] + s‚àá f (xn ) +

s(s + 1) 2
s(s + 1) ¬∑ ¬∑ ¬∑ (s + n ‚àí 1) n
‚àá f (xn ) + ¬∑ ¬∑ ¬∑ +
‚àá f (xn ).
2
n!

Si extendemos la notaci√≥n del coeficiente binomial para incluir todos los valores reales de s
al tomar
‚àís
k

=

‚àís(‚àís ‚àí 1) ¬∑ ¬∑ ¬∑ (‚àís ‚àí k + 1)
s(s + 1) ¬∑ ¬∑ ¬∑ (s + k ‚àí 1)
= (‚àí1)k
,
k!
k!

entonces
Pn (x) = f [xn ]+(‚àí1)1

‚àís
‚àís
‚àís
‚àá f (xn )+(‚àí1)2
‚àá 2 f (xn )+ ¬∑ ¬∑ ¬∑ +(‚àí1)n
‚àá n f (xn ).
1
2
n

Esto nos da el siguiente resultado.

F√≥rmula de diferencias hacia adelante de Newton
n

Pn (x) = f [xn ] +

(‚àí1)k

k=1

Ilustraci√≥n

‚àís
‚àá k f (xn )
k

(3.13)

La tabla 3.12 de diferencias divididas corresponde a los datos en el ejemplo 1.

Tabla 3.12

Primeras
diferencias
divididas
1.0

0.7651977

1.3

0.6200860

Segundas
diferencias
divididas

‚àí0.4837057

‚àí0.1087339

‚àí0.5489460
1.6

‚àí0.0494433

0.4554022
‚àí0.5786120

1.9

Terceras
diferencias
divididas

Cuartas
diferencias
divididas

0.0658784
0.0018251
0.0680685

0.2818186

0.0118183
‚àí0.5715210

2.2

0.1103623

Solamente un polinomio de interpolaci√≥n de grado, a lo sumo, cuatro, usa estos cinco puntos
de datos, pero nosotros organizaremos los nodos para obtener mejores aproximaciones de
interpolaci√≥n de grados uno, dos y tres. Esto nos dar√° un sentido de precisi√≥n para la aproximaci√≥n de cuarto grado para el valor dado de x.
Si se requiere una aproximaci√≥n para f(1.1), la opci√≥n razonable para los nodos ser√≠a x0
5 1.0, x1 5 1.3, x2 5 1.6, x3 5 1.9 y x4 5 2.2, puesto que esta opci√≥n usa lo antes posible
los puntos de datos m√°s cercanos a x 5 1.1 y tambi√©n usa la cuarta diferencia dividida. Esto
1
implica que h 5 0.3 y s 5 , por lo que la f√≥rmula de diferencias dividas hacia adelante de
3

Newton se utiliza con las diferencias divididas que tienen un subrayado s√≥lido (___) en la
tabla 3.12:

1
P4 (1.1) = P4 (1.0 + (0.3))
3
1
1
= 0.7651977 + (0.3)(‚àí0.4837057) +
3
3
+

1
3

‚àí

2
3

‚àí

5
3

(0.3)3 (0.0658784)

‚àí

2
3

(0.3)2 (‚àí0.1087339)

98

CAP√çTULO 3

Interpolaci√≥n y aproximaci√≥n polinomial

+

1
3

‚àí

2
3

‚àí

5
3

‚àí

8
3

(0.3)4 (0.0018251)

= 0.7196460.
Para aproximar un valor cuando x HVWi FHUFD GHO √ÄQDO GH ORV YDORUHV WDEXODGRV GLJDPRV
x 5 2.0, de nuevo nos gustar√≠a usar lo antes posible los puntos de datos con s = ‚àí 23 y las
). Observe que
diferencias divididas en la tabla 3.12 que tienen un subrayado ondulado (
la cuarta diferencia dividida se usa en ambas f√≥rmulas:
2
P4 (2.0) = P4 2.2 ‚àí (0.3)
3

2
2
= 0.1103623 ‚àí (0.3)(‚àí0.5715210) ‚àí
3
3
‚àí

2
3

1
3

4
3

(0.3)3 (0.0680685) ‚àí

2
3

1
3
1
3

(0.3)2 (0.0118183)
4
3

7
3

(0.3)4 (0.0018251)

= 0.2238754.

Diferencias centradas
Las f√≥rmulas de diferencias hacia adelante y hacia atr√°s de Newton no son adecuadas para
aproximar f(x) cuando x se encuentra cerca del centro de la tabla porque ninguna permitir√°
que la diferencia de orden superior tenga x0 cerca de x. Existen varias f√≥rmulas de diferencias divididas para este caso, cada una de las cuales incluye situaciones en las que se pueden
usar para una m√°xima ventaja. Estos m√©todos reciben el nombre de f√≥rmulas de diferencias
centradas. Nosotros s√≥lo consideraremos una f√≥rmula de diferencias centradas, el m√©todo
de Stirling.
Para las f√≥rmulas de diferencias centradas seleccionamos x0 cerca del punto que se va a
aproximar y etiquetamos los nodos directamente bajo x0 como x1, x2, 7 y aquellos directamente arriba como x21, x22, ‚Ä¶ Con esta convenci√≥n la f√≥rmula de Stirling est√° dada por

Pn (x) = P2m+1 (x) = f [x0 ] +
+
James Stirling (1692‚Äì1770)
public√≥ √©sta y muchas otras
f√≥rmulas en Methodus
Differentialis en 1720. En este
trabajo se incluyen las t√©cnicas
para acelerar la convergencia de
diferentes series.

sh
( f [x‚àí1 , x0 ] + f [x0 , x1 ]) + s 2 h 2 f [x‚àí1 , x0 , x1 ]
2

(3.14)

s(s 2 ‚àí 1)h 3
f [x‚àí2 , x‚àí1 , x0 , x1 ] + f [x‚àí1 , x0 , x1 , x2 ])
2

+ ¬∑ ¬∑ ¬∑ + s 2 (s 2 ‚àí 1)(s 2 ‚àí 4) ¬∑ ¬∑ ¬∑ (s 2 ‚àí (m ‚àí 1)2 )h 2m f [x‚àím , . . . , xm ]
+

s(s 2 ‚àí 1) ¬∑ ¬∑ ¬∑ (s 2 ‚àí m 2 )h 2m+1
( f [x‚àím‚àí1 , . . . , xm ] + f [x‚àím , . . . , xm+1 ]),
2

si n 5 2m 1 1 es impar. Si n 5 2m es par, usamos la misma f√≥rmula pero borramos la √∫ltima
l√≠nea. Las entradas utilizadas para esta f√≥rmula est√°n subrayadas en la tabla 3.13.

Tabla 3.13
x

f (x)

x‚àí2

f [x‚àí2 ]

x‚àí1

f [x‚àí1 ]

x0

f [x0 ]

x1

f [x1 ]

x2

f [x2 ]

Primeras
diferencias
divididas
f [x‚àí2 , x‚àí1 ]
f [x‚àí1 , x0 ]
f [x0 , x1 ]
f [x1 , x2 ]

Segundas
diferencias
divididas

f [x‚àí2 , x‚àí1 , x0 ]
f [x‚àí1 , x0 , x1 ]
f [x0 , x1 , x2 ]

Terceras
diferencias
divididas

f [x‚àí2 , x‚àí1 , x0 , x1 ]
f [x‚àí1 , x0 , x1 , x2 ]

Cuartas
diferencias
divididas

f [x‚àí2 , x‚àí1 , x0 , x1 , x2 ]

3.4

Ejemplo 2

Interpolaci√≥n de Hermite

99

Considere la tabla de datos dada en los ejemplos anteriores. Use la f√≥rmula de Stirling para
aproximar f(1.5) con x0 5 1.6.
Soluci√≥n Para aplicar la f√≥rmula de Stirling, usamos las entradas subrayadas en la tabla de
diferencias 3.14.

Tabla 3.14

Primeras
diferencias
divididas

x

f (x)

1.0

0.7651977

1.3

0.6200860

Segundas
diferencias
divididas

‚àí0.4837057

0.0658784
‚àí0.0494433

0.4554022
‚àí0.5786120

1.9

Cuartas
diferencias
divididas

‚àí0.1087339

‚àí0.5489460
1.6

Terceras
diferencias
divididas

0.0018251
0.0680685

0.2818186

0.0118183
‚àí0.5715210

2.2

0.1103623

La f√≥rmula con, h = 0.3, x0 = 1.6 y s = ‚àí 13 , se convierte en
f (1.5) ‚âà P4 1.6 + ‚àí

1
3

(0.3)

= 0.4554022 + ‚àí
+ ‚àí

1
3

1
2

‚àí

+ ‚àí

1
3

+

1
3

0.3
2

((‚àí0.5489460) + (‚àí0.5786120))

2

(0.3)2 (‚àí0.0494433)
1
3

‚àí

2

‚àí

1
3

2

1
3

‚àí 1 (0.3)3 (0.0658784 + 0.0680685)
2

‚àí 1 (0.3)4 (0.0018251) = 0.5118200.

Muchos textos sobre an√°lisis num√©rico, escritos antes del uso generalizado de las computadoras, incluyen amplios tratamientos de los m√©todos de diferencias divididas. Si se necesita
XQWUDWDPLHQWRPiVH[KDXVWLYRGHHVWHWHPDHOOLEURGH+LOGHEUDQG>+LOG@HVXQDUHIHUHQFLD
especialmente buena.
La secci√≥n Conjunto de ejercicios 3.3 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

3.4 Interpolaci√≥n de Hermite
Cuando la palabra latina
osculum, literalmente ‚Äúboca
peque√±a‚Äù o ‚Äúbeso‚Äù, se aplica
a una curva, indica que s√≥lo la
toca y tiene la misma forma.
La interpolaci√≥n de Hermite
tiene esta propiedad osculante.
Corresponde a una curva dada y
su derivada obliga a que la curva
de interpolaci√≥n ‚Äúbese‚Äù a la
curva dada

Los polinomios osculantes generalizan tanto los polinomios de Taylor como los polinomios
de Lagrange. Suponga que tenemos n + 1 n√∫meros distintos x0, x1, 7, xnHQ>a, b] y enteros
no negativos m 0 , m 1 , . . . , m n , y m = m√°x{m 0 , m 1 , . . . , m n }. El polinomio osculante que
aproxima una funci√≥n f ‚àà C m [a, b] en xi, para cada i 5 0, 7, n, es el polinomio de grado
m√≠nimo que tiene los mismos valores que la funci√≥n f y todas sus derivadas de orden menor
que o igual que mi en cada xi. El grado de este polinomio osculante es el m√°ximo
n

M=

mi + n
i=0

100

CAP√çTULO 3

Interpolaci√≥n y aproximaci√≥n polinomial
n
Ya que el n√∫mero de condiciones que se satisfacen es i=0 m i + (n + 1) y un polinomio
de grado M tiene M 1 1 FRH√ÄFLHQWHVTXHVHSXHGHQXVDUSDUDVDWLVIDFHUHVWDVFRQGLFLRQHV

DeÔ¨Ånici√≥n 3.8
Charles Hermite (1822‚Äì
1901) realiz√≥ importantes
descubrimientos matem√°ticos
a lo largo de su vida en √°reas
como el an√°lisis complejo y la
teor√≠a num√©rica, especialmente
en relaci√≥n con la teor√≠a de las
ecuaciones. Es, tal vez, mejor
conocido por probar, en 1873,
que e es trascendental; es decir,
no es la soluci√≥n para cualquier
ecuaci√≥n algebraica que tenga
FRH√ÄFLHQWHVHQWHURV(VWR
condujo, en 1882, a la prueba de
Lindemann que establece que
¬õ tambi√©n es trascendental, lo
cual demostr√≥ que es imposible
utilizar las herramientas de la
geometr√≠a est√°ndar de Euclides
para construir un cuadrado que
tenga la misma √°rea que un
c√≠rculo unitario.

Teorema 3.9

Sean x0, x1, 7, xn n 1Q~PHURVGLVWLQWRVHQ>a, b] y para cada i 5 0, 1, 7, n, sea mi un
entero no negativo. Suponga que f ‚àà C m [a, b], donde m = m√°x0‚â§i‚â§n m i .
El polinomio osculante que se aproxima a f es el polinomio P(x) de menor grado, tal que

d k P(xi )
d k f (xi )
=
,
k
dx
dxk

para cada i = 0, 1, . . . , n

Observe que cuando n 5 0, el polinomio osculante que se aproxima a f es el m0-√©simo
polinomio de Taylor para f en x0. Cuando mi 5 0 para cada i, el polinomio osculante es el
en√©simo polinomio de Lagrange que interpola f en x0, x1, 7, xn.

Polinomios de Hermite
Cuando mi 5 1, para cada i 5 0, 1, 7, n, nos da los polinomios de Hermite. Para una funci√≥n f determinada, estos polinomios concuerdan con f en x0, x1, 7, xn. Adem√°s, puesto que
sus primeras derivadas concuerdan con las de f, tienen la misma ‚Äúforma‚Äù que la funci√≥n en
(xi, f(xi)), en el sentido en el que las rectas tangentes al polinomio y la funci√≥n concuerdan.
Nosotros limitaremos nuestro estudio de los polinomios osculantes a esta situaci√≥n y primero consideraremos un teorema que describe de manera precisa la forma de los polinomios
de Hermite.
Si f ‚àà C 1 [a, b] y x0 , . . . , xn ‚àà [a, b] son distintos, el √∫nico polinomio de menor grado que
concuerda con f y f 9 en x0, 7, xn es el polinomio de Hermite de grado a lo sumo 2n 1 1
dado por
n

H2n+1 (x) =

n

f (x j )Hn, j (x) +
j=0

En 1878, Hermite dio una
descripci√≥n de un polinomio
osculante general en una carta
SDUD&DUO:%RUFKDUGWD
quien regularmente enviaba
sus resultados nuevos. Su
demostraci√≥n es una aplicaci√≥n
interesante del uso de t√©cnicas
complejas de integraci√≥n para
resolver un problema de valor
real.

y k = 0, 1, . . . , m i .

f (x j ) HÃÇn, j (x),
j=0

Donde cada Ln, j(x) denota el jpVLPRFRH√ÄFLHQWHGHOSROLQRPLRGH/DJUDQJHGHJUDGRn, y

Hn, j (x) = [1 ‚àí 2(x ‚àí x j )L n, j (x j )]L 2n, j (x)

y

HÃÇn, j (x) = (x ‚àí x j )L 2n, j (x).

Adem√°s, si f ‚àà C 2n+2 [a, b], entonces
f (x) = H2n+1 (x) +

(x ‚àí x0 )2 . . . (x ‚àí xn )2 (2n+2)
(Œæ(x)),
f
(2n + 2)!

para algunos (en general desconocidos) Œæ(x) en el intervalo (a, b).
Demostraci√≥n Primero, recuerde que

L n, j (xi ) =

0, si i = j,
1, si i = j.

Por lo tanto, cuando i = j,
Hn, j (xi ) = 0

y

HÃÇn, j (xi ) = 0,

y

HÃÇn,i (xi ) = (xi ‚àí xi ) ¬∑ 12 = 0.

Mientras que, para cada i,
Hn,i (xi ) = [1 ‚àí 2(xi ‚àí xi )L n,i (xi )] ¬∑ 1 = 1

3.4

Interpolaci√≥n de Hermite

101

Por consiguiente,
n

H2n+1 (xi ) =

n

f (x j ) ¬∑ 0 + f (xi ) ¬∑ 1 +
j=0
j =i

f (x j ) ¬∑ 0 = f (xi ),
j=0

de modo que H2n+1 concuerda con f en x0 , x1 , . . . , xn .
Para mostrar la concordancia de H2n+1 con f en los nodos, primero observe que Ln, j (x)
es un factor de Hn, j (x), por lo que Hn, j (xi ) = 0 cuando i = j Adem√°s, cuando i 5 j, tenemos L n,i (xi ) = 1, por lo que

Hn,i (xi ) = ‚àí2L n,i (xi ) ¬∑ L 2n,i (xi ) + [1 ‚àí 2(xi ‚àí xi )L n,i (xi )]2L n,i (xi )L n,i (xi )
= ‚àí2L n,i (xi ) + 2L n,i (xi ) = 0.
Por lo tanto, Hn, j (xi ) = 0 para todas las i y j.
Finalmente,
HÃÇn, j (xi ) = L 2n, j (xi ) + (xi ‚àí x j )2L n, j (xi )L n, j (xi )
= L n, j (xi )[L n, j (xi ) + 2(xi ‚àí x j )L n, j (xi )],
por lo que HÃÇn, j (xi ) = 0 si i = j y HÃÇn,i (xi ) = 1. Al combinar estos hechos, tenemos
n

H2n+1 (xi ) =

n

f (x j ) ¬∑ 0 +
j=0

f (x j ) ¬∑ 0 + f (xi ) ¬∑ 1 = f (xi ).
j=0
j =i

Por lo tanto, H2n+1 concuerda con f y H2n+1 con f en x0 , x1 , . . . , xn .
La unicidad de este polinomio y la deducci√≥n de la f√≥rmula del error se consideran en
el ejercicio 11.
Ejemplo 1

Use el polinomio de Hermite que concuerda con los datos listados en la tabla 3.5 para encontrar una aproximaci√≥n de f (1.5).

Tabla 3.15

k

xk

f (xk )

f (xk )

0
1
2

1.3
1.6
1.9

0.6200860
0.4554022
0.2818186

‚àí0.5220232
‚àí0.5698959
‚àí0.5811571

Soluci√≥n

Primero calculamos los polinomios de Lagrange y sus derivadas. Esto nos da

L 2,0 (x) =

50 2 175
152
(x ‚àí x1 )(x ‚àí x2 )
=
x ‚àí
x+
,
(x0 ‚àí x1 )(x0 ‚àí x2 )
9
9
9

L 2,1 (x) =

(x ‚àí x0 )(x ‚àí x2 )
‚àí200
‚àí100 2 320
247
320
=
x +
x‚àí
, L 2,1 (x) =
x+
;
(x1 ‚àí x0 )(x1 ‚àí x2 )
9
9
9
9
9

L 2,0 (x) =

175
100
x‚àí
;
9
9

y
L 2,2 =

50 2 145
104
(x ‚àí x0 )(x ‚àí x1 )
=
x ‚àí
x+
,
(x2 ‚àí x0 )(x2 ‚àí x1 )
9
9
9

L 2,2 (x) =

145
100
x‚àí
.
9
9

102

CAP√çTULO 3

Interpolaci√≥n y aproximaci√≥n polinomial

Los polinomios H2, j (x) y HÃÇ2, j (x) son entonces
H2,0 (x) = [1 ‚àí 2(x ‚àí 1.3)(‚àí5)]

‚àí100 2 320
247
x +
x‚àí
9
9
9

2

2

152
50 2 175
x ‚àí
x+
9
9
9

= (10x ‚àí 12)
H2,1 (x) = 1 ¬∑

152
50 2 175
x ‚àí
x+
9
9
9
,

2

,

104
50 2 145
x ‚àí
x+
9
9
9

2

H2,2 (x) = 10(2 ‚àí x)

152
50 2 175
x ‚àí
x+
9
9
9

2

HÃÇ2,0 (x) = (x ‚àí 1.3)
HÃÇ2,1 (x) = (x ‚àí 1.6)

‚àí100 2 320
247
x +
x‚àí
9
9
9

HÃÇ2,2 (x) = (x ‚àí 1.9)

50 2 145
104
x ‚àí
x+
9
9
9

,
,
2

,

y
2

.

Finalmente,
H5 (x) = 0.6200860H2,0 (x) + 0.4554022H2,1 (x) + 0.2818186H2,2 (x)
‚àí 0.5220232 HÃÇ2,0 (x) ‚àí 0.5698959 HÃÇ2,1 (x) ‚àí 0.5811571 HÃÇ2,2 (x)
y
4
27

H5 (1.5) = 0.6200860
‚àí 0.5220232

+ 0.4554022

64
81

+ 0.2818186

5
81

4
‚àí32
‚àí 0.5698959
‚àí 0.5811571
405
405

‚àí2
= 0.5118277,
405

un resultado que es apropiado para los lugares enumerados.
A pesar de que el teorema 3.9 proporciona una descripci√≥n completa de los polinomios
de Hermite, a partir del ejemplo 1 es claro que la necesidad de determinar y evaluar los polinomios de Lagrange y sus derivadas hace que el procedimientos sea tedioso para los valores
peque√±os de n.

Polinomios de Hermite usando diferencias divididas
Existe un m√©todo alterno para generar aproximaciones de Hermite que tiene sus bases en la
f√≥rmula de diferencias divididas de interpolaci√≥n de Newton (3.10) en x0, x1, 7, xn; esto es,
n

Pn (x) = f [x0 ] +

f [x0 , x1 , . . . , xk ](x ‚àí x0 ) ¬∑ ¬∑ ¬∑ (x ‚àí xk‚àí1 ).
k=1

El m√©todo alterno utiliza la conexi√≥n entre la en√©sima diferencia dividida y la en√©sima derivada de f, como se describe en el teorema 3.6 en la secci√≥n 3.3.
Suponga que los diferentes n√∫meros x0, x1, 7, xn est√°n dados junto con los valores de f
y f9HQHVWRVQ~PHURV'H√ÄQDXQDQXHYDVXFHVLyQz0, z1, ‚Ä¶, z2n11 mediante

z 2i = z 2i+1 = xi ,

para cada i = 0, 1, . . . , n,

3.4

Interpolaci√≥n de Hermite

103

y construya la tabla de diferencias divididas en la forma de la tabla 3.9 que usa z0, z1, ‚Ä¶,
z2n11.
Puesto que z 2i = z 2i+1 = xi para cada iQRSRGHPRVGH√ÄQLU f [z 2i , z 2i+1 ] con la f√≥rmula de diferencias divididas. Sin embargo, si suponemos, con base en el teorema 3.6, que la
sustituci√≥n razonable en estas situaciones es f [z 2i , z 2i+1 ] = f (z 2i ) = f (xi ), podemos usar
las entradas

f (x0 ), f (x1 ), . . . , f (xn )
HQOXJDUGHODVSULPHUDVGLIHUHQFLDVGLYLGLGDVQRGH√ÄQLGDV

f [z 0 , z 1 ], f [z 2 , z 3 ], . . . , f [z 2n , z 2n+1 ].
Las diferencias divididas restantes se producen de la manera com√∫n y las diferencias divididas adecuadas se usan en la f√≥rmula de diferencias divididas de interpolaci√≥n de Newton. La
tabla 3.16 muestra las entradas que se utilizan para las primeras tres columnas de diferencias
divididas al determinar el polinomio de Hermite H5(x) para x0, x1 y x2. Las entradas restantes
se generan de la manera que se muestra en la tabla 3.9. El polinomio de Hermite est√° dado por
2n+1

H2n+1 (x) = f [z 0 ] +

f [z 0 , . . . , z k ](x ‚àí z 0 )(x ‚àí z 1 ) ¬∑ ¬∑ ¬∑ (x ‚àí z k‚àí1 ).
k=1

8QDSUXHEDGHHVWHKHFKRSXHGHHQFRQWUDUVHHQ>3RZ@S

Tabla 3.16
z

f (z)

z 0 = x0

f [z 0 ] = f (x0 )

z 1 = x0

f [z 1 ] = f (x0 )

Primeras diferencias
divididas
f [z 0 , z 1 ] = f (x0 )

f [z 1 , z 2 ] =
z 2 = x1

Segundas diferencias
divididas

f [z 0 , z 1 , z 2 ] =

f [z 1 , z 2 ] ‚àí f [z 0 , z 1 ]
z2 ‚àí z0

f [z 1 , z 2 , z 3 ] =

f [z 2 , z 3 ] ‚àí f [z 1 , z 2 ]
z3 ‚àí z1

f [z 2 , z 3 , z 4 ] =

f [z 3 , z 4 ] ‚àí f [z 2 , z 3 ]
z4 ‚àí z2

f [z 3 , z 4 , z 5 ] =

f [z 4 , z 5 ] ‚àí f [z 3 , z 4 ]
z5 ‚àí z3

f [z 2 ] ‚àí f [z 1 ]
z2 ‚àí z1

f [z 2 ] = f (x1 )
f [z 2 , z 3 ] = f (x1 )

z 3 = x1

f [z 3 ] = f (x1 )
f [z 3 , z 4 ] =

Ejemplo 2

z 4 = x2

f [z 4 ] = f (x2 )

z 5 = x2

f [z 5 ] = f (x2 )

f [z 4 ] ‚àí f [z 3 ]
z4 ‚àí z3

f [z 4 , z 5 ] = f (x2 )

Use los datos que se proporcionan en el ejemplo 1 y el m√©todo de diferencias divididas para
determinar la aproximaci√≥n polinomial de Hermite en x 5 1.5.
Soluci√≥n

Las entradas subrayadas en las primeras tres columnas de la tabla 3.17 son los
datos que se proporcionaron en el ejemplo 1. Las entradas restantes en esta tabla se generan
con la f√≥rmula de diferencias divididas est√°ndar (3.9).
Por ejemplo, para la segunda entrada en la tercera columna usamos la segunda entrada
1.3 en la segunda columna y la primera entrada 1.6 en esa columna para obtener

0.4554022 ‚àí 0.6200860
= ‚àí0.5489460.
1.6 ‚àí 1.3

104

CAP√çTULO 3

Interpolaci√≥n y aproximaci√≥n polinomial

Para la primera entrada en la cuarta columna, usamos la primera entrada 1.3 en la tercera
columna y la primera entrada 1.6 en esa columna para obtener

‚àí0.5489460 ‚àí (‚àí0.5220232)
= ‚àí0.0897427.
1.6 ‚àí 1.3
El valor del polinomio de Hermite en 1.5 es

H5 (1.5) = f [1.3] + f (1.3)(1.5 ‚àí 1.3) + f [1.3, 1.3, 1.6](1.5 ‚àí 1.3)2
+ f [1.3, 1.3, 1.6, 1.6](1.5 ‚àí 1.3)2 (1.5 ‚àí 1.6)
+ f [1.3, 1.3, 1.6, 1.6, 1.9](1.5 ‚àí 1.3)2 (1.5 ‚àí 1.6)2
+ f [1.3, 1.3, 1.6, 1.6, 1.9, 1.9](1.5 ‚àí 1.3)2 (1.5 ‚àí 1.6)2 (1.5 ‚àí 1.9)
= 0.6200860 + (‚àí0.5220232)(0.2) + (‚àí0.0897427)(0.2)2
+ 0.0663657(0.2)2 (‚àí0.1) + 0.0026663(0.2)2 (‚àí0.1)2
+ (‚àí0.0027738)(0.2)2 (‚àí0.1)2 (‚àí0.4)
= 0.5118277.
Tabla 3.17

1.3

0.6200860

1.3

0.6200860

‚àí0.5220232
‚àí0.5489460
1.6

0.4554022
‚àí0.5698959

1.6

0.4554022

1.9

0.2818186

1.9

0.2818186

‚àí0.5786120
‚àí0.5811571

‚àí0.0897427
0.0663657
‚àí0.0698330

0.0026663
‚àí0.0027738

0.0679655
‚àí0.0290537

0.0010020
0.0685667

‚àí0.0084837

La t√©cnica que se usa en el algoritmo 3.3 se puede ampliar para su uso en la determinaci√≥n de otros polinomios osculantes. Es posible encontrar un an√°lisis conciso de los proceGLPLHQWRVHQ>3RZ@SS¬≤

ALGORITMO

3.3

Interpolaci√≥n de Hermite
3DUDREWHQHUORVFRH√ÄFLHQWHVGHOSROLQRPLRGHLQWHUSRODFLyQGH+HUPLWHH(x) en los (n 1 1)
n√∫meros distintos x0, 7, xn para la funci√≥n f :

ENTRADA n√∫meros x0 , x1 , . . . , xn ; valores f (x0 ), . . . , f (xn ) y f (x0 ), . . . , f (xn ).
SALIDA los n√∫meros Q 0,0 , Q 1,1 , . . . , Q 2n+1,2n+1 donde
H (x) = Q 0,0 + Q 1,1 (x ‚àí x0 ) + Q 2,2 (x ‚àí x0 )2 + Q 3,3 (x ‚àí x0 )2 (x ‚àí x1 )
+ Q 4,4 (x ‚àí x0 )2 (x ‚àí x1 )2 + ¬∑ ¬∑ ¬∑
+ Q 2n+1,2n+1 (x ‚àí x0 )2 (x ‚àí x1 )2 ¬∑ ¬∑ ¬∑ (x ‚àí xn‚àí1 )2 (x ‚àí xn ).
Paso 1 Para i = 0, 1, . . . , n haga los pasos 2 y 3.
Paso 2 Haga z 2i = xi ;
z 2i+1 = xi ;
Q 2i,0 = f (xi );
Q 2i+1,0 = f (xi );
Q 2i+1,1 = f (xi ).

3.5 Interpolaci√≥n de spline c√∫bico

105

Paso 3 Si i = 0 entonces haga
Q 2i,1 =

Q 2i,0 ‚àí Q 2i‚àí1,0
.
z 2i ‚àí z 2i‚àí1

Paso 4 Para i = 2, 3, . . . , 2n + 1
para j = 2, 3, . . . , i haga Q i, j =

Q i, j‚àí1 ‚àí Q i‚àí1, j‚àí1
.
z i ‚àí z i‚àí j

Paso 5 SALIDA (Q 0,0 , Q 1,1 , . . . , Q 2n+1,2n+1 );
PARE.

3.5 Interpolaci√≥n de spline c√∫bico1
Las secciones previas se preocuparon por la aproximaci√≥n de las funciones arbitrarias en
intervalos cerrados usando un polinomio individual. Sin embargo, los polinomios de orden
VXSHULRUSXHGHQRVFLODUHUUiWLFDPHQWHHVGHFLUXQD√ÅXFWXDFLyQPHQRUVREUHXQDSHTXHxD
SDUWHGHOLQWHUYDORSXHGHLQGXFLU√ÅXFWXDFLRQHVJUDQGHVVREUHWRGRHOUDQJR2EVHUYDUHPRV
XQEXHQHMHPSORGHHVWRHQOD√ÄJXUDDO√ÄQDOGHHVWDVHFFLyQ
Un enfoque alternativo es dividir el intervalo de aproximaci√≥n en un conjunto de subintervalos y construir (en general) un polinomio de aproximaci√≥n diferente en cada subintervalo. Esto se llama aproximaci√≥n polinomial por tramos.

Aproximaci√≥n de polinomio por tramos
La aproximaci√≥n polinomial por tramos m√°s simple es la interpolaci√≥n lineal por tramos, la
cual consiste en unir un conjunto de puntos de datos

{(x0 , f (x0 )), (x1 , f (x1 )), . . . , (xn , f (xn ))}
PHGLDQWHXQDVHULHGHOtQHDVUHFWDVFRPRVHPXHVWUDHQOD√ÄJXUD
Figura 3.7
y

y 5 f (x)

x1

x0

x2

...

xj

x j11

x j12

...

x n21

xn

x

Una desventaja de la aproximaci√≥n mediante funciones lineales es que probablemente no existe diferenciabilidad en los extremos de los subintervalos, lo que, en un contexto
1

Las pruebas de los teoremas en esta secci√≥n dependen de los resultados en el cap√≠tulo 6.

106

CAP√çTULO 3

Interpolaci√≥n y aproximaci√≥n polinomial

Isaac Jacob Schoenberg (1903‚Äì
1990) desarroll√≥ su trabajo
sobre splines durante la Segunda
Guerra Mundial mientras estaba
de licencia en la Universidad de
Pennsylvania para trabajar en
HO$UP\¬∑V%DOOLVWLF5HVHDUFK
Laboratory (Laboratorio de
,QYHVWLJDFLyQGH%DOtVWLFDGHO
Ej√©rcito) en Aberdeen, Maryland.
Su trabajo original implicaba
procedimientos num√©ricos
para resolver ecuaciones
diferenciales. La aplicaci√≥n
mucho m√°s amplia de los splines
en las √°reas de ajuste de datos
y dise√±o de geometr√≠a asistida
por computador se volvieron
evidentes con la disponibilidad
generalizada de las computadoras
en la d√©cada de 1960.
La ra√≠z de la palabra ‚Äúspline‚Äù
es la misma que la de ‚Äúsplint‚Äù.
Originalmente, era una tira
peque√±a de madera que se puede
utilizar para unir dos tablas. M√°s
adelante, la palabra se us√≥ para
dibujar curvas suaves y continuas
al forzar que la tira pasara a
WUDYpVGHSXQWRVHVSHFt√ÄFRV\
siguiera a lo largo de la curva.

JHRPpWULFR VLJQL√ÄFD TXH OD IXQFLyQ GH LQWHUSRODFLyQ QR HV ¬¥VXDYH¬µ$ PHQXGR HV FODUR
a partir de las condiciones f√≠sicas, que se requiere suavidad, por lo que la funci√≥n de aproximaci√≥n debe ser continuamente diferenciable.
Un procedimiento alterno es usar un polinomio por tramos de tipo Hermite. Por
ejemplo, si se conocen los valores de f y de f 9 en cada uno de los puntos x0 , x1 , 7
, xn, se puede usar un polinomio c√∫bico de Hermite en cada uno de los subintervalos
[x0 , x1 ], [x1 , x2 ], . . . , [xn‚àí1 , xn ] para obtener una funci√≥n que tiene una derivada continua
HQXQLQWHUYDOR>x0, xn].
Determinar el polinomio c√∫bico de Hermite adecuado en un intervalo dado es simplemente cuesti√≥n de calcular H3(x) para ese intervalo. Los polinomios de interpolaci√≥n de
Lagrange necesarios para determinar H3, son de primer grado, por lo que esto se puede lograr
VLQPD\RUGL√ÄFXOWDG6LQHPEDUJRSDUDXVDUORVSROLQRPLRVSRUWUDPRVGH+HUPLWHSDUDOD
interpolaci√≥n general, necesitamos conocer la derivada de la funci√≥n que se va a aproximar
y esto con frecuencia no est√° disponible.
El resto de esta secci√≥n considera que la aproximaci√≥n usa polinomios por tramos que
QRUHTXLHUHQLQIRUPDFLyQHVSHFt√ÄFDVREUHODGHULYDGDH[FHSWRWDOYH]HQORVH[WUHPRVGHO
intervalo en el que la funci√≥n se aproxima.
El tipo m√°s simple de funci√≥n polinomial por tramos diferenciable en un intervalo comSOHWR>x0, x1] es la funci√≥n obtenida al ajustar un polinomio cuadr√°tico entre cada par suFHVLYRGHQRGRV(VWRVHKDFHDOFRQVWUXLUXQDFXDGUiWLFDHQ>x0, x1] que concuerda con la
funci√≥n en x0 y x1,RWUDFXDGUiWLFDHQ>x1, x2] que concuerda con la funci√≥n en x1 y x2, y as√≠
sucesivamente. Un polinomio cuadr√°tico general tiene tres constantes arbitrarias: el t√©rmino
FRQVWDQWHHOFRH√ÄFLHQWHGHx \HOFRH√ÄFLHQWHGHx2, y s√≥lo se requieren dos condiciones para
DMXVWDUORVGDWRVHQORVH[WUHPRVGHFDGDVXELQWHUYDOR3RUORWDQWRH[LVWH√ÅH[LELOLGDGTXH
permite seleccionar las cuadr√°ticas de tal forma que el interpolante tenga una derivada contiQXDHQ>x0, xn@/DGL√ÄFXOWDGVXUJHSRUTXHJHQHUDOPHQWHQHFHVLWDPRVHVSHFL√ÄFDUFRQGLFLRQHV
sobre la derivada del interpolante en los extremos x0 y xn1RKD\XQQ~PHURVX√ÄFLHQWHGH
constantes para garantizar que las condiciones se satisfagan (consulte el ejercicio 34).

Splines c√∫bicos
La aproximaci√≥n polinomial por tramos m√°s com√∫n usa polinomios c√∫bicos entre cada par
sucesivo de nodos y recibe el nombre de interpolaci√≥n de spline c√∫bico. Un polinomio
F~ELFRJHQHUDOLPSOLFDFXDWURFRQVWDQWHVSRUORTXHH[LVWHVX√ÄFLHQWH√ÅH[LELOLGDGHQHOSURFHdimiento de spline c√∫bico para garantizar que el interpolante no s√≥lo es continuamente diferenciable en el intervalo, sino tambi√©n tiene una segunda derivada continua. Sin embargo, la
construcci√≥n del spline c√∫bico no supone que las derivadas del interpolante concuerdan con
ODVGHODIXQFLyQHQVXDSUR[LPDFLyQLQFOXVRHQORVQRGRV FRQVXOWHOD√ÄJXUD

Figura 3.8
S(x)
S n22
Sj

S1

S n21

S j11

S0
S j (x j11) 5 f (x j11) 5 S j11(x j11)
S 9j (x j11) 5 S9j11(x j11)
S j0(x j11) 5 S j11
0 (x j11)

x0

x1

x2

...

xj

x j11

x j12

...

x n22 x n21 x n

x

3.5 Interpolaci√≥n de spline c√∫bico

DeÔ¨Ånici√≥n 3.10

Un spline natural no tiene
condiciones impuestas para la
direcci√≥n en sus extremos, por
lo que la curva toma la forma de
una l√≠nea recta despu√©s de pasar
por los puntos de interpolaci√≥n
m√°s cercanos a sus extremos.
El nombre deriva del hecho de
que √©sta es la forma natural que
DVXPHXQDWLUD√ÅH[LEOHVLHV
forzada a pasar por los puntos
GHLQWHUSRODFLyQHVSHFt√ÄFRV
sin restricciones adicionales
FRQVXOWHOD√ÄJXUD

Figura 3.9

Ejemplo 1

107

Dada una funci√≥n f GH√ÄQLGDHQ>a, b] y un conjunto de nodos a = x0 < x1 < ¬∑ ¬∑ ¬∑ < xn = b,
un interpolante de spline c√∫bico S para f es una funci√≥n que satisface las siguientes condiciones:

a)

S(x) es un polinomio c√∫bico, que se denota S j (x), en el subintervalo [x j , x j+1 ] para
cada j = 0, 1, . . . , n ‚àí 1;

b)

S j (x j ) = f (x j ) y S j (x j+1 ) = f (x j+1 ) para cada j = 0, 1, . . . , n ‚àí 1;

c)

S j+1 (x j+1 ) = S j (x j+1 ) para cada j = 0, 1, . . . , n ‚àí 2; (impl√≠cito en b).)

d)

S j+1 (x j+1 ) = S j (x j+1 ) para cada j = 0, 1, . . . , n ‚àí 2;

e)

S j+1 (x j+1 ) = S j (x j+1 ) para cada j = 0, 1, . . . , n ‚àí 2;

f)

Uno de los siguientes conjuntos de condiciones de frontera se satisface:
i)
ii)

S (x0 ) = S (xn ) = 0 (frontera natural ) (o libre) ;
S (x0 ) = f (x0 ) y S (xn ) = f (xn ) (frontera condicionada).

$XQTXHORVVSOLQHVF~ELFRVVHGH√ÄQHQFRQRWUDVFRQGLFLRQHVGHIURQWHUDODVFRQGLFLRQHV
GDGDVHQODSDUWHI VRQVX√ÄFLHQWHVSDUDQXHVWURVSURSyVLWRV&XDQGRVHSUHVHQWDQFRQGLFLRnes de frontera libres, el spline recibe el nombre de spline natural\VXJUi√ÄFDVHDSUR[LPD
DODIRUPDTXHXQDYDULOOD√ÅH[LEOH\ODUJDDVXPLUtDVLIXHUDIRU]DGDDSDVDUSRUORVSXQWRVGH
datos {(x0 , f (x0 )), (x1 , f (x1 )), . . . , (xn , f (xn ))}.
En general, las condiciones de frontera condicionada conducen a aproximaciones m√°s
precisas porque incluyen m√°s informaci√≥n sobre la funci√≥n. Sin embargo, para mantener
este tipo de condici√≥n de frontera, es necesario tener, ya sea los valores de la derivada en los
extremos o una aproximaci√≥n precisa para esos valores.
Construya un spline c√∫bico natural que pase por los puntos (1, 2), (2, 3) y (3, 5).
Soluci√≥n (VWH VSOLQH FRQVLVWH HQ GRV F~ELFRV (O SULPHUR SDUD HO LQWHUYDOR > @ TXH VH

denota

S0 (x) = a0 + b0 (x ‚àí 1) + c0 (x ‚àí 1)2 + d0 (x ‚àí 1)3 ,
\HORWURSDUD>@TXHVHGHQRWD

S1 (x) = a1 + b1 (x ‚àí 2) + c1 (x ‚àí 2)2 + d1 (x ‚àí 2)3 .
Existen ocho constantes que se van a determinar y esto requiere ocho condiciones. Cuatro
condiciones a partir del hecho de que los splines deben concordar con los datos en los nodos.
Por lo tanto,

2 = f (1) = a0 ,

3 = f (2) = a0 + b0 + c0 + d0 ,

3 = f (2) = a1 ,

y

5 = f (3) = a1 + b1 + c1 + d1 .
Dos m√°s provienen del hecho de que S0 (2) = S1 (2) y S0 (2) = S1 (2). Estos son

S0 (2) = S1 (2) :

b0 + 2c0 + 3d0 = b1

y

S0 (2) = S1 (2) :

2c0 + 6d0 = 2c1 .

/RVGRV√ÄQDOHVSURYLHQHQGHODVFRQGLFLRQHVGHIURQWHUDQDWXUDO

S0 (1) = 0 :

2c0 = 0

y

S1 (3) = 0 :

2c1 + 6d1 = 0.

Al resolver este sistema de ecuaciones obtenemos el spline

S(x) =

2 + 34 (x ‚àí 1) + 14 (x ‚àí 1)3 , para x ‚àà [1, 2]
3 + 32 (x ‚àí 2) + 34 (x ‚àí 2)2 ‚àí 14 (x ‚àí 2)3 , para x ‚àà [2, 3].

108

CAP√çTULO 3

Interpolaci√≥n y aproximaci√≥n polinomial

Construcci√≥n de un spline c√∫bico
&RPRORGHPXHVWUDHOHMHPSORSUHYLRXQVSOLQHGH√ÄQLGRHQXQLQWHUYDORTXHVHKDGLYLGLGR
en n subintervalos requerir√° determinar 4n constantes. Para construir el spline c√∫bico que se
interpola para una funci√≥n dada f,ODVFRQGLFLRQHVHQODGH√ÄQLFLyQVHDSOLFDQDORVSROLQRPLRV
c√∫bicos

S j (x) = a j + b j (x ‚àí x j ) + c j (x ‚àí x j )2 + d j (x ‚àí x j )3 ,
para cada j 5 0, 1, 7, n 2 1. Puesto que S j (x j ) 5 aj 5 f (x j ), la condici√≥n c) se puede aplicar
para obtener

a j+1 = S j+1 (x j+1 ) = S j (x j+1 ) = a j + b j (x j+1 ‚àí x j ) + c j (x j+1 ‚àí x j )2 + d j (x j+1 ‚àí x j )3 ,
Sujetar un spline indica que los
H[WUHPRVGHODWLUD√ÅH[LEOHHVWiQ
√ÄMRVGHWDOIRUPDTXHFDGDXQRGH
sus extremos es forzado a tomar
XQDGLUHFFLyQHVSHFt√ÄFD(VWRHV
importante, por ejemplo, cuando
los extremos de dos funciones
spline deben concordar. Esto se
realiza de manera matem√°tica
DOHVSHFL√ÄFDUORVYDORUHVGH
la derivada de la curva en los
extremos del spline.

para cada j 5 0, 1, 7, n 2 2.
Como los t√©rminos x j+1 ‚àí x j son usados repetidamente en este desarrollo, es conveniente introducir una notaci√≥n m√°s simple

h j = x j+1 ‚àí x j ,
para cada j 5 0, 1, 7, n 26LWDPELpQGH√ÄQLPRVan 5 f(xn ), entonces la ecuaci√≥n

a j+1 = a j + b j h j + c j h 2j + d j h 3j

(3.15)

se mantiene para cada j 5 0, 1, 7, n 2 1.
'HPDQHUDVLPLODUGH√ÄQDbn = S (xn ) y observe que

S j (x) = b j + 2c j (x ‚àí x j ) + 3d j (x ‚àí x j )2
implica que S j (x j ) = b j , para cada j 5 0, 1, ‚Ä¶, n 2 1. Al aplicar la condici√≥n en la parte
d) obtenemos

b j+1 = b j + 2c j h j + 3d j h 2j ,

(3.16)

para cada j 5 0, 1, 7, n 2 1.
2WUDUHODFLyQHQWUHORVFRH√ÄFLHQWHVGHSjVHREWLHQHDOGH√ÄQLUcn 5 S 0(xn)/2 y aplicar la
condici√≥n en la parte e). Entonces, para cada j 5 0, 1, 7, n 2 1,

c j+1 = c j + 3d j h j .

(3.17)

Resolviendo para dj en la ecuaci√≥n (3.17) y sustituyendo este valor en las ecuaciones
(3.15) y (3.16) obtenemos, para cada j 5 0, 1, 7, n 2 1, las nuevas ecuaciones

a j+1 = a j + b j h j +

h 2j
3

(2c j + c j+1 )

(3.18)

y

b j+1 = b j + h j (c j + c j+1 ).

(3.19)

/DUHODFLyQ√ÄQDOTXHLQYROXFUDORVFRH√ÄFLHQWHVVHREWLHQHDOUHVROYHUODHFXDFLyQDGHFXDda en la forma de la ecuaci√≥n (3.18), primero para bj,

bj =

1
hj
(a j+1 ‚àí a j ) ‚àí (2c j + c j+1 ),
hj
3

(3.20)

3.5 Interpolaci√≥n de spline c√∫bico

109

y entonces, con una reducci√≥n del √≠ndice, para bj21. Esto nos da

b j‚àí1 =

1
h j‚àí1

(a j ‚àí a j‚àí1 ) ‚àí

h j‚àí1
(2c j‚àí1 + c j ).
3

Al sustituir estos valores en la ecuaci√≥n obtenida de la ecuaci√≥n (3.19), con el √≠ndice reducido en uno, obtenemos el sistema lineal de ecuaciones

h j‚àí1 c j‚àí1 + 2(h j‚àí1 + h j )c j + h j c j+1 =

3
3
(a j+1 ‚àí a j ) ‚àí
(a j ‚àí a j‚àí1 ),
hj
h j‚àí1

(3.21)

Para cada j = 1, 2, ‚Ä¶, n ‚Äì 1. Este sistema s√≥lo tiene los {c j }nj=0 como inc√≥gnitas. Los valores
n
n
de {h j }n‚àí1
j=0 y {a j } j=0 est√°n dados, respectivamente, por el espaciado de los nodos {x j } j=0 y
los valores de f en los nodos. Por lo que, una vez que se determinan los valores de {c j }nj=0, es
n‚àí1
sencillo encontrar el resto de las constantes {b j }n‚àí1
j=0 a partir de la ecuaci√≥n (3.20) y {d j } j=0 a

partir de la ecuaci√≥n (3.17). Entonces podemos construir los polinomios c√∫bicos {S j (x)}n‚àí1
j=0 .
La pregunta m√°s importante que surge en relaci√≥n con esta construcci√≥n es si los valores
de {c j }nj=0 se pueden encontrar usando el sistema de ecuaciones dado en la ecuaci√≥n (3.21) y,
en este caso, si estos valores son √∫nicos. Los siguientes teoremas indican que √©ste es el caso
FXDQGRVHLPSRQHFXDOTXLHUDGHODVFRQGLFLRQHVGHIURQWHUDGDGDVHQODSDUWHI GHODGH√ÄQLci√≥n. Las demostraciones de estos teoremas requieren material a partir del √°lgebra lineal, la
cual se analiza en el cap√≠tulo 6.

Splines naturales
Teorema 3.11

Si fVHGH√ÄQHHQ a = x0 < x1 < ¬∑ ¬∑ ¬∑ < xn = b, entonces f tiene un spline natural √∫nico
que interpola S en los nodos x0 , x1 , . . . , xn; es decir, un spline interpolante que satisface las
condiciones de frontera natural S (a) = 0 y S (b) = 0.
Demostraci√≥n

Las condiciones de frontera en este caso implican que cn = S (xn )/2 = 0

y que

0 = S (x0 ) = 2c0 + 6d0 (x0 ‚àí x0 ),
por lo que c0 5 0. Las dos ecuaciones c0 5 0 y cn 5 0 junto con las ecuaciones en (3.21)
producen un sistema lineal descrito por la ecuaci√≥n matriz-vector Ax = b, donde A es la
matriz (n 1 1) 3 (n 1 1)

‚é§
0 . .. .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 0.
....
‚é¢
.. ‚é•
....
‚é¢
.. ‚é•
....
‚é•
‚é¢h 0 2(h 0 + h 1 )
h1
....
.. ‚é•
‚é¢
....
‚é•
‚é¢
.
.
....
.. ‚é•
‚é¢ 0. . . .
....
. . . . h 1 . . . . . . . 2(h 1 .+. . h. .2.). h 2 . . . . . . .
.
.
A=‚é¢
.
....
. . ‚é•
....
....
.
‚é•,
‚é¢ ..
....
....
.... .......
‚é•
‚é¢.
0
....
...
....
....
‚é•
‚é¢ ..
...
‚é•
‚é¢.
.
.
.
.
.
....
‚é¢ ..
h
2(h
+
h
)
h
n‚àí2
n‚àí2
n‚àí1
n‚àí1 ‚é•
....
‚é¶
‚é£.
..
0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ... . 0.
0
1
‚é°

1

0

110

CAP√çTULO 3

Interpolaci√≥n y aproximaci√≥n polinomial

y b y x son los vectores

‚é°

0

‚é§

‚é•
‚é¢
‚é•
‚é¢
‚é•
‚é¢
‚é•
‚é¢
..
b=‚é¢
‚é•
.
‚é•
‚é¢ 3
3
‚é•
‚é¢
‚é£ h n‚àí1 (an ‚àí an‚àí1 ) ‚àí h n‚àí2 (an‚àí1 ‚àí an‚àí2 )‚é¶
0
3
(a ‚àí a1 ) ‚àí h3 (a1 ‚àí a0 )
h1 2
0

y

‚é° ‚é§
c0
‚é¢ c1 ‚é•
‚é¢ ‚é•
x = ‚é¢ . ‚é•.
‚é£ .. ‚é¶
cn

La matriz AHVHVWULFWDPHQWHGRPLQDQWHGHPDQHUDGLDJRQDOHVGHFLUHQFDGD√ÄODOD
magnitud de la entrada diagonal excede la suma de las magnitudes de todas las otras entradas
HQOD√ÄOD8QVLVWHPDOLQHDOFRQXQDPDWUL]GHHVWDIRUPDVHPRVWUDUiPHGLDQWHHOWHRUHPD
6.21 en la secci√≥n 6.6 para tener una √∫nica soluci√≥n para c0 , c1 , . . . , cn .
La soluci√≥n para el problema del spline c√∫bico con las condiciones de frontera S (x0 ) =
S (xn ) = 0 se puede obtener al aplicar el algoritmo 3.4.

ALGORITMO

3.4

Spline c√∫bico natural
Para construir el spline c√∫bico interpolante S para la funci√≥n f GH√ÄQLGR HQ ORV Q~PHURV
x0 < x1 < ¬∑ ¬∑ ¬∑ < xn , que satisfacen S (x0 ) = S (xn ) = 0:

ENTRADA n; x0 , x1 , . . . , xn ; a0 = f (x0 ), a1 = f (x1 ), . . . , an = f (xn ).
SALIDA a j , b j , c j , d j para j = 0, 1, . . . , n ‚àí 1.
(Nota: S(x) = S j (x) = a j + b j (x ‚àí x j ) + c j (x ‚àí x j )2 + d j (x ‚àí x j )3 para x j ‚â§ x ‚â§ x j+1 .)
Paso 1 Para i = 0, 1, . . . , n ‚àí 1 haga h i = xi+1 ‚àí xi .
Paso 2 Para i = 1, 2, . . . , n ‚àí 1 haga
Œ±i =

3
3
(ai+1 ‚àí ai ) ‚àí
(ai ‚àí ai‚àí1 ).
hi
h i‚àí1

Paso 3 Determine l0 = 1; (Los pasos 3, 4 y 5 y parte del paso 6 resuelven un sistema
lineal tridiagonal con un m√©todo descrito en el algoritmo 6.7. )
Œº0 = 0;
z 0 = 0.
Paso 4 Para i = 1, 2, . . . , n ‚àí 1
haga li = 2(xi+1 ‚àí xi‚àí1 ) ‚àí h i‚àí1 Œºi‚àí1 ;
Œºi = h i /li ;
z i = (Œ±i ‚àí h i‚àí1 z i‚àí1 )/li .
Paso 5 Haga ln = 1;
z n = 0;
cn = 0.
Paso 6 Para j = n ‚àí 1, n ‚àí 2, . . . , 0
haga c j = z j ‚àí Œº j c j+1 ;
b j = (a j+1 ‚àí a j )/ h j ‚àí h j (c j+1 + 2c j )/3;
d j = (c j+1 ‚àí c j )/(3h j ).
Paso 7 SALIDA (a j , b j , c j , d j para j = 0, 1, . . . , n ‚àí 1);
PARE.

3.5 Interpolaci√≥n de spline c√∫bico

Ejemplo 2

111

Al inicio del cap√≠tulo 3, proporcionamos algunos polinomios de Taylor para aproximar la exponencial f (x) = e x. Use los puntos (0, 1), (1, e), (2, e2 ), y (3, e3 ) para formar un spline
natural S(x) que se aproxima a f (x) = e x .
Tenemos n = 3, h 0 = h 1 = h 2 = 1, a0 = 1, a1 = e, a2 = e2 , y a3 = e3. Por lo
que, la matriz A y los vectores b y x determinados en el teorema 3.11 tienen las formas
‚é§
‚é° ‚é§
‚é°
‚é§
‚é°
c0
1 0 0 0
0
‚é¢c1 ‚é•
‚é¢1 4 1 0‚é•
‚é¢ 3(e2 ‚àí 2e + 1) ‚é•
‚é•
‚é¢
‚é¢
‚é•
‚é¢
, y
x=‚é£ ‚é•
.
A=‚é£
, b=‚é£ 3
c2 ‚é¶
3(e ‚àí 2e2 + e)‚é¶
0 1 4 1‚é¶
0
c3
0 0 0 1
Soluci√≥n

La ecuaci√≥n matriz-vector Ax = b es equivalente al sistema de ecuaciones

c0 = 0,
c0 + 4c1 + c2 = 3(e2 ‚àí 2e + 1),
c1 + 4c2 + c3 = 3(e3 ‚àí 2e2 + e),
c3 = 0.
Este sistema tiene la soluci√≥n c0 5 c3 5 0, y para cinco lugares decimales,

c1 =

1
1
(‚àíe3 + 6e2 ‚àí 9e + 4) ‚âà 0.75685, y c2 = (4e3 ‚àí 9e2 + 6e ‚àí 1) ‚âà 5.83007.
5
5

Al resolver para las constantes restantes obtenemos

b0 =

1
h0
(a1 ‚àí a0 ) ‚àí (c1 + 2c0 )
h0
3

1
(‚àíe3 + 6e2 ‚àí 9e + 4) ‚âà 1.46600,
15
1
h1
b1 = (a2 ‚àí a1 ) ‚àí (c2 + 2c1 )
h1
3
= (e ‚àí 1) ‚àí

1
(2e3 + 3e2 ‚àí 12e + 7) ‚âà 2.22285,
15
1
h2
b2 = (a3 ‚àí a2 ) ‚àí (c3 + 2c2 )
h2
3
= (e2 ‚àí e) ‚àí

1
(8e3 ‚àí 18e2 + 12e ‚àí 2) ‚âà 8.80977,
15
1
1
(c1 ‚àí c0 ) =
(‚àíe3 + 6e2 ‚àí 9e + 4) ‚âà 0.25228,
d0 =
3h 0
15
= (e3 ‚àí e2 ) ‚àí

d1 =

1
1
(c2 ‚àí c1 ) = (e3 ‚àí 3e2 + 3e ‚àí 1) ‚âà 1.69107,
3h 1
3

d2 =

1
1
(‚àí4e3 + 9e2 ‚àí 6e + 1) ‚âà ‚àí1.94336.
(c3 ‚àí c1 ) =
3h 2
15

y

112

CAP√çTULO 3

Interpolaci√≥n y aproximaci√≥n polinomial

El spline c√∫bico natural se describe por tramos mediante

‚éß
3
‚é™
para x ‚àà [0, 1],
‚é®1 + 1.46600x + 0.25228x ,
S(x) = 2.71828 + 2.22285(x‚àí1) + 0.75685(x‚àí1)2 + 1.69107(x‚àí1)3 , para x ‚àà [1, 2],
‚é™
‚é©
7.38906 + 8.80977(x‚àí2) + 5.83007(x‚àí2)2 ‚àí 1.94336(x‚àí2)3 , para x ‚àà [2, 3].
El spline y f (x) 5 exVHPXHVWUDQHQOD√ÄJXUD

Figura 3.10

e3

y

y = S(x)
y = ex
e2

e
1
1

2

3

x

Una vez que hemos determinado un spline para una aproximaci√≥n de una funci√≥n, podemos usarlo para aproximar otras propiedades de la funci√≥n. La siguiente ilustraci√≥n implica
la integral del spline que encontramos en el ejemplo previo.
Ilustraci√≥n

Para aproximar la integral de f (x) = e x en [0, 3], que tiene el valor
3

e x d x = e3 ‚àí 1 ‚âà 20.08553692 ‚àí 1 = 19.08553692,

0

podemos integrar por tramos el spline que aproxima f en este intervalo. Esto nos da
3
0

1

S(x) =

1 + 1.46600x + 0.25228x 3 d x

0
2

+

2.71828 + 2.22285(x ‚àí 1) + 0.75685(x ‚àí 1)2 + 1.69107(x ‚àí 1)3 d x

1
3

+
2

7.38906 + 8.80977(x ‚àí 2) + 5.83007(x ‚àí 2)2 ‚àí 1.94336(x ‚àí 2)3 d x.

3.5 Interpolaci√≥n de spline c√∫bico

113

Integrando y calculando los valores de las potencias obtenemos
3

1

S(x) = x + 1.46600

0

x4
x2
+ 0.25228
2
4 0
2

+ 2.71828(x‚àí1) + 2.22285

(x‚àí1)3
(x‚àí1)4
(x‚àí1)2
+ 0.75685
+ 1.69107
2
3
4
1
3

(x‚àí2)3
(x‚àí2)4
(x‚àí2)2
+ 5.83007
‚àí 1.94336
+ 7.38906(x‚àí2) + 8.80977
2
3
4
2
= (1 + 2.71828 + 7.38906) +
+

1
(1.46600 + 2.22285 + 8.80977)
2

1
1
(0.75685 + 5.83007) + (0.25228 + 1.69107 ‚àí 1.94336)
3
4

= 19.55229.
Puesto que los nodos est√°n espaciados de manera equivalente en este ejemplo, la aproximaci√≥n de la integral es simplemente
3

1
1
1
S(x) d x = (a0 + a1 + a2 ) + (b0 + b1 + b2 ) + (c0 + c1 + c2 ) + (d0 + d1 + d2 ).
2
3
4
0
(3.22)

Splines condicionados
Ejemplo 3

En el ejemplo 1 encontramos un spline natural S que pasa por los puntos (1, 2), (2, 3) y (3, 5).
Construir un spline condicionado s que pase por esos puntos y que cumpla s (1) = 2 y
s (3) = 1.
Soluci√≥n

Si

s0 (x) = a0 + b0 (x ‚àí 1) + c0 (x ‚àí 1)2 + d0 (x ‚àí 1)3
HVHOF~ELFRHQ>@\HOF~ELFRHQ>@HV

s1 (x) = a1 + b1 (x ‚àí 2) + c1 (x ‚àí 2)2 + d1 (x ‚àí 2)3 .
Entonces, la mayor√≠a de las condiciones para determinar ocho constantes son iguales a las
del ejemplo 1. Es decir,

2 = f (1) = a0 ,

3 = f (2) = a0 + b0 + c0 + d0 ,

3 = f (2) = a1 ,

y

5 = f (3) = a1 + b1 + c1 + d1 .
s0 (2) = s1 (2) :

b0 + 2c0 + 3d0 = b1

y

s0 (2) = s1 (2) :

2c0 + 6d0 = 2c1

114

CAP√çTULO 3

Interpolaci√≥n y aproximaci√≥n polinomial

Sin embargo, las condiciones de frontera ahora son

s0 (1) = 2 :

b0 = 2

y

s1 (3) = 1 :

b1 + 2c1 + 3d1 = 1.

Resolviendo este sistema de ecuaciones obtenemos el spline como

2 + 2(x ‚àí 1) ‚àí 52 (x ‚àí 1)2 + 32 (x ‚àí 1)3 , para x ‚àà [1, 2]

s(x) =

3 + 32 (x ‚àí 2) + 2(x ‚àí 2)2 ‚àí 32 (x ‚àí 2)3 , para x ‚àà [2, 3]

.

En el caso general de las condiciones de frontera condicionada, tenemos un resultado que
es similar al teorema para las condiciones de frontera natural descritas en el teorema 3.11.
Teorema 3.12

Si f VHGH√ÄQHHQ a = x0 < x1 < ¬∑ ¬∑ ¬∑ < xn = b y es diferenciable en a y b, entonces f tiene un √∫nico spline interpolante S condicionado en los nodos x0 , x1 , . . . , xn; es decir, un
spline interpolante que satisface las condiciones de frontera condicionada S (a) = f (a) y
S (b) = f (b).
Demostraci√≥n

Puesto que f (a) = S (a) = S (x0 ) = b0, la ecuaci√≥n (3.20) con j 5 0 impli-

ca que

f (a) =

1
h0
(a1 ‚àí a0 ) ‚àí (2c0 + c1 ).
h0
3

Por consiguiente,
2h 0 c0 + h 0 c1 =

3
(a1 ‚àí a0 ) ‚àí 3 f (a).
h0

De igual forma,
f (b) = bn = bn‚àí1 + h n‚àí1 (cn‚àí1 + cn ),
por lo que la ecuaci√≥n (3.20) con j = n ‚àí 1 implica que
f (b) =
=

an ‚àí an‚àí1
h n‚àí1
‚àí
(2cn‚àí1 + cn ) + h n‚àí1 (cn‚àí1 + cn )
h n‚àí1
3
h n‚àí1
an ‚àí an‚àí1
(cn‚àí1 + 2cn ),
+
h n‚àí1
3

y
h n‚àí1 cn‚àí1 + 2h n‚àí1 cn = 3 f (b) ‚àí

3
h n‚àí1

(an ‚àí an‚àí1 ).

Las ecuaciones (3.21) junto con las ecuaciones
2h 0 c0 + h 0 c1 =

3
(a1 ‚àí a0 ) ‚àí 3 f (a)
h0

y
h n‚àí1 cn‚àí1 + 2h n‚àí1 cn = 3 f (b) ‚àí

3
(an ‚àí an‚àí1 )
h n‚àí1

3.5 Interpolaci√≥n de spline c√∫bico

115

determinan el sistema lineal Ax = b, donde

‚é§
0 . .. .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 0.
....
‚é¢
.. ‚é•
....
‚é¢
.. ‚é•
....
‚é•
‚é¢ h 0 2(h 0 + h 1 )
h1
....
.. ‚é•
‚é¢
....
‚é•
‚é¢
....
... ‚é•
‚é¢ 0. . . . .
h 1 . . . . 2(h 1 .+. . h 2 )
h2 . . . .
.
.
.
.
‚é¢
.
.
.
.
.
.
....
....
....
....
.... . ‚é•
A =‚é¢ .
‚é•,
....
....
....
....
‚é•
‚é¢ ..
.
0
.
.
.
.
.
.
.
.
.
.
.
.
.
....
....
....
...
‚é•
‚é¢ .
....
..
...
‚é•
‚é¢ .
....
‚é•
‚é¢ ..
h
2(h
+
h
)
h
n‚àí2
n‚àí2
n‚àí1
n‚àí1
....
‚é¶
‚é£ .
..
0 ................................ 0
h n‚àí1
2h n‚àí1
‚é§
‚é°
3
(a ‚àí a0 ) ‚àí 3 f (a)
h0 1
‚é° ‚é§
3
‚é•
‚é¢
c0
(a ‚àí a1 ) ‚àí h3 (a1 ‚àí a0 )
‚é•
‚é¢
h1 2
0
‚é•
‚é¢c1 ‚é•
‚é¢
‚é•
‚é¢ ‚é•
‚é¢
..
b =‚é¢
‚é• , y x = ‚é¢ .. ‚é• .
.
‚é•
‚é¢
‚é£.‚é¶
‚é•
‚é¢ 3
3
‚é£ h n‚àí1 (an ‚àí an‚àí1 ) ‚àí h n‚àí2 (an‚àí1 ‚àí an‚àí2 )‚é¶
cn
3 f (b) ‚àí h 3 (an ‚àí an‚àí1 )
‚é°

2h 0

h0

n‚àí1

Esta matriz A tambi√©n es estrictamente dominante de manera diagonal, por lo que satisface las condiciones del teorema 6.21 en la secci√≥n 6.6. Por lo tanto, el sistema lineal tiene
una soluci√≥n √∫nica para c0 , c1 , . . . , cn .
La soluci√≥n del problema de spline c√∫bico con condiciones de frontera S (x0 ) = f (x0 )
y S (xn ) = f (xn ) se puede obtener al aplicar el algoritmo 3.5.
ALGORITMO

3.5

Spline c√∫bico condicionado
Para construir el spline c√∫bico interpolante S para la funci√≥n f GH√ÄQLGD HQ ORV Q~PHURV
x0 < x1 < ¬∑ ¬∑ ¬∑ < xn , que satisfacen S (x0 ) = f (x0 ) y S (xn ) = f (xn ):

ENTRADA n; x0 , x1 , . . . , xn ; a0 = f (x0 ), a1 = f (x1 ), . . . , an = f (xn ); FPO = f (x0 );
FPN = f (xn ).
SALIDA a j , b j , c j , d j para j = 0, 1, . . . , n ‚àí 1.
(Nota: S(x) = S j (x) = a j + b j (x ‚àí x j ) + c j (x ‚àí x j )2 + d j (x ‚àí x j )3 para x j ‚â§ x ‚â§ x j+1 .)
Paso 1 Para i = 0, 1, . . . , n ‚àí 1 haga h i = xi+1 ‚àí xi .
Paso 2 Haga Œ±0 = 3(a1 ‚àí a0 )/ h 0 ‚àí 3FPO;
Œ±n = 3FPN ‚àí 3(an ‚àí an‚àí1 )/ h n‚àí1 .
Paso 3 Para i = 1, 2, . . . , n ‚àí 1
haga Œ±i =
Paso 4 Haga l0 = 2h 0 ;

3
3
(ai+1 ‚àí ai ) ‚àí
(ai ‚àí ai‚àí1 ).
hi
h i‚àí1
(Los pasos 4, 5 y 6 y parte del paso 7 resuelven un sistema lineal
tridiagonal con un m√©todo descrito en el algoritmo 6.7. )

Œº0 = 0.5;
z 0 = Œ±0 /l0 .
Paso 5 Para i = 1, 2, . . . , n ‚àí 1
haga li = 2(xi+1 ‚àí xi‚àí1 ) ‚àí h i‚àí1 Œºi‚àí1 ;
Œºi = h i /li ;
z i = (Œ±i ‚àí h i‚àí1 z i‚àí1 )/li .

116

CAP√çTULO 3

Interpolaci√≥n y aproximaci√≥n polinomial

Paso 6 Haga ln = h n‚àí1 (2 ‚àí Œºn‚àí1 );
z n = (Œ±n ‚àí h n‚àí1 z n‚àí1 )/ln ;
cn = z n .
Paso 7 Para j = n ‚àí 1, n ‚àí 2, . . . , 0
haga c j = z j ‚àí Œº j c j+1 ;
b j = (a j+1 ‚àí a j )/ h j ‚àí h j (c j+1 + 2c j )/3;
d j = (c j+1 ‚àí c j )/(3h j ).
Paso 8 SALIDA (a j , b j , c j , d j para j = 0, 1, . . . , n ‚àí 1);
PARE.

Ejemplo 4

En el ejemplo 2 se us√≥ un spline natural y los puntos de datos (0, 1), (1, e), (2, e2) y (3, e3)
para formar una nueva funci√≥n de aproximaci√≥n S(x). Determine el spline condicionado s(x)
que utiliza estos datos y la informaci√≥n adicional, f (x) = e x , f (0) = 1 y f (3) = e3 .
Como en el ejemplo 2, tenemos n = 3, h 0 = h 1 = h 2 = 1, a0 = 0, a1 = e,
a2 = e2 , y a3 = e3 Esto, junto con la informaci√≥n que f (0) = 1 y f (3) = e3, da la matriz
A y los vectores b y x con las formas
‚é§
‚é° ‚é§
‚é°
‚é§
‚é°
c0
2 1 0 0
3(e ‚àí 2)
‚é¢c1 ‚é•
‚é¢1 4 1 0‚é•
‚é¢ 3(e2 ‚àí 2e + 1) ‚é•
‚é•
‚é¢
‚é¢
‚é•
‚é¢
, y x=‚é£ ‚é•
.
A=‚é£
, b=‚é£ 3
c2 ‚é¶
3(e ‚àí 2e2 + e)‚é¶
0 1 4 1‚é¶
c3
3e2
0 0 1 2

Soluci√≥n

La ecuaci√≥n matriz-vector Ax = b es equivalente al sistema de ecuaciones

2c0 + c1 = 3(e ‚àí 2),
c0 + 4c1 + c2 = 3(e2 ‚àí 2e + 1),
c1 + 4c2 + c3 = 3(e3 ‚àí 2e2 + e),
c2 + 2c3 = 3e2 .
Resolviendo este sistema simult√°neamente para c0 , c1 , c2 y c3 obtenemos, con cinco lugares
decimales,

1
(2e3 ‚àí 12e2 + 42e ‚àí 59) = 0.44468,
15
1
c1 = (‚àí4e3 + 24e2 ‚àí 39e + 28) = 1.26548,
15
1
c2 = (14e3 ‚àí 39e2 + 24e ‚àí 8) = 3.35087,
15
1
c3 = (‚àí7e3 + 42e2 ‚àí 12e + 4) = 9.40815.
15
c0 =

Al resolver para las constantes restantes de la misma manera que en el ejemplo 2 obtenemos

b0 = 1.00000,

b1 = 2.71016,

b2 = 7.32652,

d0 = 0.27360,

d1 = 0.69513,

d2 = 2.01909.

y

3.5 Interpolaci√≥n de spline c√∫bico

Esto nos da el spline c√∫bico condicionado
‚éß
2
3
‚é™
‚é®1 + x + 0.44468x + 0.27360x ,
s(x) = 2.71828 + 2.71016(x‚àí1) + 1.26548(x‚àí1)2 + 0.69513(x‚àí1)3 ,
‚é™
‚é©
7.38906 + 7.32652(x‚àí2) + 3.35087(x‚àí2)2 + 2.01909(x‚àí2)3 ,

117

si 0 ‚â§ x < 1,
si 1 ‚â§ x < 2,
si 2 ‚â§ x ‚â§ 3.

/DJUi√ÄFDGHOVSOLQHFRQGLFLRQDGR\ f (x) = e x son tan similares que no es posible observar
diferencias.
Tambi√©n podemos aproximar la integral de fHQ>@DOLQWHJUDUHOVSOLQHFRQGLFLRQDGR
El valor exacto de la integral es
3

e x d x = e3 ‚àí 1 ‚âà 20.08554 ‚àí 1 = 19.08554.

0

Puesto que los datos est√°n igualmente espaciados, integrar por tramos el spline condicionado
resulta en la misma f√≥rmula que en (3.22); es decir,
3

1
s(x) d x = (a0 + a1 + a2 ) + (b0 + b1 + b2 )
2
0
1
1
+ (c0 + c1 + c2 ) + (d0 + d1 + d2 ).
3
4
Por lo tanto, la aproximaci√≥n de integral es
3

1
s(x) d x = (1 + 2.71828 + 7.38906) + (1 + 2.71016 + 7.32652)
2
0
1
1
+ (0.44468 + 1.26548 + 3.35087) + (0.27360 + 0.69513 + 2.01909)
3
4
= 19.05965.
El error absoluto en la aproximaci√≥n integral usando los splines naturales y condicionados es

Natural: |19.08554 ‚àí 19.55229| = 0.46675
y
Condicionado: |19.08554 ‚àí 19.05965| = 0.02589.
Para prop√≥sitos de integraci√≥n, el spline condicionado es inmensamente superior. Esto no
deber√≠a ser una sorpresa porque las condiciones de frontera para el spline condicionado son
exactas, mientras que para el spline natural fundamentalmente asumimos que, f 0 (x) 5 ex,

0 = S (x) ‚âà f (0) = e1 = 1

y

0 = S (3) ‚âà f (3) = e3 ‚âà 20.

La siguiente ilustraci√≥n usa un spline para aproximar una curva que no tiene una representaci√≥n funcional.
Ilustraci√≥n

/D√ÄJXUDPXHVWUDXQSDWRPDOYDVLDHQYXHOR3DUDDSUR[LPDUHOSHU√ÄOVXSHULRUGHOSDWR
hemos seleccionado puntos a lo largo de la curva por los cuales queremos que pase la curva
de aproximaci√≥n. La tabla 3.18 enumera las coordenadas de 21 puntos de datos relativos al
VLVWHPDGHFRRUGHQDGDVVXSHUSXHVWRTXHVHPXHVWUDHQOD√ÄJXUD2EVHUYHTXHVHXVDQ
m√°s puntos cuando la curva cambia r√°pidamente que cuando lo hace m√°s despacio.

118

CAP√çTULO 3

Interpolaci√≥n y aproximaci√≥n polinomial

Figura 3.11

Tabla 3.18
x

0.9 1.3 1.9 2.1 2.6 3.0 3.9 4.4 4.7 5.0 6.0 7.0 8.0 9.2 10.5 11.3 11.6 12.0 12.6 13.0 13.3

f (x) 1.3 1.5 1.85 2.1 2.6 2.7 2.4 2.15 2.05 2.1 2.25 2.3 2.25 1.95 1.4 0.9 0.7 0.6 0.5 0.4 0.25

Figura 3.12
f (x)
4
3
2
1
1

2

3

4

5

6

7

8

9 10 11 12 13

x

El uso del algoritmo 3.4 para generar el spline c√∫bico natural para estos datos produce
ORVFRH√ÄFLHQWHVTXHVHPXHVWUDQHQODWDEOD(VWDFXUYDVSOLQHHVFDVLLGpQWLFDDOSHU√ÄO
FRPRVHPXHVWUDHQOD√ÄJXUD

3.5 Interpolaci√≥n de spline c√∫bico

Tabla 3.19

j

xj

aj

bj

cj

dj

0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

0.9
1.3
1.9
2.1
2.6
3.0
3.9
4.4
4.7
5.0
6.0
7.0
8.0
9.2
10.5
11.3
11.6
12.0
12.6
13.0
13.3

1.3
1.5
1.85
2.1
2.6
2.7
2.4
2.15
2.05
2.1
2.25
2.3
2.25
1.95
1.4
0.9
0.7
0.6
0.5
0.4
0.25

0.54
0.42
1.09
1.29
0.59
‚àí0.02
‚àí0.50
‚àí0.48
‚àí0.07
0.26
0.08
0.01
‚àí0.14
‚àí0.34
‚àí0.53
‚àí0.73
‚àí0.49
‚àí0.14
‚àí0.18
‚àí0.39

0.00
‚àí0.30
1.41
‚àí0.37
‚àí1.04
‚àí0.50
‚àí0.03
0.08
1.27
‚àí0.16
‚àí0.03
‚àí0.04
‚àí0.11
‚àí0.05
‚àí0.10
‚àí0.15
0.94
‚àí0.06
0.00
‚àí0.54

‚àí0.25
0.95
‚àí2.96
‚àí0.45
0.45
0.17
0.08
1.31
‚àí1.58
0.04
0.00
‚àí0.02
0.02
‚àí0.01
‚àí0.02
1.21
‚àí0.84
0.04
‚àí0.45
0.60

119

Figura 3.13
f (x)
4
3
2
1
1

2

3

4

5

6

7

8

9 10 11 12 13

x

3DUDSURSyVLWRVGHFRPSDUDFLyQOD√ÄJXUDSURSRUFLRQDXQDLOXVWUDFLyQGHODFXUYD
que se genera con un polinomio de interpolaci√≥n de Lagrange para ajustar los datos provistos
en la tabla 3.18. En este caso, el polinomio de interpolaci√≥n es de grado 20 y oscila en forma
desordenada. Produce una ilustraci√≥n muy extra√±a de la espalda del pato, en vuelo o de otra
forma.

120

CAP√çTULO 3

Interpolaci√≥n y aproximaci√≥n polinomial

Figura 3.14
f (x)
4
3
2
1
1

2

3

4

5

6

7

8

9 10 11 12

x

Al utilizar un spline condicionado para aproximar esta curva, necesitar√≠amos derivar
aproximaciones para los extremos. Incluso si estas aproximaciones est√°n disponibles, podr√≠amos esperar poca mejora debido al acuerdo cerrado del spline c√∫bico natural para la
FXUYDGHOSHU√ÄOVXSHULRU
&RQVWUXLUXQVSOLQHF~ELFRSDUDDSUR[LPDUHOSHU√ÄOLQIHULRUGHOSDWRPDOYDVtDVHUtDPiV
dif√≠cil porque la curva para esta parte no se puede expresar como una funci√≥n de x, y en ciertos puntos la curva no parece ser suave. Estos problemas se pueden resolver usando splines
separados para representar varias partes de la curva, pero en la siguiente secci√≥n se considera
XQHQIRTXHPiVH√ÄFD]SDUDDSUR[LPDUODVFXUYDVGHHVWHWLSR
En general, al aproximar funciones mediante splines c√∫bicos son preferibles las condiciones de frontera condicionada, por lo que la derivada de la funci√≥n debe conocerse o aproximarse en los extremos del intervalo. Cuando los nodos est√°n espaciados uniformemente cerca
de ambos extremos, es posible obtener las aproximaciones con cualquiera de las f√≥rmulas
adecuadas provistas en las secciones 4.1 y 4.2. Cuando los nodos no est√°n espaciados de
manera uniforme, el problema es considerablemente m√°s dif√≠cil.
Para concluir esta secci√≥n, listamos una f√≥rmula de la cota de error para el spline c√∫bico
VXMHWRDFRQGLFLRQHVGHIURQWHUD/DSUXHEDGHHVWHUHVXOWDGRVHSXHGHHQFRQWUDUHQ>6FKXO@
pp. 57‚Äì58.
Teorema 3.13

Sea f ‚àà C 4 [a, b] con m√°x a‚â§x‚â§b | f (4) (x)| = M . Si S es el √∫nico spline c√∫bico condicionado
interpolante para f respecto a los nodos a = x0 < x1 < ¬∑ ¬∑ ¬∑ < xn = b, entonces, para todas
las xHQ>a, b].

| f (x) ‚àí S(x)| ‚â§

5M
m√°x (x j+1 ‚àí x j )4 .
384 0‚â§ j‚â§n‚àí1

Un resultado de la cota del error de cuarto orden tambi√©n se mantiene para el caso de las
FRQGLFLRQHVGHIURQWHUDQDWXUDOSHURHVPiVGLItFLOGHH[SUHVDU FRQVXOWH>%'@SS¬≤ 
En general, las condiciones de frontera natural proporcionar√°n resultados menos preciVRVTXHODVGHIURQWHUDFRQGLFLRQDGDFHUFDGHORVH[WUHPRVGHOLQWHUYDOR>x0, xn] a menos que
la funci√≥n f satisfaga f (x0 ) = f (xn ) = 0. Una alternativa para la condici√≥n de frontera
natural que no requiere conocimiento de la derivada de f es la condici√≥n sin nudo (consulte
>'HE@SS¬≤ (VWDFRQGLFLyQUHTXLHUHTXHS (x) sea continua en x1 y xn21.
La secci√≥n Conjunto de ejercicios 3.5 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

3.6

Curvas param√©tricas

121

3.6 Curvas param√©tricas
Ninguna de las t√©cnicas desarrolladas en este cap√≠tulo puede usarse para generar curvas de
ODIRUPDTXHVHPXHVWUDHQOD√ÄJXUDSRUTXHHVWDFXUYDQRVHSXHGHH[SUHVDUFRPRXQD
funci√≥n de una variable coordenada en t√©rminos de la otra. En esta secci√≥n veremos c√≥mo
representar curvas generales al usar un par√°metro para expresar tanto las variables x como y.
&XDOTXLHUEXHQOLEURVREUHJUi√ÄFDVFRPSXWDFLRQDOHVPRVWUDUiFyPRHVSRVLEOHDPSOLDUHVWD
WpFQLFDSDUDUHSUHVHQWDUFXUYDVJHQHUDOHV\VXSHU√ÄFLHVHQHOHVSDFLR FRQVXOWHSRUHMHPSOR
>)9)+@

Figura 3.15
y
1

21

1

x

21

Una t√©cnica param√©trica sencilla para determinar un polinomio o un polinomio por tramos para conectar los puntos (x0 , y0 ), (x1 , y1 ), . . . , (xn , yn ) en el orden provisto consiste en
usar un par√°metro t en un intervalo [t0 , tn ], con t0 < t1 < ¬∑ ¬∑ ¬∑ < tn y construir funciones de
aproximaci√≥n con

xi = x(ti )

y

yi = y(ti ),

para cada i = 0, 1, . . . , n.

El siguiente ejemplo muestra la t√©cnica en el caso en que ambas funciones de aproximaci√≥n son polinomios de interpolaci√≥n de Lagrange.
Ejemplo 1

Construya un par de polinomios de Lagrange para aproximar la curva que se muestra en la
√ÄJXUDXVDQGRORVSXQWRVGHGDWRVHQODFXUYD
Soluci√≥n ([LVWH√ÅH[LELOLGDGDOVHOHFFLRQDUHOSDUiPHWUR\QRVRWURVHOHJLUHPRVORVSXQWRV
4
{ti }i=0
LJXDOPHQWHHVSDFLDGRVHQ>@ORFXDOSURYHHORVGDWRVHQODWDEOD

Tabla 3.20

i

0

1

2

3

4

ti
xi
yi

0
‚àí1
0

0.25
0
1

0.5
1
0.5

0.75
0
0

1
1
‚àí1

122

CAP√çTULO 3

Interpolaci√≥n y aproximaci√≥n polinomial

Esto produce los polinomios de interpolaci√≥n

x(t) =

64t ‚àí 352
t + 60 t ‚àí 14
t ‚àí1 y y(t) =
3
3

‚àí 64
t + 48 t ‚àí 116
t + 11 t.
3
3

7UD]DUHVWHVLVWHPDSDUDPpWULFRSURGXFHODJUi√ÄFDTXHVHPXHVWUDHQD]XOHQOD√ÄJXUD
3.16. Aunque pasa por los puntos requeridos y tiene la misma forma b√°sica, es una aproximaci√≥n bastante burda para la curva original. Una aproximaci√≥n m√°s precisa requerir√≠a nodos
adicionales, con el consiguiente incremento en computaci√≥n.

Figura 3.16
y
1

21

(x(t), y(t))

1

x

21

Un sistema de dise√±o
computacional exitoso necesita
estar basado en una teor√≠a
matem√°tica formal, de tal
manera que los resultados sean
predecibles, pero esta teor√≠a
deber√≠a realizarse en segundo
plano para que el artista pueda
basar el dise√±o en la est√©tica.

Las curvas param√©trica de Hermite y de spline se pueden generar de forma similar, pero
esto tambi√©n requiere un gran esfuerzo computacional.
/DVDSOLFDFLRQHVGHJUi√ÄFDVFRPSXWDFLRQDOHVUHTXLHUHQODJHQHUDFLyQUiSLGDGHFXUYDV
VXDYHVTXHVHSXHGHQPRGL√ÄFDUGHPDQHUDIiFLO\UiSLGD3RUUD]RQHVWDQWRHVWpWLFDVFRPR
computacionales, cambiar una parte de estas curvas deber√≠a tener un efecto peque√±o o ning√∫n efecto en otras partes de las curvas. Esto elimina el uso de polinomios de interpolaci√≥n
y splines ya que cambiar una parte de estas curvas afecta su totalidad.
/D VHOHFFLyQ GH OD FXUYD SDUD XVDUOD HQ JUi√ÄFDV FRPSXWDFLRQDOHV HV HQ JHQHUDO XQD
forma de polinomio Hermite c√∫bico por tramos. Cada parte de un polinomio de Hermite
F~ELFR VH FRPSOHWD WRWDOPHQWH DO HVSHFL√ÄFDU VXV H[WUHPRV \ ODV GHULYDGDV HQ HVWRV H[WUHmos. Por consiguiente, una parte de la curva se puede cambiar mientras la mayor parte de
la misma se deja igual. S√≥ORODVSDUWHVDG\DFHQWHVQHFHVLWDQPRGL√ÄFDUVHSDUDJDUDQWL]DUOD
suavidad en los extremos. Los c√°lculos se pueden realizar r√°pidamente y es posible cambiar
una secci√≥n de la curva a la vez.
(OSUREOHPDFRQODLQWHUSRODFLyQGH+HUPLWHHVODQHFHVLGDGGHHVSHFL√ÄFDUODVGHULYDGDV
en los extremos de cada secci√≥n de la curva. Suponga que la curva tiene n 1 1 puntos de datos
(x(t0 ), y(t0 )), . . . ,(x(tn ), y(tn )) y deseamos parametrizar el c√∫bico para permitir caracter√≠sWLFDVFRPSOHMDV(QWRQFHVGHEHPRVHVSHFL√ÄFDUx (ti ) y y (ti ), para cada i 5 0, 1, 7, n. Esto
no es tan dif√≠cil como parece ya que cada parte se genera de manera independiente. S√≥lo debemos garantizar que las derivadas en los extremos de cada parte coincidan con los de la parWHDG\DFHQWH(QHVHQFLDHQWRQFHVSRGHPRVVLPSOL√ÄFDUHOSURFHVRHQXQRTXHGHWHUPLQHXQ
par de polinomios de Hermite c√∫bicos en el par√°metro t, donde t0 = 0 y t1 = 1, dados los datos
del extremo (x(0), y(0)) y (x(1), y(1)) y las derivadas dy/d x (en t = 0) y dy/d x (en t = 1).

3.6

Curvas param√©tricas

123

Sin embargo, observe que s√≥ORHVSHFL√ÄFDPRVVHLVFRQGLFLRQHV\ORVSROLQRPLRVF~ELFRV
en x(t) y y(t), cada uno tiene cuatro par√°metros, para un total de ocho. Esto proporciona
√ÅH[LELOLGDGDOVHOHFFLRQDUHOSDUF~ELFRGHSROLQRPLRVGH+HUPLWHSDUDVDWLVIDFHUODVFRQdiciones porque la forma natural para determinar x(t) y y(t  UHTXLHUH TXH HVSHFL√ÄTXHPRV
x (0), x (1), y (0), y y (1). La curva de Hermite expl√≠cita en x y yUHTXLHUHHVSHFL√ÄFDUVRODmente los cocientes

dy
y (0)
(t = 0) =
dx
x (0)

y

dy
y (1)
(t = 1) =
.
dx
x (1)

Al multiplicar x9(0) y y9(0) por un factor de escala com√∫n, la recta tangente para la curva
en (x(0), y(0)) permanece igual, pero la forma de la curva var√≠a. Mientras m√°s grande sea el
factor de escala, m√°s cerca est√° la curva de aproximaci√≥n a la recta tangente en las cercan√≠as
de (x(0), y(0)) . Existe una situaci√≥n similar en el otro extremo (x(1), y(1)).
3DUDVLPSOL√ÄFDUPiVHOSURFHVRHQODVJUi√ÄFDVFRPSXWDFLRQDOHVLQWHUDFWLYDVODGHULYDGD
HQXQSXQWRH[WUHPRVHHVSHFL√ÄFDXVDQGRXQVHJXQGRSXQWROODPDGRpunto gu√≠a, en una
recta tangente deseada. Mientras m√°s lejos est√© del nodo, m√°s cerca se aproxima la curva a
la recta tangente cerca del nodo.
(QOD√ÄJXUDORVQRGRVVHSUHVHQWDQHQ x0, y0) y (x1, y1), el punto gu√≠a para (x0, y0)
es (x0 + Œ±0 , y0 + Œ≤0 ), y el punto gu√≠a para (x1 , y1 ) es (x1 ‚àí Œ±1 , y1 ‚àí Œ≤1 ). El polinomio de
Hermite c√∫bico x(t HQ>@VDWLVIDFH

x(0) = x0 ,

x(1) = x1 ,

x (0) = Œ±0 ,

y

x (1) = Œ±1 .

El √∫nico polinomio c√∫bico que satisface estas condiciones es

x(t) = [2(x0 ‚àí x1 ) + (Œ±0 + Œ±1 )]t 3 + [3(x1 ‚àí x0 ) ‚àí (Œ±1 + 2Œ±0 )]t 2 + Œ±0 t + x0 .

(3.23)

De manera similar, el √∫nico polinomio c√∫bico que satisface

y(0) = y0 ,

y(1) = y1 ,

y (0) = Œ≤0 ,

y

y (1) = Œ≤1

es

y(t) = [2(y0 ‚àí y1 ) + (Œ≤0 + Œ≤1 )]t 3 + [3(y1 ‚àí y0 ) ‚àí (Œ≤1 + 2Œ≤0 )]t 2 + Œ≤0 t + y0 .

Figura 3.17
y
(x 0 1 a 0, y0 1 b0)
(x1 2 a 1, y1 2 b1)
(x 0, y 0)
(x1, y1)
x

(3.24)

124

CAP√çTULO 3

Interpolaci√≥n y aproximaci√≥n polinomial

'HWHUPLQH OD JUi√ÄFD GH OD FXUYD SDUDPpWULFD JHQHUDGD SRU ODV HFXDFLRQHV   \  
cuando los extremos son (x0 , y0 ) = (0, 0) y (x1 , y1 ) = (1, 0) y los puntos gu√≠a respectivos,
FRPRVHPXHVWUDHQOD√ÄJXUDVRQ  \  

Ejemplo 2

y

Puntos gu√≠a
(1, 1)

(0, 1)

La informaci√≥n del extremo implica que x0 = 0, x1 = 1, y0 = 0, y y1 = 0, y los
puntos gu√≠a (1, 1) y (0, 1) implican que Œ±0 = 1, Œ±1 = 1, Œ≤0 = 1, y Œ≤1 = ‚àí1. Observe que las
pendientes de las rectas gu√≠a en (0, 0) y (1, 0) son, respectivamente,
Soluci√≥n

Nodos
(0, 0)

(1, 1)

1
Œ≤0
= =1
Œ±0
1

x

Figura 3.18

y

Œ≤1
‚àí1
= ‚àí1.
=
Œ±1
1

Las ecuaciones (3.23) y (3.24) implican que para t ‚àà [0, 1], tenemos

3LHUUH(WLHQQH%p]LHU ¬≤
1999) fue director de dise√±o y
producci√≥n de los autom√≥viles
Renault durante la mayor
parte de su vida profesional.
Comenz√≥ su investigaci√≥n en
dise√±o y fabricaci√≥n asistidos
por computadora en 1960,
desarrollando herramientas
interactivas para el dise√±o de
FXUYDV\VXSHU√ÄFLHVHLQLFLy
el bobinado generado por
computadora para modelado
de autom√≥viles. Las curvas de
%p]LHUTXHOOHYDQVXQRPEUH
tienen la ventaja de estar basadas
en una teor√≠a matem√°tica
rigurosa que no necesita ser
reconocida expl√≠citamente por
el practicante, quien s√≥lo quiere
FUHDUXQDFXUYDRVXSHU√ÄFLH
agradable desde el punto de vista
est√©tico. √âstas son las curvas que
son la base del poderoso sistema
Adobe Postscript y producen
curvas a mano alzada generadas
en la mayor√≠a de los paquetes
JUi√ÄFRVFRPSXWDFLRQDOHV
VX√ÄFLHQWHPHQWHSRWHQWHV

x(t) =[2(0 ‚àí 1) + (1 + 1)]t 3 + [3(0 ‚àí 0) ‚àí (1 + 2 ¬∑ 1)]t 2 + 1 ¬∑ t + 0 = t
y

y(t) =[2(0 ‚àí 0) + (1 + (‚àí1))]t 3 + [3(0 ‚àí 0) ‚àí (‚àí1 + 2 ¬∑ 1)]t 2 + 1 ¬∑ t + 0 = ‚àít 2 + t.
(VWDJUi√ÄFDVHPXHVWUDFRPRD HQOD√ÄJXUDMXQWRFRQDOJXQDVRWUDVSRVLELOLGDGHV
de curvas producidas por las ecuaciones (3.23) y (3.24) cuando los nodos son (0, 0) y (1, 0)
y las pendientes de estos nodos son 1 y 21, respectivamente.
(O SURFHGLPLHQWR HVWiQGDU SDUD GHWHUPLQDU ODV FXUYDV HQ XQ PRGR JUi√ÄFR LQWHUDFWLYR
es utilizar primero un mouse (o rat√≥n) o touchpad (panel t√°ctil) y establecer los nodos y
los puntos gu√≠a para generar una primera aproximaci√≥n a la curva. Esto se puede hacer de
PDQHUDPDQXDOSHURPXFKRVVLVWHPDVGHJUi√ÄFDVSHUPLWHQXWLOL]DUVXGLVSRVLWLYRGHHQWUDGD
para trazar la curva en una pantalla a mano alzada y seleccionar los nodos y puntos gu√≠a
adecuados para su curva a mano alzada.
A continuaci√≥n, los nodos y los puntos gu√≠a se pueden manipular en una posici√≥n que
genera una curva agradable desde el punto de vista est√©tico. Puesto que los c√°lculos son
m√≠nimos, la curva se puede determinar tan r√°pido que el cambio resultante se aprecia de
inmediato. Adem√°s, todos los datos necesarios para calcular las curvas est√°n incorporados
en las coordenadas de los nodos y puntos gu√≠a, por lo que no se requiere que el usuario tenga
conocimiento anal√≠tico.

Figura 3.19

y

y

(1, 1)
1

(0, 1)

1

(1, 1)

(0.75, 0.25)

1

2
a)

x

1

2
b)

x

3.6

125

Curvas param√©tricas

Figura 3.19
y

y
(2, 2)
2

2

1

1

1

(0.5, 0.5)

1
1

2

21

(2, 21)
c)

x

2

21

x

(2, 21)
d)

/RVSURJUDPDVGHJUi√ÄFRVSRSXODUHVXVDQHVWHWLSRGHVLVWHPDSDUDVXVUHSUHVHQWDFLRQHV
JUi√ÄFDVDPDQRDO]DGDHQXQDIRUPDOLJHUDPHQWHPRGL√ÄFDGD/RVF~ELFRVGH+HUPLWHVHGHVcriben como polinomios de B√©zier, los cuales incluyen un factor de escala de tres al calcular
ODVGHULYDGDVHQORVH[WUHPRV(VWRPRGL√ÄFDODVHFXDFLRQHVSDUDPpWULFDVSDUD

x(t) = [2(x0 ‚àí x1 ) + 3(Œ±0 + Œ±1 )]t 3 + [3(x1 ‚àí x0 ) ‚àí 3(Œ±1 + 2Œ±0 )]t 2 + 3Œ±0 t + x0 (3.25)
y

y(t) = [2(y0 ‚àí y1 ) + 3(Œ≤0 + Œ≤1 )]t 3 + [3(y1 ‚àí y0 ) ‚àí 3(Œ≤1 + 2Œ≤0 )]t 2 + 3Œ≤0 t + y0 , (3.26)
para 0 ‚â§ t ‚â§ 1, pero este cambio es transparente para el usuario del sistema.
(ODOJRULWPRFRQVWUX\HXQFRQMXQWRGHFXUYDVGH%p]LHUFRQEDVHHQODVHFXDFLRQHV
param√©tricas en las ecuaciones (3.25) y (3.26).
ALGORITMO

3.6

Curva de B√©zier
3DUDFRQVWUXLUODVFXUYDVF~ELFDVGH%p]LHU C0 , . . . , Cn‚àí1 en forma param√©trica, donde Ci
est√° representado por

(xi (t), yi (t)) = (a0(i) + a1(i) t + a2(i) t 2 + a3(i) t 3 , b0(i) + b1(i) t + b2(i) t 2 + b3(i) t 3 ),
para 0 ‚â§ t ‚â§ 1, como se determina mediante el extremo izquierdo (xi , yi ), el punto gu√≠a
‚àí
‚àí
(xi+ , yi+ ), el extremo derecho (xi+1 , yi+1 ), y el punto gu√≠a derecho (xi+1
, yi+1
) para cada
i = 0, 1, . . . , n ‚àí 1:
+
+
, yn‚àí1
); (x1‚àí , y1‚àí ), . . . , (xn‚àí , yn‚àí ).
ENTRADA n; (x0 , y0 ), . . . , (xn , yn ); (x0+ , y0+ ), . . . , (xn‚àí1

SALIDA coeficientes {a0 (i), a1 (i), a2 (i), a3 (i), b0 (i), b1 (i), b2 (i), b3 (i), para 0 ‚â§ i ‚â§ n ‚àí 1}.

126

CAP√çTULO 3

Interpolaci√≥n y aproximaci√≥n polinomial

Paso 1 Para cada i = 0, 1, . . . , n ‚àí 1 haga los pasos 2 y 3.
Paso 2 Haga a0(i) = xi ;
b0(i) = yi ;
a1(i) = 3(xi+ ‚àí xi );
b1(i) = 3(yi+ ‚àí yi );

‚àí
‚àí 2xi+ );
a2(i) = 3(xi + xi+1
‚àí
‚àí 2yi+ );
b2(i) = 3(yi + yi+1

‚àí
;
a3(i) = xi+1 ‚àí xi + 3xi+ ‚àí 3xi+1
‚àí
;
b3(i) = yi+1 ‚àí yi + 3yi+ ‚àí 3yi+1

Paso 3 SALIDA (a0(i) , a1(i) , a2(i) , a3(i) , b0(i) , b1(i) , b2(i) , b3(i) ).
Paso 4 PARE.

/DVFXUYDVWULGLPHQVLRQDOHVVHJHQHUDQGHODPLVPDIRUPDDOHVSHFL√ÄFDUDGLFLRQDOPHQWH
terceros componentes z0 y z1 para los nodos y z 0 +Œ≥0 y z 1 ‚àíŒ≥1 para los puntos gu√≠a. El problema m√°s dif√≠cil que implica la representaci√≥n de las curvas tridimensionales se preocupa por
la p√©rdida de la tercera dimensi√≥n cuando la curva se proyecta en una pantalla bidimensional
de computadora. Se utilizan varias t√©cnicas de proyecci√≥n, pero este tema se encuentra denWURGHOFDPSRGHODVJUi√ÄFDVSRUFRPSXWDGRU3DUDXQDLQWURGXFFLyQDHVWHWHPD\ODVPDQHUDVHQODVTXHODWpFQLFDVHSXHGHPRGL√ÄFDUSDUDODVUHSUHVHQWDFLRQHVGHVXSHU√ÄFLHFRQVXOWH
DOJXQRGHORVPXFKRVOLEURVVREUHPpWRGRVJUi√ÄFRVSRUFRPSXWDGRUDFRPR>)9)+@
La secci√≥n Conjunto de ejercicios 3.6 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

3.7 Software num√©rico y revisi√≥n del cap√≠tulo
/DVUXWLQDVGHLQWHUSRODFLyQLQFOXLGDVHQOD%LEOLRWHFD,06/VHEDVDQHQHOOLEURA practical
Guide to Splines (Una gu√≠a pr√°ctica para splines)GH&DUOGH%RRU>'HE@\XVDQODLQWHUpolaci√≥n mediante splines c√∫bicos. Existen splines c√∫bicos para minimizar las oscilaciones
y preservar la concavidad. Los m√©todos para interpolaci√≥n bidimensional mediante splines
bic√∫bicos tambi√©n se incluyen.
La biblioteca NAG contiene subrutinas para la interpolaci√≥n polinomial y de Hermite,
para interpolaci√≥n de spline c√∫bico y para interpolaci√≥n de Hermite c√∫bico por tramos. NAG
tambi√©n contiene subrutinas para funciones de interpolaci√≥n de dos variables.
La biblioteca netlib contiene las subrutinas para calcular el spline c√∫bico con varias conGLFLRQHVGHH[WUHPR8QSDTXHWHSURGXFHORVFRH√ÄFLHQWHVGHGLIHUHQFLDGLYLGLGDGH1HZWRQ
para un conjunto discreto de puntos de datos y existen diferentes rutinas para evaluar los
polinomios por tramos de Hermite.
Las secciones Preguntas de an√°lisis, Conceptos clave y Revisi√≥n del cap√≠tulo est√°n disponibles en l√≠nea. Encuentre la ruta de acceso en las p√°ginas preliminares.

CAP√çTULO

4

Diferenciaci√≥n num√©rica e integraci√≥n

Introducci√≥n
Una hoja de tejado corrugado se construye al presionar una hoja plana de aluminio dentro de
otra cuya secci√≥n transversal tiene la forma de una onda senoidal.

Se necesita una hoja corrugada de 4 pies de largo, la altura de cada onda es de 1 pulgada
desde la l√≠nea central y cada onda tiene un periodo de aproximadamente 2œÄ. El problema de
encontrar la longitud de la hoja plana inicial consiste en encontrar la longitud de la curva determinada por f (x) 5 sen x desde x 5 0 pulgadas hasta x 5 48 pulgadas. A partir del c√°lculo,
sabemos que esta longitud es
48

L=
0

48

1 + ( f (x))2 d x =

1 + (cos x)2 d x,

0

por lo que el problema se reduce a evaluar esta integral. A pesar de que la funci√≥n senoidal
es una de las funciones matem√°ticas m√°s comunes, el c√°lculo de su longitud implica una
integral el√≠ptica de segunda clase, que no se puede evaluar de manera expl√≠cita. En este cap√≠tulo se desarrollan m√©todos para aproximar la soluci√≥n a los problemas de este tipo. Este
problema particular se considera en el ejercicio 21 de la secci√≥n 4.4, en el ejercicio 15 de la
secci√≥n 4.5 y en el ejercicio 10 de la secci√≥n 4.7.
En la introducci√≥n del cap√≠tulo 3 mencionamos que una raz√≥n para usar polinomios
algebraicos para aproximar un conjunto arbitrario de datos es que, dada cualquier funci√≥n
FRQWLQXDGH√ÄQLGDGHQWURGHXQLQWHUYDORFHUUDGRH[LVWHXQSROLQRPLRTXHHVWiDUELWUDULDPHQte cerca de la funci√≥n en cada punto del intervalo. Adem√°s, las derivadas y las integrales de
los polinomios se obtienen y eval√∫an con facilidad. No deber√≠a sorprender, entonces, que
muchos procedimientos para aproximar derivadas e integrales utilicen los polinomios
que aproximan la funci√≥n.

127

128

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

4.1 Diferenciaci√≥n num√©rica
La derivada de la funci√≥n f en x0 es

f (x0 ) = l√≠m

h‚Üí0

f (x0 + h) ‚àí f (x0 )
.
h

Esta f√≥rmula proporciona una forma obvia de generar una aproximaci√≥n para f (x0 ); simplemente calcule

f (x0 + h) ‚àí f (x0 )
h
para valores peque√±os de h. Aunque esto puede ser obvio, no tiene mucho √©xito debido a
nuestro antiguo n√©mesis, el error de redondeo. Sin embargo, es un lugar para empezar.
Para aproximar f (x0 ), suponga primero que x0 ‚àà (a, b), donde f ‚àà C 2 [a, b], y
que x1 = x0 +h para alguna h = 0  TXH HV VX√ÄFLHQWHPHQWH SHTXHxD SDUD JDUDQWL]DU TXH
x1 ‚àà [a, b]. Nosotros construimos el primer polinomio de Lagrange P0,1 (x) para f determinado por x0 y x1, con su t√©rmino de error:

(x ‚àí x0 )(x ‚àí x1 )
f (Œæ(x))
2!
f (x0 )(x ‚àí x0 ‚àí h)
f (x0 + h)(x ‚àí x0 ) (x ‚àí x0 )(x ‚àí x0 ‚àí h)
=
+
+
f (Œæ(x)),
‚àíh
h
2

f (x) = P0,1 (x) +

para algunos Œæ(x) entre x0 y x1. Derivando obtenemos

f (x) =
=

(x ‚àí x0 )(x ‚àí x0 ‚àí h)
f (x0 + h) ‚àí f (x0 )
+ Dx
f (Œæ(x))
h
2
f (x0 + h) ‚àí f (x0 ) 2(x ‚àí x0 ) ‚àí h
+
f (Œæ(x))
h
2
(x ‚àí x0 )(x ‚àí x0 ‚àí h)
+
Dx ( f (Œæ(x))).
2

Borrando los t√©rminos relacionados con Œæ(x) obtenemos

f (x) ‚âà

Isaac Newton us√≥ y populariz√≥
las ecuaciones de diferencias
en el √∫ltimo cuarto del siglo
XVII, pero muchas de estas
t√©cnicas fueron desarrolladas
previamente por Thomas Harriot
(1561‚Äì1621) y Henry Briggs
(1561‚Äì1630). Harriot realiz√≥
DYDQFHVVLJQL√ÄFDWLYRVHQWpFQLFDV
de navegaci√≥n y Briggs fue la
persona m√°s responsable de
la aceptaci√≥n de logaritmos como
auxiliares para el c√°lculo.

f (x0 + h) ‚àí f (x0 )
.
h

8QDGL√ÄFXOWDGFRQHVWDIyUPXODHVTXHQRWHQHPRVLQIRUPDFLyQVREUH Dx f (Œæ(x)), por lo
que el error de truncamiento no se puede calcular. Cuando x es x0VLQHPEDUJRHOFRH√ÄFLHQWH
de Dx f (Œæ(x))HV\ODIyUPXODVHVLPSOL√ÄFDHQ

f (x0 ) =

f (x0 + h) ‚àí f (x0 ) h
‚àí f (Œæ ).
h
2

(4.1)

Para los valores peque√±os de h, el cociente de diferencia [ f (x0 + h) ‚àí f (x0 )]/ h se
puede utilizar para aproximar f (x0 ) con un error acotado por M|h|/2, donde M es una cota
de | f (x)| para x entre x0 y x0 + h. A esta f√≥rmula se le conoce como f√≥rmula de diferencias
hacia adelante si h > YpDVHOD√ÄJXUD \FRPRf√≥rmula de diferencias hacia atr√°s
si h < 0.

4.1

Diferenciaci√≥n num√©rica

129

Figura 4.1
y

Pendiente f 9(x 0)
Pendiente

x0

Ejemplo 1

f (x0 1 h) 2 f (x 0)
h

x

x0 1 h

Use la f√≥rmula de diferencias hacia adelante para aproximar la derivada de f (x) = ln x
en x0 = 1.8 mediante h = 0.1, h = 0.05, y h = 0.01 y determine las cotas para los errores
de aproximaci√≥n.
Soluci√≥n

La f√≥rmula de diferencias hacia adelante

f (1.8 + h) ‚àí f (1.8)
h
con h = 0.1 nos da
ln 1.9 ‚àí ln 1.8
0.64185389 ‚àí 0.58778667
=
= 0.5406722.
0.1
0.1
Puesto que f (x) = ‚àí1/x 2 y 1.8 < Œæ < 1.9, una cota para este error de aproximaci√≥n es
|h f (Œæ )|
|h|
0.1
= 2 <
= 0.0154321.
2
2Œæ
2(1.8)2
La aproximaci√≥n y las cotas de error cuando h 5 0.05 y h 5 0.01 se encuentran de manera
similar y los resultados se muestran en la tabla 4.1.

Tabla 4.1

h

f (1.8 + h)

f (1.8 + h) ‚àí f (1.8)
h

|h|
2(1.8)2

0.1
0.05
0.01

0.64185389
0.61518564
0.59332685

0.5406722
0.5479795
0.5540180

0.0154321
0.0077160
0.0015432

Puesto que f (x) = 1/x , el valor exacto de f (1.8) es 0.555, y en este caso la cota del
error est√° bastante cerca del verdadero error de aproximaci√≥n.
Para obtener las f√≥rmulas generales de aproximaci√≥n a la derivada, suponga que
{x0 , x1 , . . . , xn } son (n + 1) n√∫meros distintos en alg√∫n intervalo I y que f ‚àà C n+1 (I ). A
partir del teorema 3.3 en la p√°gina 83,
n

f (x) =

f (xk )L k (x) +
k=0

(x ‚àí x0 ) ¬∑ ¬∑ ¬∑ (x ‚àí xn ) (n+1)
(Œæ(x)),
f
(n + 1)!

130

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

para algunos Œæ(x) en I , donde L k (x) denota el k-√©simoFRH√ÄFLHQWHGHOSROLQRPLRGH/DJUDQge para f en x0 , x1 , . . . , xn. Al derivar esta ecuaci√≥n obtenemos
n

f (x) =

f (xk )L k (x) + Dx
k=0

+

(x ‚àí x0 ) ¬∑ ¬∑ ¬∑ (x ‚àí xn )
(n + 1!)

f (n+1) (Œæ(x))

(x ‚àí x0 ) ¬∑ ¬∑ ¬∑ (x ‚àí xn )
Dx [ f (n+1) (Œæ(x))].
(n + 1)!

De nuevo tenemos un problema al calcular el error de truncamiento a menos que x sea
uno de los n√∫meros xj. En este caso, el t√©rmino que multiplica Dx [ f (n+1) (Œæ(x))] es 0 y la
f√≥rmula se vuelve
n

n

f (x j ) =

f (xk )L k (x j ) +
k=0

f (n+1) (Œæ(x j ))
(x j ‚àí xk ),
(n + 1)! k=0

(4.2)

k= j

que recibe el nombre de f√≥rmula de (n 1 1) puntos para aproximar f (x j ).
En general, el uso de m√°s puntos de evaluaci√≥n en la ecuaci√≥n (4.2) produce mayor
precisi√≥n, a pesar de que el n√∫mero de evaluaciones funcionales y el crecimiento del error de
redondeo disuade un poco esto. Las f√≥rmulas m√°s comunes son las relacionadas con tres y
cinco puntos de evaluaci√≥n.
Primero derivamos algunas f√≥rmulas √∫tiles de tres puntos y consideramos aspectos de
sus errores. Puesto que

L 0 (x) =

(x ‚àí x1 )(x ‚àí x2 )
,
(x0 ‚àí x1 )(x0 ‚àí x2 )

tenemos L 0 (x) =

2x ‚àí x1 ‚àí x2
.
(x0 ‚àí x1 )(x0 ‚àí x2 )

De igual forma,

L 1 (x) =

2x ‚àí x0 ‚àí x2
(x1 ‚àí x0 )(x1 ‚àí x2 )

y L 2 (x) =

2x ‚àí x0 ‚àí x1
.
(x2 ‚àí x0 )(x2 ‚àí x1 )

Por lo tanto, a partir de la ecuaci√≥n (4.2),

f (x j ) = f (x0 )

2x j ‚àí x1 ‚àí x2
2x j ‚àí x0 ‚àí x2
+ f (x1 )
(x0 ‚àí x1 )(x0 ‚àí x2 )
(x1 ‚àí x0 )(x1 ‚àí x2 )
2

+ f (x2 )

1
2x j ‚àí x0 ‚àí x1
+ f (3) (Œæ j ) (x j ‚àí xk ),
(x2 ‚àí x0 )(x2 ‚àí x1 )
6
k=0

(4.3)

k= j

para cada j 5 0, 1, 2, donde la notaci√≥n Œæ j indica que este punto depende de xj.

F√≥rmulas de tres puntos
Las f√≥rmulas a partir de la ecuaci√≥n (4.3) se vuelven especialmente √∫tiles si los nodos est√°n
espaciados de manera uniforme, es decir, cuando

x1 = x0 + h

y x2 = x0 + 2h,

para algunas h = 0.

Supondremos nodos igualmente espaciados a lo largo del resto de esta secci√≥n.
Por medio de la ecuaci√≥n (4.3) con x j = x0 , x1 = x0 + h, y x2 = x0 + 2h obtenemos

f (x0 ) =

1
3
1
h 2 (3)
f (Œæ0 ).
‚àí f (x0 ) + 2 f (x1 ) ‚àí f (x2 ) +
h
2
2
3

Al hacer lo mismo para x j = x1 obtenemos

f (x1 ) =

1
1
h 2 (3)
1
f (Œæ1 )
‚àí f (x0 ) + f (x2 ) ‚àí
h
2
2
6

4.1

Diferenciaci√≥n num√©rica

131

y, para x j = x2 ,

f (x2 ) =

1 1
3
h 2 (3)
f (x0 ) ‚àí 2 f (x1 ) + f (x2 ) +
f (Œæ2 ).
h 2
2
3

Puesto que x1 = x0 + h y x2 = x0 + 2h, estas f√≥rmulas tambi√©n se pueden expresar
como

f (x0 ) =

1
3
1
h 2 (3)
f (Œæ0 ),
‚àí f (x0 ) + 2 f (x0 + h) ‚àí f (x0 + 2h) +
h
2
2
3

f (x0 + h) =

1
1
1
h 2 (3)
f (Œæ1 ),
‚àí f (x0 ) + f (x0 + 2h) ‚àí
h
2
2
6

f (x0 + 2h) =

1 1
3
h 2 (3)
f (x0 ) ‚àí 2 f (x0 + h) + f (x0 + 2h) +
f (Œæ2 ).
h 2
2
3

y

Por cuestiones de conveniencia, la sustituci√≥n de la variable x0 por x0 1 h se usa en
medio de la ecuaci√≥n para cambiar esta f√≥rmula por una aproximaci√≥n para f 9(x0). Un cambio similar, x0 para x0 1 2h, se utiliza en la √∫ltima ecuaci√≥n. Esto nos da tres f√≥rmulas para
aproximar f 9(x0)

f (x0 ) =

1
h 2 (3)
[‚àí3 f (x0 ) + 4 f (x0 + h) ‚àí f (x0 + 2h)] +
f (Œæ0 ),
2h
3

f (x0 ) =

1
h 2 (3)
[‚àí f (x0 ‚àí h) + f (x0 + h)] ‚àí
f (Œæ1 ),
2h
6

f (x0 ) =

1
h 2 (3)
[ f (x0 ‚àí 2h) ‚àí 4 f (x0 ‚àí h) + 3 f (x0 )] +
f (Œæ2 ).
2h
3

y

Finalmente, observe que la √∫ltima de estas ecuaciones se puede obtener a partir de la primera
simplemente al reemplazar h por 2h, de modo que en realidad s√≥lo son dos f√≥rmulas:

F√≥rmula del extremo de tres puntos
‚Ä¢

f (x0 ) =

1
h 2 (3)
[‚àí3 f (x0 ) + 4 f (x0 + h) ‚àí f (x0 + 2h)] +
f (Œæ0 )
2h
3

(4.4)

donde j0 se encuentra entre x0 y x0 1 2h.

F√≥rmula del punto medio de tres puntos
‚Ä¢

f (x0 ) =

1
h 2 (3)
[ f (x0 + h) ‚àí f (x0 ‚àí h)] ‚àí
f (Œæ1 ),
2h
6

(4.5)

donde j1 se encuentra entre x0 2 h y x0 1 h.
A pesar de que los errores en las dos ecuaciones (4.4) y (4.5) son O(h2), el error en la
ecuaci√≥n (4.5) es aproximadamente la mitad del error en la ecuaci√≥n (4.4). Esto porque
la ecuaci√≥n (4.5) utiliza datos en ambos lados de x0 y la ecuaci√≥n (4.4) los usa en un solo
lado. Tambi√©n observe que f se debe evaluar solamente en dos puntos en la ecuaci√≥n (4.5),
PLHQWUDVTXHHQODHFXDFLyQ  VHQHFHVLWDQWUHVHYDOXDFLRQHV/D√ÄJXUDLOXVWUDODDSURximaci√≥n producida a partir de la ecuaci√≥n (4.5). La aproximaci√≥n en la ecuaci√≥n (4.4) es
√∫til cerca de los extremos de un intervalo porque la informaci√≥n sobre f fuera del intervalo
puede no estar disponible.

132

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

Figura 4.2
y
Pendiente f 9(x 0)

Pendiente

x0 2 h

x0

1
[ f (x0 1 h) 2 f (x 0 2 h)]
2h

x

x0 1 h

F√≥rmulas de cinco puntos
Los m√©todos presentados en las ecuaciones (4.4) y (4.5) reciben el nombre de f√≥rmulas de
tres puntos (aunque el tercer punto f (x0) no aparece en la ecuaci√≥n (4.5)). De igual forma,
existen f√≥rmulas de cinco puntos que implican la evaluaci√≥n de la funci√≥n en dos puntos
adicionales. El t√©rmino de error para estas f√≥rmulas es O(h4). Una f√≥rmula de cinco puntos com√∫n se usa para determinar las aproximaciones para la derivada en el punto medio.

F√≥rmula del punto medio de cinco puntos
‚Ä¢

f (x0 ) =

1
h 4 (5)
[ f (x0 ‚àí 2h) ‚àí 8 f (x0 ‚àí h) + 8 f (x0 + h) ‚àí f (x0 + 2h)] +
f (Œæ ),
12h
30
(4.6)

donde j se encuentra entre x0 2 2h y x0 1 2h.
La deducci√≥n de esta f√≥rmula se considera en la secci√≥n 4.2. La otra f√≥rmula de cinco puntos
se usa para aproximaciones en los extremos.

F√≥rmula del extremo de cinco puntos
‚Ä¢

f (x0 ) =

1
[‚àí25 f (x0 ) + 48 f (x0 + h) ‚àí 36 f (x0 + 2h)
12h
+ 16 f (x0 + 3h) ‚àí 3 f (x0 + 4h)] +

h 4 (5)
f (Œæ ),
5

(4.7)

donde j se encuentra entre x0 1 4h.
Las aproximaciones del extremo izquierdo se encuentran mediante esta f√≥rmula con h > 0
y las aproximaciones del extremo derecho con h < 0. La f√≥rmula del extremo de cinco puntos
es especialmente √∫til para la interpolaci√≥n de spline c√∫bico condicionado de la secci√≥n 3.5.
Ejemplo 2

Los valores para f (x) = xe x se dan en la tabla 4.2. Utilice las f√≥rmulas aplicables de tres y
cinco puntos para aproximar f (2.0).

4.1

Tabla 4.2
x

f (x)

1.8
1.9
2.0
2.1
2.2

10.889365
12.703199
14.778112
17.148957
19.855030

Diferenciaci√≥n num√©rica

133

Soluci√≥n Los datos en la tabla nos permiten encontrar cuatro diferentes aproximaciones de
tres puntos. Podemos usar la f√≥rmula del extremo (4.4) con h 5 0.1 o con h 5 20.1, y la
f√≥rmula del punto medio (4.5) con h 5 0.1 o con h 5 0.2.
Mediante la f√≥rmula del extremo (4.4) con h 5 0.1 obtenemos

1
[‚àí3 f (2.0) + 4 f (2.1) ‚àí f (2.2] = 5[‚àí3(14.778112) + 4(17.148957) ‚àí 19.855030)]
0.2
= 22.032310
y con h 5 20.1 nos da 22.054525.
Por medio de la f√≥rmula del punto medio (4.5) con h 5 0.1 obtenemos

1
[ f (2.1) ‚àí f (1.9] = 5(17.148957 ‚àí 12.7703199) = 22.228790
0.2
y con h 5 0.2 nos da 22.414163.
/D~QLFDIyUPXODGHFLQFRSXQWRVSDUDODTXHODWDEODSURYHHGDWRVVX√ÄFLHQWHVHVODIyUPXOD
del punto medio (4.6) con h 5 0.1. Esto nos da

1
1
[ f (1.8) ‚àí 8 f (1.9) + 8 f (2.1) ‚àí f (2.2)] =
[10.889365 ‚àí 8(12.703199)
1.2
1.2
+ 8(17.148957) ‚àí 19.855030]
= 22.166999.
Si no tenemos m√°s informaci√≥n, aceptar√≠amos la aproximaci√≥n del punto medio de cinco
puntos mediante h 5 0.1 como la m√°s apropiada y esperamos que el valor verdadero se encuentre entre esa aproximaci√≥n y la aproximaci√≥n del punto medio de tres puntos, es decir,
en el intervalo [22.166, 22.229].
En este caso, el valor verdadero es f (2.0) = (2 + 1)e2 = 22.167168, por lo que, en
realidad, los errores de aproximaci√≥n son los siguientes:

Extremo de tres puntos con h = 0.1: 1.35 √ó 10‚àí1 ;
Extremo de tres puntos con h = ‚àí0.1: 1.13 √ó 10‚àí1 ;
Punto medio de tres puntos con h = 0.1: ‚àí6.16 √ó 10‚àí2 ;
Punto medio de tres puntos con h = 0.2: ‚àí2.47 √ó 10‚àí1 ;
Punto medio de cinco puntos con h = 0.1: 1.69 √ó 10‚àí4 .
Los m√©todos tambi√©n se pueden deducir para encontrar aproximaciones para derivadas
superiores de una funci√≥n que s√≥lo usa valores tabulados de la funci√≥n en diferentes puntos.
La deducci√≥n es algebraicamente tediosa, sin embargo, s√≥lo se presentar√° un procedimiento
representativo.
Represente una funci√≥n f mediante la expansi√≥n de un tercer polinomio de Taylor sobre
un punto x0 y eval√∫e x0 + h y x0 2 h. Entonces,

f (x0 + h) = f (x0 ) + f (x0 )h +

1
1
1 (4)
f (x0 )h 2 + f (x0 )h 3 +
f (Œæ1 )h 4
2
6
24

y

f (x0 ‚àí h) = f (x0 ) ‚àí f (x0 )h +

1
1
1 (4)
f (x0 )h 2 ‚àí f (x0 )h 3 +
f (Œæ‚àí1 )h 4 ,
2
6
24

donde x0 ‚àí h < Œæ‚àí1 < x0 < Œæ1 < x0 + h.

134

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

Si adicionamos estas ecuaciones, los t√©rminos relacionados con f (x0 ) y ‚àí f (x0 ) se
cancelan, por lo que

f (x0 + h) + f (x0 ‚àí h) = 2 f (x0 ) + f (x0 )h 2 +

1 (4)
[ f (Œæ1 ) + f (4) (Œæ‚àí1 )]h 4 .
24

Al resolver esta ecuaci√≥n para f (x0 ) obtenemos

f (x0 ) =

1
h2
[ f (x0 ‚àí h) ‚àí 2 f (x0 ) + f (x0 + h)] ‚àí [ f (4) (Œæ1 ) + f (4) (Œæ‚àí1 )]. (4.8)
2
h
24

Suponga que f (4) es continua en [x0 2 h, x0 1 h]. Puesto que 12 [ f (4) (Œæ1 ) + f (4) (Œæ‚àí1 )] se
encuentra entre f (4) (Œæ1 ) y f (4) (Œæ‚àí1 ), el teorema de valor intermedio implica que existe un
n√∫mero j entre Œæ1 y Œæ‚àí1 y, por lo tanto, en (x0 ‚àí h, x0 + h) con

f (4) (Œæ ) =

1 (4)
f (Œæ1 ) + f (4) (Œæ‚àí1 ) .
2

(VWRQRVSHUPLWHUHHVFULELUODHFXDFLyQ  HQVXIRUPD√ÄQDO

F√≥rmula del punto medio de la segunda derivada
‚Ä¢ f (x0 ) =

1
h 2 (4)
[
f
(x
‚àí
h)
‚àí
2
f
(x
)
+
f
(x
+
h)]
‚àí
f (Œæ),
0
0
0
h2
12

(4.9)

Para algunos Œæ , donde x0 ‚àí h < Œæ < x0 + h.
Si f (4) es continua en [x0 2 h, x0 1 h], tambi√©n est√° acotada, y la aproximaci√≥n es O(h2).
Ejemplo 3

Tabla 4.3
x

f (x)

1.8
1.9
2.0
2.1
2.2

10.889365
12.703199
14.778112
17.148957
19.855030

En el ejemplo 2 utilizamos los datos que se muestran en la tabla 4.3 para aproximar la primera derivada de f (x) 5 xex en x 5 2.0. Use la f√≥rmula de la segunda derivada (4.9) para
aproximar f 0(2.0).
Soluci√≥n Los datos nos permiten determinar dos aproximaciones para f 0(2.0). Usando (4.9)
con h 5 0.1 obtenemos

1
[ f (1.9) ‚àí 2 f (2.0) + f (2.1)] = 100[12.703199 ‚àí 2(14.778112) + 17.148957]
0.01
= 29.593200,
y mediante (4.9) con h 5 0.2 obtenemos

1
[ f (1.8) ‚àí 2 f (2.0) + f (2.2)] = 25[10.889365 ‚àí 2(14.778112) + 19.855030]
0.04
= 29.704275.
Puesto que f (x) = (x + 2)e x , el valor exacto es f (2.0) = 29.556224. Por lo tanto, los
errores reales son ‚àí3.70 √ó 10‚àí2 y ‚àí1.48 √ó 10‚àí1 , respectivamente.

Inestabilidad del error de redondeo
Es especialmente importante prestar atenci√≥n al error de redondeo al aproximar derivadas.
Para ilustrar la situaci√≥n, examinemos m√°s de cerca la f√≥rmula del punto medio de tres puntos, la ecuaci√≥n (4.5).

f (x0 ) =

1
h 2 (3)
[ f (x0 + h) ‚àí f (x0 ‚àí h)] ‚àí
f (Œæ1 ),
2h
6

4.1

Diferenciaci√≥n num√©rica

135

Suponga que al evaluar f (x0 + h) y f (x0 ‚àí h), encontramos los errores de redondeo
e(x0 + h) y e(x0 ‚àí h). Entonces nuestros c√°lculos en realidad usan los valores fÀú(x0 + h) y
fÀú(x0 ‚àí h), que est√°n relacionados con los valores verdaderos f(x0 1 h) y f(x0 2 h) mediante

f (x0 + h) = fÀú(x0 + h) + e(x0 + h)

y

f (x0 ‚àí h) = fÀú(x0 ‚àí h) + e(x0 ‚àí h).

El error total en la aproximaci√≥n,

f (x0 ) ‚àí

e(x0 + h) ‚àí e(x0 ‚àí h) h 2 (3)
fÀú(x0 + h) ‚àí fÀú(x0 ‚àí h)
=
‚àí
f (Œæ1 ),
2h
2h
6

se debe tanto al error de redondeo, la primera parte, como al error de truncamiento. Si suponemos que los errores de redondeo e(x0 ¬± h) est√°n acotados por alg√∫n n√∫mero Œµ > 0 y que
la tercera derivada de f est√° acotada por un n√∫mero M > 0, entonces

f (x0 ) ‚àí

Œµ
h2
fÀú(x0 + h) ‚àí fÀú(x0 ‚àí h)
‚â§ + M.
2h
h
6

Para reducir el error de truncamiento h 2 M/6, necesitamos reducir h. Pero conforme h se reduce, el error de redondeo Œµ/ h crece. En la pr√°ctica, entonces, casi nunca es ventajoso dejar
que h sea demasiado peque√±a, porque en este caso, el error de redondeo dominar√° los c√°lculos.
Ilustraci√≥n

Considere utilizar los valores en la tabla 4.4. Para aproximar f (0.900), donde f (x) = sen x.
. El verdadero valor es cos 0.900 5 0.62161. La f√≥rmula

f (0.900) ‚âà

f (0.900 + h) ‚àí f (0.900 ‚àí h)
,
2h

con diferentes valores de h, proporciona las aproximaciones en la tabla 4.5.

Tabla 4.4

x

sen x

x

sen x

0.800
0.850
0.880
0.890
0.895
0.898
0.899

0.71736
0.75128
0.77074
0.77707
0.78021
0.78208
0.78270

0.901
0.902
0.905
0.910
0.920
0.950
1.000

0.78395
0.78457
0.78643
0.78950
0.79560
0.81342
0.84147

Tabla 4.5
h

Aproximaci√≥n
para f (0.900)

Error

0.001
0.002
0.005
0.010
0.020
0.050
0.100

0.62500
0.62250
0.62200
0.62150
0.62150
0.62140
0.62055

0.00339
0.00089
0.00039
‚àí0.00011
‚àí0.00011
‚àí0.00021
‚àí0.00106

La mejor opci√≥n para h parece encontrarse entre 0.005 y 0.05. Podemos utilizar el c√°lcuORSDUDYHUL√ÄFDU FRQVXOWHHOHMHUFLFLR TXHVHSUHVHQWDXQPtQLPRSDUD

e(h) =
‚àö
en h = 3 3Œµ/M, donde
M=

m√°x

x‚àà[0.800,1.00]

| f (x)| =

h2
Œµ
+ M,
h
6

m√°x

x‚àà[0.800,1.00]

| cos x| = cos 0.8 ‚âà 0.69671.

Puesto que los valores de f est√°n determinados para cinco lugares decimales, supondremos
que el error de redondeo est√° limitado por Œµ = 5 √ó 10‚àí6. Por lo tanto, la mejor opci√≥n de h
es aproximadamente

h=

3

3(0.000005)
‚âà 0.028,
0.69671

que es consistente con los resultados en la tabla 4.6.

136

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

En la pr√°ctica no podemos calcular una h √≥ptima para utilizarla en la aproximaci√≥n de
la derivada ya que no conocemos la tercera derivada de la funci√≥n. Sin embargo, debemos
seguir siendo conscientes de que reducir el tama√±o del paso no siempre mejora la aproximaci√≥n.

Tome en cuenta que las
aproximaciones por m√©todo de
diferencias pueden ser inestables.

S√≥lo hemos considerado los problemas de error de redondeo presentados por la f√≥rPXODGHWUHVSXQWRVHFXDFLyQ  SHURVHSUHVHQWDQGL√ÄFXOWDGHVVLPLODUHVFRQWRGDVODV
f√≥rmulas de diferenciaci√≥n. La raz√≥n se puede rastrear a la necesidad de dividir entre una
potencia de h. Como encontramos en la secci√≥n 1.2 (consulte, especialmente, el ejemplo 3),
la divisi√≥n entre n√∫meros peque√±os tiende a exagerar el error de redondeo y, de ser posible,
deber√≠a evitarse esta operaci√≥n. En caso de diferenciaci√≥n num√©rica, no podemos evitar el
SUREOHPDSRUFRPSOHWRDSHVDUGHTXHORVPpWRGRVGHRUGHQVXSHULRUUHGXFHQODGL√ÄFXOWDG
Al igual que los m√©todos de aproximaci√≥n, la diferenciaci√≥n num√©rica es inestable ya
que los valores peque√±os de h necesarios para reducir el error de truncamiento tambi√©n
causan que el error de redondeo crezca. Esta es la primera clase de m√©todos inestables que
hemos encontrado y, de ser posible, estas t√©cnicas deber√≠an evitarse. Sin embargo, adem√°s
de usarse para prop√≥sitos computacionales, las f√≥rmulas son necesarias para aproximar las
soluciones de ecuaciones ordinarias y diferenciales parciales.
La secci√≥n Conjunto de ejercicios 4.1 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las paginas preliminares

4.2 Extrapolaci√≥n de Richardson
La extrapolaci√≥n de Richardson se usa para generar resultados de alta precisi√≥n mientras usa
IyUPXODVGHEDMRRUGHQ$SHVDUGHTXHHOQRPEUHDGMXQWRDOPpWRGRVHUH√ÄHUHDXQDUWtFXOR
escrito por L. F. Richardson y J. A. Gaunt [RG] en 1927, la idea detr√°s de la t√©cnica es mucho
m√°s antigua. Un art√≠culo interesante respecto a la historia y la aplicaci√≥n de la extrapolaci√≥n
se puede encontrar en [Joy].
La extrapolaci√≥n se puede aplicar siempre que se sepa que una t√©cnica de aproximaci√≥n
tiene un t√©rmino de error con un formato predecible, uno que depende de un par√°metro, normalmente el tama√±o de paso h. Suponga que para cada n√∫mero h = 0, tenemos una f√≥rmula
N1 (h) que se aproxima a una constante M desconocida y que el error de truncamiento relacionado con la aproximaci√≥n tiene la forma

M ‚àí N1 (h) = K 1 h + K 2 h 2 + K 3 h 3 + ¬∑ ¬∑ ¬∑ ,
Lewis Fry Richardson (18811953) fue el primero en aplicar
sistem√°ticamente las matem√°ticas
a la predicci√≥n del tiempo
mientras trabajaba en Inglaterra
SDUDOD2√ÄFLQD0HWHRUROyJLFD
Como opositor concienzudo
durante la Primera Guerra
Mundial, escribi√≥ ampliamente
sobre la futilidad econ√≥mica de
la guerra, mediante sistemas
de ecuaciones diferenciales para
modelar interacciones racionales
entre pa√≠ses. La t√©cnica de
extrapolaci√≥n que lleva su
nombre fue el redescubrimiento
de una t√©cnica con ra√≠ces que
son tan antiguas como Christiaan
Hugyens (1629‚Äì1695) y,
posiblemente, Arqu√≠medes
(287‚Äì212 a.C.)

para alg√∫n conjunto de constantes K 1 , K 2 , K 3 , . . . . (desconocidas).
El error de truncamiento es O(h), a menos que haya una variaci√≥n m√°s grande entre las
constantes K 1 , K 2 , K 3 , . . .,

M ‚àí N1 (0.1) ‚âà 0.1K 1 ,

M ‚àí N1 (0.01) ‚âà 0.01K 1 ,

y, en general, M ‚àí N1 (h) ‚âà K 1 h.
El objetivo de la extrapolaci√≥n es encontrar una forma f√°cil de combinarlas en lugar de
aproximaciones O(h) inapropiadas de manera adecuada para producir f√≥rmulas con un error
de truncamiento de orden superior.
Suponga, por ejemplo, que podemos combinar las f√≥rmulas N1(h) para producir una
f√≥rmula de aproximaci√≥n O(h2), N2(h), para M con

M ‚àí N2 (h) = KÃÇ 2 h 2 + KÃÇ 3 h 3 + ¬∑ ¬∑ ¬∑ ,
para alg√∫n, nuevamente desconocido, conjunto de constantes KÃÇ 2 , KÃÇ 3 , . . . . Entonces tenemos

M ‚àí N2 (0.1) ‚âà 0.01 KÃÇ 2 ,

M ‚àí N2 (0.01) ‚âà 0.0001 KÃÇ 2 ,

4.2 Extrapolaci√≥n de Richardson

137

y as√≠ sucesivamente. Si las constantes K 1 y KÃÇ 2 son aproximadamente de la misma magnitud, entonces las aproximaciones N2(h) ser√≠an mucho mejores que las aproximaciones N1(h)
correspondientes. La extrapolaci√≥n contin√∫a al combinar las aproximaciones N2(h) de manera que produce f√≥rmulas con error de truncamiento O(h3) y as√≠ sucesivamente.
3DUD REVHUYDU HVSHFt√ÄFDPHQWH FyPR SRGHPRV JHQHUDU ODV IyUPXODV GH H[WUDSRODFLyQ
considere la f√≥rmula O(h) para aproximar M

M = N1 (h) + K 1 h + K 2 h 2 + K 3 h 3 + ¬∑ ¬∑ ¬∑ .

(4.10)

Se asume que la f√≥rmula se mantiene para todas las h positivas, por lo que reemplazamos el
par√°metro h a la mitad de su valor. A continuaci√≥n, tenemos una segunda f√≥rmula de aproximaci√≥n O(h)

M = N1

h
2

+ K1

h
h2
h3
+ K2 + K3 + ¬∑ ¬∑ ¬∑ .
2
4
8

(4.11)

Restando dos veces la ecuaci√≥n (4.11) de la ecuaci√≥n (4.10) se elimina el t√©rmino relacionado con K1 y nos da

M = N1

h
2

+ N1

h
2

h2
‚àí h2
2

‚àí N2 (h) + K 2

+ K3

h3
‚àí h3
4

+ ¬∑¬∑¬∑ .
(4.12)

'H√ÄQD

N2 (h) = N1

h
2

+ N1

h
2

‚àí N1 (h) .

Entonces, la ecuaci√≥n (4.12) es una f√≥rmula de aproximaci√≥n O(h2) para M:

M = N2 (h) ‚àí
Ejemplo 1

K 2 2 3K 3 3
h ‚àí
h ‚àí ¬∑¬∑¬∑ .
2
4

(4.13)

En el ejemplo 1 de la secci√≥n 4.1, utilizamos el m√©todo de diferencias hacia adelante con
h 5 0.1 y h 5 0.05 para encontrar aproximaciones para f (1.8) para f (x) 5 ln(x). Suponga
que esta f√≥rmula tiene error de truncamiento O(h). Use la extrapolaci√≥n en estos valores para
observar si esto resulta en una mejor aproximaci√≥n.
Soluci√≥n

En el ejemplo 1 de la secci√≥n 4.1, encontramos que

con h = 0.1: f (1.8) ‚âà 0.5406722,

y

con h = 0.05: f (1.8) ‚âà 0.5479795.

Esto implica que

N1 (0.1) = 0.5406722 y

N1 (0.05) = 0.5479795.

La extrapolaci√≥n de estos resultados nos da la nueva aproximaci√≥n

N2 (0.1) = N1 (0.05) + (N1 (0.05) ‚àí N1 (0.1)) = 0.5479795 + (0.5479795 ‚àí 0.5406722)
= 0.555287.
Se encontr√≥ que los resultados h 5 0.1 y h 5 0.05 son precisos dentro de 1.5 3 1022 y 7.7
3 1023, respectivamente. Puesto que f (1.8) = 1/1.8 = 0.5, el valor extrapolado es preciso
dentro de 2.7 3 1024.
La extrapolaci√≥n se puede aplicar siempre que el error de truncamiento para una f√≥rmula
tenga la forma
m‚àí1
j=1

K j h Œ± j + O(h Œ±m )

138

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

para un conjunto de constantes Kj y cuando Œ±1 < Œ±2 < Œ±3 < ¬∑ ¬∑ ¬∑ < Œ±m. Muchas f√≥rmulas
utilizadas en la extrapolaci√≥n tienen errores de truncamiento que s√≥lo contienen potencias
pares de h, es decir, tienen la forma

M = N1 (h) + K 1 h 2 + K 2 h 4 + K 3 h 6 + ¬∑ ¬∑ ¬∑ .

(4.14)

La extrapolaci√≥n es mucho m√°s efectiva cuando todas las potencias de h est√°n presentes debido a que el proceso de promediar genera resultados con errores O(h 2 ), O(h 4 ), O(h 6 ), . . .,
con esencialmente, ning√∫n incremento en el c√°lculo, sobre los resultados con errores
O(h), O(h 2 ), O(h 3 ), . . . .
Suponga que la aproximaci√≥n tiene la forma de la ecuaci√≥n (4.14). Al reemplazar h con
h / 2 obtenemos la f√≥rmula de aproximaci√≥n O(h2)

h
2

M = N1

+ K1

h2
h4
h6
+ K2
+ K3
+ ¬∑¬∑¬∑ .
4
16
64

Al restar cuatro veces esta ecuaci√≥n de la ecuaci√≥n (4.14), se elimina el t√©rmino h2,

3M = 4N1

h
2

h4
‚àí h4
4

‚àí N1 (h) + K 2

h6
‚àí h6
16

+ ¬∑¬∑¬∑ .

K3
3

h6
‚àí h6
16

+ ¬∑¬∑¬∑ .

1
N1
3

h
2

+ K3

Dividiendo esta ecuaci√≥n entre 3, produce una f√≥rmula O(h4)

M=

1
4N1
3

h
2

K2
3

‚àí N1 (h) +

h4
‚àí h4
4

+

h
2

+

$OGH√ÄQLU

N2 (h) =

1
4N1
3

h
2

‚àí N1 (h) = N1

‚àí N1 (h)

produce la f√≥rmula de la aproximaci√≥n con error de truncamiento O(h4):

M = N2 (h) ‚àí K 2

h4
5h 6
‚àí K3
+ ¬∑¬∑¬∑ .
4
16

(4.15)

Ahora reemplace h en la ecuaci√≥n (4.15) con h/2 para producir una segunda f√≥rmula O(h4)

h
2

M = N2

‚àí K2

h4
5h 6
‚àí K3
‚àí ¬∑¬∑¬∑ .
64
1024

Al restar 16 veces esta ecuaci√≥n de la ecuaci√≥n (4.15) elimina el t√©rmino h4 y da

15M = 16N2

h
2

‚àí N2 (h) + K 3

15h 6
+ ¬∑¬∑¬∑ .
64

Al dividir esta ecuaci√≥n entre 15 produce la nueva f√≥rmula O(h6)

M=

1
16N2
15

h
2

‚àí N2 (h) + K 3

h6
+ ¬∑¬∑¬∑ .
64

Ahora tenemos la f√≥rmula de aproximaci√≥n O(h6)

N3 (h) =

1
16N2
15

h
2

‚àí N2 (h) = N2

h
2

+

1
N2
15

h
2

‚àí N2 (h) .

4.2 Extrapolaci√≥n de Richardson

139

Al continuar con este procedimiento obtenemos, para cada j = 2, 3, . . . , la aproximaci√≥n
O(h2j)
h
N j‚àí1 (h/2) ‚àí N j‚àí1 (h)
N j (h) = N j‚àí1
.
+
2
4 j‚àí1 ‚àí 1
La tabla 4.6 muestra el orden en el que se generan las aproximaciones cuando

M = N1 (h) + K 1 h 2 + K 2 h 4 + K 3 h 6 + ¬∑ ¬∑ ¬∑ .

(4.16)

Se supone de manera conservadora que el verdadero resultado es preciso por lo menos
dentro del acuerdo de los dos resultados inferiores en la diagonal, en este caso, dentro de
|N3 (h) ‚àí N4 (h)|.

Tabla 4.6

Ejemplo 2

O(h 2 )

O(h 4 )

O(h 6 )

O(h 8 )

1: N1 (h)
2: N1 ( h2 )
4: N1 ( h4 )
7: N1 ( h8 )

3: N2 (h)
5: N2 ( h2 )
8: N2 ( h4 )

6: N3 (h)
9: N3 ( h2 )

10: N4 (h)

El teorema de Taylor se puede utilizar para mostrar que la f√≥rmula de diferencias centradas
en la ecuaci√≥n (4.5) para aproximar f (x0 ) se puede expresar con una f√≥rmula de error:

f (x0 ) =

1
h2
h 4 (5)
[ f (x0 + h) ‚àí f (x0 ‚àí h)] ‚àí
f (x0 ) ‚àí
f (x0 ) ‚àí ¬∑ ¬∑ ¬∑ .
2h
6
120

Encuentre las aproximaciones de orden O(h2), O(h4), y O(h6) para f 9(2.0) cuando f 5 xex y
h 5 0.2.
Probablemente, las constantes K 1 = ‚àí f (x0 )/6, K 2 = ‚àí f (5) (x0 )/120, ¬∑ ¬∑ ¬∑ , ser√°n desconocidas, sin embargo, esto no es importante. S√≥lo necesitamos saber que estas
FRQVWDQWHVH[LVWHQFRQHO√ÄQGHDSOLFDUODH[WUDSRODFLyQ
Tenemos la aproximaci√≥n O(h2)

Soluci√≥n

f (x0 ) = N1 (h) ‚àí

h2
h 4 (5)
f (x0 ) ‚àí
f (x0 ) ‚àí ¬∑ ¬∑ ¬∑ ,
6
120

(4.17)

donde

N1 (h) =

1
[ f (x0 + h) ‚àí f (x0 ‚àí h)].
2h

Esto nos da las primeras aproximaciones O(h2)

N1 (0.2) =

1
[ f (2.2) ‚àí f (1.8)] = 2.5(19.855030 ‚àí 10.889365) = 22.414160
0.4

N1 (0.1) =

1
[ f (2.1) ‚àí f (1.9)] = 5(17.148957 ‚àí 12.703199) = 22.228786.
0.2

y

Al combinarlas para producir la primera aproximaci√≥n O(h4) obtenemos

1
1
N2 (0.2) = N1 (0.1) + (N1 (0.1) ‚àí N1 (0.2)) = 22.228786 + (22.228786 ‚àí 22.414160)
3
3
= 22.166995.

4.2 Extrapolaci√≥n de Richardson

141

y

1
1
f (x0 )h 2 ‚àí f (x0 )h 3
2
6
1 (4)
1
+
f (x0 )h 4 ‚àí
f (5) (Œæ2 )h 5 ,
24
120

f (x0 ‚àí h) = f (x0 ) ‚àí f (x0 )h +

(4.19)

donde x0 ‚àí h < Œæ2 < x0 < Œæ1 < x0 + h.
Al restar la ecuaci√≥n (4.19) de la ecuaci√≥n (4.18) obtenemos una nueva aproximaci√≥n
para f (x)):

f (x0 + h) ‚àí f (x0 ‚àí h) = 2h f (x0 ) +

h3
h 5 (5)
f (x0 ) +
[ f (Œæ1 ) + f (5) (Œæ2 )], (4.20)
3
120

lo cual implica que

f (x0 ) =

1
h2
h 4 (5)
[ f (x0 + h) ‚àí f (x0 ‚àí h)] ‚àí
f (x0 ) ‚àí
[ f (Œæ1 ) + f (5) (Œæ2 )].
2h
6
240

Si f (5) es continua en [ x0 ‚àí h, x0 + h], el teorema del valor intermedio 1.11 implica que
existe un n√∫mero ŒæÃÉ en (x0 ‚àí h, x0 + h) con

f (5) (ŒæÃÉ ) =

1 (5)
f (Œæ1 ) + f (5) (Œæ2 ) .
2

Como consecuencia, tenemos la aproximaci√≥n O(h2)

f (x0 ) =

1
h2
h 4 (5)
[ f (x0 + h) ‚àí f (x0 ‚àí h)] ‚àí
f (x0 ) ‚àí
f (ŒæÃÉ ).
2h
6
120

(4.21)

A pesar de que la aproximaci√≥n en la ecuaci√≥n (4.21) es igual a la que se obtuvo en la
f√≥rmula de tres puntos en la ecuaci√≥n (4.5), ahora, el punto desconocido de evaluaci√≥n se
presenta en f (5) en lugar de en f -. La extrapolaci√≥n aprovecha esto al reemplazar primero h
en la ecuaci√≥n (4.21) con 2h para producir la nueva f√≥rmula

f (x0 ) =

1
4h 2
16h 4 (5)
[ f (x0 + 2h) ‚àí f (x0 ‚àí 2h)] ‚àí
f (x0 ) ‚àí
f (ŒæÃÇ ),
4h
6
120

(4.22)

donde ŒæÃÇ se encuentra entre x0 ‚àí 2h y x0 + 2h.
Al multiplicar la ecuaci√≥n (4.21) por 4 y restar la ecuaci√≥n (4.22) produce

3 f (x0 ) =

2
1
[ f (x0 + h) ‚àí f (x0 ‚àí h)] ‚àí
[ f (x0 + 2h) ‚àí f (x0 ‚àí 2h)]
h
4h
‚àí

2h 4 (5)
h 4 (5)
f (ŒæÃÉ ) +
f (ŒæÃÇ ).
30
15

Incluso si f (5) es continua en [x0 ‚àí 2h, x0 + 2h], el teorema de valor intermedio 1.11 no
se puede aplicar como lo hicimos para derivar la ecuaci√≥n (4.21) porque tenemos la diferencia de t√©rminos relacionados con f (5). Sin embargo, es posible usar un m√©todo alterno para
mostrar que f (5) (ŒæÃÉ ) y f (5) (ŒæÃÇ ) se puede seguir reemplazando con un valor com√∫n f (5) (Œæ ).
Al suponer esto y dividir entre 3 produce la f√≥rmula del punto medio de cinco puntos la
ecuaci√≥n (4.6) que observamos en la secci√≥n 4.1:

f (x0 ) =

1
h 4 (5)
[ f (x0 ‚àí 2h) ‚àí 8 f (x0 ‚àí h) + 8 f (x0 + h) ‚àí f (x0 + 2h)] +
f (Œæ ).
12h
30

Otras f√≥rmulas para las primeras derivadas y las derivadas superiores se pueden deducir
de manera similar. Consulte, por ejemplo, el ejercicio 8.
A lo largo del texto se utiliza la t√©cnica de extrapolaci√≥n. Las aplicaciones m√°s prominentes se presentan al aproximar las integrales en la secci√≥n 4.5 y al determinar soluciones
aproximadas para las ecuaciones diferenciales en la secci√≥n 5.8.

142

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

La secci√≥n Conjunto de ejercicios 4.2 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

4.3 Elementos de integraci√≥n num√©rica
$PHQXGRVXUJHODQHFHVLGDGGHHYDOXDUODLQWHJUDOGH√ÄQLGDGHXQDIXQFLyQTXHQRWLHQHXQD
antiderivada o cuya antiderivada no es f√°cil de obtener. El m√©todo b√°sico asociado con la
b
aproximaci√≥n de a f (x) d x recibe el nombre de cuadratura num√©rica. √âste utiliza una
b
n
suma i=0 ai f (xi ) para aproximar a f (x) d x.
Los m√©todos de cuadratura en esta secci√≥n se basan en los polinomios de interpolaci√≥n
que se han explicado en el cap√≠tulo 3. La idea b√°sica es seleccionar un conjunto de nodos
distintos {x0 , . . . , xn } del intervalo [a, b]. Entonces integramos el polinomio interpolante de
Lagrange
n

Pn (x) =

f (xi )L i (x)
i=0

y su t√©rmino de error de truncamiento sobre [a, b] para obtener
b
a

n

b

f (x) d x =

b n

f (xi )L i (x) d x +

a

a

i=0

n

=

ai f (xi ) +
i=0

i=0

b n

1
(n + 1)!

a

(x ‚àí xi )

f (n+1) (Œæ(x))
dx
(n + 1)!

(x ‚àí xi ) f (n+1) (Œæ(x)) d x,

i=0

donde j(x) se encuentra en [a, b] para cada x y

ai =

b
a

L i (x) d x,

para cada i = 0, 1, . . . , n.

La f√≥rmula de cuadratura es, por lo tanto,
b
a

n

f (x) d x ‚âà

ai f (xi ),
i=0

con un error dado por

E( f ) =

1
(n + 1)!

b n
a

(x ‚àí xi ) f (n+1) (Œæ(x)) d x.

i=0

Antes de analizar la situaci√≥n general de las f√≥rmulas de cuadratura, consideremos las
f√≥rmulas producidas mediante el uso del primer y del segundo polinomios de Lagrange con
nodos igualmente espaciados. Esto da la regla trapezoidal y la regla de Simpson, las cuales
se presentan generalmente en cursos de c√°lculo.

La regla trapezoidal
Para derivar la regla trapezoidal (o regla del trapecio) para aproximar
x0 = a, x1 = b, h = b ‚àí a y utilice el polinomio de Lagrange

P1 (x) =

(x ‚àí x1 )
(x ‚àí x0 )
f (x0 ) +
f (x1 ).
(x0 ‚àí x1 )
(x1 ‚àí x0 )

b
a f (x) d x, sean

4.3 Elementos de integraci√≥n num√©rica

143

Entonces
b
a

f (x) d x =

x1

(x ‚àí x0 )
(x ‚àí x1 )
f (x0 ) +
f (x1 ) d x
(x0 ‚àí x1 )
(x1 ‚àí x0 )

x0

1
+
2

x1
x0

(4.23)

f (Œæ(x))(x ‚àí x0 )(x ‚àí x1 ) d x.

El producto (x ‚àí x0 )(x ‚àí x1 ) no cambia de signo en [x0, x1], por lo que el teorema del valor
promedio ponderado para integrales 1.13 se puede aplicar al t√©rmino de error para obtener,
para algunos j en (x0, x1),
x1
x0

Cuando usamos el t√©rmino
trapezoidal, nos referimos a
XQD√ÄJXUDGHFXDWURODGRVFRQ
al menos dos lados paralelos. El
WpUPLQRHXURSHRSDUDHVWD√ÄJXUD
es trapezium. Para confundir
m√°s el tema, la palabra europea
trapezoidalVHUH√ÄHUHDXQD√ÄJXUD
de cuatro lados sin ning√∫n lado
igual y la palabra estadounidense
SDUDHVWHWLSRGH√ÄJXUDHV
trapezium.

f (Œæ(x))(x ‚àí x0 )(x ‚àí x1 ) d x = f (Œæ )

x1
x0

(x ‚àí x0 )(x ‚àí x1 ) d x
x

= f (Œæ )
=‚àí

1
x3
(x1 + x0 ) 2
‚àí
x + x0 x1 x
3
2
x0

h3
f (Œæ ).
6

Por consiguiente, la ecuaci√≥n (4.23) implica que
b
a

x

f (x) d x =
=

1
(x ‚àí x1 )2
(x ‚àí x0 )2
h3
f (x0 ) +
f (x1 )
f (Œæ )
‚àí
2(x0 ‚àí x1 )
2(x1 ‚àí x0 )
12
x0

h3
(x1 ‚àí x0 )
[ f (x0 ) + f (x1 )] ‚àí
f (Œæ ).
2
12

Por medio de la notaci√≥n h = x1 ‚àí x0 obtenemos la siguiente regla:
Regla trapezoidal:

b

h
h3
[ f (x0 ) + f (x1 )] ‚àí
f (Œæ ).
2
12
a
Esto recibe el nombre de regla trapezoidal porque cuando f es una funci√≥n con valores
b
positivos, a f (x) d x se aproxima mediante el √°rea de un trapecio, como se muestra en la
√ÄJXUD
f (x) d x =

Figura 4.3
y
y 5 f (x)
y 5 P1(x)

a 5 x0

x1 5 b

x

El t√©rmino de error para la regla trapezoidal implica f 0, por lo que la regla da el resultado
exacto cuando se aplica a cualquier funci√≥n cuya segunda derivada es id√©nticamente cero, es
decir, cualquier polinomio de grado uno o menos.

144

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

Regla de Simpson
La regla de Simpson resulta de la integraci√≥n sobre [a, b] del segundo polinomio de
Lagrange con nodos igualmente espaciados x0 = a, x2 = b, y x1 = a + h, en donde h = (b ‚àí a)/2. (v√©ase OD√ÄJXUD 
Figura 4.4

y
y 5 f (x)

y 5 P2(x)

a 5 x0

x1

x

x2 5 b

Por lo tanto,
b

f (x) d x =

a

x2

(x ‚àí x1 )(x ‚àí x2 )
(x ‚àí x0 )(x ‚àí x2 )
f (x0 ) +
f (x1 )
(x0 ‚àí x1 )(x0 ‚àí x2 )
(x1 ‚àí x0 )(x1 ‚àí x2 )

x0

+
+

(x ‚àí x0 )(x ‚àí x1 )
f (x2 ) d x
(x2 ‚àí x0 )(x2 ‚àí x1 )
x2
x0

(x ‚àí x0 )(x ‚àí x1 )(x ‚àí x2 ) (3)
f (Œæ(x)) d x.
6

Al deducir la regla de Simpson de esta forma, sin embargo, da un solo t√©rmino de error O(h4)
relacionado con f (3). Al aproximar el problema de otra forma, se puede derivar otro t√©rmino
de orden superior relacionado con f (4).
Para ilustrar este m√©todo alternativo, suponga que f se expande en el tercer polinomio de
Taylor alrededor de x1. Entonces, para cada x en [x0, x2], existe un n√∫mero j(x) en (x0, x2) con

f (x) = f (x1 ) + f (x1 )(x ‚àí x1 ) +
+

f (x1 )
f (x1 )
(x ‚àí x1 )2 +
(x ‚àí x1 )3
2
6

f (4) (Œæ(x))
(x ‚àí x1 )4
24

y
x2
x0

f (x) d x =

f (x1 )(x ‚àí x1 ) +
+

f (x1 )
f (x1 )
(x ‚àí x1 )2 +
(x ‚àí x1 )3
2
6

x2
f (x1 )
1
+
(x ‚àí x1 )4
24
24
x0

x2
x0

f (4) (Œæ(x))(x ‚àí x1 )4 d x. (4.24)

Puesto que (x 2 x1)4 nunca es negativo en [x0, x2], el teorema de valor promedio ponderado
para las integrales 1.13 implica que

1
24

x2
x0

f (4) (Œæ(x))(x ‚àí x1 )4 d x =

f (4) (Œæ1 )
24

x2
x0

(x ‚àí x1 )4 d x =

x2
f (4) (Œæ1 )
(x ‚àí x1 )5 ,
120
x0

4.3 Elementos de integraci√≥n num√©rica

145

para alg√∫n n√∫mero j1 en (x0, x2).
Sin embargo, h 5 x2 2 x1 5 x1 2 x0, por lo que

(x2 ‚àí x1 )2 ‚àí (x0 ‚àí x1 )2 = (x2 ‚àí x1 )4 ‚àí (x0 ‚àí x1 )4 = 0,
mientras

(x2 ‚àí x1 )3 ‚àí (x0 ‚àí x1 )3 = 2h 3

y

(x2 ‚àí x1 )5 ‚àí (x0 ‚àí x1 )5 = 2h 5 .

Por consiguiente, la ecuaci√≥n (4.24) se puede reescribir como
x2
x0

f (x) d x = 2h f (x1 ) +

h3
f (4) (Œæ1 ) 5
f (x1 ) +
h .
3
60

Ahora, si reemplazamos f (x1 ) por medio de la aproximaci√≥n determinada en la ecuaci√≥n (4.9) de la secci√≥n 4.1, tenemos
x2
x0

f (x) d x = 2h f (x1 ) +
=

Thomas Simpson (1710‚Äì1761)
fue un matem√°tico autodidacta
que en sus primeros a√±os se
ganaba la vida como tejedor. Su
principal inter√©s fue la teor√≠a
de la probabilidad, aunque en
1750 public√≥ un libro de c√°lculo
de dos vol√∫menes titulado
The Doctrine and Application
of Fluxions (La doctrina y
DSOLFDFLyQGH√ÅX[LRQHV .

Ejemplo 1

h3
3

1
h 2 (4)
f (4) (Œæ1 ) 5
f (Œæ2 ) +
h
[ f (x0 ) ‚àí 2 f (x1 ) + f (x2 )] ‚àí
2
h
12
60

h
h 5 1 (4)
1
[ f (x0 ) + 4 f (x1 ) + f (x2 )] ‚àí
f (Œæ2 ) ‚àí f (4) (Œæ1 ) .
3
12 3
5

Con m√©todos alternos se puede mostrar (consulte el ejercicio 26) que los valores Œæ1 y Œæ2 en
esta expresi√≥n se pueden reemplazar mediante un valor com√∫n Œæ en (x0 , x2 ). Esto da la regla
de Simpson.
Regla de Simpson:
x2
x0

f (x) d x =

h
h 5 (4)
[ f (x0 ) + 4 f (x1 ) + f (x2 )] ‚àí
f (Œæ ).
3
90

El t√©rmino de error en la regla de Simpson implica la cuarta derivada de f, por lo que da
resultados exactos cuando se aplica a cualquier polinomio de grado tres o menos.
2

Compare las aproximaciones de la regla trapezoidal y de la regla de Simpson para 0 f (x) d x
cuando f (x) es

a)
d)
Soluci√≥n

x2
‚àö
1 + x2

x4
sen x

b)
e)

c)
f)

(x + 1)‚àí1
ex

En [0, 2], las reglas trapezoidal y de Simpson tiene las formas
2

Trapezoidal:

f (x) d x ‚âà f (0) + f (2)

y

0
2

De Simpson:

f (x) d x ‚âà

0

1
[ f (0) + 4 f (1) + f (2)].
3

Cuando f (x) = x 2, obtenemos
2

Trapezoidal:

f (x) d x ‚âà 02 + 22 = 4

y

0
2

De Simpson:
0

f (x) d x ‚âà

1 2
8
[(0 ) + 4 ¬∑ 12 + 22 ] = .
3
3

La aproximaci√≥n a partir de la regla de Simpson es exacta porque su error de truncamiento
implica f (4), lo cual es id√©nticamente 0 cuando f (x) = x 2 .
Los resultados a los tres lugares para las funciones se resumen en la tabla 4.7. Observe
TXHHQFDGDLQVWDQFLDODUHJODGH6LPSVRQHVVLJQL√ÄFDWLYDPHQWHVXSHULRU

146

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

Tabla 4.7

(a)

(b)

(c)

(e)

(f)

(x + 1)‚àí1

(d)
‚àö
1 + x2

f (x)

x2

x4

sen x

ex

Valor exacto
Trapezoidal
De Simpson

2.667
4.000
2.667

6.400
16.000
6.667

1.099
1.333
1.111

2.958
3.326
2.964

1.416
0.909
1.425

6.389
8.389
6.421

Precisi√≥n de medici√≥n
Las derivadas est√°ndar de las f√≥rmulas de error de cuadratura est√°n basadas al determinar la
clase de polinomios para los que estas f√≥rmulas producen resultados exactos. La siguiente
GH√ÄQLFLyQVHXWLOL]DSDUDIDFLOLWDUHODQiOLVLVGHHVWDGHULYDGD
DeÔ¨Ånici√≥n 4.1

La precisi√≥n mejorada de la
regla de Simpson sobre la regla
trapezoidal se explica de manera
intuitiva con el hecho de que la
regla de Simpson incluye una
evaluaci√≥n de punto medio que
proporciona mejor equilibrio
para la aproximaci√≥n.

El grado de precisi√≥n, o precisi√≥n, de una f√≥rmula de cuadratura es el mayor entero positivo n,
de tal forma que la f√≥rmula es exacta para xk, para cada k = 0, 1, . . . , n.
/DGH√ÄQLFLyQLPSOLFDTXHODVUHJODVWUDSH]RLGDO\GH6LPSVRQWLHQHQJUDGRVGHSUHcisi√≥n uno y tres, respectivamente.
La integraci√≥n y la sumatoria son operaciones lineales; es decir,
b
a

(Œ± f (x) + Œ≤g(x)) d x = Œ±

b
a

b

f (x) d x + Œ≤

a

g(x) d x

y
n

n

(Œ± f (xi ) + Œ≤g(xi )) = Œ±
i=0

n

f (xi ) + Œ≤
i=0

g(xi ),
i=0

para cada par de funciones integrables f y g y cada par de constantes reales a y b. Esto
implica (consulte el ejercicio 25) que
La terminolog√≠a abierta y cerrada
para los m√©todos implica que el
m√©todo abierto s√≥lo utiliza como
nodos los puntos en el intervalo
abierto (a, b) para aproximar
b
a f (x) d x. Los m√©todos
cerrados incluyen los puntos a
y b del intervalo cerrado [a, b]
como nodos.

‚Ä¢ el grado de precisi√≥n de una f√≥rmula de cuadratura es n si y s√≥lo si el error es cero para
todos los polinomios de grado k = 0, 1, . . . , n, pero no es cero para algunos polinomios
de grado n 1 1.
Las reglas trapezoidal y de Simpson son ejemplos de una clase de m√©todos conocidos como
f√≥rmulas de Newton-Cotes. Existen dos tipos de f√≥rmulas de Newton-Cotes: abiertas y cerradas.

F√≥rmulas de Newton-Cotes cerradas
La f√≥rmula cerrada de (n 1 1) puntos de Newton-Cotes utiliza nodos xi = x0 +i h, para
i = 0, 1, . . . , n, donde x0 = a, xn = b y h = (b ‚àí a)/n.  9pDVH OD √ÄJXUD   5HFLEH HO
nombre de cerrada porque los extremos del intervalo cerrado [a, b] se incluyen como nodos.
Figura 4.5

y
y = Pn(x)
y = f (x)

a 5 x0

x1

x2

xn21

xn 5 b

x

4.3 Elementos de integraci√≥n num√©rica

147

La f√≥rmula asume la forma
b
a

n

f (x) d x ‚âà

ai f (xi ),
i=0

donde

ai =

xn
x0

L i (x) d x =

xn
x0

n

(x ‚àí x j )
d x.
(xi ‚àí x j )
j=0
j =i

El siguiente teorema detalla el an√°lisis de error relacionado con las f√≥rmulas de
Newton-Cotes cerradas. Para una demostraci√≥n de este teorema, consulte [IK], p. 313.
Teorema 4.2
Roger Cotes (1682‚Äì1716)
ascendi√≥ desde un origen
humilde hasta convertirse, en
1704, en el primer profesor
plumiano en la Universidad
de Cambridge. Realiz√≥
numerosos avances en las √°reas
de matem√°ticas, incluyendo
los m√©todos num√©ricos para
la interpolaci√≥n e integraci√≥n.
Newton es famoso por decir
respecto a Cotes: ‚ÄúSi hubiera
vivido, habr√≠amos aprendido
algo‚Äù.

n
Suponga que i=0
ai f (xi ) denota la f√≥rmula cerrada de (n 1 1) puntos de Newton-Cotes
con x0 = a, xn = b, y h = b ‚àí a)/n. Existe Œæ ‚àà (a, b) para el que
n

b

f (x) d x =

a

ai f (xi ) +
i=0

h n+3 f (n+2) (Œæ )
(n + 2)!

n

t 2 (t ‚àí 1) ¬∑ ¬∑ ¬∑ (t ‚àí n) dt,

0

si n es par y f ‚àà C n+2 [a, b], y
b
a

n

f (x) d x =

ai f (xi ) +
i=0

h n+2 f (n+1) (Œæ )
(n + 1)!

n

t (t ‚àí 1) ¬∑ ¬∑ ¬∑ (t ‚àí n) dt,

0

si n es impar y f ‚àà C n+1 [a, b].
Observe que cuando n es un entero par, el grado de precisi√≥n es n 1 1, a pesar de que el
polinomio de interpolaci√≥n es de grado a lo sumo n. Cuando n es impar, el grado de precisi√≥n
s√≥lo es n.
Se listan algunas de las f√≥rmulas comunes de Newton-Cotes cerradas con sus t√©rminos
de error. Observe que, en cada caso, el valor desconocido j se encuentra en (a, b).
n 5 1: Regla trapezoidal
x1
x0

f (x) d x =

h
h3
[ f (x0 ) + f (x1 )] ‚àí
f (Œæ ),
2
12

donde

x0 < Œæ < x1 .

(4.25)

n 5 2: Regla de Simpson
x2
x0

f (x) d x =

h
h 5 (4)
[ f (x0 ) + 4 f (x1 ) + f (x2 )] ‚àí
f (Œæ ),
3
90

donde

x0 < Œæ < x2 .
(4.26)

n 5 3: Regla de tres octavos de Simpson
x3
x0

f (x) d x =

3h
3h 5 (4)
[ f (x0 ) + 3 f (x1 ) + 3 f (x2 ) + f (x3 )] ‚àí
f (Œæ ),
8
80
donde

x0 < Œæ < x3 .

(4.27)

148

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

n 5 4:
x4
x0

f (x) d x =

2h
8h 7 (6)
[7 f (x0 ) + 32 f (x1 ) + 12 f (x2 ) + 32 f (x3 ) + 7 f (x4 )] ‚àí
f (Œæ ),
45
945
x0 < Œæ < x4 .

donde

(4.28)

F√≥rmulas de Newton-Cotes abiertas
Las f√≥rmulas de Newton-Cotes abiertas no incluyen los extremos de [a, b] como nodos. √âstas
utilizan los nodos xi = x0 + i h , para cada i = 0, 1,   , n, donde h = (b ‚àí a)/(n + 2) y
x0 = a + h. Esto implica que xn = b ‚àí h, por lo que etiquetamos los extremos al establecer x‚àí1 = a y xn+1 = bFRPRVHPXHVWUDHQOD√ÄJXUD/DVIyUPXODVDELHUWDVFRQWLHQHQ
todos los nodos que se usan para la aproximaci√≥n dentro del intervalo abierto (a, b). Las
f√≥rmulas se convertir√°n en
b
a

donde ai =

b
a

f (x) d x =

xn+1
x‚àí1

n

f (x) d x ‚âà

ai f (xi ),
i=0

L i (x) d x.

Figura 4.6

y

y = Pn(x)
y = f (x)

a 5 x21 x0

x1

x2

xn

xn11 5 b

x

El siguiente teorema es an√°logo al teorema 4.2; su demostraci√≥n se encuentra en [IK],
p. 314.
Teorema 4.3

n
Suponga que i=0
ai f (xi ) denota la f√≥rmula abierta de (n 1 1) puntos de Newton-Cotes
con x‚àí1 = a, xn+1 = b, y h = (b ‚àí a)/(n + 2). Existe Œæ ‚àà (a, b) para el que
b
a

n

f (x) d x =

ai f (xi ) +
i=0

h n+3 f (n+2) (Œæ )
(n + 2)!

n+1
‚àí1

t 2 (t ‚àí 1) ¬∑ ¬∑ ¬∑ (t ‚àí n) dt,

4.3 Elementos de integraci√≥n num√©rica

149

si n es par y f ‚àà C n+2 [a, b], y
b
a

n

f (x) d x =

ai f (xi ) +
i=0

h n+2 f (n+1) (Œæ )
(n + 1)!

n+1
‚àí1

t (t ‚àí 1) ¬∑ ¬∑ ¬∑ (t ‚àí n) dt,

si n es impar y f ‚àà C n+1 [a, b].
Observe que, como en el caso de m√©todos cerrados, tenemos el grado de precisi√≥n comparativamente superior para los m√©todos pares que para los m√©todos impares.
Algunas de las f√≥rmulas de Newton-Cotes abiertas comunes con sus t√©rminos de error
son las siguientes:
n 5 0: regla del punto medio
x1
x‚àí1

f (x) d x = 2h f (x0 ) +

h3
f (Œæ ),
3

donde

x‚àí1 < Œæ < x1 .

(4.29)

n 5 1:
x2
x‚àí1

f (x) d x =

3h
3h 3
[ f (x0 ) + f (x1 )] +
f (Œæ ),
2
4

donde

x‚àí1 < Œæ < x2 . (4.30)

n 5 2:
x3
x‚àí1

f (x) d x =

4h
14h 5 (4)
[2 f (x0 ) ‚àí f (x1 ) + 2 f (x2 )] +
f (Œæ ),
3
45
donde

(4.31)

x‚àí1 < Œæ < x3 .

n 5 3:
x4
x‚àí1

f (x) d x =

5h
95 5 (4)
[11 f (x0 ) + f (x1 ) + f (x2 ) + 11 f (x3 )] +
h f (Œæ ),
24
144
donde

Ejemplo 2

(4.32)

x‚àí1 < Œæ < x4 .

Compare los resultados de las f√≥rmulas cerradas y abiertas de Newton-Cotes como la ecuaci√≥n (4.25) a la (4.28) y la ecuaci√≥n (4.29) a la (4.32) para aproximar
œÄ/4

sen x d x = 1 ‚àí

‚àö

2/2 ‚âà 0.29289322.

0

Soluci√≥n

Para las f√≥rmulas cerradas, tenemos

(œÄ/4)
œÄ
sen 0 + sen
‚âà 0.27768018
2
4
œÄ
(œÄ/8)
œÄ
sen 0 + 4 sen + sen
‚âà 0.29293264
n=2:
3
8
4
œÄ
3(œÄ/12)
œÄ
œÄ
sen 0 + 3 sen
‚âà 0.29291070
n=3:
+ 3 sen + sen
8
12
6
4
œÄ
3œÄ
œÄ
2(œÄ/16)
œÄ
+ 12 sen + 32 sen
+ 7 sen
n=4:
7 sen 0 + 32 sen
45
16
8
16
4

n=1:

‚âà 0.29289318

150

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

y para las f√≥rmulas abiertas, tenemos

œÄ
‚âà 0.30055887
8
œÄ
3(œÄ/12)
œÄ
+ sen
sen
‚âà 0.29798754
2
12
6
4(œÄ/16)
œÄ
3œÄ
œÄ
‚àí sen + 2 sen
2 sen
‚âà 0.29285866
3
16
8
16

n = 0 : 2(œÄ/8) sen
n=1:
n=2:
n=3:

œÄ
3œÄ
œÄ
œÄ
5(œÄ/20)
+ sen
+ sen
+ 11 sen
11 sen
24
20
10
20
5

‚âà 0.29286923

La tabla 4.8 resume los resultados y muestra los errores de aproximaci√≥n.
Tabla 4.8

n
F√≥rmulas cerradas
Error
F√≥rmulas abiertas
Error

0

1

2

3

4

0.29293264
0.00003942
0.29285866
0.00003456

0.29291070
0.00001748
0.29286923
0.00002399

0.29289318
0.00000004

0.30055887
0.00766565

0.27768018
0.01521303
0.29798754
0.00509432

La secci√≥n Conjunto de ejercicios 4.3 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

4.4 Integraci√≥n num√©rica compuesta

A menudo, la aproximaci√≥n por
tramos es efectiva. Recuerde que
esto se us√≥ para interpolaci√≥n de
spline.

Ejemplo 1

En general, el uso de f√≥rmulas de Newton-Cotes es inapropiado sobre largos intervalos de inWHJUDFLyQ6HUHTXHULUtDQIyUPXODVGHJUDGRVXSHULRU\ORVYDORUHVGHORVFRH√ÄFLHQWHVHQHVWDV
f√≥rmulas son dif√≠ciles de obtener. Adem√°s, las f√≥rmulas de Newton-Cotes est√°n basadas en
los polinomios de interpolaci√≥n que utilizan nodos igualmente espaciados, un procedimiento
inapropiado sobre intervalos largos debido a la naturaleza oscilatoria de los polinomios de
orden superior.
En esta secci√≥n, analizamos el enfoque por tramos (o fragmentario) para la integraci√≥n
num√©rica que usa las f√≥rmulas de Newton-Cotes de bajo orden. Estas son las t√©cnicas que se
aplican m√°s a menudo.

Use la regla de Simpson para aproximar 0 e x d x y compare esto con los resultados obteni2
4
dos mediante la suma de las aproximaciones de Simpson para 0 e x dx y 2 e x d x y al sumar
1 x
2 x
3 x
4 x
√©stas con 0 e d x, 1 e d x, 2 e d x, y 3 e d x.
4

Soluci√≥n

La regla de Simpson en [0, 4] con h 5 2 da
4
0

ex d x ‚âà

2 0
(e + 4e2 + e4 ) = 56.76958.
3

La respuesta correcta en este caso es e4 ‚àí e0 = 53.59815, y el error 23.17143 es mucho
m√°s grande de lo que aceptar√≠amos normalmente.

4.4

Integraci√≥n num√©rica compuesta

151

Al aplicar la regla de Simpson en cada uno de los intervalos [0, 2] y [2, 4] con h 5 1 da
4

2

ex d x =

0

4

ex d x +

0

ex d x

2

1 2
1 0
e + 4e + e2 +
e + 4e3 + e4
3
3
1 0
=
e + 4e + 2e2 + 4e3 + e4
3

‚âà

= 53.86385.
El error se ha reducido a 20.26570.
Para las integrales en [0, 1], [1, 2], [3, 4], y [3, 4], utilizamos la regla de Simpson cuatro
veces con h = 12, con lo que obtenemos
4

1

ex d x =

0

2

ex d x +

0

3

ex d x +

1

4

ex d x +

2

ex d x

3

1
1
e0 + 4e1/2 + e +
e + 4e3/2 + e2
6
6
1 2
1 3
+
e + 4e5/2 + e3 +
e + 4e7/2 + e4
6
6
1 0
=
e + 4e1/2 + 2e + 4e3/2 + 2e2 + 4e5/2 + 2e3 + 4e7/2 + e4
6
= 53.61622.

‚âà

El error para esta aproximaci√≥n se ha reducido a 20.01807.
b

Para generalizar este procedimiento para una integral arbitraria a f (x) d x, seleccione
un entero par n. Subdivida el intervalo [a, b] en n subintervalos y aplique la regla de Simpson
en cada par consecutivo de subintervalos (v√©ase OD√ÄJXUD 
Figura 4.7
y
y 5 f (x)

a 5 x0

x2

x2j22 x2j21

x2j

b 5 xn

x

Con h = (b ‚àí a)/n y x j = a + j h, para cada j = 0, 1, . . . , n, tenemos
b
a

n/2

x2 j

j=1

x2 j‚àí2

n/2

h
h 5 (4)
[ f (x2 j‚àí2 ) + 4 f (x2 j‚àí1 ) + f (x2 j )] ‚àí
f (Œæ j ) ,
3
90

f (x) d x =

=
j=1

f (x) d x

152

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n
4
para algunos Œæ j con x2 j‚àí2 < Œæ j < x2 j, siempre que f ‚àà C [a, b]. A trav√©s del hecho de
que para cada j = 1, 2, . . . , (n/2) ‚àí 1 tenemos f (x2 j ) TXH √ÄJXUD HQ HO WpUPLQR FRUUHVpondiente al intervalo [x2 j‚àí2 , x2 j ] y tambi√©n en el t√©rmino correspondiente al intervalo
[x2 j , x2 j+2 ], podemos reducir esta suma a

‚é§
‚é°
(n/2)‚àí1
n/2
n/2
h‚é£
h5
‚é¶
f (x) d x =
f (x2 j ) + 4
f (x2 j‚àí1 ) + f (xn ) ‚àí
f (4) (Œæ j ).
f (x0 ) + 2
3
90
a
j=1
j=1
j=1
b

El error relacionado con esta aproximaci√≥n es
n/2

E( f ) = ‚àí

h5
f (4) (Œæ j ),
90 j=1

donde x2 j‚àí2 < Œæ j < x2 j , para cada j = 1, 2, . . . , n/2.
Si f ‚àà C 4 [a, b], el teorema de valor extremo 1.9 implica que f (4) asume su m√°ximo y su
m√≠nimo en [a, b]. Puesto que

m√≠n f (4) (x) ‚â§ f (4) (Œæ j ) ‚â§ m√°x f (4) (x),

x‚àà[a,b]

x‚àà[a,b]

tenemos
n/2

n
n
f (4) (Œæ j ) ‚â§
m√≠n f (4) (x) ‚â§
m√°x f (4) (x)
x‚àà[a,b]
2 x‚àà[a,b]
2
j=1
y

m√≠n f (4) (x) ‚â§

x‚àà[a,b]

n/2

2
f (4) (Œæ j ) ‚â§ m√°x f (4) (x).
x‚àà[a,b]
n j=1

Por medio del teorema de valor intermedio 1.11, existe m ‚àà (a, b) tal que

f (4) (Œº) =

n/2

2
f (4) (Œæ j ).
n j=1

Por lo tanto,
n/2

E( f ) = ‚àí

h5
h5
n f (4) (Œº),
f (4) (Œæ j ) = ‚àí
90 j=1
180

Y, puesto que h = (b ‚àí a)/n,

E( f ) = ‚àí

(b ‚àí a) 4 (4)
h f (Œº).
180

Estas observaciones producen el siguiente resultado.
Teorema 4.4

Si f ‚àà C 4 [a, b], n es par, h = (b ‚àí a)/n, y x j = a + j h, para cada j = 0, 1, . . . , n. Existe Œº ‚àà (a, b) para los que la regla compuesta de Simpson para n subintervalos se puede
reescribir con su t√©rmino de error como
‚é§
‚é°
(n/2)‚àí1
n/2
b
h‚é£
b ‚àí a 4 (4)
f (x) d x =
f (x2 j ) + 4
f (x2 j‚àí1 ) + f (b)‚é¶ ‚àí
f (a) + 2
h f (Œº).
3
180
a
j=1
j=1

4.4

Integraci√≥n num√©rica compuesta

153

Observe que el t√©rmino de error para la regla compuesta de Simpson es O(h4), mientras
que era O(h5) para la regla est√°ndar de Simpson. Sin embargo, estos √≠ndices no son comparables ya que para la regla est√°ndar de Simpson, tenemos h √ÄMDHQh = (b ‚àí a)/2, pero para la
regla compuesta de Simpson, tenemos h = (b‚àía)/n, para n un entero par. Esto nos permite
reducir considerablemente el valor de h.
El algoritmo 4.1 utiliza la regla compuesta de Simpson en n subintervalos. √âste es el
algoritmo de cuadratura que se usa con mayor frecuencia para prop√≥sito general.

ALGORITMO

4.1

Regla compuesta de Simpson
Para aproximar la integral I =
ENTRADA

b
a f (x) d x:

extremos a, b; entero positivo n par.
aproximaci√≥n X I para I.

SALIDA

Paso 1 Haga h = (b ‚àí a)/n.
Paso 2 Haga X I 0 = f (a) + f (b);
X I 1 = 0; (Suma de f (x2i‚àí1 ).)
X I 2 = 0. (Suma de f (x2i ).)
Paso 3 Para i = 1, . . . , n ‚àí 1 realice los pasos 4 y 5.
Paso 4 Haga X = a + i h.
Paso 5 Si i es par entonces haga X I 2 = X I 2 + f (X )
Tambi√©n determine X I 1 = X I 1 + f (X ).
Paso 6 Haga X I = h(X I 0 + 2 ¬∑ X I 2 + 4 ¬∑ X I 1)/3.
Paso 7 SALIDA (X I );
PARE.

El enfoque de subdivisi√≥n se puede aplicar a cualquiera de las f√≥rmulas de NewtonCotes. Las extensiones de las reglas trapezoidal (v√©ase OD√ÄJXUD \GHSXQWRPHGLRVHGDQ
sin prueba. La regla trapezoidal s√≥lo requiere un intervalo para cada aplicaci√≥n, por lo que el
entero n puede ser tanto par como impar.
Figura 4.8
y
y 5 f (x)

a 5 x0 x1

x j21

xj

x n21

b 5 xn

x

154

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

Teorema 4.5

Sean f ‚àà C 2 [a, b], h = (b ‚àí a)/n, y x j = a + j h, para cada j = 0, 1, . . . , n. Existe m ‚àà
(a, b) para el que la regla compuesta trapezoidal para n subintervalos se puede reescribir
con este t√©rmino de error como
b
a

‚é°
f (x) d x =

‚é§

n‚àí1

h‚é£
b‚àía 2
f (a) + 2
h f (Œº).
f (x j ) + f (b)‚é¶ ‚àí
2
12
j=1

Para la regla compuesta de punto medio, n debe ser, nuevamente, par (v√©ase OD√ÄJXUD
4.9.)

Figura 4.9
y
y 5 f (x)

a 5 x21 x 0

Teorema 4.6

xn21 x n b 5 x n11

x

Si f ‚àà C 2 [a, b], n es par, h = (b ‚àí a)/(n + 2), y x j = a + ( j + 1)h para cada j 5
‚àí1, 0, . . . , n + 1. Existe m ‚àà (a, b) para el que la regla compuesta de punto medio para n 1 2
subintervalos se puede reescribir con su t√©rmino de error como
b
a

Ejemplo 2

x2j21 x2j x2j11

x1

n/2

f (x) d x = 2h

f (x2 j ) +
j=0

b‚àía 2
h f (Œº).
6

Determine valores de h que garantizar√°n un error de aproximaci√≥n menor que 0.00002 al
œÄ
aproximar 0 sen x d x y usar
a) La regla compuesta trapezoidal y b) la regla compuesta de Simpson.
Soluci√≥n a) La forma de error para la regla trapezoidal compuesta para f (x) = sen x en [0, œÄ ]

es

œÄ h2
œÄ h2
œÄ h2
f (Œº) =
(‚àí sen Œº) =
| sen Œº|.
12
12
12
3DUDJDUDQWL]DUVX√ÄFLHQWHSUHFLVLyQFRQHVWDWpFQLFDQHFHVLWDPRVWHQHU

œÄ h2
œÄ h2
| sen Œº| ‚â§
< 0.00002.
12
12
Puesto que h = œÄ/n, necesitamos

œÄ3
< 0.00002,
12n 2

lo cual implica que n >

œÄ3
12(0.00002)

1/2

‚âà 359.44,

4.4

Integraci√≥n num√©rica compuesta

155

y la regla compuesta trapezoidal requiere n ‚â• 360.
b) La forma de error para la regla compuesta de Simpson para f (x) = sen x en [0, œÄ ] es

œÄ h 4 (4)
œÄ h4
œÄ h4
f (Œº) =
sen Œº =
| sen Œº|.
180
180
180
3DUDJDUDQWL]DUVX√ÄFLHQWHSUHFLVLyQFRQHVWDWpFQLFDQHFHVLWDPRVWHQHU

œÄ h4
œÄ h4
| sen Œº| ‚â§
< 0.00002.
180
180
A trav√©s del hecho de que n = œÄ/ h nos da

œÄ5
< 0.00002,
180n 4

lo cual implica que n >

œÄ5
180(0.00002)

1/4

‚âà 17.07.

Por lo tanto, la regla compuesta de Simpson s√≥lo requiere n ‚â• 18.
La regla compuesta de Simpson con n 5 18 nos da
‚é°
‚é§
8
9
œÄ
jœÄ
(2 j ‚àí 1)œÄ ‚é¶
œÄ ‚é£
2
= 2.0000104.
sen x d x ‚âà
sen
+4
sen
54
9
18
0
j=1
j=1
Esto es preciso dentro de aproximadamente 1025 porque el valor verdadero es ‚àí cos(œÄ) ‚àí
(‚àí cos(0)) = 2.
La regla compuesta de Simpson es la selecci√≥n clara si desea minimizar los c√°lculos.
Para prop√≥sitos de comparaci√≥n, considere la regla compuesta trapezoidal por medio de
h = œÄ/18 para la integral en el ejemplo 2. Esta aproximaci√≥n utiliza las mismas evaluaciones de funci√≥n que la regla compuesta de Simpson, pero la aproximaci√≥n en este caso,
‚é§
‚é§
‚é°
‚é°
17
17
œÄ
jœÄ
jœÄ
œÄ ‚é£
œÄ
‚é¶
‚é£2
sen x d x ‚âà
sen
+ sen 0 + sen œÄ ‚é¶ =
sen
2
36
18
36
18
0
j=1
j=1

= 1.9949205,
s√≥lo es precisa para aproximadamente 5 3 1023.

Estabilidad del error de redondeo

Se espera que la integraci√≥n
num√©rica sea estable, mientras
que la diferenciaci√≥n num√©rica es
inestable.

En el ejemplo 2, observamos que garantizar una precisi√≥n de 2 3 1025 para aproximar
œÄ
0 sen x d x requer√≠a 360 subdivisiones de [0, /] para la regla compuesta trapezoidal y s√≥lo 18
para la regla compuesta de Simpson. Adem√°s del hecho de que se necesitan menos c√°lculos para la t√©cnica de Simpson, usted podr√≠a sospechar que este m√©todo tambi√©n implica
menos error de redondeo. Sin embargo, una propiedad importante compartida por todas
las t√©cnicas de integraci√≥n compuesta es una estabilidad respecto al error de redondeo.
Es decir, el error de redondeo no depende del n√∫mero c√°lculos realizados.
Para demostrar este hecho considerablemente sorprendente, suponga que aplicamos la
regla compuesta de Simpson con n subintervalos a una funci√≥n f en [a, b] y determine
la m√°xima cota para el error de redondeo. Suponga que f (xi ) se aproxima mediante fÀú(xi )
y que

f (xi ) = fÀú(xi ) + ei ,

para cada i = 0, 1, . . . , n,

156

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

donde ei denota el error de redondeo asociado con el uso de fÀú(xi ) para aproximar f (xi ).
Entonces, el error acumulado, e(h), en la regla compuesta de Simpson es

‚é°
e(h) =

h‚é£
e0 + 2
3

(n/2)‚àí1

n/2

e2 j + 4
j=1

‚é§
e2 j‚àí1 + en ‚é¶

j=1

‚é°
‚é§
(n/2)‚àí1
n/2
h‚é£
‚â§
|e0 | + 2
|e2 j | + 4
|e2 j‚àí1 | + |en |‚é¶ .
3
j=1
j=1
Si los errores de redondeo est√°n limitados de manera uniforme por Œµ, entonces

e(h) ‚â§

h
h
n
n
Œµ+2
Œµ + Œµ = 3nŒµ = nhŒµ.
‚àí1 Œµ+4
3
2
2
3

Sin embargo, nh = b ‚àí a , por lo que

e(h) ‚â§ (b ‚àí a)Œµ,
una cota independiente de h (y n (VWRVLJQL√ÄFDTXHDXQTXHTXL]iQHFHVLWHPRVGLYLGLUXQ
intervalo en m√°s partes para garantizar precisi√≥n, el c√°lculo incrementado que se requiere
no aumenta el error de redondeo. Este resultado implica que el procedimiento es estable
conforme h se aproxima a cero. Recuerde que esto no es verdad para los procedimientos de
diferenciaci√≥n num√©rica considerados al principio de este cap√≠tulo.
La secci√≥n Conjunto de ejercicios 4.4 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

4.5 Integraci√≥n de Romberg
En esta secci√≥n ilustraremos la forma en la que la extrapolaci√≥n de Richardson aplicada a los
resultados de la regla compuesta trapezoidal se puede usar para obtener aproximaciones de
alta precisi√≥n con poco costo computacional.
En la secci√≥n 4.4 encontramos que la regla compuesta trapezoidal tiene un error de truncamiento de orden O(h2 (QHVSHFt√ÄFRPRVWUDPRVTXHSDUDh = (b ‚àí a)/n y x j = a + j h,
tenemos
‚é°
‚é§
n‚àí1
b
h‚é£
(b ‚àí a) f (Œº) 2
f (a) + 2
h ,
f (x) d x =
f (x j ) + f (b)‚é¶ ‚àí
2
12
a
j=1
para alg√∫n numero m en (a, b).
Con un m√©todo alternativo, se puede mostrar (consulte [RR], pp. 136‚Äì140) que si
f ‚àà C ‚àû [a, b], la regla compuesta trapezoidal tambi√©n se puede escribir con un t√©rmino de
error de la forma
b
a

‚é°
f (x) d x =

n‚àí1

‚é§

h‚é£
f (x j ) + f (b)‚é¶ + K 1 h 2 + K 2 h 4 + K 3 h 6 + ¬∑ ¬∑ ¬∑ ,
f (a) + 2
2
j=1
(4.33)

donde Ki es una constante que depende solamente de f (2i‚àí1) (a) y f (2i‚àí1) (b).

4.5 Integraci√≥n de Romberg

157

Recuerde, de la secci√≥n 4.2, que la extrapolaci√≥n de Richardson se puede realizar en
cualquier procedimiento de aproximaci√≥n cuyo error de truncamiento es de la forma
m‚àí1

K j h Œ± j + O(h Œ±m ),

j=1

para un conjunto de constantes Kj y donde Œ±1 < Œ±2 < Œ±3 < ¬∑ ¬∑ ¬∑ < Œ±m . En esa secci√≥n realizamos demostraciones para ilustrar qu√© tan efectiva es esta t√©cnica cuando el procedimiento
de aproximaci√≥n tiene un error de truncamiento s√≥lo con potencias pares de h, es decir,
cuando el error de truncamiento tiene la forma
m‚àí1

K j h 2 j + O(h 2m ).

j=1

Werner Romberg (1909‚Äì2093)
concibi√≥ este procedimiento para
mejorar la precisi√≥n de la regla
trapezoidal al eliminar t√©rminos
sucesivos en la expansi√≥n
asint√≥tica en 1955.

Puesto que la regla trapezoidal compuesta tiene esta forma, es un candidato obvio para extrapolaci√≥n. Esto resulta en una t√©cnica conocida como integraci√≥n de Romberg.
b
Para aproximar la integral a f (x) d x, utilizamos los resultados de la regla compuesta trapezoidal con n = 1, 2, 4, 8, 16, . . . , e indicamos las aproximaciones resultantes, respectivamente mediante R1,1, R2,1, R3,1, y as√≠ sucesivamente. A continuaci√≥n, aplicamos la
extrapolaci√≥n de la forma determinada en la secci√≥n 4.2; es decir, obtenemos O(h4) aproximaciones R2,2, R3,2, R4,2, y as√≠ sucesivamente, por medio de

1
Rk,2 = Rk,1 + (Rk,1 ‚àí Rk‚àí1,1 ),
3

para k = 2, 3, . . .

y, entonces, las O(h6) aproximaciones R3,3, R4,3, R5,3, se pueden obtener mediante

Rk,3 = Rk,2 +

1
(Rk,2 ‚àí Rk‚àí1,2 ),
15

para k = 3, 4, . . .

En general, despu√©s de que se han obtenido las aproximaciones Rk, j‚àí1 adecuadas, determinamos las aproximaciones O(h 2 j ) a partir de

Rk, j = Rk, j‚àí1 +
Ejemplo 1

1
(Rk, j‚àí1 ‚àí Rk‚àí1, j‚àí1 ),
4 j‚àí1 ‚àí 1

para k = j, j + 1, . . .
œÄ

Use la regla compuesta trapezoidal para encontrar aproximaciones para 0 sen x d x con
n 5 1, 2, 4, 8 y 16. A continuaci√≥n, realice la integraci√≥n de Romberg en los resultados.
La regla compuesta trapezoidal para los diferentes valores de n proporciona las siguientes
aproximaciones para el valor verdadero 2:

œÄ
[sen 0 + sen œÄ ] = 0,
2
œÄ
œÄ
R2,1 =
sen 0 + 2 sen + sen œÄ = 1.57079633,
4
2
œÄ
3œÄ
œÄ
œÄ
R3,1 =
sen 0 + 2 sen + sen + sen
+ sen œÄ = 1.89611890,
8
4
2
4
R1,1 =

R4,1 =

œÄ
œÄ
œÄ
3œÄ
7œÄ
sen 0 + 2 sen + sen + ¬∑ ¬∑ ¬∑ + sen
+ sen
16
8
4
4
8

+ sen œÄ

=1.97423160, y
R5,1 =

œÄ
œÄ
œÄ
7œÄ
15œÄ
sen 0 + 2 sen
+ sen + ¬∑ ¬∑ ¬∑ + sen
+ sen
32
16
8
8
16

= 1.99357034.

+ sen œÄ

158

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

Las aproximaciones O(h4) son

1
R2,2 = R2,1 + (R2,1 ‚àí R1,1 ) = 2.09439511,
3
1
R3,2 = R3,1 + (R3,1 ‚àí R2,1 ) = 2.00455976,
3
1
R4,2 = R4,1 + (R4,1 ‚àí R3,1 ) = 2.00026917, y
3
1
R5,2 = R5,1 + (R5,1 ‚àí R4,1 ) = 2.00001659.
3
Las aproximaciones O(h6) son

1
(R3,2 ‚àí R2,2 ) = 1.99857073,
15
1
R4,3 = R4,2 + (R4,2 ‚àí R3,2 ) = 1.99998313, y
15
1
R5,3 = R5,2 + (R5,2 ‚àí R4,2 ) = 1.99999975.
15

R3,3 = R3,2 +

Las dos aproximaciones O(h8) son

R4,4 = R4,3 +

1
1
(R4,3 ‚àí R3,3 ) = 2.00000555 y R5,4 = R5,3 + (R5,3 ‚àí R4,3 )
63
63
= 2.00000001,

y la aproximaci√≥n O(h10 √ÄQDOHV

R5,5 = R5,4 +

1
(R5,4 ‚àí R4,4 ) = 1.99999999.
255

Estos resultados se muestran en la tabla 4.9.

Tabla 4.9

0
1.57079633
1.89611890
1.97423160
1.99357034

2.09439511
2.00455976
2.00026917
2.00001659

1.99857073
1.99998313
1.99999975

2.00000555
2.00000001

1.99999999

Observe que al generar las aproximaciones para la regla compuesta trapezoidal en el
ejemplo 1, cada aproximaci√≥n consecutiva inclu√≠a todas las evaluaciones de la funci√≥n desde la aproximaci√≥n previa. Es decir, R1,1 utilizaba evaluaciones en 0 y œÄ, y R2,1 usaba estas
evaluaciones y a√±ad√≠a una evaluaci√≥n en el punto intermedio œÄ/2. Entonces R3,1 utilizaba
las evaluaciones de R2,1 y a√±ad√≠a dos intermedios adicionales en œÄ/4 y 3œÄ/4. Este patr√≥n
contin√∫a con R4,1 mediante las mismas evaluaciones como R3,1, pero al a√±adir evaluaciones
en los cuatro puntos intermedios œÄ/8, 3œÄ/8, 5œÄ/8, 7œÄ/8, y as√≠ sucesivamente.
Este procedimiento de evaluaci√≥n para las aproximaciones de la regla compuesta tradicional se mantiene para una integral en cualquier intervalo [a, b]. En general, la regla
compuesta trapezoidal denotaba Rk+1,1 utiliza las mismas evaluaciones que Rk,1, pero a√±ade
evaluaciones en los puntos intermedios 2 k‚àí23RUORWDQWRHOFiOFXORH√ÄFLHQWHGHHVWDVDSURximaciones puede realizarse de manera recursiva.
b
Para obtener las aproximaciones de la regla compuesta trapezoidal para a f (x) d x,
k‚àí1
entonces h k = (b ‚àí a)/m k = (b ‚àí a)/2 .

R1,1 =

h1
(b ‚àí a)
[ f (a) + f (b)] =
[ f (a) + f (b)],
2
2

4.5 Integraci√≥n de Romberg

159

y

R2,1 =

h2
[ f (a) + f (b) + 2 f (a + h 2 )].
2

Al reescribir este resultado para R2,1, podemos incorporar la aproximaci√≥n previamente
determinada R1,1

R2,1 =

(b ‚àí a)
4

f (a) + f (b) + 2 f

a+

(b ‚àí a)
2

=

1
[R1,1 + h 1 f (a + h 2 )].
2

De manera similar, podemos escribir

R3,1 =

1
{R2,1 + h 2 [ f (a + h 3 ) + f (a + 3h 3 )]},
2

y, en general (v√©aseOD√ÄJXUD WHQHPRV
‚é§
‚é°
2k‚àí2
1
Rk,1 = ‚é£ Rk‚àí1,1 + h k‚àí1
f (a + (2i ‚àí 1)h k )‚é¶ ,
2
i=1

(4.34)

para cada k = 2, 3, . . . , n. (Consulte los ejercicios 18 y 19.)
Figura 4.10
y

y
R1,1

y

y 5 f (x)

a

b

y 5 f (x)

R 2,1

x

b

a

y 5 f (x)

R 3,1

x

a

b

x

Entonces, la extrapolaci√≥n se usa para producir aproximaciones O(h 2k j ) mediante

Rk, j = Rk, j‚àí1 +

1
4 j‚àí1 ‚àí 1

(Rk, j‚àí1 ‚àí Rk‚àí1, j‚àí1 ),

para k = j, j + 1, . . . ,

como se muestra en la tabla 4.10.

Tabla 4.10

k

O h 2k

O h 4k

O h 6k

O h 8k

1
2
3
4
..
.
n

R1,1
R2,1
R3,1
R4,1
..
.
Rn,1

R2,2
R3,2
R4,2
..
.
Rn,2

R3,3
R4,3
..
.
Rn,3

R4,4
..
.
Rn,4

O h 2n
k

..

.
¬∑¬∑¬∑

Rn,n

160

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

El m√©todo efectivo para construir la tabla de Romberg utiliza el orden m√°s alto de la
DSUR[LPDFLyQHQFDGDSDVR(VGHFLUFDOFXODODVHQWUDGDV√ÄODSRU√ÄODHQRUGHQR1,1, R2,1, R2,2,
R3,1, R3,2, R3,3, y as√≠ sucesivamente. Esto tambi√©n permite calcular unaQXHYD√ÄODFRPSOHWD
en la tabla al realizar s√≥lo una aplicaci√≥n adicional de la regla compuesta trapezoidal. A continuaci√≥n, usa un promediado simple de los valores previamente calculados para obtener las
HQWUDGDVUHVWDQWHVHQOD√ÄOD5HFXHUGH
¬á &DOFXODUODWDEODGH5RPEHUJXQD√ÄODFRPSOHWDDODYH]

Ejemplo 2

œÄ

$xDGDXQD√ÄODGHH[WUDSRODFLyQDGLFLRQDODODWDEODSDUD 0 sen x d x.
Soluci√≥n 3DUDREWHQHUOD√ÄODDGLFLRQDOQHFHVLWDPRVODDSUR[LPDFLyQWUDSH]RLGDO

‚é°
‚é§
24
1‚é£
œÄ
(2k ‚àí 1)œÄ ‚é¶
R6,1 =
sen
R5,1 +
= 1.99839336.
2
16 k=1
32

El valor en la tabla 4.9 da

1
1
R6,2 = R6,1 + (R6,1 ‚àí R5,1 ) = 1.99839336 + (1.99839336 ‚àí 1.99357035)
3
3
= 2.00000103,
R6,3 = R6,2 +

1
1
(R6,2 ‚àí R5,2 ) = 2.00000103 + (2.00000103 ‚àí 2.00001659)
15
15

= 2.00000000,
R6,4 = R6,3 +

1
(R6,3 ‚àí R5,3 ) = 2.00000000,
63

R6,5 = R6,4 +

1
(R6,4 ‚àí R5,4 )
255

= 2.00000000,
1
y R6,6 = R6,5 + 1023
(R6,5 ‚àí R5,5 ) = 2.00000000. La nueva tabla de extrapolaci√≥n se muestra en la tabla 4.11.

Tabla 4.11

0
1.57079633
1.89611890
1.97423160
1.99357034
1.99839336

2.09439511
2.00455976
2.00026917
2.00001659
2.00000103

1.99857073
1.99998313
1.99999975
2.00000000

2.00000555
2.00000001
2.00000000

1.99999999
2.00000000

2.00000000

2EVHUYHTXHWRGRVORVYDORUHVH[WUDSRODGRVH[FHSWRSRUHOSULPHUR HQODSULPHUD√ÄODGH
la segunda columna) son m√°s precisos que la mejor aproximaci√≥n compuesta trapezoidal (en la
~OWLPD√ÄODGHODSULPHUDFROXPQD $SHVDUGHTXHH[LVWHQHQWUDGDVHQODWDEODV√≥lo
la sexta en la columna izquierda requiere evaluaciones de funci√≥n ya que son las √∫nicas entradas generadas por la regla compuesta trapezoidal; las otras entradas se obtienen mediante
un proceso de promediado. De hecho, debido a la relaci√≥n de recurrencia de los t√©rminos
en la columna izquierda, las √∫nicas evaluaciones de funci√≥n son aquellas para calcular la
aproximaci√≥n de la regla compuesta trapezoidal. En general, Rk,1 requiere evaluaciones de
funci√≥n 1 + 2k‚àí1, por lo que en este caso se necesita 1 + 25 = 33.

4.5 Integraci√≥n de Romberg

161

El algoritmo 4.2 usa el procedimiento recursivo para encontrar las aproximaciones de la
UHJODFRPSXHVWDWUDSH]RLGDO\FDOFXODORVUHVXOWDGRVHQODWDEOD√ÄODSRU√ÄOD

ALGORITMO

4.2

Integraci√≥n de Romberg
Para aproximar la integral I =

b
a

f (x) d x, seleccione un entero n > 0.

ENTRADA extremos a, b; entero n.
SALIDA

un arreglo R. (Calcule R por filas; s√≥lo se almacenan las √∫ltimas dos filas.)

Paso 1 Haga h = b ‚àí a;
R1,1 = h2 ( f (a) + f (b)).
Paso 2 SALIDA (R1,1 ).
Paso 3 Para i = 2, . . . , n realice los pasos 4‚Äì8.
‚é°
‚é§
2i‚àí2
1‚é£
R1,1 + h
f (a + (k ‚àí 0.5)h)‚é¶.
Paso 4 Haga R2,1 =
2
k=1
(Aproximaci√≥n a partir del m√©todo trapezoidal.)
Paso 5 Para j = 2, . . . , i
haga R2, j = R2, j‚àí1 +

R2, j‚àí1 ‚àí R1, j‚àí1
.
4 j‚àí1 ‚àí 1

(Extrapolaci√≥n.)

Paso 6 SALIDA (R2, j para j = 1, 2, . . . , i).
Paso 7 Haga h = h/2.
Paso 8 Para j = 1, 2, . . . , i determine R1, j = R2, j .

(Actualice la fila 1 de R.)

Paso 9 PARE.

El adjetivo cauteloso que se usa
en la descripci√≥n de un m√©todo
num√©rico indica que se incluye
XQDYHUL√ÄFDFLyQSDUDGHWHUPLQDU
si las hip√≥tesis de continuidad
tienen alguna probabilidad de ser
verdaderas.

El algoritmo 4.2 requiere un entero preestablecido n para determinar el n√∫mero de
√ÄODV TXH VH YD D JHQHUDU7DPELpQ HVWDEOHFHUHPRV XQD WROHUDQFLD GH HUURU SDUD OD DSUR[Lmaci√≥n y generar n, dentro de alg√∫na cota superior hasta que las entradas diagonales consecutivas Rn‚àí1,n‚àí1 y Rn,n concuerden dentro de la tolerancia. Para evitar la posibilidad de
TXHGRVHOHPHQWRVGH√ÄODFRQVHFXWLYRVFRQFXHUGHQXQRVFRQRWURVSHURQRFRQHOYDORUGH
la integral que se est√° aproximando, es com√∫n generar aproximaciones hasta que no s√≥lo
|Rn‚àí1,n‚àí1 ‚àí Rn,n | est√© dentro de la tolerancia, sino tambi√©n |Rn‚àí2,n‚àí2 ‚àí Rn‚àí1,n‚àí1 |. A pesar
de que no es una protecci√≥n universal, esto garantizar√° que dos conjuntos de aproximaciones
JHQHUDGRVGHPDQHUDGLIHUHQWHFRQFXHUGHQGHQWURGHODWROHUDQFLDHVSHFL√ÄFDGDDQWHVGHRn,n
VHDFHSWHFRPRVX√ÄFLHQWHPHQWHSUHFLVR
La integraci√≥n de Romberg aplicada a una funci√≥n f en el intervalo [a, b] depende de la
suposici√≥n de que la regla compuesta trapezoidal tienen un t√©rmino de error que se puede
expresar en la forma de la ecuaci√≥n (4.33); es decir, debemos tener f ‚àà C 2k+2 [a, b] para la
k-√©sima√ÄODTXHVHYDDJHQHUDU/RVDOJRULWPRVGHSURSyVLWRJHQHUDOSRUPHGLRGHODLQWHJUDFLyQGH5RPEHUJLQFOX\HQXQDYHUL√ÄFDFLyQHQFDGDHWDSDSDUDJDUDQWL]DUHOFXPSOLPLHQWR
de esta suposici√≥n. Estos m√©todos se conocen como algoritmos cautelosos de Romberg y
se describen en [Joh]. Esta referencia tambi√©n describe m√©todos para utilizar la t√©cnica de
Romberg como un procedimiento adaptable, similar a la regla adaptable de Simpson, la cual
se analizar√° en la secci√≥n 4.6.
La secci√≥n Conjunto de ejercicios 4.5 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en p√°ginas preliminares.

162

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

4.6 M√©todos de cuadratura adaptable
Las f√≥rmulas compuestas son muy efectivas en muchas situaciones, pero ocasionalmente sufren porque requieren el uso de nodos igualmente espaciados. Esto es inadecuado al integrar
una funci√≥n sobre un intervalo que contiene ambas regiones con gran variaci√≥n funcional y
regiones con variaci√≥n funcional peque√±a.
Ilustraci√≥n

La √∫nica soluci√≥n a la ecuaci√≥n diferencial y + 6y + 25 = 0 que adicionalmente satisface y(0) = 0 y y (0) = 4 es y(x) = e‚àí3x sen 4x. Las funciones de este tipo son comunes
en ingenier√≠a mec√°nica porque describen ciertas caracter√≠sticas de sistemas absorbentes de
muelle y amortiguador y en ingenier√≠a el√©ctrica porque son soluciones comunes a los proEOHPDVIXQGDPHQWDOHVGHFLUFXLWRV/DJUi√ÄFDGHy(x) para x en el intervalo [0, 4] se muestra
HQOD√ÄJXUD

Figura 4.11
y
0.5
0.4
0.3
0.2

y (x) = e23xsen 4x

0.1
2

1
2

3

4

x

20.1

Suponga que necesitamos la integral de y(x HQ>@/DJUi√ÄFDLQGLFDTXHODLQWHJUDO
en [3, 4] debe estar muy cerca de 0 y en [2, 3] tampoco se esperar√≠a que fuera grande. Sin
HPEDUJRHQ>@H[LVWHYDULDFLyQVLJQL√ÄFDWLYDGHODIXQFLyQ\QRHVGHOWRGRFODUDFXiOHVOD
integral en este intervalo. Este es un ejemplo de una situaci√≥n en donde la integraci√≥n compuesta ser√≠a inadecuada. Se podr√≠a usar un m√©todo de orden bajo en [2, 4], pero se necesitar√≠a
un m√©todo de orden superior en [0, 2].
La pregunta que considerar√≠amos en esta secci√≥n es:
‚Ä¢ ¬øC√≥mo podemos determinar la t√©cnica que deber√≠amos aplicar en las diferentes partes del
LQWHUYDORGHLQWHJUDFLyQ\TXpWDQSUHFLVDSRGHPRVHVSHUDUTXHVHDODDSUR[LPDFLyQ√ÄQDO"
Veremos que en ciertas condiciones razonables, podemos responder esta pregunta y tambi√©n
determinar aproximaciones que satisfacen requisitos de precisi√≥n determinados.
Si el error de aproximaci√≥n para una integral en un intervalo determinado est√° distribuido de manera equitativa, se necesita un tama√±o de paso m√°s peque√±o para las grandes
UHJLRQHVGHYDULDFLyQTXHSDUDDTXHOODVFRQPHQRVYDULDFLyQ8QDWpFQLFDH√ÄFLHQWHSDUDHVWH

4.6 M√©todos de cuadratura adaptable

163

tipo de problema deber√≠a predecir la cantidad de variaci√≥n funcional y adaptar el tama√±o de
paso conforme sea necesario. Estos m√©todos reciben el nombre de m√©todos de cuadratura
adaptable. Los m√©todos adaptables son especialmente populares para la inclusi√≥n en paTXHWHVSURIHVLRQDOHVGHVRIWZDUHSRUTXHDGHPiVGHVHUH√ÄFLHQWHVHQJHQHUDOSURSRUFLRQDQ
DSUR[LPDFLRQHVTXHVHHQFXHQWUDQGHQWURGHXQDWROHUDQFLDHVSHFt√ÄFDGHWHUPLQDGD
En esta secci√≥n, consideramos un m√©todo de cuadratura y veremos c√≥mo se puede usar
para reducir el error de aproximaci√≥n y tambi√©n predecir un error calculado para la aproximaci√≥n que no depende del conocimiento de las derivadas superiores de la funci√≥n. El
m√©todo que analizamos est√° basado en la regla compuesta de Simpson, pero la t√©cnica se
PRGL√ÄFDIiFLOPHQWHSDUDXWLOL]DURWURVSURFHGLPLHQWRVFRPSXHVWRV
b
Suponga que queremos aproximar a f (x) d xGHQWURGHXQDWROHUDQFLDHVSHFt√ÄFDŒµ > 0.
El primer paso es aplicar la regla de Simpson con tama√±o de paso h = (b ‚àí a)/2. Esto produce (v√©aseOD√ÄJXUD
b
a

f (x) d x = S(a, b) ‚àí

h 5 (4)
f (Œæ ),
90

para algunos Œæ en(a, b),

(4.35)

donde denotamos la aproximaci√≥n de la regla de Simpson en [a, b] mediante

S(a, b) =

h
[ f (a) + 4 f (a + h) + f (b)].
3

Figura 4.12
y

S(a, b)

a

y 5 f (x)

h

x

b

h

El siguiente paso es determinar una aproximaci√≥n de precisi√≥n que no requiere f (4) (Œæ ).
Para hacerlo aplicamos la regla compuesta de Simpson con n 5 4 y el tama√±o de paso
(b‚àía)/4 = h/2, lo cual nos da
b
a

f (x) d x =

h
6

f (a) + 4 f

‚àí

h
2

4

a+

h
2

+ 2 f (a + h) + 4 f

a+

3h
2

(b ‚àí a) (4)
f (ŒæÃÉ ),
180

(4.36)

para algunos j en (a, b 3DUDVLPSOL√ÄFDUODQRWDFLyQVL

S a,

a+b
2

=

h
6

f (a) + 4 f

+ f (b)

a+

h
2

+ f (a + h)

164

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

y
S

a+b
,b
2

=

h
6

f (a + h) + 4 f

a+

3h
2

+ f (b) .

Entonces, la ecuaci√≥n (4.36) se puede reescribir (v√©ase la figura 4.13) como
b
a

f (x) d x = S a,

a+b
2

+S

(

(

a+b
1
,b ‚àí
2
16

h5
90

f (4) (ŒæÃÉ ).

(4.37)

Figura 4.13
y

(

a1b
a1b
1S
, b
2
2

S a,

(
y 5 f (x)

a
h
2

x

b

a1b
2

El c√°lculo de error se deriva al suponer que Œæ ‚âà ŒæÃÉ o, con mayor precisi√≥n, que f (4) (Œæ ) ‚âà
f (ŒæÃÉ ), y el √©xito de la t√©cnica depende de la precisi√≥n de esta suposici√≥n. Si es precisa,
entonces, al equiparar las integrales en las ecuaciones (4.35) y (4.37) obtenemos
(4)

S a,

a+b
2

+S

a+b
1
,b ‚àí
2
16

h5
90

f (4) (Œæ ) ‚âà S(a, b) ‚àí

h 5 (4)
f (Œæ ),
90

por lo que
16
a+b
h 5 (4)
S(a, b) ‚àí S a,
f (Œæ ) ‚âà
90
15
2

‚àíS

a+b
,b
2

.

Al utilizar esta t√©cnica en la ecuaci√≥n (4.37) produce el c√°lculo de error
b
a

‚âà

f (x) d x ‚àí S a,

1
16

h5
90

a+b
2

f (4) (Œæ ) ‚âà

‚àíS

a+b
,b
2

1
a+b
S(a, b) ‚àí S a,
15
2

‚àíS

a+b
,b
2

.

4.6 M√©todos de cuadratura adaptable

165

b

Esto implica que S(a, (a + b)/2) + S((a + b)/2, b) se aproxima a a f (x) d x aproximadamente 15 veces mejor que lo que concuerda con el valor calculado S(a, b). Por lo tanto, si

S(a, b) ‚àí S a,

a+b
2

a+b
,b
2

‚àíS

< 15Œµ,

(4.38)

esperamos tener
b
a

f (x) d x ‚àí S a,

a+b
2

a+b
,b
2

‚àíS

< Œµ,

(4.39)

y

S a,

a+b
2

a+b
,b
2

+S

b

VHVXSRQHTXHHVXQDDSUR[LPDFLyQVX√ÄFLHQWHPHQWHSUHFLVDSDUD a f (x) d x.
Ejemplo 1

9HUL√ÄTXHODSUHFLVLyQGHOFiOFXORGHHUURUGHWHUPLQDGRSRUODVGHVLJXDOGDGHV  \  
al aplicar la integral
œÄ/2

sen x d x = 1

0

al comparar

1
œÄ
S 0,
15
2

‚àí S 0,

œÄ
4

‚àíS

œÄ/2

œÄ œÄ
,
4 2

con

sen x d x ‚àí S 0,

0

œÄ
4

‚àíS

œÄ œÄ
,
4 2

.

Soluci√≥n 7HQHPRV

œÄ
2

S 0,

=

œÄ
œÄ/4
œÄ
œÄ ‚àö
(2 2 + 1) = 1.002279878
sen 0 + 4 sen + sen
=
3
4
2
12

y

S 0,

œÄ
4

+S

œÄ œÄ
,
4 2

=

œÄ
3œÄ
œÄ
œÄ/8
œÄ
+ sen
sen 0 + 4 sen + 2 sen + 4 sen
3
8
4
8
2

= 1.000134585.
Por lo que,

S 0,

œÄ
2

‚àí S 0,

œÄ
4

‚àíS

œÄ œÄ
,
4 2

= |1.002279878 ‚àí 1.000134585| = 0.002145293.

El c√°lculo para el error obtenido al utilizar S(a, (a + b)) + S((a + b), b) para aproximar
b
a f (x) d x es, por consiguiente,

1
œÄ
S 0,
15
2

‚àí S 0,

œÄ
4

‚àíS

œÄ œÄ
,
4 2

= 0.000143020,

166

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

el cual se aproxima de cerca al error real
œÄ/2

sen x d x ‚àí 1.000134585 = 0.000134585,

0

aunque Dx4 sen x = sen xYDUtDVLJQL√ÄFDWLYDPHQWHHQHOLQWHUYDOR(0, œÄ/2).

Es buena idea incluir un margen
de seguridad cuando es imposible
YHUL√ÄFDUODVVXSRVLFLRQHVGH
precisi√≥n.

ALGORITMO

&XDQGR ODV DSUR[LPDFLRQHV HQ OD GHVLJXDOGDG   GL√ÄHUHQ SRU PiV GH 15Œµ, podemos aplicar la t√©cnica de la regla de Simpson de manera individual a los subintervalos
[ a, (a + b)/2] y [(a + b)/2, b]. A continuaci√≥n, usamos el procedimiento de c√°lculo de
error para determinar si la aproximaci√≥n para la integral en cada subintervalo se encuentra
dentro de una tolerancia de Œµ/2. En este caso, sumamos las aproximaciones para producir
b
una aproximaci√≥n de a f (x) d x dentro de la tolerancia Œµ.
Si la aproximaci√≥n en uno de los subintervalos no se encuentra dentro de la tolerancia
Œµ/2, entonces ese subintervalo se subdivide en s√≠ mismo y el procedimiento se reaplica a los
dos subintervalos para determinar si la aproximaci√≥n en cada subintervalo es precisa dentro de
Œµ/4. Este procedimiento de reducci√≥n a la mitad contin√∫a hasta que cada parte est√° dentro
de la tolerancia requerida.
Se pueden construir problemas para los que esta tolerancia nunca se cumplir√°, pero
normalmente la t√©cnica es exitosa porque cada subdivisi√≥n aumenta la precisi√≥n de la aproximaci√≥n en un factor de 16 mientras requiere un factor de precisi√≥n aumentado en s√≥lo 2.
El algoritmo 4.3 describe detalladamente este procedimiento de cuadratura adaptable
SDUDODUHJODGH6LPSVRQDSHVDUGHTXHODLPSOHPHQWDFLyQGL√ÄHUHOLJHUDPHQWHGHODQiOLVLV
anterior. Por ejemplo, en el paso 1, la tolerancia se ha establecido en 10Œµ en lugar de en 15Œµ en
la desigualdad (4.38). Esta cota se elige de manera conservadora para compensar el error
en la suposici√≥n f (4) (Œæ ) ‚âà f (4) (ŒæÃÉ ). En problemas donde se sabe que f (4) var√≠a ampliamente,
esta cota deber√≠a disminuir todav√≠a m√°s.
El procedimiento listado en el algoritmo, primero aproxima la integral en el subinterYDORVLWXDGRPiVDODL]TXLHUGDHQXQDVXEGLYLVLyQ(VWRUHTXLHUHDOPDFHQDPLHQWRH√ÄFLHQWH
y recordar las evaluaciones funcionales que se han calculado antes para los nodos en los
subintervalos situados a la mitad derecha. Los pasos 3, 4 y 5 contienen un procedimiento de
apilamiento con un indicador para seguir los datos requeridos para calcular la aproximaci√≥n
en el subintervalo inmediatamente adyacente y a la derecha del subintervalo en el que se
genera la aproximaci√≥n. El m√©todo es m√°s f√°cil de implementar por medio de lenguaje de
programaci√≥n recursivo.

Cuadratura adaptable

4.3
Para aproximar la integral I =

b
a

f (x) d x dentro de una tolerancia determinada:

ENTRADA extremos a, b; tolerancia TOL; cota N para diferentes niveles.
SALIDA aproximaci√≥n APP o mensaje N superado.
Paso 1 Haga APP = 0;
i = 1;
TOLi = 10 TOL;
ai = a;
h i = (b ‚àí a)/2;
FAi = f (a);
FCi = f (a + h i );
FBi = f (b);
Si = h i (FAi + 4FCi + FBi )/3; (Aproximaci√≥n a partir del m√©todo
de Simpson para intervalo completo.)
L i = 1.
Paso 2 Si i > 0 haga los pasos 3‚Äì5.

4.6 M√©todos de cuadratura adaptable

167

Paso 3 Haga FD = f (ai + h i /2);
FE = f (ai + 3h i /2);
S1 = h i (FAi + 4FD + FCi )/6; (Aproximaciones para el m√©todo
de Simpson para mitades de
subintervalos.)
S2 = h i (FCi + 4FE + FBi )/6;
v1 = ai ; (Guardar datos en este nivel.)
v2 = FAi ;
v3 = FCi ;
v4 = FBi ;
v5 = h i ;
v6 = TOLi ;
v7 = Si ;
v8 = L i .
Paso 4 Haga i = i ‚àí 1. (Borrar el nivel.)
Paso 5 Si|S1 + S2 ‚àí v7 | < v6
entonces determine APP = APP + (S1 + S2)
tambi√©n
si (v8 ‚â• N )
entonces
SALIDA ('NIVEL EXCEDIDO'); (Falla del procedimiento.)
PARE.
tambi√©n (A√±adir un nivel. )
haga i = i + 1; (Datos para subintervalo mitad derecha.)
ai = v1 + v5 ;
FAi = v3 ;
FCi = FE;
FBi = v4 ;
h i = v5 /2;
TOLi = v6 /2;
Si = S2;
L i = v8 + 1;
haga i = i + 1; (Datos para subintervalo mitad izquierda.)
ai = v1 ;
FAi = v2 ;
FCi = FD;
FBi = v3 ;
h i = h i‚àí1 ;
TOLi = TOLi‚àí1 ;
Si = S1;
L i = L i‚àí1 .
Paso 6 SALIDA (APP);
PARE.

Ilustraci√≥n

(APP se aproxima a I dentro de TOL. )

/DJUi√ÄFDGHODIXQFLyQ f (x) = (100/x 2 ) sen(10/x) para xHQ>@VHPXHVWUDHQOD√Ägura 4.13. Por medio del algoritmo de cuadratura adaptable 4.3 con tolerancia 1024 para
3
aproximar 1 f (x) d x produce 21.426014, un resultado preciso dentro de 1.1 √ó 10‚àí5.
La aproximaci√≥n requer√≠a que la regla de Simpson con n 5 4 se realice sobre los 23 subinWHUYDORVFX\RVH[WUHPRVVHPXHVWUDQHQHOHMHKRUL]RQWDOHQOD√ÄJXUD(OQ~PHURWRWDOGH
evaluaciones funcionales requerido para esta aproximaci√≥n es 93.

168

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

Figura 4.14
y

60
50
40
y = f (x) =

30

( (

100
10
sen
x2
x

20
10
2.75 3.0
1.0

1.25 1.5

1.75 2.0 2.25 2.5

x

210
220
230
240
250
260

El valor m√°s grande de h para el que la regla compuesta de Simpson est√°ndar provee
precisi√≥n dentro de 1024 es h = 1/88. Esta aplicaci√≥n requiere 177 evaluaciones de funci√≥n,
aproximadamente dos veces m√°s que la cuadratura adaptable.
La secci√≥n Conjunto de ejercicios 4.6 est√° disponible en l√≠nea: Encuentre la ruta de
acceso en las p√°ginas preliminares.

4.7 Cuadratura gaussiana
Las f√≥rmulas de Newton-Cotes en la secci√≥n 4.3 se dedujeron al integrar polinomios de interpolaci√≥n. El t√©rmino de error en el polinomio interpolante de grado n implica la derivada
(n 1 1) de la funci√≥n que se va a aproximar, por lo que la f√≥rmula Newton-Cotes es exacta
al aproximar la integral de cualquier polinomio de grado menor o igual a n.
7RGDVODVIyUPXODVGH1HZWRQ&RWHVXWLOL]DQYDORUHVGHODIXQFLyQHQSXQWRVLJXDOPHQWH
espaciados. Esta restricci√≥n es conveniente cuando se combinan las f√≥rmulas para formar las
UHJODVFRPSXHVWDVTXHFRQVLGHUDPRVHQODVHFFLyQSHURSXHGHGLVPLQXLUVLJQL√ÄFDWLYDmente la precisi√≥n de la aproximaci√≥n. Considere, por ejemplo, la regla trapezoidal aplicada
SDUDGHWHUPLQDUODVLQWHJUDOHVGHODVIXQFLRQHVFX\DVJUi√ÄFDVVHPXHVWUDQHQOD√ÄJXUD

4.7

Cuadratura gaussiana

169

Figura 4.15
y

y

y

y 5 f (x)

a 5 x1

y 5 f (x)

y 5 f (x)

x2 5 b x

x2 5 b x

a 5 x1

x2 5 b x

a 5 x1

La regla trapezoidal aproxima la integral de la funci√≥n al integrar la funci√≥n lineal que
XQHORVH[WUHPRVGHODJUi√ÄFDGHODIXQFLyQ3HURSUREDEOHPHQWHQRHVODPHMRUOtQHDSDUD
DSUR[LPDUODLQWHJUDO(QPXFKRVFDVRVODVOtQHDVFRPRODVPRVWUDGDVHQOD√ÄJXUDSURbablemente proporcionar√≠an mucho mejores aproximaciones.
Figura 4.16
y

y

y

y 5 f (x)

y 5 f (x)

y 5 f (x)

a x1

x2 b

x

a x1

x2 b

x

a x1

x2 b

x

En la cuadratura Gaussiana, los puntos para evaluaci√≥n se seleccionan de manera √≥ptima
en lugar de nodos igualmente espaciados. Los nodos x1 , x2 , . . . , xn en el intervalo [a, b], y
ORVFRH√ÄFLHQWHV c1 , c2 , . . . , cn se seleccionan para minimizar el error esperado obtenido en
la aproximaci√≥n
Gauss demostr√≥ su m√©todo de
LQWHJUDFLyQQXPpULFDH√ÄFLHQWH
en un documento presentado
ante la G√∂ttingen Society en
1814. √âl permiti√≥ que los nodos,
DVtFRPRORVFRH√ÄFLHQWHVGHODV
evaluaciones de funci√≥n, fueran
par√°metros en la f√≥rmula de
suma y encontr√≥ la colocaci√≥n
√≥ptima de los nodos. Goldstine
[Golds], pp. 224‚Äì232, tiene una
descripci√≥n interesante de este
desarrollo.

b
a

n

f (x) d x ‚âà

ci f (xi ).
i=1

Para medir esta precisi√≥n, suponemos que la mejor elecci√≥n de estos valores produce el
resultado exacto para la clase de polinomios de mayor grado, es decir, la selecci√≥n que da
el grado m√°s alto de precisi√≥n.
/RVFRH√ÄFLHQWHVc1 , c2 , . . . , cn en la f√≥rmula de aproximaci√≥n son arbitrarios y los nodos
x1 , x2 , . . . , xn est√°n restringidos solamente por el hecho de que deben encontrarse en [a, b],
el intervalo de integraci√≥n. Esto nos da 2n SDUiPHWURV D HOHJLU 6L ORV FRH√ÄFLHQWHV GH XQ
polinomio se consideran par√°metros, la clase de polinomios de grado por lo menos 2n 2 1
tambi√©n contiene 2n par√°metros. Esto, entonces, es la mayor clase de polinomios para los
que es razonable esperar que una f√≥rmula sea exacta. Con la elecci√≥n adecuada de valores y
constantes, se puede obtener la precisi√≥n en este conjunto.

170

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

Para ilustrar el procedimiento para elegir los par√°metros adecuados, mostraremos c√≥mo
VHOHFFLRQDUORVFRH√ÄFLHQWHV\QRGRVFXDQGRn 5 2 y el intervalo de integraci√≥n es [21, 1].
Entonces, analizaremos la situaci√≥n m√°s general para una selecci√≥n arbitraria de nodos y
FRH√ÄFLHQWHV\PRVWUDUHPRVFyPRVHPRGL√ÄFDODWpFQLFDDOLQWHJUDUVREUHXQLQWHUYDORDUELtrario.
Suponga que queremos determinar c1, c2, x1, y x2 de tal forma que la f√≥rmula de integraci√≥n
1
‚àí1

f (x) d x ‚âà c1 f (x1 ) + c2 f (x2 )

da el resultado exacto siempre que f (x) es un polinomio de grado 2(2) 2 1 5 3 o menor, es
decir, cuando

f (x) = a0 + a1 x + a2 x 2 + a3 x 3 ,
para alg√∫n conjunto de constantes a0, a1, a2, y a3. Puesto que

(a0 + a1 x + a2 x 2 + a3 x 3 ) d x = a0

1 d x + a1

x d x + a2

x 2 d x + a3

x 3 d x,

esto es equivalente a mostrar que la f√≥rmula da resultados exactos cuando f (x) es 1, x, x2, y
x3. Por lo tanto, necesitamos c1, c2, x1, y x2, de tal forma que

c1 ¬∑ 1 + c 2 ¬∑ 1 =
c1 ¬∑ x12 + c2 ¬∑ x22 =

1
‚àí1
1
‚àí1

1 d x = 2,

c1 ¬∑ x1 + c2 ¬∑ x2 =

2
,
3

y c1 ¬∑ x13 + c2 ¬∑ x23 =

x2 dx =

1
‚àí1
1
‚àí1

x d x = 0,
x 3 d x = 0.

Un poco de √°lgebra muestra que este sistema de ecuaciones tiene una soluci√≥n √∫nica
‚àö
‚àö
3
3
c1 = 1, c2 = 1, x1 = ‚àí
, y x2 =
,
3
3
lo cual da la f√≥rmula de aproximaci√≥n
1
‚àí1

f (x) d x ‚âà f

‚àö
‚àí 3
3

+ f

‚àö
3
3

.

(4.40)

Esta f√≥rmula tiene grado de precisi√≥n tres; es decir, produce el resultado exacto para cada
polinomio de grado tres o menor.

Polinomios de Legendre
/DWpFQLFDTXHKHPRVGHVFULWRVHSXHGHXWLOL]DUSDUDGHWHUPLQDUORVQRGRV\FRH√ÄFLHQWHVSDUD
las f√≥rmulas que dan resultados exactos para polinomios de grado superior, pero un m√©todo
alterno los obtiene con mayor facilidad. En las secciones 8.2 y 8.3, consideraremos diferentes
conjuntos de polinomios ortogonales, funciones que tienen la propiedad de que una integral
GH√ÄQLGDGHOSURGXFWRGHFXDOTXLHUDGHGRVGHHOODVHV(OFRQMXQWRUHOHYDQWHSDUDQXHVWUR
problema son los polinomios de Legendre, un conjunto {P0 (x), P1 (x), . . . , Pn (x), . . . , }
con las siguientes propiedades:
(1) Para cada n, Pn (x) es un polinomio m√≥nico de grado n.
1

(2)
‚àí1

P(x)Pn (x) d x = 0 siempre que P(x) sea un polinomio de grado menor a n.

4.7
Recuerde que los polinomios
m√≥nicosWLHQHQXQFRH√ÄFLHQWH
principal de 1.

Cuadratura gaussiana

171

Los primeros polinomios de Legendre son

P0 (x) = 1,

1
P2 (x) = x 2 ‚àí ,
3
6
3
P4 (x) = x 4 ‚àí x 2 + .
7
35

P1 (x) = x,

3
P3 (x) = x 3 ‚àí x,
5

y

Adrien-Marie Legendre (1752‚Äì
1833) introdujo este conjunto
GHSROLQRPLRVHQ7XYR
numerosas disputas prioritarias
con Gauss, principalmente
debido a la falla de Gauss al
publicar muchos de sus resultados
originales mucho despu√©s de que
los hab√≠a descubierto.

Las ra√≠ces de estos polinomios son distintas, se encuentran en el intervalo (21, 1), tienen
una simetr√≠a respecto al origen y, m√°s importante, son la elecci√≥n correcta para determinar
ORVSDUiPHWURVTXHQRVSURSRUFLRQDQORVQRGRV\FRH√ÄFLHQWHVSDUDQXHVWURPpWRGRGHFXDdratura.
Los nodos x1 , x2 , . . . , xn necesarios para producir una f√≥rmula de aproximaci√≥n integral
que proporcione resultados exactos para cualquier polinomio de grado menor a 2n son las
ra√≠ces del polinomio de Legendre de en√©simo grado. Esto se establece mediante el siguiente
resultado.

Teorema 4.7

Suponga que x1 , x2 , . . . , xn son las ra√≠ces del n-√©simo polinomio de Legendre Pn (x) y que
para cada i = 1, 2, . . . , n, los n√∫meros ciHVWiQGH√ÄQLGRVSRU

ci =

n

x ‚àí xj
d x.
‚àí1 j=1 x i ‚àí x j
1

j =i

Si P(x) es cualquier polinomio de grado menor a 2n, entonces
1
‚àí1

n

P(x) d x =

ci P(xi ).
i=1

Demostraci√≥n Consideramos primero la situaci√≥n para un polinomio P(x) de grado menor a n.

Reescriba P(x HQWpUPLQRVGHORVSROLQRPLRVGHFRH√ÄFLHQWHVGH/DJUDQJH n 2 1)-√©simos
con nodos en las ra√≠ces del n-√©simo polinomio de Legendre Pn(x). El t√©rmino de error para
esta representaci√≥n implica la n-√©sima derivada de P(x). Puesto que P(x) es de grado menor
a n, la n-√©sima derivada de P(x) es 0 y esta representaci√≥n es exacta. Por lo tanto,
n

P(x) =

n

P(xi )L i (x) =
i=1

n

x ‚àí xj
P(xi )
x ‚àí xj
i=1 j=1 i
j =i

y

‚é§

‚é°
1
‚àí1

P(x) d x =

1 ‚é¢ n
‚àí1

‚é¢
‚é£

n

‚é•
x ‚àí xj
P(xi )‚é•
‚é¶ dx
x ‚àí xj
i=1 j=1 i
j =i

‚é°
n

=
i=1

‚é¢
‚é¢
‚é£

‚é§

n
‚é•
x ‚àí xj
dx‚é•
P(x
)
=
ci P(xi ).
i
‚é¶
‚àí1 j=1 x i ‚àí x j
i=1
1

n

j =i

Por lo tanto, el resultado es verdad para polinomios de grado menor a n.
Ahora considere un polinomio P(x) de grado por lo menos n pero menor a 2n. Divida
P(x) entre el n-√©simo polinomio de Legendre Pn(x). Esto proporciona dos polinomios Q(x) y
R(x), cada uno de grado menor a n, con

P(x) = Q(x)Pn (x) + R(x).

172

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

Observe que xi es una ra√≠z de Pn(x) para cada i = 1, 2, . . . , n, por lo que tenemos

P(xi ) = Q(xi )Pn (xi ) + R(xi ) = R(xi ).
Ahora, recurrimos a la potencia √∫nica de los polinomios de Legendre. En primer lugar, el
grado del polinomio Q(x) es menor a n, por lo que (mediante la propiedad (2) de Legendre),
1
‚àí1

Q(x)Pn (x) d x = 0.

A continuaci√≥n, puesto que R(x) es un polinomio de grado menor a n, el argumento de apertura implica que
1
‚àí1

n

R(x) d x =

ci R(xi ).
i=1

$OMXQWDUHVWRVKHFKRVYHUL√ÄFDPRVTXHODIyUPXODHVH[DFWDSDUDHOSROLQRPLRP(x):
1
‚àí1

P(x) d x =

1
‚àí1

[Q(x)Pn (x) + R(x)] d x =

1
‚àí1

n

R(x) d x =

n

ci R(xi ) =
i=1

ci P(xi ).
i=1

Las constantes ci necesarias para la regla de cuadratura se pueden generar a partir de la
ecuaci√≥n en el teorema 4.7, pero tanto las constantes como las ra√≠ces de los polinomios de
Legendre se tabulan ampliamente. La tabla 4.12 enumera estos valores para n 5 2, 3, 4 y 5.

Tabla 4.12

n

Ra√≠ces rn,i

Coeficientes cn,i

2

0.5773502692
‚àí0.5773502692
0.7745966692
0.0000000000
‚àí0.7745966692
0.8611363116
0.3399810436
‚àí0.3399810436
‚àí0.8611363116
0.9061798459
0.5384693101
0.0000000000
‚àí0.5384693101
‚àí0.9061798459

1.0000000000
1.0000000000
0.5555555556
0.8888888889
0.5555555556
0.3478548451
0.6521451549
0.6521451549
0.3478548451
0.2369268850
0.4786286705
0.5688888889
0.4786286705
0.2369268850

3

4

5

1

Ejemplo 1

Aproxime
Soluci√≥n

‚àí1

e x cos x d x mediante cuadratura gaussiana con n 5 3.

Las entradas en la tabla 4.12 nos dan
1
‚àí1

e x cos x d x ‚âà 0.5e0.774596692 cos 0.774596692
+ 0.8 cos 0 + 0.5e‚àí0.774596692 cos(‚àí0.774596692)
= 1.9333904.

La integraci√≥n por partes se puede utilizar para mostrar que el valor verdadero de la integral
es 1.9334214, por lo que el error absoluto es menor a 3.2 3 1025.

4.7

Cuadratura gaussiana

173

Cuadratura gaussiana en intervalos arbitrarios
b

Una integral a f (x) d x sobre un intervalo arbitrario [a, b] se puede transformar en una
integral sobre [2@DOXWLOL]DUHOFDPELRGHYDULDEOHV YpDVHOD√ÄJXUD 

t=

Figura 4.17

2x ‚àí a ‚àí b
1
‚áê‚áí x = [(b ‚àí a)t + a + b].
b‚àía
2
t
(b, 1)

1
t5

2x 2 a 2 b
b2a

a

21

b

x

(a, 21)

Esto permite aplicar la cuadratura gaussiana a cualquier intervalo [a, b] porque
b
a

f (x) d x =
3

Ejemplo 2

Considere la integral

1
‚àí1

f

(b ‚àí a)t + (b + a)
2

(b ‚àí a)
dt.
2

(4.41)

x 6 ‚àí x 2 sen(2x) d x = 317.3442466.

1

a)
b)

Compare los resultados de la f√≥rmula cerrada de Newton-Cotes con n 5 1, la
f√≥rmula abierta de Newton-Cotes con n 5 1, y la cuadratura gaussiana cuando n 5 2.
Compare los resultados de la f√≥rmula cerrada de Newton-Cotes con n 5 2, la
f√≥rmula abierta de Newton-Cotes con n 5 2, y la cuadratura gaussiana cuando n 5 3.

Soluci√≥n

a) Cada una de las f√≥rmulas en esta parte requiere dos evaluaciones de la funci√≥n
f (x) = x 6 ‚àí x 2 sen(2x). Las aproximaciones de Newton-Cotes son

Cerrada n = 1 :
Abierta n = 1 :

2
[ f (1) + f (3)] = 731.6054420;
2
3(2/3)
[ f (5/3) + f (7/3)] = 188.7856682.
2

La cuadratura gaussiana aplicada a este problema requiere que la integral primero se transforme en un problema cuyo intervalo de integraci√≥n es [21, 1]. Por medio de la ecuaci√≥n
(4.41) obtenemos
3

x 6 ‚àí x 2 sen(2x) d x =

1

1
‚àí1

(t + 2)6 ‚àí (t + 2)2 sen(2(t + 2)) dt.

Entonces, la cuadratura con n 5 2 da
3

x 6 ‚àí x 2 sen(2x) d x ‚âà f (‚àí0.5773502692 + 2) + f (0.5773502692 + 2)

1

= 306.8199344.

174

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

b) Cada una de las f√≥rmulas en esta parte requiere tres evaluaciones de funci√≥n. Las aproximaciones de Newton-Cotes son

Cerrada n = 2 :
Abierta n = 2 :

1
[ f (1) + 4 f (2) + f (3)] = 333.2380940;
3
4(1/2)
[2 f (1.5) ‚àí f (2) + 2 f (2.5)] = 303.5912023.
3

La cuadratura gaussiana con n 5 3, una vez que la transformaci√≥n se ha realizado, obtenemos
3

x 6 ‚àí x 2 sen(2x) d x ‚âà 0.5 f (‚àí0.7745966692 + 2)

1

+ 0.8 f (2) + 0.5 f (‚àí0.7745966692 + 2) = 317.2641516.
Los resultados de la cuadratura gaussiana son claramente superiores en cada instancia.
La secci√≥n Conjunto de ejercicios 4.7 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

4.8 Integrales m√∫ltiples
/DVWpFQLFDVDQDOL]DGDVHQODVVHFFLRQHVSUHYLDVVHSXHGHQPRGL√ÄFDUSDUDXWLOL]DUVHHQOD
aproximaci√≥n de integrales m√∫ltiples. Considere la integral doble

f (x, y) d A,
R

cuando R = { (x, y) | a ‚â§ x ‚â§ b, c ‚â§ y ‚â§ d }, para algunas constantes a, b, c y d, es
XQDUHJLyQUHFWDQJXODUHQHOSODQR 9pDVHOD√ÄJXUD
Figura 4.18
z
z 5 f (x, y)

a

b

R

c

d
y

x

La siguiente ilustraci√≥n muestra la manera en la que la regla compuesta trapezoidal por
medio de dos subintervalos en cada direcci√≥n coordinada se aplicar√≠a a esta integral.

4.8

Ilustraci√≥n

Integrales m√∫ltiples

175

Al escribir la integral doble como una integral iterada obtenemos

f (x, y) d A =
R

b

d

a

c

f (x, y) dy

d x.

3DUDVLPSOL√ÄFDUODQRWDFLyQVL k = (d ‚àíc)/2 y h = (b‚àía)/2. Aplique la regla compuesta
trapezoidal a la integral interior para obtener
d
c

f (x, y) dy ‚âà

k
2

f (x, c) + f (x, d) + 2 f

x,

c+d
2

.

Esta aproximaci√≥n es de orden O (d ‚àí c)3 . A continuaci√≥n, aplique la regla compuesta trapezoidal nuevamente para aproximar la integral de esta funci√≥n de x:
b

d

a

f (x, y) dy

c

dx ‚âà

b

d ‚àíc
4

a

f (x, c) + 2 f

x,

=

b‚àía
4

d ‚àíc
4

+

b‚àía
4

2

+

b‚àía
4

d ‚àíc
4

=

(b ‚àí a)(d ‚àí c)
f (a, c) + f (a, d) + f (b, c) + f (b, d)
16

+2
+4f

f

f (a, c) + 2 f

d ‚àíc
4

f

c+d
2

a+b
,c + 2 f
2

f (b, c) + 2 f

a+b
,c + f
2

a,

a+b
,d
2

b,

c+d
2

+ f

c+d
2

+ f (x, d) d x

+ f (a, d)
a+b c+d
,
2
2

a+b
,d
2

+ f

+ f (b, d)

a,

c+d
2

+ f

b,

c+d
2

a+b c+d
,
2
2

Esta aproximaci√≥n es de orden O (b ‚àí a)(d ‚àí c) (b ‚àí a)2 + (d ‚àí c)2  /D √ÄJXUD
4.19 muestra una cuadr√≠cula con el n√∫mero de evaluaciones funcionales en cada uno de los
nodos utilizados en la aproximaci√≥n.

Figura 4.19
y
d
1
2 (c 1 d)

c
a

1

2

1

2

4

2

1

2

1

1
2 (a 1 b)

b

x

176

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

Como muestra la ilustraci√≥n, el procedimiento es bastante sencillo. Sin embargo, el
n√∫mero de evaluaciones de funci√≥n crece con el cuadrado del n√∫mero requerido para una
sola integral. En una situaci√≥n pr√°ctica, no esperar√≠amos utilizar un m√©todo tan b√°sico como
la regla compuesta trapezoidal con n 5 2. En cambio, usaremos la regla compuesta de Simpson que es m√°s apropiada para ilustrar la t√©cnica de aproximaci√≥n general, a pesar de que se
podr√≠a usar cualquier otra f√≥rmula en su lugar.
Para aplicar la regla compuesta de Simpson, dividimos la regi√≥n R al subdividir [a, b]
y [c, d@HQXQQ~PHURSDUGHVXELQWHUYDORV3DUDVLPSOL√ÄFDUODQRWDFLyQVHOHFFLRQDPRVHQteros pares n y m y subdividimos [a, b] y [c, d] con puntos de malla igualmente espaciados
x0 , x1 , . . . , xn y y0 , y1 , . . . , ym, respectivamente. Estas subdivisiones determinan tama√±os
de pasos h = (b ‚àí a)/n y k = (d ‚àí c)/m. Al escribir la integral doble como la integral
iterada
b

f (x, y) d A =

d

a

R

c

f (x, y) dy

d x,

primero utilizamos la regla compuesta de Simpson para aproximar
d
c

f (x, y) dy,

tomando x como una constante.
Si y j = c + jk, para cada j = 0, 1, . . . , m. Entonces
d
c

‚é°
‚é§
(m/2)‚àí1
m/2
k‚é£
f (x, y0 ) + 2
f (x, y) dy =
f (x, y2 j ) + 4
f (x, y2 j‚àí1 ) + f (x, ym )‚é¶
3
j=1
j=1
‚àí

(d ‚àí c)k 4 ‚àÇ 4 f (x, Œº)
,
180
‚àÇ y4

para algunas m en (c, d). Por lo tanto,
b
a

d
c

f (x, y) d y d x =

b

k
3

a

(m/2)‚àí1

f (x, y0 ) d x + 2

j=1

‚àí

a

j=1

m/2

+4

b
a

(d ‚àí c)k 4
180

f (x, y2 j‚àí1 ) d x +
b
a

b

b
a

f (x, y2 j ) d x

f (x, ym ) d x

‚àÇ 4 f (x, Œº)
d x.
‚àÇ y4

Ahora, utilizamos la regla compuesta de Simpson para las integrales en esta ecuaci√≥n. Si
xi = a + i h, para cada i = 0, 1, . . . , n. Entonces, para cada j = 0, 1, . . . , m tenemos
b
a

f (x, y j ) d x =

(n/2)‚àí1

n/2

h
3

f (x0 , y j ) + 2

‚àí

(b ‚àí a)h 4 ‚àÇ 4 f
(Œæ j , y j ),
180
‚àÇx4

f (x2i , y j ) + 4
i=1

f (x2i‚àí1 , y j ) + f (xn , y j )
i=1

4.8

Integrales m√∫ltiples

177

para algunos Œæ j en (a, b). La aproximaci√≥n resultante tiene la forma
b
a

d
c

(n/2)‚àí1

hk
f (x, y) d y d x ‚âà
9

f (x0 , y0 ) + 2

f (x2i , y0 )
i=1

n/2

+4

f (x2i‚àí1 , y0 ) + f (xn , y0 )
i=1
(m/2)‚àí1

(m/2)‚àí1 (n/2)‚àí1

+2

f (x0 , y2 j ) + 2

f (x2i , y2 j )

j=1

j=1

(m/2)‚àí1 n/2

+4

i=1

(m/2)‚àí1

f (x2i‚àí1 , y2 j ) +
j=1

f (xn , y2 j )

i=1

j=1

m/2

m/2 (n/2)‚àí1

+4

f (x0 , y2 j‚àí1 ) + 2
j=1
m/2

f (x2i , y2 j‚àí1 )
j=1

n/2

i=1
m/2

+4

f (x2i‚àí1 , y2 j‚àí1 ) +
j=1 i=1

f (xn , y2 j‚àí1 )
j=1

(n/2)‚àí1

+ f (x0 , ym ) + 2

n/2

f (x2i , ym ) + 4
i=1

+ f (xn , ym )

f (x2i‚àí1 , ym )
i=1

.

El t√©rmino de error E est√° dado por

E=

‚àík(b ‚àí a)h 4 ‚àÇ 4 f (Œæ0 , y0 )
+2
540
‚àÇx4

(m/2)‚àí1

‚àÇ 4 f (Œæm , ym )
(d ‚àí c)k 4
‚àí
‚àÇx4
180

b

+

j=1

a

m/2

‚àÇ 4 f (Œæ2 j , y2 j )
‚àÇ 4 f (Œæ2 j‚àí1 , y2 j‚àí1 )
+
4
‚àÇx4
‚àÇx4
j=1

‚àÇ 4 f (x, Œº)
d x.
‚àÇ y4

Si ‚àÇ 4 f /‚àÇ x 4 es continua, el teorema del valor intermedio 1.11 se puede aplicar repetidamente para mostrar que la evaluaci√≥n de las derivadas parciales respecto a x se pueden
reemplazar con un valor com√∫n y que

E=

‚àík(b ‚àí a)h 4
‚àÇ4 f
(d ‚àí c)k 4
3m 4 (Œ∑, Œº) ‚àí
540
‚àÇx
180

b
a

‚àÇ 4 f (x, Œº)
d x,
‚àÇ y4

para algunos (Œ∑, Œº) en R. Si ‚àÇ 4 f /‚àÇ y 4 tambi√©n es continua, el teorema de valor medio para
integrales implica que
b
a

‚àÇ4 f
‚àÇ 4 f (x, Œº)
d
x
=
(b
‚àí
a)
(Œ∑ÃÇ, ŒºÃÇ),
‚àÇ y4
‚àÇ y4

178

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

para algunos (Œ∑ÃÇ, ŒºÃÇ) en R. Puesto que m = (d ‚àí c)/k, el t√©rmino de error tiene la forma

E=

‚àík(b ‚àí a)h 4
‚àÇ4 f
(d ‚àí c)(b ‚àí a) 4 ‚àÇ 4 f
3m 4 (Œ∑, Œº) ‚àí
(Œ∑ÃÇ, ŒºÃÇ),
k
540
‚àÇx
180
‚àÇ y4

ORFXDOVHVLPSOL√ÄFDHQ

E =‚àí

4
(d ‚àí c)(b ‚àí a) 4 ‚àÇ 4 f
4‚àÇ f
h
(Œ∑,
Œº)
+
k
(Œ∑ÃÇ, ŒºÃÇ) ,
180
‚àÇx4
‚àÇ y4

para algunos (Œ∑, Œº) y (Œ∑ÃÇ, ŒºÃÇ) en R.
Ejemplo 1

Utilice la regla compuesta de Simpson con n 5 4 y m 5 2 para aproximar
2.0

1.5

1.4

1.0

ln(x + 2y) d y d x.

Soluci√≥n Los tama√±os de paso para esta aplicaci√≥n son h = (2.0 ‚àí 1.4)/4 = 0.15 y k =

(1.5 ‚àí 1.0)/2 = 0.25. La regi√≥n de integraci√≥n RVHPXHVWUDHQOD√ÄJXUDMXQWRFRQORV
nodos (xi , y j ), donde i = 0, 1, 2, 3, 4 y j = 0, 1, 2.  7DPELpQ PXHVWUD TXH ORV FRH√ÄFLHQWHV
w i, j de f (xi , yi ) = ln(xi +2yi ) en la suma que da la aproximaci√≥n de la regla compuesta de
Simpson para la integral.
Figura 4.20

y
1.50
1.25
1.00

1

4

2

4

1

4

16

8

16

4

1

4

2

4

1

1.40

1.55

1.70

1.85

2.00

x

La aproximaci√≥n es
2.0

1.5

1.4

1.0

4

ln(x + 2y) d y d x ‚âà

2

(0.15)(0.25)
w i, j ln(xi + 2y j )
9
i=0 j=0

= 0.4295524387.
tenemos

‚àÇ4 f
‚àí6
(x, y) =
4
‚àÇx
(x + 2y)4

y

‚àÇ4 f
‚àí96
(x, y) =
,
4
‚àÇy
(x + 2y)4

y los valores m√°ximos de los valores absolutos de estas derivadas parciales se presentan en
R cuando x 5 1.4 y y 5 1.0. Por lo que el error est√° acotado por

|E| ‚â§

6
96
(0.5)(0.6)
(0.15)4 m√°x
+ (0.25)4 m√°x
‚â§ 4.72 √ó 10‚àí6 .
4
(x,y)inR (x + 2y)
(x,y)inR (x + 2y)4
180

4.8

Integrales m√∫ltiples

179

El valor real de la integral para 10 lugares decimales es
2.0

1.5

1.4

1.0

ln(x + 2y) d y d x = 0.4295545265,

por lo que la aproximaci√≥n es precisa dentro de 2.1 3 1026.
Las mismas t√©cnicas se pueden aplicar para la aproximaci√≥n de las integrales triples
as√≠ como integrales superiores para las funciones de m√°s de tres variables. El n√∫mero de
evaluaciones requeridas para la aproximaci√≥n es el producto del n√∫mero de evaluaciones
requeridas cuando se aplica el m√©todo a cada variable.

Cuadratura gaussiana para aproximaci√≥n de integral doble
3DUDUHGXFLUHOQ~PHURGHHYDOXDFLRQHVIXQFLRQDOHVVHSXHGHQLQFOXLUPpWRGRVPiVH√ÄFLHQtes, como la cuadratura gaussiana, la integraci√≥n de Romberg o la cuadratura adaptable, en
lugar de las f√≥rmulas de Newton-Cotes. El siguiente ejemplo ilustra el uso de cuadratura
gaussiana para la integral considerada en el ejemplo 1.
Ejemplo 2

Utilice la cuadratura gaussiana con n 5 3 en ambas dimensiones para aproximar la integral
2.0

1.5

1.4

1.0

ln(x + 2y) d y d x.

Soluci√≥n Antes de emplear la cuadratura gaussiana para aproximar esta integral, necesitamos transformar la regi√≥n de integraci√≥n

R = { (x, y) | 1.4 ‚â§ x ‚â§ 2.0, 1.0 ‚â§ y ‚â§ 1.5 }

en

RÃÇ = { (u, v) | ‚àí1 ‚â§ u ‚â§ 1, ‚àí1 ‚â§ v ‚â§ 1 }.
Las transformaciones lineales que cumplen esto son

1
(2x ‚àí 1.4 ‚àí 2.0)
2.0 ‚àí 1.4

u=

1
(2y ‚àí 1.0 ‚àí 1.5),
1.5 ‚àí 1.0

y v=

o, de manera equivalente x = 0.3u + 1.7 y y = 0.25v + 1.25. Al emplear este cambio de
variables obtenemos una integral en la que se puede aplicar la cuadratura gaussiana:
2.0

1.5

1.4

1.0

ln(x + 2y) d y d x = 0.075

1

1

‚àí1

‚àí1

ln(0.3u + 0.5v + 4.2) dv du.

La f√≥rmula de cuadratura gaussiana para n 5 3 tanto en u como en v requiere que utilicemos
los nodos

u 1 = v1 = r3,2 = 0,

u 0 = v0 = r3,1 = ‚àí0.7745966692,

y

u 2 = v2 = r3,3 = 0.7745966692.
Los pesos asociados con c3,2 = 0.8 y c3,1 = c3,3 = 0.5. (Estos se muestran en la tabla 4.12
en la p√°gina 232.) La aproximaci√≥n resultante es
2.0

1.5

1.4

1.0

3

3

ln(x + 2y) d y d x ‚âà 0.075

c3,i c3, j ln(0.3r3,i + 0.5r3, j + 4.2)
i=1 j=1

= 0.4295545313.

180

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

A pesar de que este resultado s√≥lo requiere nueve evaluaciones funcionales en comparaci√≥n
con 15 para la regla compuesta de Simpson considerada en el ejemplo 1, √©ste es preciso dentro de 4.8√ó10‚àí9, en comparaci√≥n con la precisi√≥n 2.1 √ó 10‚àí6 en el ejemplo 1.

Regiones no rectangulares
El uso de m√©todos de aproximaci√≥n para integrales dobles no est√° limitado a integrales con
regiones rectangulares de integraci√≥n. Las t√©cnicas analizadas previamente se pueden modi√ÄFDUSDUDDSUR[LPDUODVLQWHJUDOHVGREOHVGHODIRUPD
b

d(x)

a

c(x)

d

b(y)

f (x, y) d y dx

(4.42)

f (x, y) d x d y.

(4.43)

o

c

a(y)

De hecho, las integrales en las regiones que no son de este tipo, tambi√©n se pueden aproximar
al realizar subdivisiones adecuadas de la regi√≥n. (Consulte el ejercicio 10.)
Para describir la t√©cnica relacionada con la aproximaci√≥n de una integral de la forma
b
a

d(x)
c(x)

f (x, y) d y d x,

utilizaremos la regla b√°sica de Simpson para integrar respecto a ambas variables. El tama√±o
de paso para la variable x es h = (b ‚àí a)/2, pero el tama√±o de paso para y var√≠a con x (v√©ase
OD√ÄJXUD \VHHVFULEH

k(x) =

d(x) ‚àí c(x)
.
2

Figura 4.21
z
z 5 f (x, y)

y
d(a)
d(b)

A(x)

y 5 d(x)

k(a)

k(b)

c(b)
c(a)

a

y 5 c(x)

k(a 1 h)
a

a1h
a)

b

x

y

b
x

R
y 5 c(x)
b)

y 5 d(x)

4.8

Integrales m√∫ltiples

181

Esto nos da
b
a

d(x)

f (x, y) d y d x ‚âà

c(x)

‚âà

b
a

k(x)
[ f (x, c(x)) + 4 f (x, c(x) + k(x)) + f (x, d(x))] d x
3

h
3

k(a)
[ f (a, c(a)) + 4 f (a, c(a) + k(a)) + f (a, d(a))]
3

4k(a + h)
[ f (a + h, c(a + h)) + 4 f (a + h, c(a + h)
3
+ k(a + h)) + f (a + h, d(a + h))]
+

+

k(b)
[ f (b, c(b)) + 4 f (b, c(b) + k(b)) + f (b, d(b))] .
3

El algoritmo 4.4. aplica la regla compuesta de Simpson para una integral de la forma (4.42).
Las integrales de la forma (4.43) pueden, por supuesto, manejarse de manera similar.
ALGORITMO

4.4

Integral doble de Simpson
Para aproximar la integral

I =
ENTRADA

b
a

d(x)
c(x)

f (x, y) d y d x :

extremos a, b: enteros positivos pares m, n.

SALIDA aproximaci√≥n J para I.
Paso 1 Haga h = (b ‚àí a)/n;
J1 = 0; (T√©rminos finales.)
J2 = 0; (T√©rminos pares.)
J3 = 0. (T√©rminos impares.)
Paso 2 Para i = 0, 1, . . . , n haga los pasos 3‚Äì8.
Paso 3 Haga x = a + i h; (M√©todo compuesto de Simpson para x.)
HX = (d(x) ‚àí c(x))/m;
K 1 = f (x, c(x)) + f (x, d(x)); (T√©rminos finales.)
K 2 = 0; (T√©rminos pares.)
K 3 = 0. (T√©rminos impares.)
Paso 4 Para j = 1, 2, . . . , m ‚àí 1 haga los pasos 5 y 6.
Paso 5 Haga y = c(x) + jHX;
Q = f (x, y).
Paso 6 Si j incluso determine entonces K 2 = K 2 + Q
tambi√©n determine K 3 = K 3 + Q.
Paso 7 Haga L = (K 1 + 2K 2 + 4K 3 )HX/3.
L‚âà

d(xi )
c(xi )

f (xi , y) dy

mediante el m√©todo compuesto de Simpson .

Paso 8 Si i = 0 o i = n entonces detemine J1 = J1 + L
tambi√©n si i incluso determine entonces J2 = J2 + L
tambi√©n determine J3 = J3 + L. (Paso final 2)
Paso 9 Haga J = h(J1 + 2J2 + 4J3 )/3.
Paso 10 SALIDA ( J );
PARE.

182

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

Aplicando la cuadratura gaussiana a la integral doble
b

d(x)

a

c(x)

f (x, y) dy dx

primero debemos transformar, para cada x en [a, b], la variable y en el intervalo [c(x), d(x)]
en la variable t en el intervalo [21, 1]. Esta transformaci√≥n lineal nos da

f (x, y) = f
El c√°lculo reducido hace que, en
general, valga la pena aplicar la
cuadratura gaussiana en lugar
de una t√©cnica de Simpson al
aproximar las integrales dobles.

x,

(d(x) ‚àí c(x))t + d(x) + c(x)
2

y

dy =

d(x) ‚àí c(x)
dt.
2

Entonces, para cada x en [a, b], aplicamos la cuadratura gaussiana a la integral resultante
d(x)

1

(d(x) ‚àí c(x))t + d(x) + c(x)
2

f

x,

d(x) ‚àí c(x)
cn, j f
2
j=1

x,

c(x)

f (x, y) dy =

‚àí1

dt

para producir
b
a

d(x)
c(x)

‚âà

b
a

f (x, y) d y d x
n

(d(x) ‚àí c(x))rn, j + d(x) + c(x)
2

d x,

mientras, como antes, las ra√≠ces rn,j\ORVFRH√ÄFLHQWHVcn,j provienen de la tabla 4.12 en la
p√°gina 172. Ahora, el intervalo [a, b] se transforma en [21, 1], y la cuadratura gaussiana se
aplica para aproximar la integral en el lado derecho de esta ecuaci√≥n. Los detalles se incluyen en el algoritmo 4.5.

ALGORITMO

4.5

Integral doble gaussiana
Para aproximar la integral
b
a

d(x)
c(x)

f (x, y) d y d x :

ENTRADA extremos a, b: enteros positivos m, n.
(Las ra√≠ces ri, j y los coeficientes c i, j necesitan estar disponibles para i = m√°x{m, n}
y para 1 ‚â§ j ‚â§ i.)
SALIDA

aproximaci√≥n J para I.

Paso 1 Haga h 1 = (b ‚àí a)/2;
h 2 = (b + a)/2;
J = 0.
Paso 2 Para i = 1, 2, . . . , m haga los pasos 3‚Äì5.
Paso 3 Haga J X = 0;
x = h 1rm,i + h 2 ;
d1 = d(x);
c1 = c(x);
k1 = (d1 ‚àí c1 )/2;
k2 = (d1 + c1 )/2.
Paso 4 Para j = 1, 2, . . . , n haga
y = k1rn, j + k2 ;
Q = f (x, y);
J X = J X + cn, j Q.

4.8

Paso 5 Haga J = J + cm,i k1 J X .

Integrales m√∫ltiples

183

(Paso final 2.)

Paso 6 Haga J = h 1 J .
Paso 7 SALIDA (J );
PARE.

Ilustraci√≥n

(OYROXPHQGHOVyOLGRHQOD√ÄJXUDVHDSUR[LPDDSOLFDQGRHODOJRULWPRGHODLQWHJUDO
doble de Simpson con n 5 m 5 10 a
0.5

x2

0.1

x3

e y/x d y d x.

Esto requiere 121 evaluaciones de la funci√≥n f (x, y) = e y/x y produce el valor
HOFXDODSUR[LPDHOYROXPHQGHOVyOLGRPRVWUDGRHQOD√ÄJXUDDDSUR[LPDGDmente siete lugares decimales. Al aplicar el algoritmo de cuadratura gaussiana con n 5 m 5 5
s√≥lo se requieren 25 evaluaciones de funci√≥n y da la aproximaci√≥n 0.03330556611, lo cual
es preciso para 11 lugares decimales.
Figura 4.22
z

(0.1, 0.01, e0.1)

1

(0.5, 0.25, e0.5)

(0.1, 0.001, e0.01)
y
(0.5, 0.125, e0.25)

0.25
0.125

0.1

R

(0.5, 0.25, 0)
(0.5, 0.125, 0)

0.5
x

Aproximaci√≥n de integral triple
Las integrales triples de la forma
El c√°lculo reducido casi siempre
hace que valga la pena aplicar
la cuadratura gaussiana en lugar
de la t√©cnica de Simpson al
aproximar integrales triples o
superiores.

b
a

d(x)

Œ≤(x,y)

c(x)

Œ±(x,y)

f (x, y, z) dz dy dx

9pDVHOD√ÄJXUD VHDSUR[LPDQGHPDQHUDVLPLODU'HELGRDOQ~PHURGHFiOFXORVLPSOLcados, la cuadratura gaussiana es el m√©todo de elecci√≥n. El algoritmo 4.6 implementa este
procedimiento.

184

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

Figura 4.23
z

z 5 b(x, y)

z 5 a(x, y)
a

x

y

y 5 c(x)

b

R

y 5 d(x)

x

ALGORITMO

4.6

Integral triple gaussiana
Para aproximar la integral
b
a

d(x)

Œ≤(x,y)

c(x)

Œ±(x,y)

f (x, y, z) dz dy d x :

ENTRADA extremos a, b; enteros positivos m, n, p.
(Las ra√≠ces ri, j y los coeficientes ci, j necesitan estar disponibles para i = m√°x{n, m, p}
y para 1 ‚â§ j ‚â§ i.)
SALIDA

aproximaci√≥n J para I.

Paso 1 Haga h 1 = (b ‚àí a)/2;
h 2 = (b + a)/2;
J = 0.
Paso 2 Para i = 1, 2, . . . , m haga los pasos 3‚Äì8.
Paso 3 Haga JX = 0;
x = h 1rm,i + h 2 ;
d1 = d(x);
c1 = c(x);
k1 = (d1 ‚àí c1 )/2;
k2 = (d1 + c1 )/2.
Paso 4 Para j = 1, 2, . . . , n haga los pasos 5‚Äì7.
Paso 5 Haga JY = 0;
y = k1rn, j + k2 ;
Œ≤1 = Œ≤(x, y);
Œ±1 = Œ±(x, y);
l1 = (Œ≤1 ‚àí Œ±1 )/2;
l2 = (Œ≤1 + Œ±1 )/2.

4.8

Integrales m√∫ltiples

185

Paso 6 Para k = 1, 2, . . . , p haga
Haga z = l1r p,k + l2 ;
Q = f (x, y, z);
JY = JY + c p,k Q.
Paso 7 Haga JX = JX + cn, j l1 JY.
Paso 8 Haga J = J + cm,i k1 JX.

(Fin del paso 4)

(Fin del paso 2)

Paso 9 Haga J = h 1 J .
Paso 10 SALIDA (J );
PARE.
El siguiente ejemplo requiere la evaluaci√≥n de cuatro integrales triples.
Ilustraci√≥n

El centro de masa de una regi√≥n s√≥lida DFRQIXQFLyQGHGHQVLGDG∆±VHSUHVHQWDHQ

M yz Mx z Mx y
,
,
M
M
M

(x, y, z) =

,

donde

M yz =

D

xœÉ (x, y, z) d V,

Mx z =

D

yœÉ (x, y, z) d V

y

Mx y =

D

zœÉ (x, y, z) d V

son los momentos alrededor de los planos coordenados y la masa de D es

M=

D

œÉ (x, y, z) d V.

(OVyOLGRPRVWUDGRHQOD√ÄJXUDHVWiDFRWDGRSRUODSDUWHVXSHULRUGHOFRQRz 2 = x 2 + y 2
y el plano z 5 2. Suponga que este s√≥lido tiene una funci√≥n de densidad dada por

œÉ (x, y, z) =

x 2 + y2.

Figura 4.24
z

2
1

x

2

1

1
2
y

186

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

Al aplicar el algoritmo de integral triple gaussiana 4.6 con n 5 m 5 p 5 5 requiere 125 evaluaciones de funci√≥n por integral y se obtienen las siguientes aproximaciones
‚àö

M=

Mx y =

‚àö

4‚àíx 2

2
‚àí2
2
‚àí2
2
‚àí2

2

‚àö

‚àö

4‚àíx 2

2
0

Mx z =

‚àí

‚àí2

=4
M yz =

4‚àíx 2

2

‚àö

4‚àíx 2

‚àí

‚àö

‚àö

4‚àíx 2

4‚àíx 2

‚àí

‚àö

‚àö

4‚àíx 2

4‚àíx 2

‚àí

2

‚àö

0

‚àö

4‚àíx 2

x 2 + y 2 dz dy d x

x 2 +y 2

x 2 +y 2

2

‚àö

x 2 +y 2

2

‚àö

x 2 +y 2

2

‚àö

x 2 +y 2

x 2 + y 2 dz dy d x ‚âà 8.37504476,

x

x 2 + y 2 dz dy d x ‚âà ‚àí5.55111512 √ó 10‚àí17 ,

y

x 2 + y 2 dz dy d x ‚âà ‚àí8.01513675 √ó 10‚àí17 y

z

x 2 + y 2 dz dy d x ‚âà 13.40038156.

Esto implica que la ubicaci√≥n aproximada del centro de la masa es

(x, y, z) = (0, 0, 1.60003701).
Estas integrales son bastante f√°ciles de evaluar de manera directa. Si lo hace, descubrir√° que
el centro de masa exacto se presenta en (0, 0, 1.6).
La secci√≥n Conjunto de ejercicios 4.8 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

4.9 Integrales impropias
Las integrales impropias resultan cuando la noci√≥n de integraci√≥n se ampl√≠a, ya sea en un
intervalo de integraci√≥n en el que la funci√≥n no est√° acotada en un intervalo con uno o m√°s
H[WUHPRVLQ√ÄQLWRV(QFXDOTXLHUFLUFXQVWDQFLDODVUHJODVQRUPDOHVGHXQDDSUR[LPDFLyQLQWHJUDOVHGHEHQPRGL√ÄFDU

Singularidad del extremo izquierdo
Primero consideraremos la situaci√≥n en la que el integrando no est√° limitado en el extremo
L]TXLHUGRGHOLQWHUYDORGHLQWHJUDFLyQFRPRVHPXHVWUDHQOD√ÄJXUD(QHVWHFDVRGHcimos que f tiene una singularidad en el extremo a. A continuaci√≥n, mostramos c√≥mo se
pueden reducir las integrales impropias para problemas de este tipo.
Figura 4.25

y

y 5 f(x)

a

b

x

4.9

Integrales impropias

187

En c√°lculo se muestra que la integral impropia con una singularidad en el extremo izquierdo,
b
dx
,
p
a (x ‚àí a)
converge si y s√≥lo si 0 < p < 1, \HQHVWHFDVRGH√ÄQLPRV
b
a

x=b

(b ‚àí a)1‚àí p
1
(x ‚àí a)1‚àí p
d
x
=
l√≠m
=
.
(x ‚àí a) p
1‚àí p
1‚àí p
M‚Üía +
x=M
1

Ejemplo 1

1
‚àö d x converge pero que
x
0
Soluci√≥n Para la primera integral, tenemos

1

Muestre que la integral impropia

1
0

1
‚àö d x = l√≠m
x
M‚Üí0+

1
M

0

1
d x diverge.
x2

x=1

x ‚àí1/2 d x = l√≠m 2x 1/2 x=M = 2 ‚àí 0 = 2,
M‚Üí0+

pero la segunda integral
1
0

1
d x = l√≠m
x2
M‚Üí0+

1
M

x=1

x ‚àí2 d x = l√≠m ‚àíx ‚àí1 x=M
M‚Üí0+

HVLQ√ÄQLWD
Si f es una funci√≥n que se puede escribir en la forma

f (x) =

g(x)
,
(x ‚àí a) p

donde 0 < p < 1 y g es continua en [a, b], entonces la integral impropia
b
a

f (x) d x

tambi√©n existe. Nosotros aproximaremos esta integral mediante la regla compuesta de Simpson, siempre que g ‚àà C 5 [a, b].  (Q HVH FDVR FRQVWUXLPRV HO FXDUWR SROLQRPLR GH 7D\ORU
P4(x), para g alrededor de a,

P4 (x) = g(a) + g (a)(x ‚àí a) +

g (a)
g (a)
g (4) (a)
(x ‚àí a)2 +
(x ‚àí a)3 +
(x ‚àí a)4 ,
2!
3!
4!

y escribimos
b
a

f (x) d x =

b
a

b

g(x) ‚àí P4 (x)
dx +
(x ‚àí a) p

a

P4 (x)
d x.
(x ‚àí a) p

(4.44)

Puesto que P(x) es un polinomio, podemos determinar con exactitud el valor de
b
a

b

4

P4 (x)
dx =
(x ‚àí a) p
k=0

a

g (k) (a)
g (k) (a)
(x ‚àí a)k‚àí p d x =
(b ‚àí a)k+1‚àí p .
k!
k!(k
+
1
‚àí
p)
k=0
4

(4.45)

188

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

Por lo general, la parte dominante de la aproximaci√≥n, en especial cuando el polinomio de
7D\ORUP4(x) concuerda de cerca con g(x) en todo el intervalo [a, b].
Para aproximar la integral de f, debemos a√±adir este valor a la aproximaci√≥n de
b
a

g(x) ‚àí P4 (x)
d x.
(x ‚àí a) p

3DUDGHWHUPLQDUHVWRSULPHURGH√ÄQLPRV

G(x) =

g(x)‚àíP4 (x)
,
(x‚àía) p

si a < x ‚â§ b,
si x = a.

0,

(k)
Esto nos da una funci√≥n continua en [a, b]. De hecho, 0 < p < 1 y P4 (a) concuerda con
g (k) (a) para cada k = 0, 1, 2, 3, 4, por lo que tenemos G ‚àà C 4 [a, b]. Esto implica que la
regla compuesta de Simpson se puede aplicar para aproximar la integral de G sobre [a, b].
Al a√±adir esta aproximaci√≥n al valor de la ecuaci√≥n (4.45) obtenemos una aproximaci√≥n
para la integral impropia de f en [a, b], dentro de la precisi√≥n de la aproximaci√≥n de la regla
compuesta de Simpson.

Ejemplo 2

Utilice la regla compuesta de Simpson con h 5 0.25 para aproximar el valor de la integral
impropia
1
0

ex
‚àö d x.
x

Soluci√≥n (OFXDUWRSROLQRPLRGH7D\ORUSDUDex alrededor de x 5 0 es

P4 (x) = 1 + x +

x3
x4
x2
+
+ ,
2
6
24
1

por lo que la parte dominante de la aproximaci√≥n para
0
1
0

P4 (x)
‚àö dx =
x

1

1
1
1
x ‚àí1/2 + x 1/2 + x 3/2 + x 5/2 + x 7/2
2
6
24

0

= l√≠m

M‚Üí0+

=2+

1
1
2 1
+ +
+
‚âà 2.9235450.
3 5 21 108
1

0

0

G(x)

0.00
0.25
0.50
0.75
1.00

0
0.0000170
0.0004013
0.0026026
0.0099485

ex
‚àö d x , necesitamos aproximar
x

G(x) d x , donde
‚éß
‚é® ‚àö1 e x ‚àí P (x) , si 0 < x ‚â§ 1,
4
x
G(x) =
‚é©
0,
si x = 0.

Tabla 4.13
x

dx

2
1
1
1 9/2 1
x
2x 1/2 + x 3/2 + x 5/2 + x 7/2 +
3
5
21
108
M

Para la segunda parte de la aproximaci√≥n para
1

ex
‚àö d x es
x

La tabla 4.13 enumera los valores necesarios para la regla compuesta de Simpson para esta
aproximaci√≥n.
Por medio de estos datos y la regla compuesta de Simpson obtenemos
1
0

G(x) d x ‚âà

0.25
[0 + 4(0.0000170) + 2(0.0004013) + 4(0.0026026) + 0.0099485]
3

= 0.0017691.

4.9

Integrales impropias

189

Por lo tanto,
1
0

ex
‚àö d x ‚âà 2.9235450 + 0.0017691 = 2.9253141.
x

Este resultado es preciso dentro de la precisi√≥n de la aproximaci√≥n de la regla compuesta
de Simpson para la funci√≥n G. Puesto que |G (4) (x)| < 1 en [0, 1], el error est√° acotado por

1‚àí0
(0.25)4 = 0.0000217.
180

Singularidad en el extremo derecho
Para aproximar la integral impropia con una singularidad en el extremo derecho, podemos
desarrollar una t√©cnica similar, pero expandiendo los t√©rminos del extremo derecho b en lugar del extremo izquierdo a. De forma alternativa, podemos realizar la sustituci√≥n

z = ‚àíx,

dz = ‚àí d x

para cambiar la integral impropia en una de la forma
b
a

f (x) d x =

‚àía
‚àíb

f (‚àíz) dz,

(4.46)

la cual tiene su singularidad en el extremo izquierdo. Entonces, podemos aplicar la t√©cnica
GHVLQJXODULGDGGHOH[WUHPRL]TXLHUGRTXH\DKDEtDPRVGHVDUUROODGR 9pDVHOD√ÄJXUD
Figura 4.26
y

y

Para z 5 2x

y 5 f (2z)

y 5 f (x)

a

x

b

2b

2a

z

Una integral impropia con una singularidad en c, donde a < c < b, se trata como la
suma de las integrales impropias con singularidades en los extremos ya que
b
a

f (x) d x =

c
a

f (x) d x +

b
c

f (x) d x.

Singularidad inÔ¨Ånita
(ORWURWLSRGHLQWHJUDOLPSURSLDLPSOLFDOtPLWHVLQ√ÄQLWRVGHLQWHJUDFLyQ/DLQWHJUDOEiVLFD
de este tipo tiene la forma
‚àû
a

1
d x,
xp

190

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

para p > 1. Esto se convierte en una integral con singularidad de extremo izquierdo en 0 al
realizar la sustituci√≥n de integraci√≥n

t = x ‚àí1 ,

dt = ‚àíx ‚àí2 d x,

por lo que d x = ‚àíx 2 dt = ‚àít ‚àí2 dt.

Entonces
‚àû
a

tp
‚àí 2 dt =
t
1/a
0

1
dx =
xp

1/a
0

1
dt.
t 2‚àí p

De forma similar, el cambio de variable t = x ‚àí1 convierte la integral
‚àû
a f (x) d x en una que tiene singularidad de extremo izquierdo en cero:
‚àû
a

1/a

f (x) d x =

t ‚àí2 f

0

1
t

dt.

impropia

(4.47)

Ahora, se puede aproximar por medio de la f√≥rmula de cuadratura del tipo descrito anteriormente.
Ejemplo 3

Aproxime el valor de la integral impropia
‚àû

I =

x ‚àí3/2 sen

1

1
d x.
x

Soluci√≥n Primero realizamos el cambio de variable t = x ‚àí1, el cual convierte la singulari-

GDGLQ√ÄQLWDHQXQDFRQXQDVLQJXODULGDGGHH[WUHPRL]TXLHUGR(QWRQFHV

dt = ‚àíx ‚àí2 d x,

1
por lo que d x = ‚àíx 2 dt = ‚àí 2 dt,
t

y

I =

x=‚àû
x=1

x ‚àí3/2 sen

1
dx =
x

t=0

1
t

t=1

‚àí3/2

sen t

1
‚àí 2 dt
t

1

=
0

(OFXDUWRSROLQRPLRGH7D\ORUP4(t), para sen t alrededor de 0 es

1
P4 (t) = t ‚àí t 3 ,
6
por lo tanto,

‚éß
1 3
‚é™
‚é® sen t ‚àí t + 6 t ,
G(t) =
t 1/2
‚é™
‚é©0,

si 0 < t ‚â§ 1
si t = 0

est√° en C 4 [0, 1], y tenemos

I =

1

1
t ‚àí1/2 t ‚àí t 3
6
0

dt +
0

1

=

2 3/2
1
t ‚àí t 7/2 +
3
21
0

= 0.61904761 +
0

0

1 sen t ‚àí t + 1 t 3
6
dt
t 1/2

1 sen t ‚àí t + 1 t 3
6
dt
t 1/2

1 sen t ‚àí t + 1 t 3
6
dt.
t 1/2

t ‚àí1/2 sen t dt.

4.10 Software num√©rico y revisi√≥n del cap√≠tulo

191

El resultado a partir de la regla compuesta de Simpson con n 5 16 para la integral restante
HV(VWRGDXQDDSUR[LPDFLyQ√ÄQDOGH

I = 0.0014890097 + 0.61904761 = 0.62053661,
lo cual es preciso dentro de 4.0 √ó 10‚àí8 .
La secci√≥n Conjunto de ejercicios 4.9 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

4.10 Software num√©rico y revisi√≥n del cap√≠tulo
La mayor√≠a del software para integrar una funci√≥n de una √∫nica variable real est√° basado
ya sea en el enfoque adaptable o en f√≥rmulas gaussianas extremadamente precisas. La inWHJUDFLyQFDXWHORVDGH5RPEHUJHVXQDWpFQLFDDGDSWDEOHTXHLQFOX\HXQDYHUL√ÄFDFLyQSDUD
garantizar que el integrando se comporta suavemente sobre los subintervalos de la integral
de integraci√≥n. Este m√©todo se ha utilizado con √©xito en bibliotecas de software. En general,
m√∫ltiples integrales se aproximan con la ampliaci√≥n de buenos m√©todos adaptables hasta
dimensiones superiores. La cuadratura del tipo gaussiano tambi√©n se recomienda para disminuir el n√∫mero de evaluaciones de funci√≥n.
Las rutinas principales en las bibliotecas IMSL y NAG est√°n basadas en QUADPACK:
A Subroutine Package for Automatic Integration (QUADPACK: Un paquete de subrutinas
para integraci√≥n autom√°tica) de R. Piessens, E. de Doncker-Kapenga, C. W. Uberhuber y D.
K. Kahaner y publicado por Springer-Verlag en 1983 [PDUK].
La biblioteca IMSL contiene un esquema de integraci√≥n adaptable con base en la regla
Gaussiana-Kronrod de 21 puntos mediante la regla gaussiana de 10 puntos para c√°lculo
de error. Las reglas gaussianas utilizan 10 puntos x1 , . . . , x10 y pesos w 1 , . . . , w 10 para la
b
10
f√≥rmula de cuadratura i=1 w i f (xi ) para aproximar a f (x) d x . A continuaci√≥n se utilizan
los puntos adicionales x11 , . . . , x21, y los pesos nuevos v1 , . . . , v21, en la f√≥rmula de Kron21
rod i=1 vi f (xi ). Los resultados de las dos f√≥rmulas se comparan para eliminar el error.
La ventaja de utilizar x1 , . . . , x10 en cada f√≥rmula es que f s√≥lo necesita evaluarse en 21
puntos. Si se utilizaran reglas gaussianas independientes de 10 y 21 puntos, se necesitar√≠an
31 evaluaciones de funci√≥n. Este procedimiento permite singularidades de extremo en el
integrando.
2WUDVVXEUXWLQDV,06/SHUPLWHQVLQJXODULGDGHVGHH[WUHPRVLQJXODULGDGHVHVSHFL√ÄFDGDV
SRUHOXVXDULRHLQWHUYDORVLQ√ÄQLWRVGHLQWHJUDFLyQ$GHPiVH[LVWHQUXWLQDVSDUDDSOLFDUUHJODVGH
Gauss-Kronrod para integrar una funci√≥n de dos variables y una rutina para utilizar cuadratura
para integrar una funci√≥n de n variables sobre n intervalos de la forma [ai, bi].
La Biblioteca NAG incluye una rutina para calcular la integral de f sobre el intervalo
[a, b] mediante un m√©todo adaptable con base en cuadratura gaussiana mediante reglas de
.URQURGGHSXQWRV\GH*DXVVGHSXQWRV7DPELpQWLHQHXQDUXWLQDSDUDDSUR[LPDU
una integral mediante una familia de f√≥rmulas tipo gaussianas con base en 1, 3, 5, 7, 15, 31,
63, 127 y 255 nodos. Estas reglas entrelazadas de alta precisi√≥n se deben a Patterson [Pat]
y se utilizan de manera adaptable. NAG incluye muchas otras subrutinas para aproximar
integrales.
A pesar de que la diferenciaci√≥n num√©rica es inestable, se necesitan f√≥rmulas de aproximaci√≥n de derivadas para resolver ecuaciones diferenciales. La Biblioteca NAG incluye una
subrutina para la diferenciaci√≥n num√©rica de una funci√≥n de una variable real con diferenciaci√≥n para que la catorceava derivada sea posible. IMSL tiene una funci√≥n que usa un cambio
DGDSWDEOHHQWDPDxRGHSDVRSDUDGLIHUHQFLDV√ÄQLWDVSDUDDSUR[LPDUODSULPHUDVHJXQGDR
tercera derivada de f en x dentro de una tolerancia determinada. IMSL tambi√©n incluye una
VXEUXWLQDSDUDFDOFXODUODVGHULYDGDVGHXQDIXQFLyQGH√ÄQLGDHQXQFRQMXQWRGHSXQWRVPH-

192

CAP√çTULO 4

Diferenciaci√≥n num√©rica e integraci√≥n

diante interpolaci√≥n cuadr√°tica. Ambos paquetes permiten la diferenciaci√≥n e integraci√≥n de
splines c√∫bicos interpolantes construidos por las subrutinas mencionadas en la secci√≥n 3.5.
Las secciones Preguntas de an√°lisis, Conceptos clave y Revisi√≥n del cap√≠tulo est√°n disponibles en l√≠nea. Encuentre la ruta de acceso en las p√°ginas preliminares.

CAP√çTULO

5

Problemas de valor inicial para ecuaciones de
diferenciales ordinarias
Introducci√≥n
El movimiento de un p√©ndulo balance√°ndose de acuerdo con ciertas suposiciones de simpli√ÄFDFLyQVHGHVFULEHSRUPHGLRGHODHFXDFLyQGLIHUHQFLDOGHVHJXQGRRUGHQ

d 2Œ∏
g
+ sen Œ∏ = 0,
dt 2
L

L
u

donde L HV OD ORQJLWXG GHO SpQGXOR g ‚âà 32.17 pies/s2 HV OD FRQVWDQWH JUDYLWDFLRQDO GH OD
7LHUUD\uHVHOiQJXORGHOSpQGXORFRQODYHUWLFDO6LDGHPiVHVSHFL√ÄFDPRVODSRVLFLyQGHO
SpQGXORFXDQGRHOPRYLPLHQWRHPSLH]DŒ∏(t0 ) = Œ∏0\VXYHORFLGDGHQHVHSXQWRŒ∏ (t0 ) = Œ∏0
tenemos lo que recibe el nombre de problema de valor inicial.
Para los valores peque√±os de uODDSUR[LPDFLyQŒ∏ ‚âà sen Œ∏ se puede utilizar para simpli√ÄFDUHOSUREOHPDGHYDORULQLFLDOOLQHDO

d 2Œ∏
g
+ Œ∏ = 0,
2
dt
L

Œ∏ (t0 ) = Œ∏0 ,

Œ∏ (t0 ) = Œ∏0 .

(VWHSUREOHPDVHSXHGHUHVROYHUFRQXQDWpFQLFDGHHFXDFLyQGLIHUHQFLDOHVWiQGDU3DUDORV
YDORUHVPiVJUDQGHVGHuODVXSRVLFLyQGHTXHŒ∏ = sen Œ∏QRHVUD]RQDEOHSRUORTXHGHEHQ
XVDUVHORVPpWRGRVGHDSUR[LPDFLyQ8QSUREOHPDGHHVWHWLSRVHFRQVLGHUDHQHOHMHUFLFLR
GHODVHFFLyQ
&XDOTXLHUOLEURGHWH[WRVREUHHFXDFLRQHVGLIHUHQFLDOHVGHWDOODGLIHUHQWHVPpWRGRVSDUD
HQFRQWUDUVROXFLRQHVDORVSUREOHPDVGHYDORULQLFLDOGHSULPHURUGHQGHPDQHUDH[SOtFLWD
6LQHPEDUJRHQODSUiFWLFDSRFRVSUREOHPDVTXHVHRULJLQDQDSDUWLUGHOHVWXGLRGHIHQyPHQRVItVLFRVVHSXHGHQUHVROYHUFRQH[DFWLWXG
193

196

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

HQ VX VHJXQGD YDULDEOH \ OD FRQGLFLyQ   HV JHQHUDOPHQWH PiV IiFLO GH DSOLFDU TXH OD
GH√ÄQLFLyQ 6LQ HPEDUJR GHEHUtDPRV REVHUYDU TXH HO WHRUHPD  VRODPHQWH SURSRUFLRQD
FRQGLFLRQHVVX√ÄFLHQWHVSDUDPDQWHQHUODFRQGLFLyQGH/LSVFKLW]/DIXQFLyQHQHOHMHPSOR
SRU HMHPSOR VDWLVIDFH OD FRQGLFLyQ GH /LSVFKLW] SHUR OD GHULYDGD SDUFLDO UHVSHFWR Dy no
existe cuando y 5
(OVLJXLHQWHWHRUHPDHVXQDYHUVLyQGHOWHRUHPDIXQGDPHQWDOGHH[LVWHQFLD\XQLFLGDG
SDUDHFXDFLRQHVGLIHUHQFLDOHVRUGLQDULDVGHSULPHURUGHQ$SHVDUGHTXHHOWHRUHPDVHSXHGHSUREDUUHGXFLHQGRODKLSyWHVLVGHDOJXQDIRUPDHVWDYHUVLyQHVVX√ÄFLHQWHSDUDQXHVWURV
SURSyVLWRV /DSUXHEDGHOWHRUHPDHQXQFLDGRGHHVWDIRUPDVHSXHGHHQFRQWUDUHQ>%L5@
SS¬≤
Teorema 5.4

6XSRQJDTXHD = { (t, y) | a ‚â§ t ‚â§ b y ‚àí ‚àû < y < ‚àû } y que f (t, y) es continua en D.6Lf
VDWLVIDFHODFRQGLFLyQGH/LSVFKLW]HQD en la variable y, entonces el problema de valor inicial

y (t) = f (t, y),

a ‚â§ t ‚â§ b,

y(a) = Œ±,

tiene una √∫nica VROXFLyQy(t) para a ‚â§ t ‚â§ b.
Ejemplo 2

8WLOLFHHOWHRUHPDSDUDPRVWUDUTXHH[LVWHXQD√∫nica VROXFLyQSDUDHOSUREOHPDGHYDORU
inicial

y = 1 + t sen(t y),

0 ‚â§ t ‚â§ 2,

y(0) = 0.

Soluci√≥n Al mantener tFRQVWDQWH\DSOLFDUHOWHRUHPDGHYDORUPHGLRSDUDODIXQFLyQ

f (t, y) = 1 + t sen(t y),
encontramos que cuando y < yH[LVWHXQQ~PHURj en (yy) con

‚àÇ
f (t, y2 ) ‚àí f (t, y1 )
=
f (t, Œæ ) = t 2 cos(Œæ t).
y2 ‚àí y1
‚àÇy
3RUORWDQWR

| f (t, y2 ) ‚àí f (t, y1 )| = |y2 ‚àí y1 ||t 2 cos(Œæ t)| ‚â§ 4|y2 ‚àí y1 |,
\fVDWLVIDFHODFRQGLFLyQGH/LSVFKLW]HQODYDULDEOHy FRQFRQVWDQWHGH/LSVFKLW]L 5
$GHPiVf (ty) es continua cuando 0 ‚â§ t ‚â§ 2 y ‚àí‚àû < y < ‚àûSRUORTXHHOWHRUHPD
implica que existe una √∫nica VROXFLyQSDUDHVWHSUREOHPDGHYDORULQLFLDO
6LXVWHGKDFRPSOHWDGRXQFXUVRVREUHHFXDFLRQHVGLIHUHQFLDOHVSRGUtDLQWHQWDUHQFRQWUDUODVROXFLyQH[DFWDSDUDHVWHSUREOHPD

Problemas bien planteados
$KRUDTXHKHPRVKDVWDFLHUWRSXQWRDWHQGLGRODFXHVWLyQGHFXDQGRORVSUREOHPDVGHYDORU
LQLFLDO WLHQHQ VROXFLRQHV ~QLFDV SRGHPRV PRYHU OD VHJXQGD FRQVLGHUDFLyQ LPSRUWDQWH DO
DSUR[LPDUODVROXFLyQSDUDXQSUREOHPDGHYDORULQLFLDO(QJHQHUDOORVSUREOHPDVGHYDORU
LQLFLDOREWHQLGRVDOREVHUYDUHOIHQyPHQRItVLFRVRODPHQWHDSUR[LPDQODYHUGDGHUDVLWXDFLyQ
SRUORTXHQHFHVLWDPRVVDEHUVLORVSHTXHxRVFDPELRVHQODGHFODUDFLyQGHOSUREOHPDLQWURGXFHQSHTXHxRVFDPELRVHQODVROXFLyQHQODPLVPDPHGLGD(VWRWDPELpQHVLPSRUWDQWH
GHELGRDODLQWURGXFFLyQGHOHUURUGHUHGRQGHRFXDQGRVHXVDQPpWRGRVQXPpULFRV(VWRHV
¬á 3UHJXQWD¬¢&yPRGHWHUPLQDPRVVLXQSUREOHPDSDUWLFXODUWLHQHODSURSLHGDGGHTXHSHTXHxRVFDPELRVRDOWHUDFLRQHVGHOSUREOHPDLQWURGX]FDQSHTXHxRVFDPELRVHQODVROXFLyQ
en la misma medida?
&RPRVLHPSUHSULPHURQHFHVLWDPRVSURSRUFLRQDUXQDGH√ÄQLFLyQSUiFWLFDSDUDH[SUHVDUHVWH
FRQFHSWR

194

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

/D SULPHUD SDUWH GH HVWH FDStWXOR VH SUHRFXSD SRU DSUR[LPDU OD VROXFLyQy(t) para un
SUREOHPDGHODIRUPD

dy
= f (t, y),
dt

para a ‚â§ t ‚â§ b,

VXMHWRDODFRQGLFLyQLQLFLDO y(a) = Œ±. M√°s adelante en este cap√≠tulo tratamos con la extenVLyQGHHVWRVPpWRGRVKDFLDXQVLVWHPDGHHFXDFLRQHVGLIHUHQFLDOHVGHSULPHURUGHQGHOD
IRUPD

dy1
= f 1 (t, y1 , y2 , . . . , yn ),
dt
dy2
= f 2 (t, y1 , y2 , . . . , yn ),
dt
..
.
dyn
= f n (t, y1 , y2 , . . . , yn ),
dt
para a ‚â§ t ‚â§ bVXMHWRDODVFRQGLFLRQHVLQLFLDOHV

y1 (a) = Œ±1 ,

y2 (a) = Œ±2 ,

... ,

yn (a) = Œ±n .

7DPELpQH[DPLQDPRVODUHODFLyQGHXQVLVWHPDGHHVWHWLSRFRQHOSUREOHPDGHYDORULQLFLDO
GHHQpVLPRRUGHQGHODIRUPD

y (n) = f (t, y, y , y , . . . , y (n‚àí1) ),
para a ‚â§ t ‚â§ bVXMHWRDODVFRQGLFLRQHVLQLFLDOHV

y(a) = Œ±1 ,

y (a) = Œ±2 ,

... ,

y n‚àí1 (a) = Œ±n .

5.1 Teor√≠a elemental de problemas de valor inicial
/DVHFXDFLRQHVGLIHUHQFLDOHVVHXVDQSDUDPRGHODUSUREOHPDVHQODFLHQFLD\ODLQJHQLHUtD
TXHLPSOLFDQHOFDPELRGHDOJXQDYDULDEOHUHVSHFWRDRWUD0XFKRVGHHVWRVSUREOHPDVUHTXLHUHQ OD VROXFLyQ GH XQ problema de valor inicial HV GHFLU OD VROXFLyQ D XQD HFXDFLyQ
GLIHUHQFLDOTXHVDWLVIDFHXQDFRQGLFLyQLQLFLDOGHWHUPLQDGD
(QVLWXDFLRQHVGHODYLGDUHDOFRP~QODHFXDFLyQGLIHUHQFLDOTXHPRGHODHOSUREOHPDHV
GHPDVLDGRFRPSOLFDGDSDUDUHVROYHUVHGHPDQHUDH[DFWD\VHWRPDXQRGHGRVHQIRTXHVSDUD
DSUR[LPDUODVROXFLyQ(OSULPHUHQIRTXHHVPRGL√ÄFDUHOSUREOHPDDOVLPSOL√ÄFDUODHFXDFLyQ
GLIHUHQFLDO SRU XQD TXH VH SXHGD UHVROYHU GH PDQHUD H[DFWD \ D FRQWLQXDFLyQ XWLOL]DU OD
VROXFLyQGHODHFXDFLyQVLPSOL√ÄFDGDSDUDDSUR[LPDUODVROXFLyQSDUDHOSUREOHPDRULJLQDO
(ORWURHQIRTXHTXHVHH[DPLQDUiHQHVWHFDStWXORXVDPpWRGRVSDUDDSUR[LPDUODVROXFLyQ
del SUREOHPDRULJLQDO(VWHHVHOHQIRTXHTXHVHWRPDFRQPiVIUHFXHQFLDGHELGRDTXHORV
PpWRGRVGHDSUR[LPDFLyQGDQUHVXOWDGRVPiVSUHFLVRVHLQIRUPDFLyQGHOHUURUPiVUHDOLVWD
/RVPpWRGRVTXHFRQVLGHUDPRVHQHVWHFDStWXORQRSURGXFHQXQDDSUR[LPDFLyQFRQWLQXD
SDUDODVROXFLyQGHOSUREOHPDGHYDORULQLFLDO0iVELHQODVDSUR[LPDFLRQHVVHHQFXHQWUDQ
HQFLHUWRVSXQWRVHVSHFt√ÄFRV\DPHQXGRLJXDOPHQWHHVSDFLDGRV1RUPDOPHQWHVHXVDDOJ~Q
PpWRGRGHLQWHUSRODFLyQSRUORJHQHUDOHOGH+HUPLWHVLVHQHFHVLWDQYDORUHVLQWHUPHGLRV
1RVRWURVQHFHVLWDPRVDOJXQDVGH√ÄQLFLRQHV\UHVXOWDGRVDSDUWLUGHODWHRUtDGHODVHFXDFLRQHVGLIHUHQFLDOHVRUGLQDULDVDOFRQVLGHUDUPpWRGRVSDUDDSUR[LPDUODVVROXFLRQHVDORV
SUREOHPDVGHYDORULQLFLDO

5.1 Teor√≠a elemental de problemas de valor inicial

DeÔ¨Ånici√≥n 5.1

195

6HGLFHTXHXQDIXQFLyQ f (t, y)VDWLVIDFHODcondici√≥n de Lipschitz en la variable y en un
FRQMXQWRD ‚äÇ R2 si existe una constante L > 0 con

| f (t, y1 ) ‚àí f (t, y2 , )| ‚â§ L|y1 ‚àí y2 |,
siempre que (t, y1 ) y (t, y2 ) est√©n en D/DFRQVWDQWH L recibe el nombre de constante de
Lipschitz para f.
Ejemplo 1

Muestre que f (t, y) = t|y|VDWLVIDFHODFRQGLFLyQGH/LSVFKLW]HQHOLQWHUYDOR D = { (t, y)
| 1 ‚â§ t ‚â§ 2 y ‚àí 3 ‚â§ y ‚â§ 4 }.
Soluci√≥n

Para cada par de puntos (t, y1 ) y (t, y2 ) en DWHQHPRV

| f (t, y1 ) ‚àí f (t, y2 )| = |t|y1 | ‚àí t|y2 || = |t| ||y1 | ‚àí |y2 || ‚â§ 2|y1 ‚àí y2 |.
3RUORWDQWR fVDWLVIDFHODFRQGLFLyQGH/LSVFKLW]HQD en la variable yFRQODFRQVWDQWHGH
/LSVFKLW](OYDORUPiVSHTXHxRSRVLEOHSDUDODFRQVWDQWHGH/LSVFKLW]SDUDHVWHSUREOHPDHV
L 5SRUTXHSRUHMHPSOR

| f (2, 1) ‚àí f (2, 0)| = |2 ‚àí 0| = 2|1 ‚àí 0|.
DeÔ¨Ånici√≥n 5.2

6HGLFHTXHXQFRQMXQWRD ‚äÇ R2 es convexo siempre que (t1 , y1 ) y (t2 , y2 ) pertenezcan a D,
entonces ((1 ‚àí Œª)t1 + Œªt2 , (1 ‚àí Œª)y1 + Œªy2 ) tambi√©n pertenece a D para cada Œª en [0, 1].
(Q WpUPLQRV JHRPpWULFRV OD GH√ÄQLFLyQ  HVWDEOHFH TXH XQ FRQMXQWR HV FRQYH[R D
FRQGLFLyQ GH TXH VLHPSUH TXH GRV SXQWRV SHUWHQH]FDQ D XQ FRQMXQWR WRGR VHJPHQWR GH
OtQHD UHFWD HQWUH ORV SXQWRV WDPELpQ SHUWHQH]FD DO FRQMXQWR FRQVXOWH OD √ÄJXUD  \ HO
HMHUFLFLR (QJHQHUDOORVFRQMXQWRVTXHFRQVLGHUDPRVHQHVWHFDStWXORVRQGHODIRUPD
D = { (t, y) | a ‚â§ t ‚â§ b y ‚àí ‚àû < y < ‚àû }SDUDDOJXQDVFRQVWDQWHVa\b(VIiFLOYHUL√ÄFDU
FRQVXOWHHOHMHUFLFLR TXHHVWRVFRQMXQWRVVRQFRQYH[RV

Figura 5.1

(t 2, y 2)

(t 2, y 2)

(t1, y1)

(t1, y1)

Convexo

Teorema 5.3

5XGROI/LSVFKLW] ¬≤ 
WUDEDMyHQPXFKDVUDPDVGHODV
PDWHPiWLFDVLQFOX\HQGRODWHRUtD
QXPpULFDODVVHULHVGH)RXULHU
ODVHFXDFLRQHVGLIHUHQFLDOHVOD
PHFiQLFDDQDOtWLFD\ODWHRUtDGHO
SRWHQFLDO(VPHMRUFRQRFLGRSRU
HVWDJHQHUDOL]DFLyQGHOWUDEDMR
GH$XJXVWLQ¬≤/RXLV&DXFK\
¬≤ \*XLVHSSH3HDQR
¬≤ 

No convexo

6XSRQJDTXH f (t, y)VHGH√ÄQHVREUHXQFRQMXQWRFRQYH[R D ‚äÇ R26LH[LVWHXQDFRQVWDQWH
L > 0 con

‚àÇf
(t, y) ‚â§ L ,
‚àÇy

para todo (t, y) ‚àà D,



entonces f VDWLVIDFH OD FRQGLFLyQ GH /LSVFKLW] HQ D HQ OD YDULDEOH \ FRQ FRQVWDQWH L de
/LSVFKLW]
/DGHPRVWUDFLyQGHOWHRUHPDVHDQDOL]DHQHOHMHUFLFLRHVVLPLODUDODSUXHEDGHO
UHVXOWDGRFRUUHVSRQGLHQWHSDUDIXQFLRQHVGHXQDYDULDEOHDQDOL]DGDVHQHOHMHUFLFLRGHOD
VHFFLyQ
&RPRYHUHPRVHQHOVLJXLHQWHWHRUHPDDPHQXGRHVGHLQWHUpVVLJQL√ÄFDWLYRSDUDGHWHUPLQDUVLODIXQFLyQLPSOLFDGDHQHOSUREOHPDGHYDORULQLFLDOVDWLVIDFHODFRQGLFLyQGH/LSVFKLW]

5.1 Teor√≠a elemental de problemas de valor inicial

DeÔ¨Ånici√≥n 5.5

197

El problema de valor inicial

dy
= f (t, y),
dt

a ‚â§ t ‚â§ b,

y(a) = Œ±,



se dice que es un problema bien planteadoVL
‚Ä¢ Existe una √∫nica VROXFLyQy(t \
‚Ä¢ Existen constantes Œµ0 > 0 y k > 0 WDOHVTXHSDUDFXDOTXLHUŒµ en (0, Œµ0 )VLHPSUHTXHŒ¥(t)
es continua con |Œ¥(t)| < Œµ para toda t en [a, b], y cuando |Œ¥0 | < ŒµHOSUREOHPDGHYDORU
inicial

dz
= f (t, z) + Œ¥(t),
dt

a ‚â§ t ‚â§ b,

z(a) = Œ± + Œ¥0 ,



tiene una √∫nicaVROXFLyQz(t TXHVDWLVIDFH

|z(t) ‚àí y(t)| < kŒµ

para toda t en [a, b].

(OSUREOHPDHVSHFL√ÄFDGRSRUODHFXDFLyQ  UHFLEHHOQRPEUHGHproblema perturbadoUHODFLRQDGRFRQHOSUREOHPDRULJLQDOHQODHFXDFLyQ  6XSRQHODSRVLELOLGDGGHXQ
HUURULQWURGXFLGRHQODGHFODUDFLyQGHODHFXDFLyQGLIHUHQFLDODVtFRPRXQHUURU»∂ presente
HQODFRQGLFLyQLQLFLDO
Los m√©todos num√©ricos quiz√° impliquen resolver un problema perturbado debido a que
FXDOTXLHUHUURUGHUHGRQGHRLQWURGXFLGRHQODUHSUHVHQWDFLyQSHUWXUEDHOSUREOHPDRULJLQDO
$PHQRVTXHVHSODQWHHHOSUREOHPDRULJLQDOH[LVWHQSRFDVUD]RQHVSDUDHVSHUDUTXHODVROXFLyQQXPpULFDSDUDXQSUREOHPDGHHVWHWLSRVHDSUR[LPDUtDFRQSUHFLVLyQDODVROXFLyQGHO
SUREOHPDRULJLQDO
(O VLJXLHQWH WHRUHPD HVSHFL√ÄFD ODV FRQGLFLRQHV TXH JDUDQWL]DQ XQ SUREOHPD GH YDORU
LQLFLDOELHQSODQWHDGR/DGHPRVWUDFLyQGHHVWHWHRUHPDVHSXHGHHQFRQWUDUHQ>%L5@SS
¬≤
Teorema 5.6

6XSRQJD TXH D = { (t, y) | a ‚â§ t ‚â§ b y ‚àí‚àû < y < ‚àû } 6L f HV FRQWLQXD \ VDWLVIDFH OD
FRQGLFLyQGH/LSVFKLW]HQODYDULDEOHyVREUHHOFRQMXQWRDHQWRQFHVHOSUREOHPDGHYDORU
inicial

dy
= f (t, y),
dt

a ‚â§ t ‚â§ b,

y(a) = Œ±

0 ‚â§ t ‚â§ 2,

y(0) = 0.5,

HVWiELHQSODQWHDGR
Ejemplo 3

Muestre que el problema de valor inicial

dy
= y ‚àí t 2 + 1,
dt



est√° bien planteado en D = { (t, y) | 0 ‚â§ t ‚â§ 2 y ‚àí ‚àû < y < ‚àû }.
Soluci√≥n Puesto que

‚àÇ(y ‚àí t 2 + 1)
= |1| = 1,
‚àÇy
HO WHRUHPD  LPSOLFD TXH f (t, y) = y ‚àí t 2 + 1  VDWLVIDFH OD FRQGLFLyQ GH  /LSVFKLW] HQ y
sobre DFRQODFRQVWDQWHGH/LSVFKLW]3XHVWRTXHf es continua en DHOWHRUHPDLPSOLFD
TXHHOSUREOHPDHVWiELHQSODQWHDGR
&RPRLOXVWUDFLyQFRQVLGHUHODVROXFLyQSDUDHOSUREOHPDSHUWXUEDGR

dz
= z ‚àí t 2 + 1 + Œ¥,
dt

0 ‚â§ t ‚â§ 2,

z(0) = 0.5 + Œ¥0 ,



198

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

donde Œ¥ y Œ¥0VRQFRQVWDQWHV/DVVROXFLRQHVDODVHFXDFLRQHV  \  VRQ

y(t) = (t + 1)2 ‚àí 0.5et

z(t) = (t + 1)2 + (Œ¥ + Œ¥0 ‚àí 0.5)et ‚àí Œ¥,

y

UHVSHFWLYDPHQWH
6XSRQJDTXHŒµHVXQQ~PHURSRVLWLYR6L|Œ¥| < Œµ y |Œ¥0 | < Œµ, entonces

|y(t) ‚àí z(t)| = |(Œ¥ + Œ¥0 )et ‚àí Œ¥| ‚â§ |Œ¥ + Œ¥0 |e2 + |Œ¥| ‚â§ (2e2 + 1)Œµ
para todas las t(VWRLPSOLFDTXHHOSUREOHPD  HVWiELHQSODQWHDGRFRQk(Œµ) = 2e2 + 1 para
todas las Œµ > 0.
La secci√≥n Conjunto de ejercicios 5.1 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

5.2 M√©todo de Euler
El mpWRGRGH(XOHUHVODWpFQLFDGHDSUR[LPDFLyQPiVEiVLFDSDUDUHVROYHUproblemas de
YDORULQLFLDO$SHVDUGHTXHUDUDYH]VHXVDHQODSUiFWLFDODVLPSOLFLGDGGHVXGHULYDFLyQ
VHSXHGHXWLOL]DUSDUDLOXVWUDUODVWpFQLFDVUHODFLRQDGDVFRQODFRQVWUXFFLyQGHDOJXQDVGHODV
WpFQLFDVPiVDYDQ]DGDVVLQHOiOJHEUDHQJRUURVDTXHDFRPSDxDHVWDVFRQVWUXFFLRQHV
(OREMHWLYRGHOPpWRGRGH(XOHUHVREWHQHUDSUR[LPDFLRQHVSDUDHOSUREOHPDGHYDORU
inicial bien planteado

dy
= f (t, y),
dt

a ‚â§ t ‚â§ b,

y(a) = Œ±.



1RVHREWHQGUiXQDDSUR[LPDFLyQFRQWLQXDDODVROXFLyQy(t HQVXOXJDUODVDSUR[LPDciones para yVHJHQHUDUiQHQYDULRVYDORUHVOODPDGRVpuntos de mallaHQHOLQWHUYDOR>ab@
8QDYH]TXHVHREWLHQHODVROXFLyQDSUR[LPDGDHQORVSXQWRVODVROXFLyQDSUR[LPDGDHQ
RWURVSXQWRVHQHOLQWHUYDORVHSXHGHHQFRQWUDUDWUDYpVGHLQWHUSRODFLyQ
3ULPHURHVWLSXODPRVTXHORVSXQWRVGHPDOODHVWiQLJXDOPHQWHHVSDFLDGRVDORODUJRGHO
LQWHUYDOR>ab@(VWDFRQGLFLyQVHJDUDQWL]DDOVHOHFFLRQDUXQHQWHURSRVLWLYRNDOHVWDEOHFHU
h = (b ‚àí a)/N \VHOHFFLRQDUORVSXQWRVGHPDOOD

ti = a + i h,
(OXVRGHPpWRGRVGHGLIHUHQFLD
EiVLFDSDUDDSUR[LPDUODVROXFLyQ
DODVHFXDFLRQHVGLIHUHQFLDOHV
IXHXQRGHORVGLIHUHQWHVWHPDV
matem√°ticos que se presentaron
primero al p√∫blico por el m√°s
SUROt√ÄFRGHORVPDWHPiWLFRV
/HRQKDUG(XOHU ¬≤ 

para cada i = 0, 1, 2, . . . , N .

La distancia com√∫n entre los puntos h = ti+1 ‚àí ti recibe el nombre de tama√±o de paso
8VDUHPRVHOWHRUHPDGH7D\ORUSDUDGHGXFLUHOPpWRGRGH(XOHU6XSRQJDTXHy(t OD
~QLFD VROXFLyQ SDUD   WLHQH GRV GHULYDGDV FRQWLQXDV HQ >a b@ GH WDO IRUPD TXH FDGD
i = 0, 1, 2, . . . , N ‚àí 1,

y(ti+1 ) = y(ti ) + (ti+1 ‚àí ti )y (ti ) +

(ti+1 ‚àí ti )2
y (Œæi ),
2

SDUDDOJ~QQ~PHURŒæi en (ti , ti+1 )3XHVWRTXHh = ti+1 ‚àí tiWHQHPRV

y(ti+1 ) = y(ti ) + hy (ti ) +

h2
y (Œæi ),
2

\\DTXHy(t VDWLVIDFHODHFXDFLyQGLIHUHQFLDO  

y(ti+1 ) = y(ti ) + h f (ti , y(ti )) +

h2
y (Œæi ).
2



5.2

M√©todo de Euler

199

(OPpWRGRGH(XOHUFRQVWUX\Hw i ‚âà y(ti ), para cada i = 1, 2, . . . , N DOERUUDUHOWpUPLQR
UHVWDQWH3RUORWDQWRHOPpWRGRGH(XOHUHV

w 0 = Œ±,
w i+1 = w i + h f (ti , w i ),
Ilustraci√≥n

para cada i = 0, 1, . . . , N ‚àí 1.



(QHOHMHPSORXVDUHPRVXQDOJRULWPRSDUDHOPpWRGRGH(XOHUSDUDDSUR[LPDUODVROXFLyQGH

y = y ‚àí t 2 + 1,

0 ‚â§ t ‚â§ 2,

y(0) = 0.5,

en t 5 Aqu√≠ simplemente ilustraremos los pasos en la t√©cnica cuando tenemos h 5
Para este problema f (t, y) = y ‚àí t 2 + 1; SRUORTXH

w 0 = y(0) = 0.5;
w 1 = w 0 + 0.5 w 0 ‚àí (0.0)2 + 1 = 0.5 + 0.5(1.5) = 1.25;
w 2 = w 1 + 0.5 w 1 ‚àí (0.5)2 + 1 = 1.25 + 0.5(2.0) = 2.25;
w 3 = w 2 + 0.5 w 2 ‚àí (1.0)2 + 1 = 2.25 + 0.5(2.25) = 3.375;
\

y(2) ‚âà w 4 = w 3 + 0.5 w 3 ‚àí (1.5)2 + 1 = 3.375 + 0.5(2.125) = 4.4375.
/DHFXDFLyQ  UHFLEHHOQRPEUHGHecuaci√≥n de diferencia relacionada con el m√©WRGRGH(XOHU&RPRYHUHPRVPiVDGHODQWHHQHVWHFDStWXORODWHRUtD\ODVROXFLyQGHHFXDFLRQHVGHGLIHUHQFLDVRQSDUDOHODVHQPXFKDVIRUPDVDODWHRUtD\VROXFLyQGHHFXDFLRQHV
GLIHUHQFLDOHV(ODOJRULWPRLPSOHPHQWDHOPpWRGRGH(XOHU

ALGORITMO

5.1

M√©todo de Euler
3DUDDSUR[LPDUODVROXFLyQGHOSUREOHPDGHYDORULQLFLDO

y = f (t, y),

a ‚â§ t ‚â§ b,

y(a) = Œ±,

en (N + 1) n√∫meros igualmente espaciados en el intervalo [a, b]:
ENTRADA extremos a, b; entero N ; condici√≥n inicial Œ±.
SALIDA aproximaci√≥n w para y en (N + 1) valores de t.
Paso 1 Determine h = (b ‚àí a)/N ;
t = a;
w = Œ±;
SALIDA (t, w).
Paso 2 Para i = 1, 2, . . . , N haga los pasos 3, 4.
Paso 3 Determine w = w + h f (t, w); (Calcule w i .)
t = a + i h. (Calcule t i .)
Paso 4 SALIDA (t, w).
Paso 5 PARE.
3DUDLQWHUSUHWDUHOPpWRGRGH(XOHUGHPDQHUDJHRPpWULFDREVHUYHTXHFXDQGRwi es
XQDDSUR[LPDFLyQFHUFDQDSDUDy (t i ODVXSRVLFLyQGHTXHHOSUREOHPDHVWiELHQSODQWHDGR
implica que

f (ti , w i ) ‚âà y (ti ) = f (ti , y(ti )).

200

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

/D JUi√ÄFD GH OD IXQFLyQ TXH UHVDOWD y (t i  VH PXHVWUD HQ OD √ÄJXUD  8Q SDVR HQ HO
PpWRGRGH(XOHUDSDUHFHHQODILJXUD\XQDVHULHGHSDVRVDSDUHFHHQOD√ÄJXUD
Figura 5.2
y
y9 5 f (t, y),
y(a) 5 a

...

y(t N) 5 y(b)

y(t 2)
y(t 1)
y(t 0) 5 a
t0 5 a

Figura 5.3

t1

t2

. . . tN 5 b

t

Figura 5.4
y

y
y9 5 f (t, y),
y(a) 5 a

Pendiente y9(a) 5 f (a, a)

w2
w1
a

w1
a
t0 5 a

t1

Ejemplo 1

t2

. . . tN 5 b

y9 5 f (t, y),
y(a) 5 a

y(b)
wN

t

t0 5 a

t1

t2

. . . tN 5 b

t

(OPpWRGRGH(XOHUVHXVyHQODSULPHUDLOXVWUDFLyQFRQh 5SDUDDSUR[LPDUODVROXFLyQ
al problema de valor inicial

y = y ‚àí t 2 + 1,

0 ‚â§ t ‚â§ 2,

y(0) = 0.5.

8WLOLFHHODOJRULWPRFRQN 5SDUDGHWHUPLQDUDSUR[LPDFLRQHV\FRPSiUHODVFRQORV
valores exactos dados por y(t) = (t + 1)2 ‚àí 0.5et .
Soluci√≥n Con N 5WHQHPRVh = 0.2, ti = 0.2i, w 0 = 0.5, y

w i+1 = w i + h(w i ‚àí ti2 + 1) = w i + 0.2[w i ‚àí 0.04i 2 + 1] = 1.2w i ‚àí 0.008i 2 + 0.2,
para i = 0, 1, . . . , 9. 3RUORTXH

w 1 = 1.2(0.5) ‚àí 0.008(0)2 + 0.2 = 0.8,

w 2 = 1.2(0.8) ‚àí 0.008(1)2 + 0.2 = 1.152,

\DVtVXFHVLYDPHQWH/DWDEODPXHVWUDODFRPSDUDFLyQHQWUHORVYDORUHVDSUR[LPDGRVHQti
\ORVYDORUHVUHDOHV

5.2

Tabla 5.1

ti

wi

yi = y(ti )

|yi ‚àí w i |

0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0

0.5000000
0.8000000
1.1520000
1.5504000
1.9884800
2.4581760
2.9498112
3.4517734
3.9501281
4.4281538
4.8657845

0.5000000
0.8292986
1.2140877
1.6489406
2.1272295
2.6408591
3.1799415
3.7324000
4.2834838
4.8151763
5.3054720

0.0000000
0.0292986
0.0620877
0.0985406
0.1387495
0.1826831
0.2301303
0.2806266
0.3333557
0.3870225
0.4396874

M√©todo de Euler

201

2EVHUYHTXHHOHUURUFUHFHOLJHUDPHQWHFRQIRUPHHOYDORUGHt DXPHQWD(VWHFUHFLPLHQWR
GHHUURUFRQWURODGRHVXQDFRQVHFXHQFLDGHODHVWDELOLGDGGHOPpWRGRGH(XOHUORFXDOLPSOLFDTXHVHHVSHUDTXHHOHUURUQRFUH]FDGHXQDPDQHUDPHMRUDODIRUPDOLQHDO

Cotas del error para el m√©todo de Euler
$SHVDUGHTXHHOPpWRGRGH(XOHUQRHVSRUFRPSOHWRDSURSLDGRSDUDJDUDQWL]DUVXXVRHQ
ODSUiFWLFDHVVX√ÄFLHQWHPHQWHEiVLFRSDUDDQDOL]DUHOHUURUSURGXFLGRDSDUWLUGHHVWDDSOLFDFLyQ(ODQiOLVLVGHHUURUSDUDORVPpWRGRVPiVSUHFLVRVTXHFRQVLGHUDPRVHQODVVHFFLRQHV
VXEVLJXLHQWHVVLJXHHOPLVPRSDWUyQSHURHVPiVFRPSOLFDGR
3DUD GHULYDU XQD FRWD GHO HUURU SDUD HO PpWRGR GH (XOHU QHFHVLWDPRV GRV OHPDV GH
FiOFXOR
Lema 5.7

Para toda x ‚â• ‚àí1 \FXDOTXLHUmSRVLWLYDWHQHPRV0 ‚â§ (1 + x)m ‚â§ emx .
Demostraci√≥n $ODSOLFDUHOWHRUHPDGH7D\ORUFRQ f (x) = e x , x 0 = 0, y n = 1 obtenemos

1
e x = 1 + x + x 2 eŒæ ,
2
donde j est√° entre x\FHUR3RUORWDQWR

1
0 ‚â§ 1 + x ‚â§ 1 + x + x 2 eŒæ = e x ,
2
\SXHVWRTXH1 + x ‚â• 0, tenemos

0 ‚â§ (1 + x)m ‚â§ (e x )m = emx .
Lema 5.8

k
6Ls\tVRQQ~PHURVUHDOHVSRVLWLYRV{ai }i=0
HVXQDVXFHVLyQTXHVDWLVIDFHa0 ‚â• ‚àít/s\

ai+1 ‚â§ (1 + s)ai + t,

para cada i = 0, 1, 2, . . . , k ‚àí 1,

entonces

ai+1 ‚â§ e(i+1)s a0 +

t
s

t
‚àí .
s

Demostraci√≥n 3DUDXQHQWHUR√ÄMRiODGHVLJXDOGDG  LPSOLFDTXH

ai+1 ‚â§ (1 + s)ai + t
‚â§ (1 + s)[(1 + s)ai‚àí1 + t] + t = (1 + s)2 ai‚àí1 + [1 + (1 + s)]t
‚â§ (1 + s)3 ai‚àí2 + 1 + (1 + s) + (1 + s)2 t
..
.
‚â§ (1 + s)i+1 a0 + 1 + (1 + s) + (1 + s)2 + ¬∑ ¬∑ ¬∑ + (1 + s)i t.



202

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

Pero

1 + (1 + s) + (1 + s)2 + ¬∑ ¬∑ ¬∑ + (1 + s)i =

i

(1 + s) j

j=0

HVXQDVHULHJHRPpWULFDFRQUDGLR 1 s) que se suma a

1 ‚àí (1 + s)i+1
1
= [(1 + s)i+1 ‚àí 1].
1 ‚àí (1 + s)
s
3RUORWDQWR

ai+1 ‚â§ (1 + s)i+1 a0 +

(1 + s)i+1 ‚àí 1
t
t = (1 + s)i+1 a0 +
s
s

t
‚àí ,
s

\SRUHOOHPDFRQx 51 s obtenemos

ai+1 ‚â§ e(i+1)s a0 +

Teorema 5.9

t
s

t
‚àí .
s

6XSRQJDTXHfHVFRQWLQXD\VDWLVIDFHODFRQGLFLyQGH/LSVFKLW]FRQFRQVWDQWHL en

D = { (t, y) | a ‚â§ t ‚â§ b y ‚àí ‚àû < y < ‚àû }
\TXHH[LVWHXQDFRQVWDQWHM con

|y (t)| ‚â§ M,

para todas las t ‚àà [a, b],

donde y(t GHQRWDOD~QLFDVROXFLyQSDUDHOSUREOHPDGHYDORULQLFLDO

y = f (t, y),

a ‚â§ t ‚â§ b,

y(a) = Œ±.

6HDQw 0 , w 1 , . . . , w N ODVDSUR[LPDFLRQHVJHQHUDGDVSRUHOPpWRGRGH(XOHUSDUDXQHQWHUR
positivo N(QWRQFHVSDUDFDGDi = 0, 1, 2, . . . , N ,

|y(ti ) ‚àí w i | ‚â§

h M L(ti ‚àía)
e
‚àí1 .
2L



Demostraci√≥n Cuando i 5HOUHVXOWDGRHVFODUDPHQWHYHUGDGHUR\DTXHy(t0 ) = w 0 = Œ±.

$SDUWLUGHODHFXDFLyQ  WHQHPRV

y(ti+1 ) = y(ti ) + h f (ti , y(ti )) +

h2
y (Œæi ),
2

para i = 0, 1, . . . , N ‚àí 1, \DSDUWLUGHODVHFXDFLRQHVHQ 

w i+1 = w i + h f (ti , w i ).
8VDQGRODQRWDFLyQyi = y(ti ) y yi+1 = y(ti+1 )UHVWDPRVHVWDVGRVHFXDFLRQHVSDUDREWHQHU

yi+1 ‚àí w i+1 = yi ‚àí w i + h[ f (ti , yi ) ‚àí f (ti , w i )] +

h2
y (Œæi )
2

|yi+1 ‚àí w i+1 | ‚â§ |yi ‚àí w i | + h| f (ti , yi ) ‚àí f (ti , w i )| +

h2
|y (Œæi )|.
2

3RUORWDQWR

5.2

M√©todo de Euler

203

$KRUD f VDWLVIDFH OD FRQGLFLyQ GH /LSVFKLW] HQ OD VHJXQGD YDULDEOH FRQ FRQVWDQWHL \
|y (t)| ‚â§ M, por lo que

|yi+1 ‚àí w i+1 | ‚â§ (1 + h L)|yi ‚àí w i | +

h2 M
.
2

'H DFXHUGR FRQ HO OHPD  \ KDFLHQGR s = h L, t = h 2 M/2, y a j = |y j ‚àí w j |, para
cada j = 0, 1, . . . , N REVHUYDUHPRVTXH

|yi+1 ‚àí w i+1 | ‚â§ e(i+1)h L

|y0 ‚àí w 0 | +

h2 M
2h L

‚àí

h2 M
.
2h L

Puesto que |y0 ‚àí w 0 | = 0 y (i + 1)h = ti+1 ‚àí t0 = ti+1 ‚àí aHVWRLPSOLFDTXH

|yi+1 ‚àí w i+1 | ‚â§

h M (ti+1 ‚àía)L
(e
‚àí 1),
2L

para cada i = 0, 1, . . . , N ‚àí 1.
/DGHELOLGDGGHOWHRUHPDGHSHQGHGHOUHTXLVLWRGHFRQRFHUXQDFRWDSDUDODVHJXQGD
GHULYDGDGHODVROXFLyQ$SHVDUGHTXHDPHQXGRHVWDFRQGLFLyQQRVSURKtEHREWHQHUXQD
FRWDGHHUURUUHDOLVWDVHGHEHUtDREVHUYDUTXHVLH[LVWH‚àÇ f /‚àÇt y ‚àÇ f /‚àÇ yODUHJODGHODFDGHQD
SDUDODGLIHUHQFLDFLyQSDUFLDOLPSOLFDTXH

y (t) =

df
‚àÇf
‚àÇf
dy
(t) =
(t, y(t)) =
(t, y(t)) +
(t, y(t)) ¬∑ f (t, y(t)).
dt
dt
‚àÇt
‚àÇy

3RUORWDQWRDOJXQDVYHFHVHVSRVLEOHREWHQHUXQDFRWDGHHUURUSDUDy 0(t) sin conocer expl√≠citamente y (t 
Ejemplo 2

/DVROXFLyQSDUDHOSUREOHPDGHYDORULQLFLDO

y = y ‚àí t 2 + 1,

0 ‚â§ t ‚â§ 2,

y(0) = 0.5,

VHDSUR[LPyHQHOHMHPSORFRQHOPpWRGRGH(XOHUFRQh 58WLOLFHODGHVLJXDOGDGHQ
HOWHRUHPDSDUDHQFRQWUDUXQDFRWDSDUDORVHUURUHVGHDSUR[LPDFLyQ\FRPSiUHORVFRQ
ORVHUURUHVUHDOHV
Puesto que f (t, y) = y ‚àí t 2 + 1, tenemos ‚àÇ f (t, y)/‚àÇ y = 1 para todas las ySRU
lo que L 53DUDHVWHSUREOHPDODVROXFLyQH[DFWDHV y(t) = (t + 1)2 ‚àí 0.5et , por lo que
y (t) = 2 ‚àí 0.5et\
Soluci√≥n

|y (t)| ‚â§ 0.5e2 ‚àí 2,

para todas las t ‚àà [0, 2].

3RUPHGLRGHGHVLJXDOGDGHQODFRWDGHHUURUSDUDHOPpWRGRGH(XOHUFRQh 5L 5\
M = 0.5e2 ‚àí 2 da

|yi ‚àí w i | ‚â§ 0.1(0.5e2 ‚àí 2)(eti ‚àí 1).
Por lo tanto

|y(0.2) ‚àí w 1 | ‚â§0.1(0.5e2 ‚àí 2)(e0.2 ‚àí 1) = 0.03752,
|y(0.4) ‚àí w 2 | ‚â§0.1(0.5e2 ‚àí 2)(e0.4 ‚àí 1) = 0.08334,
\DVtVXFHVLYDPHQWH/DWDEODHQXPHUDHOHUURUUHDOHQFRQWUDGRHQHOHMHPSORMXQWRFRQ
ODFRWDGHHUURU2EVHUYHTXHDXQTXHVHXVyODFRWDYHUGDGHUDSDUDODVHJXQGDGHULYDGDGHOD
VROXFLyQODFRWDGHHUURUHVFRQVLGHUDEOHPHQWHVXSHULRUTXHHOHUURUUHDOHQHVSHFLDOSDUDORV
YDORUHVPD\RUHVGHt

204

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

Tabla 5.2
ti

0.2

0.4

0.6

0.8

1.0

1.2

1.4

1.6

1.8

2.0

Error real
Cota de error

0.02930
0.03752

0.06209
0.08334

0.09854
0.13931

0.13875
0.20767

0.18268
0.29117

0.23013
0.39315

0.28063
0.51771

0.33336
0.66985

0.38702
0.85568

0.43969
1.08264

/DSULQFLSDOLPSRUWDQFLDGHODIyUPXODGHODFRWDGHHUURUGHWHUPLQDGDHQHOWHRUHPDHV
que la cota depende linealmente del tama√±o de paso h3RUFRQVLJXLHQWHGLVPLQXLUHOWDPDxR
GHSDVRGHEHUtDSURSRUFLRQDUPD\RUSUHFLVLyQSDUDODVDSUR[LPDFLRQHVHQODPLVPDPHGLGD
2OYLGDGRHQHOUHVXOWDGRGHOWHRUHPDHVWiHOHIHFWRTXHHOHUURUGHUHGRQGHRUHSUHVHQWDHQODVHOHFFLyQGHOWDPDxRGHSDVR&RQIRUPHhVHYXHOYHPiVSHTXHxRVHQHFHVLWDQ
PiV FiOFXORV \ VH HVSHUD PiV HUURU GH UHGRQGHR (QWRQFHV HQ OD DFWXDOLGDG OD IRUPD GH
HFXDFLyQGHGLIHUHQFLD

w 0 = Œ±,
w i+1 = w i + h f (ti , w i ),

para cada i = 0, 1, . . . , N ‚àí 1,

QRVHXWLOL]DSDUDFDOFXODUODDSUR[LPDFLyQDODVROXFLyQyi en un punto de malla ti(QVX
OXJDUXVDPRVXQDHFXDFLyQGHODIRUPD

u 0 = Œ± + Œ¥0 ,
u i+1 = u i + h f (ti , u i ) + Œ¥i+1 ,

para cada i = 0, 1, . . . , N ‚àí 1,



donde Œ¥i denota el error de redondeo asociado con ui$OXWLOL]DUPpWRGRVVLPLODUHVDDTXHOORV
HQODSUXHEDGHOWHRUHPDSRGHPRVSURGXFLUXQDFRWDGHHUURUSDUDODVDSUR[LPDFLRQHVGH
GtJLWRV√ÄQLWRVSDUDyiSURYLVWRVSRUHOPpWRGRGH(XOHU
Teorema 5.10

6Ly(t GHQRWDOD~QLFDVROXFLyQSDUDHOSUREOHPDGHYDORULQLFLDO

y = f (t, y),

a ‚â§ t ‚â§ b,

y(a) = Œ±,



\ u 0 , u 1 , . . . , u N VRQODVDSUR[LPDFLRQHVREWHQLGDVGHODHFXDFLyQ  6L |Œ¥i | < Œ¥ para
cada i = 0, 1, . . . , N \ODKLSyWHVLVGHOWHRUHPDVRQDSOLFDEOHVDODHFXDFLyQ  HQtonces

|y(ti ) ‚àí u i | ‚â§

1
L

Œ¥
hM
+
2
h

[e L(ti ‚àía) ‚àí 1] + |Œ¥0 |e L(ti ‚àía) ,



para cada i = 0, 1, . . . , N 
/DFRWDGHHUURU  \DQRHVOLQHDOHQh'HKHFKRSXHVWRTXH

l√≠m

h‚Üí0

Œ¥
hM
+
2
h

= ‚àû,

VHHVSHUDUtDTXHHOHUURUVHYXHOYDPiVJUDQGHSDUDORVYDORUHVVX√ÄFLHQWHPHQWHSHTXHxRVGHh
(O FiOFXOR VH SXHGH XVDU SDUD GHWHUPLQDU XQD FRWD LQIHULRU SDUD HO WDPDxR GH SDVR h 6L
E(h) = (h M/2) + (Œ¥/ h) implica que E (h) = (M/2) ‚àí (Œ¥/ h 2 ):

Si h <

2Œ¥/M, entonces E (h) < 0 y E(h) disminuye.

Si h >

2Œ¥/M, entonces E (h) > 0 y E(h) aumenta.

El valor m√≠nimo de E(h) se presenta cuando

h=

2Œ¥
.
M



5.3 M√©todos de Taylor de orden superior

205

/DGLVPLQXFLyQGHh m√°s all√° de este valor tiende a incrementar el error total en la aproxiPDFLyQ3RUORJHQHUDOVLQHPEDUJRHOYDORUGHŒ¥HVVX√ÄFLHQWHPHQWHSHTXHxRSDUDTXHHVWD
FRWDLQIHULRUSDUDhQRDIHFWHODRSHUDFLyQGHOPpWRGRGH(XOHU
La secci√≥n Conjunto de ejercicios 5.2 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

5.3 M√©todos de Taylor de orden superior
3XHVWRTXHHOREMHWLYRGHXQDWpFQLFDQXPpULFDHVGHWHUPLQDUDSUR[LPDFLRQHVSUHFLVDVFRQ
PtQLPRHVIXHU]RQHFHVLWDPRVPHGLRVSDUDFRPSDUDUODH√ÄFLHQFLDGHORVGLIHUHQWHVPpWRGRVGHDSUR[LPDFLyQ(OSULPHUGLVSRVLWLYRTXHFRQVLGHUDPRVUHFLEHHOQRPEUHGHerror de
truncamiento localGHOPpWRGR
(OHUURUGHWUXQFDPLHQWRORFDOHQXQSDVRHVSHFt√ÄFRPLGHODFDQWLGDGSRUODTXHODVROXFLyQH[DFWDSDUDODHFXDFLyQGLIHUHQFLDOIDOODHQFXDQWRDVDWLVIDFHUODHFXDFLyQGHGLIHUHQFLD
TXHVHXVDSDUDODDSUR[LPDFLyQHQHVHSDVR(VWRSRGUtDSDUHFHUXQDIRUPDSRFRSUREDEOH
GHFRPSDUDUHOHUURUGHYDULRVPpWRGRV(QUHDOLGDGTXHUHPRVVDEHUTXpWDQELHQVDWLVIDFHQ
ODVDSUR[LPDFLRQHVJHQHUDGDVFRQORVPpWRGRVODHFXDFLyQGLIHUHQFLDOQRDOUHYpV6LQHPEDUJRQRFRQRFHPRVODVROXFLyQH[DFWDSRUORTXHHQJHQHUDOQRSRGHPRVGHWHUPLQDUOR\
HOHUURUGHWUXQFDPLHQWRORFDOVHUYLUiGHPDQHUDDGHFXDGDSDUDGHWHUPLQDUQRVyORHOHUURU
ORFDOGHXQPpWRGRVLQRWDPELpQHOHUURUGHDSUR[LPDFLyQUHDO
Considere el problema de valor inicial

y = f (t, y),
DeÔ¨Ånici√≥n 5.11

a ‚â§ t ‚â§ b,

y(a) = Œ±.

(OPpWRGRGHGLIHUHQFLD

w0 = Œ±
w i+1 = w i + hœÜ(ti , w i ),

para cada i = 0, 1, . . . , N ‚àí 1,

tiene error de truncamiento local

œÑi+1 (h) =

yi+1 ‚àí yi
yi+1 ‚àí (yi + hœÜ(ti , yi ))
=
‚àí œÜ(ti , yi ),
h
h

para cada i = 0, 1, . . . , N ‚àí 1, donde yi y yi+1GHQRWDQODVROXFLyQGHODHFXDFLyQGLIHrencial en ti y ti+1UHVSHFWLYDPHQWH
3RUHMHPSORHOPpWRGRGH(XOHUWLHQHHUURUGHWUXQFDPLHQWRHQHOi-√©simo paso

œÑi+1 (h) =

yi+1 ‚àí yi
‚àí f (ti , yi ),
h

para cada i = 0, 1, . . . , N ‚àí 1.

Este error es un error localSRUTXHPLGHODSUHFLVLyQGHOPpWRGRHQXQSDVRHVSHFt√ÄFR
DOVXSRQHUTXHHOPpWRGRHUDH[DFWRHQHOSDVRDQWHULRU&RPRWDOGHSHQGHGHODHFXDFLyQ
GLIHUHQFLDOGHOWDPDxRGHSDVR\GHOSDVRSDUWLFXODUHQODDSUR[LPDFLyQ
$OFRQVLGHUDUODHFXDFLyQ  HQODVHFFLyQSUHYLDREVHUYDPRVTXHHOPpWRGRGH(XOHU
tiene

œÑi+1 (h) =

h
y (Œæi ),
2

para algunas Œæi en(ti , ti+1 ).

Cuando se sabe que y (t) est√° acotada por una constante MHQ>ab@HVWRLPSOLFD

|œÑi+1 (h)| ‚â§
por lo que el error de truncamiento local es O(h 

h
M,
2

206

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

/RVPpWRGRVHQHVWDVHFFLyQ
XWLOL]DQSROLQRPLRVGH7D\ORU\
el conocimiento de la derivada en
un nodo para aproximar el valor
GHODIXQFLyQHQXQQRGRQXHYR

8QDIRUPDGHVHOHFFLRQDUPpWRGRVGHHFXDFLyQGHGLIHUHQFLDSDUDUHVROYHUHFXDFLRQHV
GLIHUHQFLDOHVRUGLQDULDVGHWDOIRUPDTXHVXVHUURUHVGHWUXQFDPLHQWRORFDOVRQO(hp) para un
valor de p WDQJUDQGHFRPRVHDSRVLEOHPLHQWUDVVHPDQWLHQHQHOQ~PHUR\ODFRPSOHMLGDG
GHORVFiOFXORVGHORVPpWRGRVGHQWURGHXQOtPLWHUD]RQDEOH
3XHVWRTXHHOPpWRGRGH(XOHUVHGHULYyGHOWHRUHPDGH7D\ORUFRQn  para aproximar
ODVROXFLyQGHODHFXDFLyQGLIHUHQFLDOQXHVWURSULPHULQWHQWRSDUDHQFRQWUDUPpWRGRVSDUD
PHMRUDUODVSURSLHGDGHVGHFRQYHUJHQFLDGHORVPpWRGRVGHGLIHUHQFLDHVDPSOLDUHVWDWpFQLFD
GHGHULYDFLyQDYDORUHVPiVJUDQGHVGH n
6XSRQJDTXHODVROXFLyQy(t) para el problema de valor inicial

y = f (t, y),

a ‚â§ t ‚â§ b,

y(a) = Œ±,

tiene (n 1 GHULYDGDVFRQWLQXDV6LDPSOLDPRVODVROXFLyQy(t HQWpUPLQRVGHVXHQpVLPR
SROLQRPLRGH7D\ORUDOUHGHGRUGHti\VHHYDO~DQHQti+1REWHQHPRV

y(ti+1 ) = y(ti ) + hy (ti ) +

h2
h n (n)
h n+1 (n+1)
y (ti ) + ¬∑ ¬∑ ¬∑ +
y (ti ) +
y
(Œæi ),
2
n!
(n + 1)!



SDUDDOJXQDVŒæi en (ti , ti+1 ).
/DGLIHUHQFLDFLyQVXFHVLYDGHODVROXFLyQy(t GD

y (t) = f (t, y(t)),

y (t) = f (t, y(t)),

y, en general,

y (k) (t) = f (k‚àí1) (t, y(t)).

$OVXVWLWXLUHVWRVUHVXOWDGRVHQODHFXDFLyQ  REWHQHPRV

y(ti+1 ) = y(ti ) + h f (ti , y(ti )) +
+

h2
f (ti , y(ti )) + ¬∑ ¬∑ ¬∑
2



h n+1
h n (n‚àí1)
(ti , y(ti )) +
f
f (n) (Œæi , y(Œæi )).
n!
(n + 1)!

(OPpWRGRGHHFXDFLyQGHGLIHUHQFLDFRUUHVSRQGLHQWHDODHFXDFLyQ  VHREWLHQHDO
borrar el t√©rmino restante relacionado con Œæi .
M√©todo de Taylor de orden n

w 0 = Œ±,
w i+1 = w i + hT (n) (ti , w i ),

para cada i = 0, 1, . . . , N ‚àí 1,



donde

T (n) (ti , w i ) = f (ti , w i ) +

h
h n‚àí1 (n‚àí1)
f (ti , w i ) + ¬∑ ¬∑ ¬∑ +
f
(ti , w i ).
2
n!

(OPpWRGRGH(XOHUHVXQPpWRGRGH7D\ORUGHRUGHQXQR
Ejemplo 1

$SOLTXHHOPpWRGRGH7D\ORUGHyUGHQHVa \b FRQN 5DOSUREOHPDGHYDORULQLFLDO

y = y ‚àí t 2 + 1,

0 ‚â§ t ‚â§ 2,

y(0) = 0.5.

a 3DUDHOPpWRGRGHRUGHQQHFHVLWDPRVODSULPHUDGHULYDGDGH f (t, y(t)) =
y(t) ‚àí t 2 + 1 respecto a la variable t3XHVWRTXHy = y ‚àí t 2 + 1, tenemos

Soluci√≥n

f (t, y(t)) =

d
(y ‚àí t 2 + 1) = y ‚àí 2t = y ‚àí t 2 + 1 ‚àí 2t,
dt

5.3 M√©todos de Taylor de orden superior

207

por lo que

h
h
f (ti , w i ) = w i ‚àí ti2 + 1 + (w i ‚àí ti2 + 1 ‚àí 2ti )
2
2

T (2) (ti , w i ) = f (ti , w i ) +
=

1+

h
2

(w i ‚àí ti2 + 1) ‚àí hti .

Puesto que N 5WHQHPRVh = 0.2, y ti = 0.2i para cada i = 1, 2, . . . , 10. Por lo tanWRHOPpWRGRGHVHJXQGRRUGHQVHYXHOYH

w 0 = 0.5,
w i+1 = w i + h

1+

= w i + 0.2

h
2

1+

w i ‚àí ti2 + 1 ‚àí hti
0.2
2

(w i ‚àí 0.04i 2 + 1) ‚àí 0.04i

= 1.22w i ‚àí 0.0088i 2 ‚àí 0.008i + 0.22.
Los primeros dos pasos dan las aproximaciones

y(0.2) ‚âà w 1 = 1.22(0.5) ‚àí 0.0088(0)2 ‚àí 0.008(0) + 0.22 = 0.83
\

Tabla 5.3

ti

Orden 2
de Taylor
Error
wi
|y(ti ) ‚àí w i |

0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0

0.500000
0.830000
1.215800
1.652076
2.132333
2.648646
3.191348
3.748645
4.306146
4.846299
5.347684

0
0.000701
0.001712
0.003135
0.005103
0.007787
0.011407
0.016245
0.022663
0.031122
0.042212

y(0.4) ‚âà w 2 = 1.22(0.83) ‚àí 0.0088(0.2)2 ‚àí 0.008(0.2) + 0.22 = 1.2158.
7RGDVODVDSUR[LPDFLRQHV\VXVHUURUHVVHPXHVWUDQHQODWDEOD
b  3DUD HO PpWRGR GH 7D\ORU GH RUGHQ  QHFHVLWDPRV ODV SULPHUDV WUHV GHULYDGDV GH
f (t, y(t)) respecto a t.'HQXHYRSRUPHGLRGHy = y ‚àí t 2 + 1, tenemos

f (t, y(t)) = y ‚àí t 2 + 1 ‚àí 2t,
f (t, y(t)) =

d
(y ‚àí t 2 + 1 ‚àí 2t) = y ‚àí 2t ‚àí 2
dt

= y ‚àí t 2 + 1 ‚àí 2t ‚àí 2 = y ‚àí t 2 ‚àí 2t ‚àí 1,
y
f (t, y(t)) =

d
(y ‚àí t 2 ‚àí 2t ‚àí 1) = y ‚àí 2t ‚àí 2 = y ‚àí t 2 ‚àí 2t ‚àí 1,
dt

por lo que
T (4) (ti , w i ) = f (ti , w i ) +

h
h2
h3
f (ti , w i ) +
f (ti , w i ) +
f (ti , w i )
2
6
24

= w i ‚àí ti2 + 1 +
+
=

h
h2
(w i ‚àí ti2 + 1 ‚àí 2ti ) + (w i ‚àí ti2 ‚àí 2ti ‚àí 1)
2
6

h3
(w i ‚àí ti2 ‚àí 2ti ‚àí 1)
24

1+

h2
h3
h
+
+
2
6
24

+1+

h2
h3
h
‚àí
‚àí .
2
6
24

(w i ‚àí ti2 ) ‚àí 1 +

h2
h
+
3 12

(hti )

208

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

Por lo tanto, el m√©todo de Taylor de orden 4 es
w 0 = 0.5,
w i+1 = w i + h
+1+

1+

h2
h3
h
+
+
2
6
24

(w i ‚àí ti2 ) ‚àí 1 +

h2
h
+
3 12

hti

h2
h3
h
‚àí
‚àí
,
2
6
24

para i = 0, 1, . . . , N ‚àí 1.
Puesto que N = 10 y h = 0.2, el m√©todo se vuelve
w i+1 = w i + 0.2
‚àí 1+

1+

0.2 0.04 0.008
+
+
2
6
24

0.2 0.04
+
3
12

(0.04i) + 1 +

(w i ‚àí 0.04i 2 )
0.2 0.04 0.008
‚àí
‚àí
2
6
24

= 1.2214w i ‚àí 0.008856i 2 ‚àí 0.00856i + 0.2186,
para cada i = 0, 1, . . . , 9. Los primeros dos pasos proporcionan las aproximaciones
y(0.2) ‚âà w 1 = 1.2214(0.5) ‚àí 0.008856(0)2 ‚àí 0.00856(0) + 0.2186 = 0.8293
\

Tabla 5.4

ti

Orden 4
de Taylor
Error
wi
|y(ti ) ‚àí w i |

0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0

0.500000
0.829300
1.214091
1.648947
2.127240
2.640874
3.179964
3.732432
4.283529
4.815238
5.305555

0
0.000001
0.000003
0.000006
0.000010
0.000015
0.000023
0.000032
0.000045
0.000062
0.000083

/DLQWHUSRODFLyQGH+HUPLWH
requiere tanto el valor de la
IXQFLyQFRPRVXGHULYDGD
HQFDGDQRGR(VWRFUHDXQ
PpWRGRGHLQWHUSRODFLyQQDWXUDO
para aproximar ecuaciones
GLIHUHQFLDOHV\DTXHHVWRVGDWRV
HVWiQGLVSRQLEOHV

y(0.4) ‚âà w 2 = 1.2214(0.8293) ‚àí 0.008856(0.2)2 ‚àí 0.00856(0.2) + 0.2186 = 1.214091.
7RGDVODVDSUR[LPDFLRQHV\VXVHUURUHVVHPXHVWUDQHQODWDEOD
&RPSDUHHVWRVUHVXOWDGRVFRQORVGHOPpWRGRGH7D\ORUGHRUGHQHQODWDEOD\REVHUYDUiTXHORVUHVXOWDGRVGHFXDUWRRUGHQVRQLQPHQVDPHQWHVXSHULRUHV
/RVGDWRVGHODWDEODLQGLFDQTXHORVUHVXOWDGRVGHOPpWRGRGH7D\ORUGHRUGHQVRQ
EDVWDQWHSUHFLVRVHQORVQRGRV\DVtVXFHVLYDPHQWH3HURVXSRQJDTXHQHFHVLWDPRV
GHWHUPLQDUXQDDSUR[LPDFLyQSDUDXQSXQWRLQWHUPHGLRHQODWDEODSRUHMHPSORHQt 5
6LXVDPRVLQWHUSRODFLyQOLQHDOVREUHHOPpWRGRGH7D\ORUGHRUGHQSDUDDSUR[LPDFLRQHVHQ
t 5\ t 5WHQHPRV

y(1.25) ‚âà

1.25 ‚àí 1.4
1.2 ‚àí 1.4

3.1799640 +

1.25 ‚àí 1.2
1.4 ‚àí 1.2

3.7324321 = 3.3180810.

El verdadero valor es y  5SRUORTXHHVWDDSUR[LPDFLyQWLHQHXQHUURUGH
ORFXDOHVDOUHGHGRUGHYHFHVHOSURPHGLRGHORVHUURUHVGHDSUR[LPDFLyQHQ
\
3RGHPRVPHMRUDUHQIRUPDVLJQL√ÄFDWLYDODDSUR[LPDFLyQSRUPHGLRGHODLQWHUSRODFLyQ
F~ELFDGH+HUPLWH/DGHWHUPLQDFLyQGHHVWDDSUR[LPDFLyQSDUDy  UHTXLHUHDSUR[LPDciones para y9  \y9  DVtFRPRSDUDy  \y  6LQHPEDUJRODVDSUR[LPDFLRnes para y   \ y   HVWiQ HQ OD WDEOD \ ODV DSUR[LPDFLRQHV GH OD GHULYDGD HVWiQ GLVSRQLEOHVDSDUWLUGHODHFXDFLyQGLIHUHQFLDO\DTXH y (t) = f (t, y(t))(QQXHVWURHMHPSOR
y (t) = y(t) ‚àí t 2 + 1, por lo que

y (1.2) = y(1.2) ‚àí (1.2)2 + 1 ‚âà 3.1799640 ‚àí 1.44 + 1 = 2.7399640
\

y (1.4) = y(1.4) ‚àí (1.4)2 + 1 ‚âà 3.7324327 ‚àí 1.96 + 1 = 2.7724321.
(OSURFHGLPLHQWRGHGLIHUHQFLDGLYLGLGDHQODVHFFLyQSURYHHODLQIRUPDFLyQHQOD
WDEOD/DVHQWUDGDVVXEUD\DGDVSURYLHQHQGHORVGDWRV\ODVRWUDVHQWUDGDVXWLOL]DQIyUPXODVGHGLIHUHQFLDGLYLGLGD

5.4

Tabla 5.5

1.2

3.1799640

1.2

3.1799640

M√©todo Runge-Kutta

209

2.7399640
0.1118825
‚àí0.3071225

2.7623405
1.4

3.7324321

1.4

3.7324321

0.0504580
2.7724321

(OSROLQRPLRF~ELFRGH+HUPLWHHV

y(t) ‚âà 3.1799640 + (t ‚àí 1.2)2.7399640 + (t ‚àí 1.2)2 0.1118825
+ (t ‚àí 1.2)2 (t ‚àí 1.4)(‚àí0.3071225),
por lo que

y(1.25) ‚âà 3.1799640 + 0.1369982 + 0.0002797 + 0.0001152 = 3.3173571,
XQ UHVXOWDGR SUHFLVR GHQWUR GH  (VWR HV DSUR[LPDGDPHQWH HO SURPHGLR GH ORV
HUURUHVHQ\\VyORGHOHUURUREWHQLGRPHGLDQWHLQWHUSRODFLyQOLQHDO(VWDPHMRUD
GHODSUHFLVLyQFLHUWDPHQWHMXVWL√ÄFDORVFiOFXORVDGLFLRQDOHVUHTXHULGRVSDUDHOPpWRGRGH
+HUPLWH
Teorema 5.12

6LVHXVDHOPpWRGRGH7D\ORUGHRUGHQnSDUDDSUR[LPDUODVVROXFLyQGH

y (t) = f (t, y(t)),

a ‚â§ t ‚â§ b,

y(a) = Œ±,

con tama√±o de paso h\VLy ‚àà C n+1 [a, b], entonces el error de truncamiento local es O(h n ).
Demostraci√≥n 2EVHUYHTXHODHFXDFLyQ  HQODSiJLQDVHSXHGHUHHVFULELUFRPR

yi+1 ‚àí yi ‚àí h f (ti , yi ) ‚àí

h2
h n (n‚àí1)
h n+1
(ti , yi ) =
f (ti , yi ) ‚àí ¬∑ ¬∑ ¬∑ ‚àí
f
f (n) (Œæi , y(Œæi )),
2
n!
(n + 1)!

SDUDDOJXQDVŒæi en (ti , ti+1 )3RUORTXHHOHUURUGHWUXQFDPLHQWRHV

œÑi+1 (h) =

yi+1 ‚àí yi
hn
‚àí T (n) (ti , yi ) =
f (n) (Œæi , y(Œæi )),
h
(n + 1)!

para cada i = 0, 1, . . . , N ‚àí 1. Puesto que y ‚àà C n+1 [a, b], tenemos y (n+1) (t) =
f (n) (t, y(t))HVWiDFRWDGDHQ[ a, b] y œÑi (h) = O(h n ), para cada i = 1, 2, . . . , N .
La secci√≥n Conjunto de ejercicios 5.3 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las paginas preliminares.

5.4 M√©todo Runge-Kutta
/RVPpWRGRVGH7D\ORUGHVFULWRVHQODVHFFLyQDQWHULRUWLHQHQODSURSLHGDGGHVHDEOHGHHUURU
GHWUXQFDPLHQWRORFDOGHRUGHQVXSHULRUSHURODGHVYHQWDMDGHUHTXHULUHOFiOFXOR\ODHYDOXDFLyQGHODVGHULYDGDVGHf (ty (VWHHVXQSURFHGLPLHQWRFRPSOLFDGR\TXHWRPDPXFKR
WLHPSRSDUDODPD\RU√≠a GHORVSUREOHPDVSRUORTXHORVPpWRGRVGH7D\ORUUDUDYH]VHXVDQ
HQODSUiFWLFD
$√ÄQDOHVGHODGpFDGDGH
Los m√©todos Runge-Kutta tienen el error de truncamiento local de orden superior a los
&DUO5XQJH ¬≤ XWLOL]y
PpWRGRVVLPLODUHVDORVTXHVHKDQ PpWRGRVGH7D\ORUSHURHOLPLQDQODQHFHVLGDGGHFDOFXODU\HYDOXDUODVGHULYDGDVGHf (ty 
XVDGRHQHVWDVHFFLyQSDUDGHULYDU
$QWHVGHSUHVHQWDUODVLGHDVGHWUiVGHVXGHULYDFLyQQHFHVLWDPRVFRQVLGHUDUHOWHRUHPDGH
YDULDVIyUPXODVSDUDDSUR[LPDU
ODVROXFLyQGHSUREOHPDVGHYDORU 7D\ORUHQGRVYDULDEOHV/DSUXHEDGHHVWHUHVXOWDGRVHSXHGHHQFRQWUDUHQFXDOTXLHUOLEUR
HVWiQGDUVREUHFiOFXORDYDQ]DGR FRQVXOWHSRUHMHPSOR>)X@S 
LQLFLDO

210

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

Teorema 5.13

(Q0DUWLQ:LOKHP.XWWD
¬≤ JHQHUDOL]yORV
PpWRGRVTXH5XQJHGHVDUUROOy
HQSDUDLQFRUSRUDUVLVWHPDV
GHHFXDFLRQHVGLIHUHQFLDOHVGH
SULPHURUGHQ(VWDVWpFQLFDV
GL√ÄHUHQOLJHUDPHQWHGHORTXHHQ
la actualidad llamamos m√©todos
GH5XQJH.XWWD

6XSRQJDTXHf (ty \WRGDVVXVGHULYDGDVSDUFLDOHVGHRUGHQPHQRURLJXDODn 1VRQFRQtinuas en D = { (t, y) | a ‚â§ t ‚â§ b, c ‚â§ y ‚â§ d } \VL(t0 , y0 ) ‚àà D3DUDFDGD(t, y) ‚àà D,
existe j entre t\t\ Œº entre y \y con

f (t, y) = Pn (t, y) + Rn (t, y),
donde

Pn (t, y) = f (t0 , y0 ) + (t ‚àí t0 )

‚àÇf
‚àÇf
(t0 , y0 ) + (y ‚àí y0 ) (t0 , y0 )
‚àÇt
‚àÇy

‚àÇ2 f
(t ‚àí t0 )2 ‚àÇ 2 f
(t
,
y
)
+
(t
‚àí
t
)(y
‚àí
y
)
(t0 , y0 )
0
0
0
0
2
‚àÇt 2
‚àÇt‚àÇ y

+

(y ‚àí y0 )2 ‚àÇ 2 f
(t0 , y0 ) + ¬∑ ¬∑ ¬∑
2
‚àÇ y2
‚é°
‚é§
n
n
1
n
‚àÇ
f
(t ‚àí t0 )n‚àí j (y ‚àí y0 ) j n‚àí j j (t0 , y0 )‚é¶
+‚é£
n! j=0 j
‚àÇt ‚àÇ y

+

\
n+1

n+1
‚àÇ n+1 f
1
Rn (t, y) =
(t ‚àí t0 )n+1‚àí j (y ‚àí y0 ) j n+1‚àí j j (Œæ, Œº).
(n + 1)! j=0
‚àÇt
‚àÇy
j
/DIXQFLyQ Pn (t, y) recibe el nombre del en√©simo polinomio de Taylor en dos variablesSDUDODIXQFLyQf cerca de (t0 , y0 ), y Rn (t, y) es el t√©rmino restante asociado con Pn (t, y)
Ejemplo 1

Determine P2 (t, y)HOVHJXQGRSROLQRPLRGH7D\ORUFHUFDGH  SDUDODIXQFLyQ

f (t, y) = exp ‚àí

(y ‚àí 3)2
(t ‚àí 2)2
‚àí
cos(2t + y ‚àí 7).
4
4

Soluci√≥n Para determinar P2 (t, y)QHFHVLWDPRVORVYDORUHVGHf\VXSULPHUD\VHJXQGDGHULYDGDVSDUFLDOHVHQ  7HQHPRVORVLJXLHQWH

f (t, y) = exp ‚àí

(t ‚àí 2)2
(y ‚àí 3)2
exp ‚àí
cos(2(t ‚àí 2) + (y ‚àí 3))
4
4

f (2, 3) = e(‚àí0 /4‚àí0 /4) cos(4 + 3 ‚àí 7) = 1,
2

2

(t ‚àí 2)2
(y ‚àí 3)2
‚àÇf
(t, y) = exp ‚àí
exp ‚àí
‚àÇt
4
4
+

1
(t ‚àí 2) cos(2(t ‚àí 2) + (y ‚àí 3))
2

1
(sen(2(t ‚àí 2) + (y ‚àí 3))
2

‚àÇf
(2, 3) = 0,
‚àÇt
(t ‚àí 2)2
(y ‚àí 3)2
‚àÇf
(t, y) = exp ‚àí
exp ‚àí
‚àÇy
4
4

1
(y ‚àí 3) cos(2(t ‚àí 2)
2

+ (y ‚àí 3)) + sen(2(t ‚àí 2) + (y ‚àí 3))
‚àÇf
(2, 3) = 0,
‚àÇy

5.4

‚àÇ2 f
(t ‚àí 2)2
(y ‚àí 3)2
(t,
y)
=
exp
‚àí
exp
‚àí
‚àÇt 2
4
4

‚àí

M√©todo Runge-Kutta

211

9 (t ‚àí 2)2
+
2
4

√ó cos(2(t ‚àí 2) + (y ‚àí 3)) + 2(t ‚àí 2) sen(2(t ‚àí 2) + (y ‚àí 3))
9
‚àÇ2 f
(2, 3) = ‚àí ,
‚àÇt 2
2
(t ‚àí 2)2
(y ‚àí 3)2
‚àÇ2 f
(t,
y)
=
exp
‚àí
exp
‚àí
‚àÇ y2
4
4

‚àí

3 (y ‚àí 3)2
+
2
4

√ó cos(2(t ‚àí 2) + (y ‚àí 3)) + (y ‚àí 3) sen(2(t ‚àí 2) + (y ‚àí 3))
3
‚àÇ2 f
(2, 3) = ‚àí ,
‚àÇ y2
2
\

‚àÇ2 f
(t ‚àí 2)2
(y ‚àí 3)2
(t, y) = exp ‚àí
exp ‚àí
‚àÇt‚àÇ y
4
4
√ó cos(2(t ‚àí 2) + (y ‚àí 3)) +

‚àí2+

(t ‚àí 2)(y ‚àí 3)
4

(t ‚àí 2)
+ (y ‚àí 3) sen(2(t ‚àí 2) + (y ‚àí 3)
2

‚àÇ2 f
(2, 3) = ‚àí2.
‚àÇt‚àÇ y
Por lo que,
P2 (t, y) = f (2, 3) + (t ‚àí 2)
+ (t ‚àí 2)(y ‚àí 3)

‚àÇf
(t ‚àí 2)2 ‚àÇ 2 f
‚àÇf
(2, 3) + (y ‚àí 3) (2, 3) +
(2, 3)
‚àÇt
‚àÇy
2
‚àÇt 2

(y ‚àí 3) ‚àÇ 2 f
‚àÇ2 f
(2, 3) +
(2, 3)
‚àÇt‚àÇ y
2
‚àÇ y2

9
3
= 1 ‚àí (t ‚àí 2)2 ‚àí 2(t ‚àí 2)(y ‚àí 3) ‚àí (y ‚àí 3)2 .
4
4
8QDLOXVWUDFLyQGHODSUHFLVLyQGHP2 (t, y)FHUFDGH  VHREVHUYDHQOD√ÄJXUD
Figura 5.5

P2(t, y) 5 12
f (t, y)

9
3
(t 2 2)2 2 2(t 2 2)(y 2 3) 2
4
4
t

(y 2 3)2

f(t, y) 5 exp {2(t 2 2) 2/4 2 (y 2 3) 2/4} cos (2t 1 y 2 7)

y

212

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

M√©todos de Runge-Kutta de orden 2
(O SULPHU SDVR SDUD GHGXFLU XQ PpWRGR 5XQJH.XWWD HV GHWHUPLQDU ORV YDORUHV SDUD
a1 , Œ±1 , y Œ≤1 con la propiedad de que a1 f (t + Œ±1 , y + Œ≤1 ) se aproxima a

T (2) (t, y) = f (t, y) +

h
f (t, y),
2

FRQHUURUQRPD\RUDO(h TXHHVLJXDODORUGHQGHOHUURUGHWUXQFDPLHQWRORFDOSDUDHO
PpWRGRGH7D\ORUGHRUGHQ<DTXH

f (t, y) =

‚àÇf
‚àÇf
df
(t, y) =
(t, y) +
(t, y) ¬∑ y (t)
dt
‚àÇt
‚àÇy

y

y (t) = f (t, y),

tenemos

T (2) (t, y) = f (t, y) +

h ‚àÇf
h ‚àÇf
(t, y) +
(t, y) ¬∑ f (t, y).
2 ‚àÇt
2 ‚àÇy



Al expandir f (t + Œ±1 , y + Œ≤1 ) HQ VX SROLQRPLR GH 7D\ORU GH JUDGR  FHUFD GH t y)
obtenemos

a1 f (t + Œ±1 , y + Œ≤1 ) = a1 f (t, y) + a1 Œ±1
+ a 1 Œ≤1

‚àÇf
(t, y)
‚àÇt

‚àÇf
(t, y) + a1 ¬∑ R1 (t + Œ±1 , y + Œ≤1 ),
‚àÇy



donde

R1 (t + Œ±1 , y + Œ≤1 ) =

Œ≤12 ‚àÇ 2 f
‚àÇ2 f
Œ±12 ‚àÇ 2 f
(Œæ,
Œº)
+
(Œæ,
Œº)
+
Œ±
Œ≤
(Œæ, Œº),
1
1
2 ‚àÇt 2
‚àÇt‚àÇ y
2 ‚àÇ y2



SDUDDOJXQDVj entre t\ t + Œ±1 y Œº entre y\ y + Œ≤1 .
$ODMXVWDUORVFRH√ÄFLHQWHVGHf\VXVGHULYDGDVHQODVHFXDFLRQHV  \  REWHQHmos las tres ecuaciones

f (t, y) : a1 = 1;

‚àÇf
h
(t, y) : a1 Œ±1 = ;
‚àÇt
2

y

‚àÇf
h
(t, y) : a1 Œ≤1 = f (t, y).
‚àÇy
2

Los par√°metros a1 , Œ±1 , y Œ≤1VRQSRUORWDQWR

a1 = 1,

Œ±1 =

h
,
2

y Œ≤1 =

h
f (t, y),
2

por lo que

T (2) (t, y) = f

t+

h
h
h
h
, y + f (t, y) ‚àí R1 t + , y + f (t, y) ,
2
2
2
2

\DSDUWLUGHODHFXDFLyQ 

R1 t +

h
h
, y + f (t, y)
2
2

=

h2 ‚àÇ 2 f
h2
‚àÇ2 f
(Œæ, Œº) +
f (t, y)
(Œæ, Œº)
2
8 ‚àÇt
4
‚àÇt‚àÇ y
+

‚àÇ2 f
h2
( f (t, y))2 2 (Œæ, Œº).
8
‚àÇy

5.4

M√©todo Runge-Kutta

213

6LWRGDVODVGHULYDGDVSDUFLDOHVGHVHJXQGRRUGHQGHfHVWiQDFRWDGDVHQWRQFHV

R1 t +

h
h
, y + f (t, y)
2
2

es O(h (QFRQVHFXHQFLD
¬á (ORUGHQGHHUURUSDUDHVWHQXHYRPpWRGRHVLJXDODOGHOPpWRGRGH7D\ORUGHRUGHQ
(OPpWRGRGHHFXDFLyQGHGLIHUHQFLDTXHUHVXOWDGHUHHPSOD]DUT (2) (t, y) en el m√©todo
GH7D\ORUGHRUGHQSRU f (t + (h/2), y + (h/2) f (t, y))HVXQPpWRGR5XQJH.XWWDHVSHFt√ÄFRFRQRFLGRFRPRm√©todo de punto medio.

M√©todo de punto medio
w 0 = Œ±,
w i+1 = w i + h f

ti +

h
h
, w i + f (ti , w i ) ,
2
2

para i = 0, 1, . . . , N ‚àí 1.

6RODPHQWHVHHQFXHQWUDQWUHVSDUiPHWURVHQ a1 f (t + Œ±1 , y + Œ≤1 )\WRGRVVRQQHFHVDULRVSDUDDMXVWDUT  3RUORTXHVHUHTXLHUHXQDIRUPDPiVFRPSOLFDGDSDUDVDWLVIDFHUODV
FRQGLFLRQHVSDUDFXDOTXLHUDGHORVPpWRGRVGH7D\ORUGHRUGHQVXSHULRU
/DIRUPDGHFXDWURSDUiPHWURVPiVDGHFXDGDSDUDDSUR[LPDU

T (3) (t, y) = f (t, y) +

h
h2
f (t, y) +
f (t, y)
2
6

es

a1 f (t, y) + a2 f (t + Œ±2 , y + Œ¥2 f (t, y)),



HLQFOXVRFRQHVWRQRKD\VX√ÄFLHQWH√ÅH[LELOLGDGSDUDDMXVWDUHOWpUPLQR
2
h2 ‚àÇ f
(t, y) f (t, y),
6 ‚àÇy

ORFXDOUHVXOWDHQODH[SDQVLyQGH (h 2 /6) f (t, y)3RUFRQVLJXLHQWHORPHMRUTXHVHSXHGH
REWHQHUDOXVDU  VRQPpWRGRVFRQHUURUGHWUXQFDPLHQWRORFDOO(h 
6LQ HPEDUJR HO KHFKR GH TXH   WHQJD FXDWUR SDUiPHWURV SURSRUFLRQD XQD √ÅH[LELOLGDG HQ VX HOHFFLyQ SRU OR TXH VH SXHGH GHULYDU XQD VHULH GH PpWRGRV O(h  8QR GH
los m√°s importantes es el PpWRGR PRGL√ÄFDGR GH (XOHU TXH FRUUHVSRQGH D VHOHFFLRQDU
a1 = a2 = 12 y Œ±2 = Œ¥2 = h. eVWHWLHQHODVLJXLHQWHIRUPDGHHFXDFLyQGHGLIHUHQFLD

M√©todo modiÔ¨Åcado de Euler
w 0 = Œ±,
w i+1 = w i +
Ejemplo 2

h
[ f (ti , w i ) + f (ti+1 , w i + h f (ti , w i ))],
2

para i = 0, 1, . . . , N ‚àí 1.

8VHORV m√©todoVGHSXQWRPHGLR\PRGL√ÄFDGRGH(XOHUFRQ N = 10, h = 0.2, ti = 0.2i,\
w 5SDUDDSUR[LPDUODVROXFLyQGHQXHVWURHMHPSORKDELWXDO

y = y ‚àí t 2 + 1,

0 ‚â§ t ‚â§ 2,

y(0) = 0.5.

214

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias
Soluci√≥n /DVHFXDFLRQHVGHGLIHUHQFLDSURGXFLGDVDSDUWLUGHODVGLIHUHQWHVIyUPXODVVRQ

mpWRGRGHSXQWRPHGLRw i+1 = 1.22w i ‚àí 0.0088i 2 ‚àí 0.008i + 0.218
\
mpWRGRPRGL√ÄFDGRGH(XOHUw i+1 = 1.22w i ‚àí 0.0088i 2 ‚àí 0.008i + 0.216,
Para cada i = 0, 1, . . . , 9. Los primeros dos pasos de estos m√©todos nos dan
mpWRGRGHSXQWRPHGLRw 1 = 1.22(0.5) ‚àí 0.0088(0)2 ‚àí 0.008(0) + 0.216 = 0.826
\
mpWRGRPRGL√ÄFDGRGH(XOHUw 1 = 1.22(0.5) ‚àí 0.0088(0)2 ‚àí 0.008(0) + 0.218 = 0.828
\
mpWRGRGHSXQWRPHGLRw 2 = 1.22(0.828) ‚àí 0.0088(0.2)2 ‚àí 0.008(0.2) + 0.218

= 1.21136
\
mpWRGRPRGL√ÄFDGRGH(XOHUw 2 = 1.22(0.826) ‚àí 0.0088(0.2)2 ‚àí 0.008(0.2) + 0.216

= 1.20692.
/DWDEODHQXPHUDWRGRVORVUHVXOWDGRVGHORVFiOFXORV3DUDHVWHSUREOHPDHOPpWRGR
GHSXQWRPHGLRHVVXSHULRUDOPpWRGRPRGL√ÄFDGRGH(XOHU

Tabla 5.6
ti

y(ti )

M√©todo de
punto medio

Error

M√©todo modificado
de Euler

Error

0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0

0.5000000
0.8292986
1.2140877
1.6489406
2.1272295
2.6408591
3.1799415
3.7324000
4.2834838
4.8151763
5.3054720

0.5000000
0.8280000
1.2113600
1.6446592
2.1212842
2.6331668
3.1704634
3.7211654
4.2706218
4.8009586
5.2903695

0
0.0012986
0.0027277
0.0042814
0.0059453
0.0076923
0.0094781
0.0112346
0.0128620
0.0142177
0.0151025

0.5000000
0.8260000
1.2069200
1.6372424
2.1102357
2.6176876
3.1495789
3.6936862
4.2350972
4.7556185
5.2330546

0
0.0032986
0.0071677
0.0116982
0.0169938
0.0231715
0.0303627
0.0387138
0.0483866
0.0595577
0.0724173

M√©todos de Runge-Kutta de orden superior
El t√©rmino T  (ty) se puede aproximar con error O(h PHGLDQWHXQDH[SUHVLyQGHODIRUPD

f (t + Œ±1 , y + Œ¥1 f (t + Œ±2 , y + Œ¥2 f (t, y))),

.DUO+HXQ ¬≤ IXH
XQSURIHVRUGHOD7HFKQLFDO
8QLYHUVLW\RI.DUOVUXKH3UHVHQWy
esta t√©cnica en un art√≠culo
SXEOLFDGRHQ>+HX@

UHODFLRQDGDFRQFXDWURSDUiPHWURV\HOiOJHEUDLPSOLFDGDHQODGHWHUPLQDFLyQGHŒ±1 , Œ¥1 , Œ±2
\Œ¥2HVEDVWDQWHWHGLRVD(OPpWRGRO(h) m√°s com√∫n es el GH+HXQGDGRSRU

w0 = Œ±
, w i + 2h
f ti + h3 , w i + h3 f (ti , w i )
w i+1 = w i + h4 f (ti , w i ) + 3 f ti + 2h
3
3
para i = 0, 1, . . . , N ‚àí 1.

,

5.4

Ilustraci√≥n

M√©todo Runge-Kutta

215

$ODSOLFDUHOPpWRGRGH+HXQFRQN = 10, h = 0.2, ti = 0.2i, y w 0 = 0.5 para aproximar
ODVROXFLyQDQXHVWURHMHPSORKDELWXDO

y = y ‚àí t 2 + 1,

0 ‚â§ t ‚â§ 2,

y(0) = 0.5,

GDORVYDORUHVHQODWDEOD2EVHUYHHOHUURUUHGXFLGRDORODUJRGHOUDQJRVREUHODVDSUR[LPDFLRQHVGHSXQWRPHGLR\(XOHUPRGL√ÄFDGR

Tabla 5.7
ti

y(ti )

M√©todo
de Heun

Error

0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0

0.5000000
0.8292986
1.2140877
1.6489406
2.1272295
2.6408591
3.1799415
3.7324000
4.2834838
4.8151763
5.3054720

0.5000000
0.8292444
1.2139750
1.6487659
2.1269905
2.6405555
3.1795763
3.7319803
4.2830230
4.8146966
5.3050072

0
0.0000542
0.0001127
0.0001747
0.0002390
0.0003035
0.0003653
0.0004197
0.0004608
0.0004797
0.0004648

(QJHQHUDOORVPpWRGRVGH5XQJH.XWWDGHRUGHQQRVHXVDQ(OPpWRGR5XQJH.XWWD
TXHVHXVDGHPDQHUDFRP~QHVGHRUGHQHQIRUPDGHHFXDFLyQGHGLIHUHQFLDGDGRFRPR
VLJXH
Runge-Kutta de orden 4

w 0 = Œ±,
k1 = h f (ti , w i ),
k2 = h f

ti +

h
1
, w i + k1 ,
2
2

k3 = h f

ti +

h
1
, w i + k2 ,
2
2

k4 = h f (ti+1 , w i + k3 ),
1
w i+1 = w i + (k1 + 2k2 + 2k3 + k4 ),
6
para cada i = 0, 1, . . . , N ‚àí 1. Este m√©todo tiene error de truncamiento local O(h) siemSUH\FXDQGRODVROXFLyQy(t WHQJDFLQFRGHULYDGDVFRQWLQXDV,QWURGXFLPRVHQHOPpWRGROD
QRWDFLyQkkkkSDUDHOLPLQDUODQHFHVLGDGGHDQLGDGRVXFHVLYRHQODVHJXQGDYDULDEOH
de f (ty (OHMHUFLFLRPXHVWUDTXpWDQFRPSOLFDGRVHYXHOYHHVWHDQLGDGR
(ODOJRULWPRLPSOHPHQWDHOPpWRGR5XQJH.XWWDGHRUGHQ
ALGORITMO

5.2

M√©todo Runge-Kutta (orden 4)
3DUDDSUR[LPDUODVROXFLyQGHOSUREOHPDGHYDORULQLFLDO

y = f (t, y),

a ‚â§ t ‚â§ b,

y(a) = Œ±,

en (N + 1) n√∫meros espaciados equitativamente en el intervalo[a, b]:
ENTRADA extremos a, b; entero N; condici√≥n inicial Œ±.
SALIDA aproximaci√≥n w para y en los valores (N + 1) de t.

216

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

Paso 1 Determine h = (b ‚àí a)/N ;
t = a;
w = Œ±;
SALIDA (t, w).
Paso 2 Para i = 1, 2, . . . , N haga los pasos 3‚Äì5.
Paso 3 Determine K 1 = h f (t, w);
K 2 = h f (t + h/2, w + K 1 /2);
K 3 = h f (t + h/2, w + K 2 /2);
K 4 = h f (t + h, w + K 3 ).
Paso 4 Determine w = w + (K 1 + 2K 2 + 2K 3 + K 4 )/6; (calcule wi .)
t = a + i h. (Calcule ti .)
Paso 5 SALIDA (t, w).
Paso 6 PARE.

Ejemplo 3

8WLOLFHHOPpWRGR5XQJH.XWWDGHRUGHQFRQ h = 0.2, N = 10, y ti = 0.2i para obtener
DSUR[LPDFLRQHVSDUDODVROXFLyQGHOSUREOHPDGHYDORULQLFLDO

y = y ‚àí t 2 + 1,

0 ‚â§ t ‚â§ 2,

y(0) = 0.5.

Soluci√≥n /DDSUR[LPDFLyQSDUDy  VHREWLHQHPHGLDQWH

w 0 = 0.5
k1 = 0.2 f (0, 0.5) = 0.2(1.5) = 0.3
k2 = 0.2 f (0.1, 0.65) = 0.328
k3 = 0.2 f (0.1, 0.664) = 0.3308
k4 = 0.2 f (0.2, 0.8308) = 0.35816
1
w 1 = 0.5 + (0.3 + 2(0.328) + 2(0.3308) + 0.35816) = 0.8292933.
6
/RVUHVXOWDGRVUHVWDQWHV\VXVHUURUHVVHPXHVWUDQHQODWDEOD

Tabla 5.8
ti

Exacto
yi = y(ti )

Runge-Kutta
orden 4
wi

Error
|yi ‚àí w i |

0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0

0.5000000
0.8292986
1.2140877
1.6489406
2.1272295
2.6408591
3.1799415
3.7324000
4.2834838
4.8151763
5.3054720

0.5000000
0.8292933
1.2140762
1.6489220
2.1272027
2.6408227
3.1798942
3.7323401
4.2834095
4.8150857
5.3053630

0
0.0000053
0.0000114
0.0000186
0.0000269
0.0000364
0.0000474
0.0000599
0.0000743
0.0000906
0.0001089

5.4

M√©todo Runge-Kutta

217

Comparaciones computacionales
(OSULQFLSDOHVIXHU]RFRPSXWDFLRQDODODSOLFDUORVPpWRGRVGH5XQJH.XWWDHVODHYDOXDFLyQ
de f(QORVPpWRGRVGHVHJXQGRRUGHQHOHUURUGHWUXQFDPLHQWRORFDOHVO(h \HOFRVWRHV
GRVHYDOXDFLRQHVGHIXQFLyQSRUSDVR(OPpWRGR5XQJH.XWWDGHRUGHQUHTXLHUHFXDWUR
HYDOXDFLRQHVSRUSDVR\HOHUURUGHWUXQFDPLHQWRORFDOHVO(h %XWFKHU FRQVXOWH>%XW@
SDUDXQUHVXPHQ KDHVWDEOHFLGRODUHODFLyQHQWUHHOQ~PHURGHHYDOXDFLRQHVSRUSDVR\HO
RUGHQGHOHUURUGHWUXQFDPLHQWRORFDOPRVWUDGRHQODWDEOD(VWDWDEODLQGLFDSRUTXpORV
PpWRGRVGHRUGHQPHQRUDFLQFRFRQWDPDxRGHSDVRPiVSHTXHxRVHXVDQSUHIHUHQWHPHQWH
SDUDORVPpWRGRVGHRUGHQVXSHULRUSRUPHGLRGHXQWDPDxRGHSDVRPiVJUDQGH

Tabla 5.9

Evaluaciones por paso

2

3

4

5‚â§n‚â§7

8‚â§n‚â§9

10 ‚â§ n

Mejor error de truncamiento local posible

O(h 2 )

O(h 3 )

O(h 4 )

O(h n‚àí1 )

O(h n‚àí2 )

O(h n‚àí3 )

8QDPHGLGDSDUDFRPSDUDUORVPpWRGRVGH5XQJH.XWWDGHRUGHQLQIHULRUVHGHVFULEH
FRPRVLJXH
¬á 0LHQWUDV HO PpWRGR 5XQJH.XWWD GH RUGHQ  UHTXLHUH FXDWUR HYDOXDFLRQHV SRU SDVR HO
PpWRGR GH (XOHU VyOR UHTXLHUH XQD HYDOXDFLyQ 3RU OR WDQWR VL HO PpWRGR 5XQJH.XWWD
GHRUGHQYDDVHUVXSHULRUGHEHUtDQSURSRUFLRQDUVHUHVSXHVWDVPiVSUHFLVDVTXHODVGHO
PpWRGRGH(XOHUFRQXQFXDUWRGHOWDPDxRGHSDVR'HLJXDOIRUPDVLHOPpWRGR5XQJH
.XWWDGHRUGHQYDDVHUVXSHULRUDORVPpWRGRVGH5XQJH.XWWDGHVHJXQGRRUGHQTXH
UHTXLHUHQGRVHYDOXDFLRQHVSRUSDVRGHEHUtDSURSRUFLRQDUPD\RUSUHFLVLyQFRQXQWDPDxR
de paso hTXHXQPpWRGRGHVHJXQGRRUGHQFRQXQWDPDxRGHSDVRh/2.
/RVLJXLHQWHLOXVWUDODVXSHULRULGDGGHOPpWRGR5XQJH.XWWDGHFXDUWRRUGHQSRUPHGLR
GHHVWDPHGLGDSDUDHOSUREOHPDGHYDORULQLFLDOTXHKHPRVFRQVLGHUDGR
Ilustraci√≥n

Para el problema

y = y ‚àí t 2 + 1,

0 ‚â§ t ‚â§ 2,

y(0) = 0.5,

el m√©todo de Euler con h 5HOPpWRGRGHSXQWRPHGLRFRQh 5\HOPpWRGR
5XQJH.XWWDGHFXDUWRRUGHQFRQh 5VHFRPSDUDQHQORVSXQWRVGHPDOODFRPXQHVGH
HVWRVPpWRGRV\&DGDXQDGHHVWDVWpFQLFDVUHTXLHUHHYDOXDFLRQHV
GHIXQFLyQSDUDGHWHUPLQDUORVYDORUHVHQXPHUDGRVHQODWDEODSDUDDSUR[LPDUy  
(QHVWHHMHPSORHOPpWRGRGHFXDUWRRUGHQHVFODUDPHQWHVXSHULRU

Tabla 5.10
ti

Exacto

Euler
h = 0.025

0.0
0.1
0.2
0.3
0.4
0.5

0.5000000
0.6574145
0.8292986
1.0150706
1.2140877
1.4256394

0.5000000
0.6554982
0.8253385
1.0089334
1.2056345
1.4147264

Euler
modificado
h = 0.05

Runge-Kutta
de orden 4
h = 0.1

0.5000000
0.6573085
0.8290778
1.0147254
1.2136079
1.4250141

0.5000000
0.6574144
0.8292983
1.0150701
1.2140869
1.4256384

La secci√≥n Conjunto de ejercicios 5.4 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

218

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

5.5 Control de error y m√©todo Runge-Kutta-Fehlberg

7DOYH]OHJXVWDUtDUHYLVDU
el material sobre cuadratura
DGDSWDEOHHQODVHFFLyQDQWHV
GHFRQVLGHUDUHVWHPDWHULDO

(QODVHFFLyQREVHUYDPRVHOXVRDSURSLDGRGHWDPDxRVGHSDVRYDULDEOHVSDUDTXHODV
DSUR[LPDFLRQHVGHLQWHJUDOHVSURGX]FDQPpWRGRVH√ÄFLHQWHV3RUVtPLVPRVTXL]iQRIXHUDQ
VX√ÄFLHQWHVSDUDIDYRUHFHUHVWRVPpWRGRVGHELGRDODXPHQWRGHFRPSOLFDFLyQTXHVXUJHDO
DSOLFDUORV6LQHPEDUJRWLHQHQRWUDFDUDFWHUtVWLFDTXHOHVGDJUDQYDORU(QHOSURFHGLPLHQWR
GHWDPDxRGHSDVRVHLQFOX\HXQFiOFXORGHOHUURUGHWUXQFDPLHQWRTXHQRUHTXLHUHODDSUR[LPDFLyQGHGHULYDGDVVXSHULRUHVGHODIXQFLyQ(VWRVPpWRGRVUHFLEHQHOQRPEUHGHadaptablesSRUTXHDGDSWDQHOQ~PHUR\ODSRVLFLyQGHORVQRGRVXWLOL]DGRVHQODDSUR[LPDFLyQSDUD
JDUDQWL]DUTXHHOHUURUGHWUXQFDPLHQWRVHPDQWLHQHGHQWURGHXQOtPLWHHVSHFt√ÄFR
([LVWH XQD FRQH[LyQ FHUFDQD HQWUH HO SUREOHPD GH DSUR[LPDFLyQ GHO YDORU GH XQD LQWHJUDOGH√ÄQLGD\HOGHDSUR[LPDFLyQGHODVROXFLyQGHXQSUREOHPDGHYDORULQLFLDO1RHV
VRUSUHQGHQWH HQWRQFHV TXH H[LVWDQ PpWRGRV DGDSWDEOHV SDUD DSUR[LPDU ODV VROXFLRQHV GH
ORVSUREOHPDVGHYDORULQLFLDO\TXHHVWRVPpWRGRVQRVyORVHDQH√ÄFLHQWHVVLQRWDPELpQTXH
LQFOX\DQHOFRQWUROGHHUURU
&XDOTXLHUPpWRGRGHXQSDVRSDUDDSUR[LPDUODVROXFLyQy(t GHOSUREOHPDGHYDORU
inicial

y = f (t, y),

para a ‚â§ t ‚â§ b,

con y(a) = Œ±,

VHSXHGHH[SUHVDUHQODIRUPD

w i+1 = w i + h i œÜ(ti , w i , h i ),

para i = 0, 1, . . . , N ‚àí 1,

SDUDDOJXQDIXQFLyQœÜ
8QPpWRGRLGHDOSDUDODHFXDFLyQGHGLIHUHQFLD

w i+1 = w i + h i œÜ(ti , w i , h i ),

i = 0, 1, . . . , N ‚àí 1,

SDUDDSUR[LPDUODVROXFLyQy(t GHOSUREOHPDGHYDORULQLFLDO

y = f (t, y),

a ‚â§ t ‚â§ b,

y(a) = Œ±,

WHQGUtDODSURSLHGDGGHTXHGDGDXQDWROHUDQFLD Œµ > 0, un n√∫mero m√≠nimo de puntos de
PDOODVHSXHGHXVDUSDUDJDUDQWL]DUTXHHOHUURUJOREDO |y(ti ) ‚àí w i |QRH[FHGD∆§SDUDQLQJXQDi 5   N1RVRUSUHQGHTXHWHQHUXQQ~PHURPtQLPRGHSXQWRVGHPDOOD\
WDPELpQFRQWURODUHOHUURUJOREDOGHXQPpWRGRGHGLIHUHQFLDVHDLQFRQVLVWHQWHFRQORVSXQWRV
LJXDOPHQWHHVSDFLDGRVHQHOLQWHUYDOR(QHVWDVHFFLyQH[DPLQDPRVWpFQLFDVTXHVHXVDQSDUD
FRQWURODUHOHUURUGHXQPpWRGRGHHFXDFLyQGHGLIHUHQFLDGHPDQHUDH√ÄFLHQWHPHGLDQWHOD
HOHFFLyQDGHFXDGDGHSXQWRVGHPDOOD
$SHVDUGHTXHHQJHQHUDOQRSRGDPRVGHWHUPLQDUHOHUURUJOREDOGHXQPpWRGRHQOD
VHFFLyQREVHUYDUHPRVTXHH[LVWHXQDFRQH[LyQFHUFDQDHQWUHHOHUURUGHWUXQFDPLHQWR
ORFDO\HOHUURUJOREDO$OXWLOL]DUPpWRGRVGHGLIHUHQWHRUGHQSRGHPRVSUHGHFLUHOHUURUGH
WUXQFDPLHQWRORFDO\DOXVDUHVWDSUHGLFFLyQVHOHFFLRQDUXQWDPDxRGHSDVRTXHFRQWUROHDO
HUURUJOREDO
3DUDLOXVWUDUODWpFQLFDVXSRQJDTXHWHQHPRVGRVWpFQLFDVGHDSUR[LPDFLyQ/DSULPHUD
VHREWLHQHDSDUWLUGHOPpWRGRGH7D\ORUGHHQpVLPRRUGHQGHODIRUPD

y(ti+1 ) = y(ti ) + hœÜ(ti , y(ti ), h) + O(h n+1 )
\SURGXFHDSUR[LPDFLRQHVFRQHUURUGHWUXQFDPLHQWRORFDO œÑi+1 (h) = O(h n )eVWDVVRQGDdas por

w0 = Œ±
w i+1 = w i + hœÜ(ti , w i , h),

para i > 0.

(QJHQHUDOHOPpWRGRVHJHQHUDDODSOLFDUODPRGL√ÄFDFLyQGH5XQJH.XWWDSDUDHOPpWRGRGH
7D\ORUSHURODGHULYDFLyQHVSHFt√ÄFDQRHVLPSRUWDQWH

5.5 Control de error y m√©todo Runge-Kutta-Fehlberg

219

(O VHJXQGR PpWRGR HV VLPLODU SHUR GH XQ RUGHQ VXSHULRU SURYLHQH GH XQ PpWRGR GH
7D\ORUGH n 1 RUGHQGHODIRUPD

y(ti+1 ) = y(ti ) + h œÜÃÉ(ti , y(ti ), h) + O(h n+2 )
\SURGXFHDSUR[LPDFLRQHVFRQHOHUURUGHWUXQFDPLHQWRORFDO œÑÃÉi+1 (h) = O(h n+1 )(VWRHV
dado por

wÃÉ 0 = Œ±
wÃÉ i+1 = wÃÉ i + h œÜÃÉ(ti , wÃÉ i , h),

para i > 0.

Primero suponemos que w i ‚âà y(ti ) ‚âà wÃÉ i\VHOHFFLRQDPRVXQWDPDxRGHSDVR√ÄMRh para
JHQHUDUODVDSUR[LPDFLRQHVw i+1 y wÃÉ i+1 to y(ti+1 ). Entonces

y(ti+1 ) ‚àí y(ti )
‚àí œÜ(ti , y(ti ), h)
h
y(ti+1 ) ‚àí w i
=
‚àí œÜ(ti , w i , h)
h
y(ti+1 ) ‚àí [w i + hœÜ(ti , w i , h)]
=
h
1
= (y(ti+1 ) ‚àí w i+1 ).
h

œÑi+1 (h) =

'HPDQHUDVLPLODUWHQHPRV

œÑÃÉi+1 (h) =

1
(y(ti+1 ) ‚àí wÃÉ i+1 ).
h

&RPRFRQVHFXHQFLDWHQHPRV

1
(y(ti+1 ) ‚àí w i+1 )
h
1
= [(y(ti+1 ) ‚àí wÃÉ i+1 ) + (wÃÉ i+1 ‚àí w i+1 )]
h
1
= œÑÃÉi+1 (h) + (wÃÉ i+1 ‚àí w i+1 ).
h

œÑi+1 (h) =

Pero œÑi+1 (h) es O(h n ) y œÑÃÉi+1 (h) es O(h n+1 ) SRU OR TXH OD SDUWH VLJQL√ÄFDWLYD GH œÑi+1 (h)
debe provenir de

1
(wÃÉ i+1 ‚àí w i+1 ) .
h
(VWRQRVGDXQDDSUR[LPDFLyQTXHVHSXHGHFDOFXODUIiFLOPHQWHSDUDHOPpWRGRGHHUURUGH
truncamiento local de O(hn 

œÑi+1 (h) ‚âà

1
(wÃÉ i+1 ‚àí w i+1 ) .
h

6HDR = h1 |wÃÉ i+1 ‚àí w i+1 |.
6LQHPEDUJRHOREMHWLYRQRHVVLPSOHPHQWHFDOFXODUHOHUURUGHWUXQFDPLHQWRORFDOVLQR
DMXVWDUHOWDPDxRGHSDVRSDUDPDQWHQHUORGHQWURGHXQDFRWDHVSHFt√ÄFD3DUDKDFHUORDVXmimos que como œÑi+1 (h) es O(hn H[LVWHXQQ~PHURKLQGHSHQGLHQWHGHhFRQ

œÑi+1 (h) ‚âà K h n .

220

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

(QWRQFHV HO HUURU GH WUXQFDPLHQWR ORFDO SURGXFLGR DO DSOLFDU HO PpWRGR GH HQpVLPR
orden con un tama√±o de paso qhVHSXHGHFDOFXODUDWUDYpVGHODVDSUR[LPDFLRQHVRULJLQDOHV
w i+1 y wÃÉ i+1 :

œÑi+1 (qh) ‚âà K (qh)n = q n (K h n ) ‚âà q n œÑi+1 (h) ‚âà

qn
(wÃÉ i+1 ‚àí w i+1 ).
h

La cota œÑi+1 (qh) para ŒµVHOHFFLRQDPRVqGHWDOIRUPDTXH

qn
|wÃÉ i+1 ‚àí w i+1 | ‚âà |œÑi+1 (qh)| ‚â§ Œµ,
h
HVGHFLUWDOTXH

q‚â§

Œµh
|wÃÉ i+1 ‚àí w i+1 |

1/n

=

1/n

R

.



M√©todo Runge-Kutta-Fehlberg
(UZLQ)HKOEHUJGHVDUUROOypVWD
\RWUDVWpFQLFDVGHFRQWUROGH
HUURUPLHQWUDVWUDEDMDEDHQODV
LQVWDODFLRQHVGHOD1$6$HQ
+XQWVYLOOH$ODEDPDGXUDQWHOD
GpFDGDGH(QUHFLELy
SRUVXWUDEDMRODPHGDOODSRU
ORJURVFLHQWt√ÄFRVH[FHSFLRQDOHV
GHOD1$6$

8QDWpFQLFDSRSXODUTXHXVDODGHVLJXDOGDG  SDUDFRQWUROGHHUURUHVHOm√©todo RungeKutta-Fehlberg FRQVXOWH>)H@ (VWDWpFQLFDXWLOL]DHOPpWRGR5XQJH.XWWDFRQHUURUGH
WUXQFDPLHQWRORFDOGHRUGHQ

wÃÉ i+1 = w i +

16
6656
28561
9
2
k1 +
k3 +
k4 ‚àí k5 + k6 ,
135
12825
56430
50
55

SDUDFDOFXODUHOHUURUORFDOHQXQPpWRGR5XQJH.XWWDGHRUGHQGDGRSRU

w i+1 = w i +

25
1408
2197
1
k1 +
k3 +
k4 ‚àí k5 ,
216
2565
4104
5

GRQGHORVFRH√ÄFLHQWHVGHODVHFXDFLRQHVVRQ

k1 = h f (ti , w i ),
k2 = h f

ti +

h
1
, w i + k1 ,
4
4

k3 = h f

ti +

3h
3
9
, w i + k1 + k2 ,
8
32
32

k4 = h f

ti +

12h
1932
7200
7296
, wi +
k1 ‚àí
k2 +
k3 ,
13
2197
2197
2197

k5 = h f

ti + h, w i +

k6 = h f

ti +

439
3680
845
k1 ‚àí 8k2 +
k3 ‚àí
k4 ,
216
513
4104

h
8
3544
1859
11
, w i ‚àí k1 + 2k2 ‚àí
k3 +
k4 ‚àí k5 .
2
27
2565
4104
40

8QDYHQWDMDGHHVWHPpWRGRHVTXHVyORVHUHTXLHUHQVHLVHYDOXDFLRQHVGHfSRUSDVR/RV
PpWRGRVDUELWUDULRVGH5XQJH.XWWDGHyUGHQHV\XWLOL]DGRVMXQWRV FRQVXOWHODWDEOD
HQODSiJLQD UHTXLHUHQSRUORPHQRVFXDWURHYDOXDFLRQHVGHf para el m√©todo de cuarto
RUGHQ\XQVH[WRDGLFLRQDOSDUDHOPpWRGRGHTXLQWRRUGHQSDUDXQWRWDOGHSRUORPHQRV
HYDOXDFLRQHVGHIXQFLyQ3RUORTXHHOPpWRGR5XQJH.XWWD)HKOEHUJWLHQHDOPHQRV
GHGLVPLQXFLyQHQHOQ~PHURGHHYDOXDFLRQHVGHIXQFLyQVREUHHOXVRGHXQSDUGHPpWRGRV
DUELWUDULRVGHFXDUWR\TXLQWRRUGHQ
(QODWHRUtDGHFRQWUROGHHUURUXQYDORULQLFLDOGHh en el i-√©simo paso se usa para encontrar los primeros valores de w i+1 y wÃÉ i+1ORFXDOFRQGXFHDODGHWHUPLQDFLyQGHq para
HVHSDVR\HQWRQFHVVHUHSLWHQORVFiOFXORV(VWHSURFHGLPLHQWRUHTXLHUHHOGREOHGHHYDOXDFLRQHVGHIXQFLyQSRUSDVRVLQFRQWUROGHHUURU(QODSUiFWLFDHOYDORUGHq que se utilizar√°

5.5 Control de error y m√©todo Runge-Kutta-Fehlberg

221

VHVHOHFFLRQDGHXQDIRUPDGLIHUHQWHFRQHO√ÄQGHKDFHUTXHHOFRVWRGHODHYDOXDFLyQGH
IXQFLyQLQFUHPHQWDGRYDOJDODSHQD(OYDORUGHq determinado en el i-√©simo paso se utiliza
SDUDGRVSURSyVLWRV
‚Ä¢ Cuando R
UHFKD]DPRVODVHOHFFLyQLQLFLDOGHh en el i-√©simoSDVR\UHSHWLPRVORV
c√°lculos mediante qh\
‚Ä¢ Cuando R ‚â§ DFHSWDPRVHOYDORUFDOFXODGRHQHOi-√©simo paso mediante el tama√±o de
paso h, pero cambiamos el tama√±o de paso para qh para el (i  SDVR
'HELGRDODGHVYHQWDMDHQWpUPLQRVGHHYDOXDFLRQHVGHIXQFLyQTXHVHGHEHSDJDUVLORVSDVRV
VHUHSLWHQqWLHQGHDVHUVHOHFFLRQDGRGHPDQHUDFRQVHUYDGRUD'HKHFKRSDUDHOPpWRGR
5XQJH.XWWD)HKOEHUJFRQn 5XQDHOHFFLyQFRP~QHV

q=

Œµh
2|wÃÉ i+1 ‚àí w i+1 |

1/4

= 0.84

Œµh
|wÃÉ i+1 ‚àí w i+1 |

1/4

= 0.84

1/n

R

.

(QHODOJRULWPRSDUDHOPpWRGR5XQJH.XWWD)HKOEHUJVHDxDGHHOSDVRSDUDHOLPLQDU
JUDQGHVPRGL√ÄFDFLRQHVHQHOWDPDxRGHSDVR(VWRVHKDFHSDUDQRSDVDUGHPDVLDGRWLHPSR
FRQWDPDxRVGHSDVRSHTXHxRVHQUHJLRQHVFRQLUUHJXODULGDGHVHQODVGHULYDGDVGHy\SDUD
HYLWDUWDPDxRVGHSDVRJUDQGHVORFXDOSXHGHUHVXOWDUHQODRPLVLyQGHUHJLRQHVVHQVLEOHV
HQWUH ORV SDVRV (O SURFHGLPLHQWR GH LQFUHPHQWR GHO WDPDxR GH SDVR VH SXHGH HYLWDU SRU
FRPSOHWRDSDUWLUGHODOJRULWPR\HOSURFHGLPLHQWRGHGLVPLQXFLyQGHOWDPDxRGHSDVRTXH
VHXVDVyORFXDQGRVHDQHFHVDULRSDUDFRQWURODUHOHUURU

ALGORITMO

5.3

M√©todo Runge-Kutta-Fehlberg
3DUDDSUR[LPDUODVROXFLyQGHOSUREOHPDGHYDORULQLFLDO

y = f (t, y),

a ‚â§ t ‚â§ b,

y(a) = Œ±,

FRQHUURUGHWUXQFDPLHQWRORFDOGHQWURGHXQDWROHUDQFLDGHWHUPLQDGD

ENTRADA extremos a, b; condici√≥n inicial Œ±; tolerancia TOL; tama√±o de paso m√°ximo
hm√°x; tama√±o de paso m√≠nimo hm√≠n.
SALIDA t, w, h, donde w se aproxima a y(t) y se utiliza el tama√±o de paso h o un mensaje
de que se excede el tama√±o m√≠nimo de paso.
Paso 1 Determine t = a;
w = Œ±;
h = hm√°x;
FLAG = 1;
SALIDA (t, w).
Paso 2 Mientras (FLAG = 1) haga los pasos 3‚Äì11.
Paso 3 Determine K 1 = h f (t, w);
K 2 = h f t + 14 h, w + 14 K 1 ;
3
9
K 1 + 32
K2 ;
K 3 = h f t + 38 h, w + 32

h, w + 1932
K ‚àí 7200
K + 7296
K ;
K 4 = h f t + 12
13
2197 1
2197 2
2197 3
845
K 5 = h f t + h, w + 439
K ‚àí 8K 2 + 3680
K 3 ‚àí 4104
K4 ;
216 1
513
8
K 1 + 2K 2 ‚àí 3544
K + 1859
K ‚àí 11
K .
K 6 = h f t + 12 h, w ‚àí 27
2565 3
4104 4
40 5

222

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias
1
128
2197
1
2
Paso 4 Determine R = h1 | 360
K 1 ‚àí 4275
K 3 ‚àí 75240
K 4 + 50
K 5 + 55
K 6 |.

(Nota: R = h1 |wÃÉ i+1 ‚àí w i+1 | ‚âà |œÑi+1 (h)|.)
Paso 5 Si R ‚â§ TOL entonces haga los pasos 6 y 7.
Paso 6 Determine t = t + h; (aproximaci√≥n aceptada.)
25
K 1 + 1408
K + 2197
K ‚àí 15 K 5 .
w = w + 216
2565 3
4104 4

Paso 7 SALIDA (t, w, h). (Fin del paso 5 )
Paso 8 Determine Œ¥ = 0.84(TOL/R)1/4 .
Paso 9 Si Œ¥ ‚â§ 0.1 entonces haga h = 0.1h
tambi√©n si Œ¥ ‚â• 4 entonces haga h = 4h
si no haga h = Œ¥h. (Calcular h nueva.)
Paso 10 Si h > hm√°x entonces haga h = hm√°x.
Paso 11 Si t ‚â• b entonces haga FLAG = 0
tambi√©n si t + h > b entonces haga h = b ‚àí t
tambi√©n si h < hm√≠n entonces
determine FLAG = 0;
SALIDA (‚Äòh m√≠nima excedida‚Äô).
(Procedimiento completado sin √©xito.)
(Fin del paso 3)
Paso 12 (El procedimiento est√° completo.)
PARE.

Ejemplo 1

8WLOLFHHOPpWRGR5XQJH.XWWD)HKOEHUJFRQXQDWROHUDQFLDTOL 52XQWDPDxRGHSDVR
m√°ximo hm√°x 5XQWDPDxRGHSDVRPtQLPRhm√≠n 5SDUDDSUR[LPDUODVROXFLyQ
del problema de valor inicial

y = y ‚àí t 2 + 1,

0 ‚â§ t ‚â§ 2,

y(0) = 0.5,

\FRPSDUHORVUHVXOWDGRVFRQODVROXFLyQH[DFWDy(t) = (t + 1)2 ‚àí 0.5et .
Soluci√≥n 7UDEDMDUHPRVDWUDYpVGHOSULPHUSDVRGHORVFiOFXORV\DFRQWLQXDFLyQDSOLFDUHPRVHODOJRULWPRSDUDGHWHUPLQDUORVUHVXOWDGRVUHVWDQWHV/DFRQGLFLyQLQLFLDOGDt 5
\w 53DUDGHWHUPLQDUw mediante w usando h 5HOWDPDxRGHSDVRPi[LPR
SHUPLVLEOHFDOFXODPRV

k1 = hf (t0 , w 0 ) = 0.25 0.5 ‚àí 02 + 1 = 0.375,
1
1
k2 = hf t0 + h, w 0 + k1
4
4

= 0.25 f

1
1
0.25, 0.5 + 0.375
4
4

= 0.3974609,

3
3
9
k3 = hf t0 + h, w 0 + k1 + k2
8
32
32
= 0.25 f
k4 = hf t0 +
= 0.25 f

0.09375, 0.5 +

9
3
0.375 + 0.3974609
32
32

= 0.4095383,

12
1932
7200
7296
h, w 0 +
k1 ‚àí
k2 +
k3
13
2197
2197
2197
0.2307692, 0.5 +

= 0.4584971,

7200
7296
1932
0.375 ‚àí
0.3974609 +
0.4095383
2197
2197
2197

5.5 Control de error y m√©todo Runge-Kutta-Fehlberg

k5 = hf t0 + h, w 0 +

223

439
3680
845
k1 ‚àí 8k2 +
k3 ‚àí
k4
216
513
4104

= 0.25 f 0.25, 0.5 +

3680
845
439
0.375 ‚àí 8(0.3974609) +
0.4095383 ‚àí
0.4584971
216
513
4104

= 0.4658452,
y
1
8
3544
1859
11
k3 +
k4 ‚àí k5
k6 = hf t0 + h, w 0 ‚àí k1 + 2k2 ‚àí
2
27
2565
4104
40
= 0.25 f
‚àí

0.125, 0.5 ‚àí

8
3544
1859
0.375 + 2(0.3974609) ‚àí
0.4095383 +
0.4584971
27
2565
4104

11
0.4658452
40

= 0.4204789.
Entonces, se encuentra que las dos aproximaciones para y(0.25) son
16
6656
28561
9
2
k1 +
k3 +
k4 ‚àí k5 + k6
135
12825
56430
50
55
16
6656
28561
9
= 0.5 +
0.375 +
0.4095383 +
0.4584971 ‚àí 0.4658452
135
12825
56430
50
2
+ 0.4204789
55

wÃÉ 1 = w 0 +

= 0.9204870,
y
25
1408
2197
1
k1 +
k3 +
k4 ‚àí k5
216
2565
4104
5
1408
2197
1
25
0.375 +
0.4095383 +
0.4584971 ‚àí 0.4658452
= 0.5 +
216
2565
4104
5

w1 = w0 +

= 0.9204886.
Esto tambi√©n implica que
1
128
2197
1
2
1
k1 ‚àí
k3 ‚àí
k4 + k5 + k6
0.25 360
4275
75240
50
55

R=

=4
+

1
128
2197
0.375 ‚àí
0.4095383 ‚àí
0.4584971
360
4275
75240
2
1
0.4658452 + 0.4204789
50
55

= 0.00000621388,
y
q = 0.84

Œµ
R

1/4

= 0.84

0.00001
0.00000621388

1/4

= 0.9461033291.

CAP√çTULO 5

224

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

Puesto que R ‚â§ 10‚àí5 SRGHPRV DFHSWDU OD DSUR[LPDFLyQ  SDUD y   SHUR
GHEHUtDPRV DMXVWDU HO WDPDxR GH SDVR SDUD OD VLJXLHQWH LWHUDFLyQ SDUD h = 0.9461033291
(0.25) ‚âà 0.23652586LQHPEDUJRVyORVHHVSHUDUtDTXHORVSULPHURVFLQFRGtJLWRVGHHVWH
resultado sean precisos porque R tiene solamente cinco GtJLWRV GH SUHFLVLyQ 3XHVWR TXH
HVWDPRV UHVWDQGR HIHFWLYDPHQWH ORV Q~PHURV FDVL LJXDOHV w i y wÃÉ i cuando calculamos R
H[LVWHXQDEXHQDSUREDELOLGDGGHHUURUGHUHGRQGHReVWDHVXQDUD]yQDGLFLRQDOSDUDVHU
conservador al calcular q.
/RV UHVXOWDGRV D SDUWLU GHO DOJRULWPR VH PXHVWUDQ HQ OD WDEOD  (O LQFUHPHQWR GH
SUHFLVLyQVHKDXVDGRSDUDJDUDQWL]DUTXHORVFiOFXORVVRQSUHFLVRVSDUDWRGRVORVOXJDUHV
OLVWDGRV/DV~OWLPDVGRVFROXPQDVHQODWDEODPXHVWUDQORVUHVXOWDGRVGHOPpWRGRGH
TXLQWRRUGHQ3DUDYDORUHVSHTXHxRVGHtHOHUURUHVPHQRUDOHUURUHQHOPpWRGRGHFXDUWR
RUGHQSHURVXSHUDHOGHOPpWRGRGHFXDUWRRUGHQFXDQGRtDXPHQWD

Tabla 5.11
ti

yi = y(ti )

RKF-4
wi

hi

Ri

|yi ‚àí w i |

RKF-5
wÃÇ i

|yi ‚àí wÃÇ i |

0
0.2500000
0.4865522
0.7293332
0.9793332
1.2293332
1.4793332
1.7293332
1.9793332
2.0000000

0.5
0.9204873
1.3964884
1.9537446
2.5864198
3.2604520
3.9520844
4.6308127
5.2574687
5.3054720

0.5
0.9204886
1.3964910
1.9537488
2.5864260
3.2604605
3.9520955
4.6308268
5.2574861
5.3054896

0.2500000
0.2365522
0.2427810
0.2500000
0.2500000
0.2500000
0.2500000
0.2500000
0.0206668

6.2 √ó 10‚àí6
4.5 √ó 10‚àí6
4.3 √ó 10‚àí6
3.8 √ó 10‚àí6
2.4 √ó 10‚àí6
7 √ó 10‚àí7
1.5 √ó 10‚àí6
4.3 √ó 10‚àí6

0.5
1.3 √ó 10‚àí6
2.6 √ó 10‚àí6
4.2 √ó 10‚àí6
6.2 √ó 10‚àí6
8.5 √ó 10‚àí6
1.11 √ó 10‚àí5
1.41 √ó 10‚àí5
1.73 √ó 10‚àí5
1.77 √ó 10‚àí5

0.9204870
1.3964900
1.9537477
2.5864251
3.2604599
3.9520954
4.6308272
5.2574871
5.3054896

2.424 √ó 10‚àí7
1.510 √ó 10‚àí6
3.136 √ó 10‚àí6
5.242 √ó 10‚àí6
7.895 √ó 10‚àí6
1.096 √ó 10‚àí5
1.446 √ó 10‚àí5
1.839 √ó 10‚àí5
1.768 √ó 10‚àí5

La secci√≥n Conjunto de ejercicios 5.5 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

5.6 M√©todos multipasos
/RV PpWRGRV TXH VH KDQ DQDOL]DGR KDVWD HVWH SXQWR HQ HO FDStWXOR UHFLEHQ HO QRPEUH GH
m√©todos de un pasoSRUTXHODDSUR[LPDFLyQSDUDHOSXQWRGHPDOOD ti+1LQYROXFUDLQIRUPDFLyQGHXQVRORSXQWRGHPDOODSUHYLRti$SHVDUGHTXHHVWRVPpWRGRVSRGUtDQXWLOL]DU
ODLQIRUPDFLyQGHHYDOXDFLyQGHIXQFLyQHQORVSXQWRVHQWUHti \ti+1QRODUHWLHQHQSDUDXVR
GLUHFWRHQDSUR[LPDFLRQHVIXWXUDV7RGDODLQIRUPDFLyQTXHVHKDXVDGRFRQHVWRVPpWRGRV
VHREWLHQHGHQWURGHOVXELQWHUYDORVREUHHOTXHODVROXFLyQVHDSUR[LPD
/DVROXFLyQDSUR[LPDGDHVWiGLVSRQLEOHHQFDGDXQRGHORVSXQWRVGHPDOODt0 , t1 , . . . , ti
DQWHVGHREWHQHUODDSUR[LPDFLyQHQti+1\SRUTXHHOHUURU|w j ‚àí y(t j )| tiende a incrementar
con jSRUORTXHSDUHFHUD]RQDEOHGHVDUUROODUPpWRGRVTXHXVHQGDWRVSUHYLRVPiVSUHFLVRV
DODSUR[LPDUODVROXFLyQti+1
Los m√©todoV TXH XWLOL]DQ OD DSUR[LPDFLyQ HQ PiV GH XQ SXQWR GH PDOOD SUHYLR SDUD
GHWHUPLQDUODDSUR[LPDFLyQHQHOVLJXLHQWHSXQWRUHFLEHQHOQRPEUHGHPpWRGRVmultipasos
$FRQWLQXDFLyQVHSUHVHQWDODGH√ÄQLFLyQSUHFLVDGHHVWRVPpWRGRVMXQWRFRQODGH√ÄQLFLyQGH
ORVGRVWLSRVGHPpWRGRVPXOWLSDVRV

5.6

DeÔ¨Ånici√≥n 5.14

M√©todos multipasos

225

8Qm√©todo multipasos de paso m para resolver el problema de valor inicial

y = f (t, y),

a ‚â§ t ‚â§ b,

y(a) = Œ±,



WLHQHXQDHFXDFLyQGHGLIHUHQFLDSDUDHQFRQWUDUODDSUR[LPDFLyQ w i+1 en el punto de malla
ti+1UHSUHVHQWDGRSRUODVLJXLHQWHHFXDFLyQGRQGHmHVXQHQWHURPD\RUTXH

w i+1 = am‚àí1 w i + am‚àí2 w i‚àí1 + ¬∑ ¬∑ ¬∑ + a0 w i+1‚àím



+ h[bm f (ti+1 , w i+1 ) + bm‚àí1 f (ti , w i )
+ ¬∑ ¬∑ ¬∑ + b0 f (ti+1‚àím , w i+1‚àím )],
para i = m‚àí1, m, . . . , N ‚àí1, donde h = (b‚àía)/N , a0 , a1 , . . . , am‚àí1 y b0 , b1 , . . . , bm son
FRQVWDQWHV\VHHVSHFL√ÄFDQORVYDORUHVLQLFLDOHVHVSHFt√ÄFRV

w 0 = Œ±,

w 1 = Œ±1 ,

w 2 = Œ±2 ,

... ,

w m‚àí1 = Œ±m‚àí1

6RQHVSHFL√ÄFDGRV
Cuando bm 5HOPpWRGRUHFLEHHOQRPEUHGHexpl√≠citoRabierto\DTXHODHFXDFLyQ
 SURSRUFLRQDw i+1 de manera expl√≠cita en t√©rminos de valores previamente determinaGRV&XDQGRbm = 0, HOPpWRGRUHFLEHHOQRPEUHGHimpl√≠citoRcerradoSRUTXHw i+1 se
SUHVHQWDHQDPERVODGRVGHODHFXDFLyQ  SRUORTXH w i+1VRODPHQWHVHHVSHFL√ÄFDGH
PDQHUDLPSOtFLWD
3RUHMHPSORODVHFXDFLRQHV

/DVWpFQLFDVGH$GDPV%DVKIRUWK
VHGHEHQD-RKQ&RXFK$GDPV
¬≤ TXLHQUHDOL]y
WUDEDMRVVLJQL√ÄFDWLYRVHQ
PDWHPiWLFDV\DVWURQRPtD
'HVDUUROOyHVWDVWpFQLFDV
num√©ricas para aproximar la
VROXFLyQGHXQSUREOHPDGH
√ÅXMRGH√ÅXLGRSURSXHVWRSRU
%DVKIRUWK

w 0 = Œ±,
w i+1 = w i +

w 1 = Œ±1 ,

w 2 = Œ±2 ,

w 3 = Œ±3 ,

h
[55 f (ti , w i ) ‚àí 59 f (ti‚àí1 , w i‚àí1 ) + 37 f (ti‚àí2 , w i‚àí2 ) ‚àí 9 f (ti‚àí3 , w i‚àí3 )],
24


para cada i = 3, 4, . . . , N ‚àí1, GH√ÄQHQHOPpWRGRexpl√≠cito de cuatro pasos conocido como
t√©cnica Adams-Bashforth de cuarto orden Las ecuaciones

w 0 = Œ±,
w i+1 = w i +

w 1 = Œ±1 ,

w 2 = Œ±2 ,

h
[9 f (ti+1 , w i+1 ) + 19 f (ti , w i ) ‚àí 5 f (ti‚àí1 , w i‚àí1 ) + f (ti‚àí2 , w i‚àí2 )],
24


)RUHVW5D\0RXOWRQ ¬≤ 
HVWDEDDFDUJRGHEDOtVWLFDHQ
$EHUGHHQ3URYLQJ*URXQGV
HQ0DU\ODQGGXUDQWHOD3ULPHUD
*XHUUD0XQGLDO(UDXQDXWRU
SUROt√ÄFRHVFULELyQXPHURVRV
OLEURVVREUHPDWHPiWLFDV\
DVWURQRPtD\GHVDUUROOyPpWRGRV
multipasos para resolver
HFXDFLRQHVEDOtVWLFDV

para cada i = 2, 3, . . . , N ‚àí 1, GH√ÄQHQXQPpWRGRimpl√≠cito de tres pasos conocido como la
t√©cnica de Adams-Moulton de cuarto orden
/RV YDORUHV LQLFLDOHV \D VHD HQ OD HFXDFLyQ   FRPR HQ OD HFXDFLyQ   GHEHQ
HVSHFL√ÄFDUVHHQJHQHUDODOVXSRQHU w 0 = Œ±\JHQHUDUORVYDORUHVUHVWDQWHV\DVHDFRQHO
PpWRGR 5XQJH.XWWD R HO PpWRGR GH7D\ORU 2EVHUYDUHPRV TXH HQ JHQHUDO ORV PpWRGRV
LPSOtFLWRVVRQPiVSUHFLVRVTXHORVH[SOtFLWRVSHURSDUDDSOLFDUGLUHFWDPHQWHXQRLPSOtFLWR
FRPRHO  GHEHPRVUHVROYHUODHFXDFLyQLPSOtFLWDSDUDw i+1(VWRQRVLHPSUHHVSRVLEOH
HLQFOXVRFXDQGRVHSXHGHKDFHUODVROXFLyQSDUDw i+1SXHGHQRVHU~QLFD

Ejemplo 1

(QHOHMHPSORGHODVHFFLyQ FRQVXOWHODWDEODHQODSiJLQD XVDPRVHOPpWRGR
5XQJH.XWWDGHRUGHQFRQh 5SDUDDSUR[LPDUVROXFLRQHVSDUDHOSUREOHPDGHYDORU
inicial

y = y ‚àí t 2 + 1,

0 ‚â§ t ‚â§ 2,

y(0) = 0.5.

226

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

6H HQFRQWUy TXH ODV SULPHUDV FXDWUR DSUR[LPDFLRQHV VRQ y(0) = w 0 = 0.5, y(0.2) ‚âà
w 1 = 0.8292933, y(0.4) ‚âà w 2 = 1.2140762, y y(0.6) ‚âà w 3 = 1.6489220√öselas como
YDORUHVLQLFLDOHVSDUDHOPpWRGR$GDP%DVKIRUWKGHFXDUWRRUGHQSDUDFDOFXODUQXHYDVDSURximaciones para y  \y  \FRPSiUHODVFRQODVSURGXFLGDVPHGLDQWHHOPpWRGR5XQJH
.XWWDGHFXDUWRRUGHQ
Soluci√≥n

3DUDHOPpWRGR$GDP%DVKIRUWKGHFXDUWRRUGHQWHQHPRV

0.2
(55 f (0.6, w 3 ) ‚àí 59 f (0.4, w 2 ) + 37 f (0.2, w 1 ) ‚àí 9 f (0, w 0 ))
24
0.2
(55 f (0.6, 1.6489220) ‚àí 59 f (0.4, 1.2140762)
= 1.6489220 +
24

y(0.8) ‚âà w 4 = w 3 +

+ 37 f (0.2, 0.8292933) ‚àí 9 f (0, 0.5))
= 1.6489220 + 0.0083333(55(2.2889220) ‚àí 59(2.0540762)
+ 37(1.7892933) ‚àí 9(1.5))
= 2.1272892
y
0.2
(55 f (0.8, w 4 ) ‚àí 59 f (0.6, w 3 ) + 37 f (0.4, w 2 ) ‚àí 9 f (0.2, w 1 ))
24
0.2
(55 f (0.8, 2.1272892) ‚àí 59 f (0.6, 1.6489220)
= 2.1272892 +
24

y(1.0) ‚âà w 5 = w 4 +

+ 37 f (0.4, 1.2140762) ‚àí 9 f (0.2, 0.8292933))
= 2.1272892 + 0.0083333(55(2.4872892) ‚àí 59(2.2889220)
+ 37(2.0540762) ‚àí 9(1.7892933))
= 2.6410533.
El error para estas aproximaciones en t 5\t 5VRQUHVSHFWLYDPHQWH

|2.1272295 ‚àí 2.1272892| = 5.97 √ó 10‚àí5 y |2.6410533 ‚àí 2.6408591| = 1.94 √ó 10‚àí4 .
Las aproximaciones correspondientes tienen los errores

|2.1272027 ‚àí 2.1272892| = 2.69 √ó 10‚àí5 y |2.6408227 ‚àí 2.6408591| = 3.64 √ó 10‚àí5 .

Adams estaba especialmente
LQWHUHVDGRHQXVDUHVWDKDELOLGDG
para los c√°lculos num√©ricos
SUHFLVRVHQODLQYHVWLJDFLyQ
GHODVyUELWDVGHORVSODQHWDV
3UHGLMRODH[LVWHQFLDGH1HSWXQR
DODQDOL]DUODVLUUHJXODULGDGHV
HQHOSODQHWD8UDQR\
GHVDUUROOyQXPHURVDVWpFQLFDV
GHLQWHJUDFLyQQXPpULFDSDUD
DSUR[LPDUODVROXFLyQGHODV
HFXDFLRQHVGLIHUHQFLDOHV

3DUDFRPHQ]DUODGHGXFFLyQGHXQPpWRGRPXOWLSDVRVREVHUYHTXHODVROXFLyQSDUDHO
problema de valor inicial

y = f (t, y),

a ‚â§ t ‚â§ b,

y(a) = Œ±,

VLVHLQWHJUDVREUHHOLQWHUYDOR[ ti , ti+1 ], tiene la propiedad de que

y(ti+1 ) ‚àí y(ti ) =

ti+1
ti

y (t) dt =

ti+1
ti

f (t, y(t)) dt.

3RUFRQVLJXLHQWH

y(ti+1 ) = y(ti ) +

ti+1
ti

f (t, y(t)) dt.



5.6

227

M√©todos multipasos

6LQ HPEDUJR QR SRGHPRV LQWHJUDU f (t y(t)) sin conocer y(t  OD VROXFLyQ GHO SUREOHPD SRU OR TXH HQ VX OXJDU LQWHJUDPRV XQ SROLQRPLR LQWHUSRODQWH P(t) para f (t y(t 
XQR TXH HVWi GHWHUPLQDGR SRU DOJXQRV GH ORV SXQWRV GH GDWRV SUHYLDPHQWH REWHQLGRV
(t0 , w 0 ), (t1 , w 1 ), . . . , (ti , w i ). &XDQGR VXSRQHPRV DGHPiV TXH y(ti ) ‚âà w i OD HFXDFLyQ
 VHFRQYLHUWHHQ

y(ti+1 ) ‚âà w i +

ti+1
ti

P(t) dt.



$SHVDUGHTXHQLQJXQDIRUPDGHSROLQRPLRGHLQWHUSRODFLyQVHSXHGHXVDUSDUDODGHULYDFLyQHVPiVFRQYHQLHQWHXWLOL]DUODIyUPXODGHGLIHUHQFLDUHJUHVLYDSRUTXHHVWDIRUPDLQFRUSRUDFRQPD\RUIDFLOLGDGORVGDWRVFDOFXODGRVPiVUHFLHQWHPHQWH
Para derivar una t√©cnica de m SDVRV GH$GDP%DVKIRUWK IRUPDPRV HO SROLQRPLR GH
GLIHUHQFLDUHJUHVLYDPm‚àí1 (t) con

(ti , f (ti , y(ti ))),

(ti‚àí1 , f (ti‚àí1 , y(ti‚àí1 ))), . . . ,

(ti+1‚àím , f (ti+1‚àím , y(ti+1‚àím ))).

Puesto que Pm‚àí1 (t)HVXQSROLQRPLRGHLQWHUSRODFLyQGHJUDGRm 2H[LVWHDOJ~QQ~PHURŒæi
en (ti+1‚àím , ti ) con

f (t, y(t)) = Pm‚àí1 (t) +

f (m) (Œæi , y(Œæi ))
(t ‚àí ti )(t ‚àí ti‚àí1 ) ¬∑ ¬∑ ¬∑ (t ‚àí ti+1‚àím ).
m!

$O LQWURGXFLU OD VXVWLWXFLyQ GH OD YDULDEOH t = ti + sh, con dt = h ds, en Pm‚àí1 (t) \ HO
t√©rmino de error implica que
ti+1
ti

f (t, y(t)) dt =

ti+1 m‚àí1
ti

‚àís
‚àá k f (ti , y(ti )) dt
k

(‚àí1)k

k=0
ti+1

+

ti

m‚àí1

=

f (m) (Œæi , y(Œæi ))
(t ‚àí ti )(t ‚àí ti‚àí1 ) ¬∑ ¬∑ ¬∑ (t ‚àí ti+1‚àím ) dt
m!

0

k=0

+

‚àís
k

1

‚àá k f (ti , y(ti ))h(‚àí1)k

h m+1
m!

1

ds

s(s + 1) ¬∑ ¬∑ ¬∑ (s + m ‚àí 1) f (m) (Œæi , y(Œæi )) ds.

0

SDUDGLIHUHQWHVYDORUHVGH kVHHYDO~DQIiFLOPHQWH\VRQOLVWDGDV
/DVLQWHJUDOHV(‚àí1)k 0 ‚àís
k
HQODWDEOD3RUHMHPSORFXDQGRk 5
1

Tabla 5.12
1

k

(‚àí1)k

0

‚àís
ds
k

1

(‚àí1)3
0

0
1
2
3
4
5

1
1
2
5
12
3
8
251
720
95
288

‚àís
3

1

ds = ‚àí
0

=

1
6

1

(‚àís)(‚àís ‚àí 1)(‚àís ‚àí 2)
ds
1¬∑2¬∑3
(s 3 + 3s 2 + 2s) ds

0
1

=

1 s4
1
+ s3 + s2 =
6 4
6
0

9
4

=

3
.
8

3RUFRQVLJXLHQWH
ti+1
ti

1
5
f (t, y(t)) dt = h f (ti , y(ti )) + ‚àá f (ti , y(ti )) + ‚àá 2 f (ti , y(ti )) + ¬∑ ¬∑ ¬∑
2
12
+

h m+1
m!

1
0

s(s + 1) ¬∑ ¬∑ ¬∑ (s + m ‚àí 1) f (m) (Œæi , y(Œæi )) ds.



228

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

Puesto que s(s + 1) ¬∑ ¬∑ ¬∑ (s + m ‚àí 1) QR FDPELD GH VLJQR HQ > @ HO WHRUHPD GH
YDORUPHGLRSRQGHUDGRSDUDLQWHJUDOHVVHSXHGHXVDUSDUDGHGXFLUTXHSDUDDOJ~QQ~PHUR
Œºi , donde ti+1‚àím < Œºi < ti+1HOWpUPLQRGHHUURUHQODHFXDFLyQ  VHFRQYLHUWHHQ

h m+1
m!

1

s(s + 1) ¬∑ ¬∑ ¬∑ (s + m ‚àí 1) f (m) (Œæi , y(Œæi )) ds

0

=

h m+1 f (m) (Œºi , y(Œºi ))
m!

1

s(s + 1) ¬∑ ¬∑ ¬∑ (s + m ‚àí 1) ds.

0

3RUORWDQWRHOHUURUHQODHFXDFLyQ  VHVLPSOL√ÄFDHQ
1

h m+1 f (m) (Œºi , y(Œºi ))(‚àí1)m
0

6LQ HPEDUJR y(ti+1 ) ‚àí y(ti ) =
escribir como

‚àís
m

ds.



ti+1
f (t, y(t)) dt SRU OR TXH OD HFXDFLyQ   VH SXHGH
ti

1
5
y(ti+1 ) = y(ti ) + h f (ti , y(ti )) + ‚àá f (ti , y(ti )) + ‚àá 2 f (ti , y(ti )) + ¬∑ ¬∑ ¬∑
2
12
1

+ h m+1 f (m) (Œºi , y(Œºi ))(‚àí1)m
0

Ejemplo 2

‚àís
ds.
m



8VHODHFXDFLyQ  FRQm 5SDUDGHULYDUODWpFQLFDGH$GDPV%DVKIRUWKGHWUHVSDVRV
Soluci√≥n Tenemos

1
5
y(ti+1 ) ‚âà y(ti ) + h f (ti , y(ti )) + ‚àá f (ti , y(ti )) + ‚àá 2 f (ti , y(ti ))
2
12
= y(ti ) + h
+

1
f (ti , y(ti )) + [ f (ti , y(ti )) ‚àí f (ti‚àí1 , y(ti‚àí1 ))]
2

5
[ f (ti , y(ti )) ‚àí 2 f (ti‚àí1 , y(ti‚àí1 )) + f (ti‚àí2 , y(ti‚àí2 ))]
12

= y(ti ) +

h
[23 f (ti , y(ti )) ‚àí 16 f (ti‚àí1 , y(ti‚àí1 )) + 5 f (ti‚àí2 , y(ti‚àí2 ))].
12

(OPpWRGR$GDPV%DVKIRUWKGHWUHVSDVRVHVSRUFRQVLJXLHQWH

w 0 = Œ±,
w i+1 = w i +

w 1 = Œ±1 ,

w 2 = Œ±2 ,

h
[23 f (ti , w i ) ‚àí 16 f (ti‚àí1 , w i‚àí1 )] + 5 f (ti‚àí2 , w i‚àí2 )],
12

para i = 2, 3, . . . , N ‚àí 1.
/RVPpWRGRVPXOWLSDVRVWDPELpQVHSXHGHQGHULYDUSRUPHGLRGHODVHULHGH7D\ORU8Q
HMHPSORGHOSURFHGLPLHQWRLPSOLFDGRVHFRQVLGHUDHQHOHMHUFLFLR8QDGHULYDFLyQSRU
PHGLRGHOSROLQRPLRGHLQWHUSRODFLyQGH/DJUDQJHVHDQDOL]DHQHOHMHUFLFLR
(O HUURU GH WUXQFDPLHQWR ORFDO SDUD PpWRGRV PXOWLSDVRV VH GH√ÄQH GH PDQHUD DQiORJD
DOGHOPpWRGRGHXQSDVR(QHOFDVRGHORVPpWRGRVGHXQSDVRHOHUURUGHWUXQFDPLHQWR
ORFDOSURYHHXQDPHGLGDGHODIRUPDHQODTXHODVROXFLyQGHODHFXDFLyQGLIHUHQFLDOQRORJUD
UHVROYHUODHFXDFLyQGHGLIHUHQFLD
DeÔ¨Ånici√≥n 5.15

6Ly(t HVODVROXFLyQDOSUREOHPDGHYDORULQLFLDO

y = f (t, y),

a ‚â§ t ‚â§ b,

y(a) = Œ±,

5.6

229

M√©todos multipasos

\

w i+1 = am‚àí1 w i + am‚àí2 w i‚àí1 + ¬∑ ¬∑ ¬∑ + a0 w i+1‚àím
+ h[bm f (ti+1 , w i+1 ) + bm‚àí1 f (ti , w i ) + ¬∑ ¬∑ ¬∑ + b0 f (ti+1‚àím , w i+1‚àím )]
es el (i 1 pVLPRSDVRHQXQPpWRGRPXOWLSDVRVHOerror de truncamiento local en este
paso es

œÑi+1 (h) =

y(ti+1 ) ‚àí am‚àí1 y(ti ) ‚àí ¬∑ ¬∑ ¬∑ ‚àí a0 y(ti+1‚àím )
h



‚àí [bm f (ti+1 , y(ti+1 )) + ¬∑ ¬∑ ¬∑ + b0 f (ti+1‚àím , y(ti+1‚àím ))],
para cada i = m ‚àí 1, m, . . . , N ‚àí 1.
Ejemplo 3

'HWHUPLQH HO HUURU GH WUXQFDPLHQWR ORFDO SDUD HO PpWRGR$GDPV%DVKIRUWK GH WUHV SDVRV
GHULYDGRHQHOHMHPSOR
Soluci√≥n $OFRQVLGHUDUODIRUPDGHOHUURUSURYLVWRHQODHFXDFLyQ  ODHQWUDGDDGHFXD-

GDHQODWDEODGD
1

h 4 f (3) (Œºi , y(Œºi ))(‚àí1)3
0

‚àís
3

ds =

3h 4 (3)
f (Œºi , y(Œºi )).
8

$WUDYpVGHOKHFKRGHTXH f (3) (Œºi , y(Œºi )) = y (4) (Œºi )\ODHFXDFLyQGHGLIHUHQFLDGHULYDGD
HQHOHMHPSORWHQHPRV

œÑi+1 (h) =
=

y(ti+1 ) ‚àí y(ti )
1
‚àí [23 f (ti , y(ti )) ‚àí 16 f (ti‚àí1 , y(ti‚àí1 )) + 5 f (ti‚àí2 , y(ti‚àí2 ))]
h
12

3h 3 (4)
1 3h 4 (3)
f (Œºi , y(Œºi )) =
y (Œºi ),
h 8
8

para algunos Œºi ‚àà (ti‚àí2 , ti+1 ).

M√©todos expl√≠citos de Adams-Bashforth
$OJXQRVGHORVPpWRGRVH[SOtFLWRVGH$GDPV%DVKIRUWKMXQWRFRQVXVYDORUHVLQLFLDOHV\
HUURUHVGHWUXQFDPLHQWRORFDOUHTXHULGRVVRQORVVLJXLHQWHV/DGHULYDFLyQGHHVWDVWpFQLFDV
HVVLPLODUDOSURFHGLPLHQWRHQORVHMHPSORV\
M√©todo expl√≠cito de dos pasos de Adams-Bashforth

w 0 = Œ±,
w i+1 = w i +

w 1 = Œ±1 ,
h
[3 f (ti , w i ) ‚àí f (ti‚àí1 , w i‚àí1 )],
2



5
y (Œºi )h 2SDUD
donde i = 1, 2, . . . , N ‚àí 1. El error de truncamiento local es œÑi+1 (h) = 12
DOJXQRVŒºi ‚àà (ti‚àí1 , ti+1 ).

M√©todo expl√≠cito de tres pasos de Adams-Bashforth

w 0 = Œ±,
w i+1 = w i +

w 1 = Œ±1 ,

w 2 = Œ±2 ,

h
[23 f (ti , w i ) ‚àí 16 f (ti‚àí1 , w i‚àí1 ) + 5 f (ti‚àí2 , w i‚àí2 )],
12



donde i = 2, 3, . . . , N ‚àí 1. El error de truncamiento local es œÑi+1 (h) = 38 y (4) (Œºi )h 3SDUD
DOJXQRVŒºi ‚àà (ti‚àí2 , ti+1 ).

230

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

M√©todo expl√≠cito de cuatro pasos de Adams-Bashforth

w 0 = Œ±,

w 1 = Œ±1 ,

w i+1 = w i +

w 2 = Œ±2 ,

w 3 = Œ±3 ,

h
[55 f (ti , w i ) ‚àí 59 f (ti‚àí1 , w i‚àí1 ) + 37 f (ti‚àí2 , w i‚àí2 ) ‚àí 9 f (ti‚àí3 , w i‚àí3 )],
24


y (5) (Œºi )h 4SDUD
donde i = 3, 4, . . . , N ‚àí 1. El error de truncamiento local es œÑi+1 (h) = 251
720
DOJXQRVŒºi ‚àà (ti‚àí3 , ti+1 ).
M√©todo expl√≠cito de cinco pasos de Adams-Bashforth

w 0 = Œ±,
w i+1 = w i +

w 1 = Œ±1 ,

w 2 = Œ±2 ,

w 3 = Œ±3 ,

w 4 = Œ±4 ,

h
[1901 f (ti , w i ) ‚àí 2774 f (ti‚àí1 , w i‚àí1 )
720

+ 2616 f (ti‚àí2 , w i‚àí2 ) ‚àí 1274 f (ti‚àí3 , w i‚àí3 ) + 251 f (ti‚àí4 , w i‚àí4 )],



95 (6)
y (Œºi )h 5SDUD
donde i = 4, 5, . . . , N ‚àí 1. El error de truncamiento local es œÑi+1 (h) = 288
DOJXQRVŒºi ‚àà (ti‚àí4 , ti+1 ).

M√©todos impl√≠citos de Adams-Moulton
Los m√©todos impl√≠citos se derivan a trav√©s de (ti+1 , f (ti+1 , y(ti+1 ))) como un nodo de interSRODFLyQDGLFLRQDOHQODDSUR[LPDFLyQGHODLQWHJUDO
ti+1
ti

f (t, y(t)) dt.

$OJXQRVGHORVPpWRGRVLPSOtFLWRVPiVFRPXQHVVRQORVVLJXLHQWHV
M√©todos impl√≠citos de dos pasos de Adams-Moulton

w 0 = Œ±,
w i+1 = w i +

w 1 = Œ±1 ,
h
[5 f (ti+1 , w i+1 ) + 8 f (ti , w i ) ‚àí f (ti‚àí1 , w i‚àí1 )],
12



1 (4)
y (Œºi )h 3
donde i = 1, 2, . . . , N ‚àí 1. El error de truncamiento local es œÑi+1 (h) = ‚àí 24
SDUDDOJXQRVŒºi ‚àà (ti‚àí1 , ti+1 ).

M√©todos impl√≠citos de tres pasos de Adams-Moulton

w 0 = Œ±,
w i+1 = w i +

w 1 = Œ±1 ,

w 2 = Œ±2 ,

h
[9 f (ti+1 , w i+1 ) + 19 f (ti , w i ) ‚àí 5 f (ti‚àí1 , w i‚àí1 ) + f (ti‚àí2 , w i‚àí2 )],
24


19 (5)
y (Œºi )h 4
donde i = 2, 3, . . . , N ‚àí 1. El error de truncamiento local es œÑi+1 (h) = ‚àí 720
SDUDDOJXQRVŒºi ‚àà (ti‚àí2 , ti+1 ).

5.6

231

M√©todos multipasos

M√©todos impl√≠citos de cuatro pasos de Adams-Moulton

w 0 = Œ±,
w i+1 = w i +

w 1 = Œ±1 ,

w 2 = Œ±2 ,

w 3 = Œ±3 ,

h
[251 f (ti+1 , w i+1 ) + 646 f (ti , w i ) ‚àí 264 f (ti‚àí1 , w i‚àí1 )
720

+ 106 f (ti‚àí2 , w i‚àí2 ) ‚àí 19 f (ti‚àí3 , w i‚àí3 )],



3 (6)
y (Œºi )h 5
donde i = 3, 4, . . . , N ‚àí 1. El error de truncamiento local es œÑi+1 (h) = ‚àí 160
SDUDDOJXQRVŒºi ‚àà (ti‚àí3 , ti+1 ).
(VLQWHUHVDQWHFRPSDUDUXQPpWRGRH[SOtFLWRGH$GDPV%DVKIRUWKGHm pasos con un
m√©todo impl√≠cito de Adams-Moulton de (m 2 SDVRV$PERVLPSOLFDQHYDOXDFLRQHVGHf
SRU SDVR \ DPERV WLHQHQ WpUPLQRV y (m+1) (Œºi )h m HQ VXV HUURUHV GH WUXQFDPLHQWR ORFDO (Q
JHQHUDOORVFRH√ÄFLHQWHVGHORVWpUPLQRVUHODFLRQDGRVFRQf en el error de truncamiento local
VRQPiVSHTXHxRVSDUDORVPpWRGRVLPSOtFLWRVTXHSDUDORVH[SOtFLWRV(VWRFRQGXFHDXQD
PD\RUHVWDELOLGDG\HUURUHVGHUHGRQGHRPiVSHTXHxRVSDUDORVPpWRGRVLPSOtFLWRV

Ejemplo 4

Considere el problema de valor inicial

y = y ‚àí t 2 + 1,

0 ‚â§ t ‚â§ 2,

y(0) = 0.5.

8WLOLFHORVYDORUHVH[DFWRVSURSRUFLRQDGRVSRUy(t) = (t + 1)2 ‚àí 0.5et como valores iniciaOHV\h 5SDUDFRPSDUDUODVDSUR[LPDFLRQHVDSDUWLUGHa) el m√©todo expl√≠cito de cuatro
SDVRVGH$GDPV%DVKIRUWK\b)HOPpWRGRLPSOtFLWRGHWUHVSDVRVGH$GDPV0RXOWRQ
Soluci√≥n

a)(OPpWRGR$GDPV%DVKIRUWKWLHQHODHFXDFLyQGHGLIHUHQFLD

w i+1 = w i +

h
[55 f (ti , w i ) ‚àí 59 f (ti‚àí1 , w i‚àí1 ) + 37 f (ti‚àí2 , w i‚àí2 ) ‚àí 9 f (ti‚àí3 , w i‚àí3 )],
24

para i = 3, 4, . . . , 9. $O VLPSOL√ÄFDU PHGLDQWH f (t, y) = y ‚àí t 2 + 1, h = 0.2, y ti = 0.2i,
se convierte en

w i+1 =

1
[35w i ‚àí 11.8w i‚àí1 + 7.4w i‚àí2 ‚àí 1.8w i‚àí3 ‚àí 0.192i 2 ‚àí 0.192i + 4.736].
24

b)(OPpWRGR$GDPV0RXOWRQWLHQHODHFXDFLyQGHGLIHUHQFLD

w i+1 = w i +

h
[9 f (ti+1 , w i+1 ) + 19 f (ti , w i ) ‚àí 5 f (ti‚àí1 , w i‚àí1 ) + f (ti‚àí2 , w i‚àí2 )],
24

para i = 2, 3, . . . , 9. Esto se reduce a

w i+1 =

1
[1.8w i+1 + 27.8w i ‚àí w i‚àí1 + 0.2w i‚àí2 ‚àí 0.192i 2 ‚àí 0.192i + 4.736].
24

3DUDXVDUHVWHPpWRGRGHPDQHUDH[SOtFLWDQHFHVLWDPRVUHVROYHUH[SOtFLWDPHQWHODHFXDFLyQSDUDw i+1 . Esto nos da

w i+1 =

1
[27.8w i ‚àí w i‚àí1 + 0.2w i‚àí2 ‚àí 0.192i 2 ‚àí 0.192i + 4.736],
22.2

para i = 2, 3, . . . , 9.
/RV UHVXOWDGRV HQ OD WDEOD  VH REWXYLHURQ D SDUWLU GH YDORUHV H[DFWRV GH
y(t) = (t + 1)2 ‚àí 0.5et para Œ±, Œ±1 , Œ±2 y Œ±3HQHOFDVRH[SOtFLWRGH$GDPV%DVKIRUWK\SDUD
Œ±, Œ±1 y Œ±2HQHOFDVRLPSOtFLWRGH$GDPV0RXOWRQ2EVHUYHTXHHOPpWRGR$GDPV0RXOWRQ
SURSRUFLRQDUHVXOWDGRVFRQVLVWHQWHPHQWHPHMRUHV

232

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

Tabla 5.13
ti

Exacto

AdamsBashforth
wi

0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0

0.5000000
0.8292986
1.2140877
1.6489406
2.1272295
2.6408591
3.1799415
3.7324000
4.2834838
4.8151763
5.3054720

2.1273124
2.6410810
3.1803480
3.7330601
4.2844931
4.8166575
5.3075838

Error

AdamsMoulton
wi

Error

0.0000828
0.0002219
0.0004065
0.0006601
0.0010093
0.0014812
0.0021119

1.6489341
2.1272136
2.6408298
3.1798937
3.7323270
4.2833767
4.8150236
5.3052587

0.0000065
0.0000160
0.0000293
0.0000478
0.0000731
0.0001071
0.0001527
0.0002132

M√©todos indicador-corrector
En el ejemplo 4, el m√©todo Adams‚ÄìMoulton da mejores resultados que el m√©todo expl√≠cito
de Adams‚ÄìBashforth del mismo orden. A pesar de que, en general, es el caso, los m√©todos
impl√≠citos tienen la debilidad inherente de tener que convertir primero el m√©todo de manera
algebraica para una representaci√≥n expl√≠cita de w i+1. Este procedimiento no siempre es posible, como se observa al considerar el problema fundamental de valor inicial

y = ey ,

0 ‚â§ t ‚â§ 0.25,

y(0) = 1.

Puesto que f (t, y) = e y, el m√©todo de tres pasos de Adams‚ÄìMoulton tiene

w i+1 = w i +

h
[9ew i+1 + 19ew i ‚àí 5ew i‚àí1 + ew i‚àí2 ]
24

como ecuaci√≥n de diferencia y esta ecuaci√≥n no se puede resolver de manera algebraica para
w i+1.
Podr√≠amos usar el m√©todo de Newton o el m√©todo de secante para aproximar w i+1, pero
esto complica considerablemente el procedimiento. En la pr√°ctica, los m√©todos impl√≠citos
multipasos no se utilizan de acuerdo con lo descrito antes. M√°s bien, se usan para mejorar
las aproximaciones obtenidas con los m√©todos expl√≠citos. La combinaci√≥n de un m√©todo
expl√≠cito para predecir y uno impl√≠cito para mejorar la predicci√≥n recibe el nombre de
m√©todo indicador-corrector.
Considere el siguiente m√©todo de cuarto orden para resolver un problema de valor inicial. El primer paso es calcular los valores iniciales w0, w1, w2 y w3 para el m√©todo de cuatro
pasos de Adams‚ÄìBashforth. Para hacerlo usamos el m√©todo de un paso de cuarto orden, el
m√©todo Runge-Kutta de cuarto orden. Lo siguiente es calcular una aproximaci√≥n w4p, para
y(t4) por medio del m√©todo expl√≠cito de Adams-Bashforth como indicador:

w4p = w3 +

h
[55 f (t3 , w 3 ) ‚àí 59 f (t2 , w 2 ) + 37 f (t1 , w 1 ) ‚àí 9 f (t0 , w 0 )].
24

Esta aproximaci√≥n mejora al insertar w4p en el lado derecho del m√©todo impl√≠cito de tres
pasos de Adams‚ÄìMoulton y usar ese m√©todo como corrector. Esto nos da

w4 = w3 +

h
[9 f (t4 , w 4 p ) + 19 f (t3 , w 3 ) ‚àí 5 f (t2 , w 2 ) + f (t1 , w 1 )].
24

La √∫nica evaluaci√≥n de funci√≥n nueva requerida en este procedimiento es f (t4 , w 4 p ) en la
ecuaci√≥n de correcci√≥n; todos los dem√°s valores de f se han calculado para aproximaciones
previas.

5.6

M√©todos multipasos

233

A continuaci√≥n se usa el valor w4 como la aproximaci√≥n para y(t4), y la t√©cnica para
utilizar el m√©todo Adams‚ÄìBashforth como indicador y el m√©todo Adams‚ÄìMoulton como
corrector se repite para encontrar w5p y w5ODVDSUR[LPDFLRQHVLQLFLDO\√ÄQDOSDUDy(t5). Este
proceso contin√∫a hasta que obtenemos una aproximaci√≥n wN para y(t N ) = y(b).
Las aproximaciones mejoradas para y(ti+1 ) se pueden obtener al iterar la f√≥rmula de
Adams‚ÄìMoulton, pero esto converge para la aproximaci√≥n provista por la f√≥rmula impl√≠cita
en lugar de la soluci√≥n y(ti+1 )3RUORWDQWRQRUPDOPHQWHHVPiVH√ÄFLHQWHXVDUXQDUHGXFci√≥n en el tama√±o de paso si se necesita precisi√≥n mejorada.
El algoritmo 5.4 est√° basado en el m√©todo Adams-Bashforth de cuarto orden como indicador y una iteraci√≥n para el m√©todo Adams‚ÄìMoulton como corrector, con los valores
iniciales obtenidos a partir del m√©todo Runge-Kutta de cuarto orden.

ALGORITMO

5.4

Indicador-corrector de Adams de cuarto orden
Para aproximar la soluci√≥n del problema de valor inicial

y = f (t, y),

a ‚â§ t ‚â§ b,

y(a) = Œ±,

en(N + 1) n√∫meros igualmente espaciados en el intervalo [a, b]:
ENTRADA extremos a, b; entero N; condici√≥n inicial Œ±.
SALIDA aproximaci√≥n w para y en los valores (N + 1) de t.
Paso 1 Determine h = (b ‚àí a)/N ;
t0 = a;
w 0 = Œ±;
SALIDA (t0 , w 0 ).
Paso 2 Para i = 1, 2, 3, haga los pasos 3‚Äì5.
(Calcule los valores iniciales con el m√©todo Runge-Kutta.)
Paso 3 Determine K 1 = h f (ti‚àí1 , w i‚àí1 );
K 2 = h f (ti‚àí1 + h/2, w i‚àí1 + K 1 /2);
K 3 = h f (ti‚àí1 + h/2, w i‚àí1 + K 2 /2);
K 4 = h f (ti‚àí1 + h, w i‚àí1 + K 3 ).
Paso 4 Determine w i = w i‚àí1 + (K 1 + 2K 2 + 2K 3 + K 4 )/6;
ti = a + i h.
Paso 5 SALIDA (ti , w i ).
Paso 6 Para i = 4, . . . , N haga los pasos 7‚Äì10.
Paso 7 Determine t = a + i h;
w = w 3 + h[55 f (t3 , w 3 ) ‚àí 59 f (t2 , w 2 ) + 37 f (t1 , w 1 )
(Prediga wi .)
‚àí 9 f (t0 , w 0 )]/24;
w = w 3 + h[9 f (t, w) + 19 f (t3 , w 3 ) ‚àí 5 f (t2 , w 2 )
(Corrija wi .)
+ f (t1 , w 1 )]/24.
Paso 8 SALIDA (t, w).
Paso 9 Para j = 0, 1, 2
Determine t j = t j+1 ; (Prepara la siguiente iteraci√≥n.)
w j = w j+1 .
Paso 10 Determine t3 = t;
w 3 = w.
Paso 11 PARE.

234

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

Ejemplo 5

Aplique el m√©todo indicador-corrector de cuarto orden de Adams con h 5 0.2 y los valores
iniciales del m√©todo de cuarto orden de Runge-Kutta para el problema de valor inicial

y = y ‚àí t 2 + 1,

0 ‚â§ t ‚â§ 2,

y(0) = 0.5.

Soluci√≥n eVWDHVXQDFRQWLQXDFLyQ\PRGL√ÄFDFLyQGHOSUREOHPDFRQVLGHUDGRHQHOHMHPSOR
al inicio de la secci√≥n. En ese ejemplo, encontramos que las aproximaciones de inicio a partir
de Runge-Kutta son

y(0) = w 0 = 0.5, y(0.2) ‚âà w 1 = 0.8292933, y(0.4) ‚âà w 2 = 1.2140762, y
y(0.6) ‚âà w 3 = 1.6489220,
y el m√©todo Adams‚ÄìBashforth de cuarto orden nos da

0.2
(55 f (0.6, w 3 ) ‚àí 59 f (0.4, w 2 ) + 37 f (0.2, w 1 ) ‚àí 9 f (0, w 0 ))
24
0.2
(55 f (0.6, 1.6489220) ‚àí 59 f (0.4, 1.2140762)
= 1.6489220 +
24

y(0.8) ‚âà w 4 p = w 3 +

+ 37 f (0.2, 0.8292933) ‚àí 9 f (0, 0.5))
= 1.6489220 + 0.0083333(55(2.2889220) ‚àí 59(2.0540762)
+ 37(1.7892933) ‚àí 9(1.5))
= 2.1272892.
Ahora utilizaremos w4p como indicador de la aproximaci√≥n para y(0.8) y determinaremos el
valor correcto w4, a partir del m√©todo impl√≠cito de Adams‚ÄìMoulton. Esto nos da

0.2
9 f (0.8, w 4 p ) + 19 f (0.6, w 3 ) ‚àí 5 f (0.4, w 2 ) + f (0.2, w 1 )
24
0.2
(9 f (0.8, 2.1272892) + 19 f (0.6, 1.6489220)
= 1.6489220 +
24

y(0.8) ‚âà w 4 = w 3 +

‚àí 5 f (0.4, 1.2140762) + f (0.2, 0.8292933))
= 1.6489220 + 0.0083333(9(2.4872892) + 19(2.2889220) ‚àí 5(2.0540762)
+ (1.7892933))
= 2.1272056.
Ahora utilizamos esta aproximaci√≥n para determinar el indicador w5p, para y(1.0) como

0.2
(55 f (0.8, w 4 ) ‚àí 59 f (0.6, w 3 ) + 37 f (0.4, w 2 ) ‚àí 9 f (0.2, w 1 ))
24
0.2
(55 f (0.8, 2.1272056) ‚àí 59 f (0.6, 1.6489220)
= 2.1272056 +
24

y(1.0) ‚âà w 5 p = w 4 +

+ 37 f (0.4, 1.2140762) ‚àí 9 f (0.2, 0.8292933))
= 2.1272056 + 0.0083333(55(2.4872056) ‚àí 59(2.2889220)
+ 37(2.0540762) ‚àí 9(1.7892933))
= 2.6409314

5.6

M√©todos multipasos

235

y corregimos esto con

0.2
9 f (1.0, w 5 p ) + 19 f (0.8, w 4 ) ‚àí 5 f (0.6, w 3 ) + f (0.4, w 2 )
24
0.2
(9 f (1.0, 2.6409314) + 19 f (0.8, 2.1272892)
= 2.1272056 +
24

y(1.0) ‚âà w 5 = w 4 +

‚àí 5 f (0.6, 1.6489220) + f (0.4, 1.2140762))
= 2.1272056 + 0.0083333(9(2.6409314) + 19(2.4872056) ‚àí 5(2.2889220)
+ (2.0540762))
= 2.6408286.
En el ejemplo 1, encontramos que utilizar el m√©todo expl√≠cito de Adams‚ÄìBashforth s√≥lo produjo resultados que eran inferiores a los de Runge-Kutta. Sin embargo, estas aproximaciones
para y(0.8) y y(1.0) son precisas, respectivamente, dentro de

|2.1272295 ‚àí 2.1272056| = 2.39 √ó 10‚àí5 y |2.6408286 ‚àí 2.6408591| = 3.05 √ó 10‚àí5 ,
en comparaci√≥n con las de Runge-Kutta, lo cual es preciso, respectivamente, dentro de

|2.1272027 ‚àí 2.1272892| = 2.69 √ó 10‚àí5 y |2.6408227 ‚àí 2.6408591| = 3.64 √ó 10‚àí5 .
Las aproximaciones restantes del indicador-corrector se generaron mediante el algoritmo 5.4
y se muestran en la tabla 5.14.

Tabla 5.14

Edward Arthur Milne (1896‚Äì
1950) trabaj√≥ en investigaci√≥n
bal√≠stica durante la Primera
Guerra Mundial y, despu√©s,
para Solar Physics Observatory,
en Cambridge. En 1929, fue
nombrado W. W. Rouse Ball
Chair en el Wadham College en
Oxford.

El nombre de Simpson est√°
asociado con esta t√©cnica porque
se basa en la regla para la
integraci√≥n de Simpson.

ti

yi = y(ti )

wi

Error
|yi ‚àí w i |

0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0

0.5000000
0.8292986
1.2140877
1.6489406
2.1272295
2.6408591
3.1799415
3.7324000
4.2834838
4.8151763
5.3054720

0.5000000
0.8292933
1.2140762
1.6489220
2.1272056
2.6408286
3.1799026
3.7323505
4.2834208
4.8150964
5.3053707

0
0.0000053
0.0000114
0.0000186
0.0000239
0.0000305
0.0000389
0.0000495
0.0000630
0.0000799
0.0001013

Otros m√©todos multipasos se pueden derivar por medio de integraci√≥n de polinomios de
interpolaci√≥n sobre intervalos de la forma [ t j , ti+1 ], para j ‚â§ i ‚àí 1, para obtener una aproximaci√≥n para y(ti+1 ). Cuando se integra un polinomio de interpolaci√≥n sobre [ti‚àí3 , ti+1 ], el
resultado es el m√©todo expl√≠cito de Milne:

w i+1 = w i‚àí3 +

4h
[2 f (ti , w i ) ‚àí f (ti‚àí1 , w i‚àí1 ) + 2 f (ti‚àí2 , w i‚àí2 )],
3

h 4 y (5) (Œæi ), para algunos Œæi ‚àà (ti‚àí3 , ti+1 ).
que tiene un error de truncamiento local 14
45
El m√©todo de Milne se usa ocasionalmente como indicador para el m√©todo impl√≠cito
de Simpson,
w i+1 = w i‚àí1 +

h
[ f (ti+1 , w i+1 ) + 4 f (ti , w i ) + f (ti‚àí1 , w i‚àí1 )],
3

que tiene un error de truncamiento local ‚àí(h 4 /90)y (5) (Œæi ), para algunos Œæi ‚àà (ti‚àí1 , ti+1 ), y
se obtiene al integrar un polinomio de interpolaci√≥n sobre [ ti‚àí1 , ti+1 ].

236

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

En general, el error de truncamiento local relacionado con el m√©todo indicador corrector
del tipo Milne-Simpson es m√°s peque√±o que el del m√©todo Adams‚ÄìBashforth-Moulton. Pero
la t√©cnica tiene uso limitado debido a los problemas de error de redondeo, los cuales no se
presentan con el procedimiento de Adams. Esta elaboraci√≥n se estudia en la secci√≥n 5.10.
La secci√≥n Conjunto de ejercicios 5.6 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

5.7 M√©todo multipasos de tama√±o de paso variable
El m√©todo Runge-Kutta-Fehlberg se utiliza para el control de error porque en cada paso
ofrece, con un costo adicional peque√±o, dos aproximaciones que se pueden comparar y relacionar con el error de truncamiento local. Las t√©cnicas de indicador-corrector generan dos
aproximaciones en cada paso, por lo que son candidatos naturales para la adaptaci√≥n de
control de error.
Para demostrar el procedimiento de control de error, construimos un m√©todo indicadorcorrector de tama√±o de paso variable mediante el m√©todo expl√≠cito Adams-Bashforth
de cuatro pasos como indicador y el m√©todo impl√≠cito Adams-Moulton de tres pasos como
corrector.
El m√©todo de Adams-Bashforth de cuatro pasos proviene de la relaci√≥n

h
[55 f (ti , y(ti )) ‚àí 59 f (ti‚àí1 , y(ti‚àí1 ))
24
251 (5)
y (ŒºÃÇi )h 5 ,
+ 37 f (ti‚àí2 , y(ti‚àí2 )) ‚àí 9 f (ti‚àí3 , y(ti‚àí3 ))] +
720

y(ti+1 ) = y(ti ) +

para algunos ŒºÃÇi ‚àà (ti‚àí3 , ti+1 ). La suposici√≥n de que las aproximaciones w 0 , w 1 , . . . , w i son
todas exactas, implica que el error de truncamiento local Adams-Bashforth es

y(ti+1 ) ‚àí w p,i+1
251 (5)
=
y (ŒºÃÇi )h 4 .
h
720

(5.40)

Un an√°lisis similar del m√©todo Adams-Moulton de tres pasos, que proviene de

h
[9 f (ti+1 , y(ti+1 )) + 19 f (ti , y(ti )) ‚àí 5 f (ti‚àí1 , y(ti‚àí1 ))
24
19 (5)
y (ŒºÃÉi )h 5 ,
+ f (ti‚àí2 , y(ti‚àí2 ))] ‚àí
720

y(ti+1 ) = y(ti ) +

para algunos ŒºÃÉi ‚àà (ti‚àí2 , ti+1 ), conduce al error de truncamiento local

19 (5)
y(ti+1 ) ‚àí w i+1
=‚àí
y (ŒºÃÉi )h 4 .
h
720

(5.41)

Para avanzar m√°s, debemos suponer que para valores peque√±os de h, tenemos

y (5) (ŒºÃÇi ) ‚âà y (5) (ŒºÃÉi ).
La efectividad de la t√©cnica de control de error depende directamente de esta suposici√≥n.
Si restamos la ecuaci√≥n (5.41) de la ecuaci√≥n (5.40), tenemos

w i+1 ‚àí w p,i+1
3
h4
=
[251y (5) (ŒºÃÇi ) + 19y (5) (ŒºÃÉi )] ‚âà h 4 y (5) (ŒºÃÉi ),
h
720
8

5.7 M√©todo multipasos de tama√±o de paso variable

237

por lo que

y (5) (ŒºÃÉi ) ‚âà

8
(w i+1 ‚àí w p,i+1 ).
3h 5

(5.42)

El uso de este resultado para eliminar el t√©rmino relacionado con y (5) (ŒºÃÉi )h 4 a partir de la
ecuaci√≥n (5.41) provee la aproximaci√≥n para el error de truncamiento local Adams-Moulton

|œÑi+1 (h)| =

19h 4 8
|y(ti+1 ) ‚àí w i+1 |
19|w i+1 ‚àí w p,i+1 |
‚âà
¬∑ 5 |w i+1 ‚àí w p,i+1 | =
.
h
720 3h
270h

Suponga que ahora reconsideramos (ecuaci√≥n 5.41) con un nuevo tama√±o de paso qh al
generar aproximaciones nuevas wÃÇ p,i+1 y wÃÇ i+1 . El objetivo es seleccionar q de tal forma que
el error de truncamiento local provisto en la ecuaci√≥n (5.41) est√© acotada por una tolerancia
prescrita Œµ. Si suponemos que el valor y (5) (Œº) en la ecuaci√≥n (5.41) relacionado con qh tambi√©n se aproxima mediante la ecuaci√≥n (5.42), entonces

|y(ti + qh) ‚àí wÃÇ i+1 |
19q 4 h 4 8
19q 4 h 4 (5)
|w i+1 ‚àí w p,i+1 |
=
|y (Œº)| ‚âà
qh
720
720
3h 5
=

19q 4 |w i+1 ‚àí w p,i+1 |
,
270
h

y necesitamos seleccionar q de tal forma que

|y(ti + qh) ‚àí wÃÇ i+1 |
19q 4 |w i+1 ‚àí w p,i+1 |
‚âà
< Œµ.
qh
270
h
Es decir, seleccione q de tal forma que

q<

hŒµ
270
19 |w i+1 ‚àí w p,i+1 |

1/4

‚âà2

hŒµ
|w i+1 ‚àí w p,i+1 |

1/4

.

En este desarrollo se ha realizado una serie de suposiciones de aproximaci√≥n, por lo que
en la pr√°ctica q se selecciona de manera conservadora, a menudo como

q = 1.5

hŒµ
|w i+1 ‚àí w p,i+1 |

1/4

.

Un cambio en el tama√±o de paso para un m√©todo multipasos es m√°s costoso en t√©rminos
de evaluaciones de funci√≥n que para un m√©todo de un paso porque se deben calcular valores
iniciales igualmente espaciados. En consecuencia, es una pr√°ctica com√∫n ignorar el cambio de
tama√±o de paso siempre que el error de truncamiento local est√© entre Œµ/10 y Œµ, es decir, cuando

Œµ
19|w i+1 ‚àí w p,i+1 |
|y(ti+1 ) ‚àí w i+1 |
< |œÑi+1 (h)| =
‚âà
< Œµ.
10
h
270h
Adem√°s, a q suele asign√°rsele una cota superior para garantizar que una aproximaci√≥n precisa √∫nica poco com√∫n no resulte en un tama√±o de paso demasiado grande. El algoritmo 5.5
incluye esta protecci√≥n con una cota superior de 4.
Recuerde que los m√©todos multipasos requieren tama√±os de paso iguales para los valores iniciales. Por lo que cualquier cambio en el tama√±o de paso necesita recalcular de nuevo
los valores iniciales en ese punto. En los pasos 3, 16 y 19 del algoritmo 5.5, esto se hace
OODPDQGRXQVXEDOJRULWPR5XQJH.XWWD DOJRULWPR FRQ√ÄJXUDGRHQHOSDVR

238

CAP√çTULO 5

ALGORITMO

5.5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

Indicador-corrector de tama√±o de paso variable Adams
Para aproximar la soluci√≥n del problema de valor inicial

y = f (t, y),

a ‚â§ t ‚â§ b,

y(a) = Œ±

con error de truncamiento local dentro de una tolerancia determinada:

ENTRADA extremos a, b; condici√≥n inicial Œ±; tolerancia TOL; tama√±o m√°ximo de
paso hm√°x; tama√±o m√≠nimo de paso hm√≠n.
SALIDA i, ti , w i , h, donde en el i-√©simo paso w i se aproxima a y(ti ) y se utiliza el
tama√±o de paso h, o un mensaje de que se excedi√≥ el tama√±o m√≠nimo de paso.
Paso 1 Configurar un subalgoritmo para el m√©todo Runge-Kutta de cuarto orden llamado a
R K 4(h, v0 , x0 , v1 , x1 , v2 , x2 , v3 , x3 ) que acepta como entrada un tama√±o de paso h
y valores iniciales v0 ‚âà y(x0 ) y regresa{(x j , v j ) | j = 1, 2, 3} definidos mediante
lo siguiente:
para j = 1, 2, 3
determine K 1 = h f (x j‚àí1 , v j‚àí1 );
K 2 = h f (x j‚àí1 + h/2, v j‚àí1 + K 1 /2)
K 3 = h f (x j‚àí1 + h/2, v j‚àí1 + K 2 /2)
K 4 = h f (x j‚àí1 + h, v j‚àí1 + K 3 )
v j = v j‚àí1 + (K 1 + 2K 2 + 2K 3 + K 4 )/6;
x j = x0 + j h.
Paso 2 Determine t0 = a;
w 0 = Œ±;
h = hm√°x;
FLAG = 1; (FLAG se usar√° para salir del ciclo en el paso 4.)
L AST = 0; (LAST indicar√° cuando se calcula la √∫ltima variable..)
SALIDA (t0 , w 0 ).
Paso 3 Llame R K 4(h, w 0 , t0 , w 1 , t1 , w 2 , t2 , w 3 , t3 );
Determine NFLAG = 1; (Indique el c√°lculo de R K 4.)
i = 4;
t = t3 + h.
Paso 4 Mientras (FLAG = 1) haga los pasos 5‚Äì20.
h
[55 f (ti‚àí1 , w i‚àí1 ) ‚àí 59 f (ti‚àí2 , w i‚àí2 )
24
+ 37 f (ti‚àí3 , w i‚àí3 ) ‚àí 9 f (ti‚àí4 , w i‚àí4 )]; (Prediga wi .)

Paso 5 Determine W P = w i‚àí1 +

h
[9 f (t, W P) + 19 f (ti‚àí1 , w i‚àí1 )
24
‚àí 5 f (ti‚àí2 , w i‚àí2 ) + f (ti‚àí3 , w i‚àí3 )]; (Corrija w i .)

W C = w i‚àí1 +

œÉ = 19|W C ‚àí W P|/(270h).
Paso 6 Si œÉ ‚â§ TOL entonces haga los pasos 7‚Äì16 (Resultado aceptado.)
tambi√©n haga los pasos 17‚Äì19. (Resultado rechazado.)
Paso 7 Determine w i = W C; (Resultado aceptado.)
ti = t.
Paso 8 Si NFLAG = 1 entonces para j = i ‚àí 3, i ‚àí 2, i ‚àí 1, i
SALIDA ( j, t j , w j , h);
(Resultados previos tambi√©n aceptados.)
tambi√©n SALIDA (i, ti , w i , h).
(Resultados previos ya aceptados.)

5.7 M√©todo multipasos de tama√±o de paso variable

Paso 9 Si LAST = 1 entonces determine FLAG = 0
tambi√©n haga los pasos 10‚Äì16.

239

(El siguiente paso es 20.)

Paso 10 Determine i = i + 1;
NFLAG = 0.
Paso 11 Si œÉ ‚â§ 0.1 TOL o ti‚àí1 + h > b entonces haga los pasos 12‚Äì16.
(Incrementa h si es m√°s preciso que lo requerido o disminuye
h para incluir b como punto de malla.)
Paso 12 Determine q = (TOL/(2œÉ ))1/4 .
Paso 13 Si q > 4 entonces determine h = 4h
tambi√©n determine h = qh.
Paso 14 Si h > hm√°x entonces determine h = hm√°x.
Paso 15 Si ti‚àí1 + 4h > b entonces
determine h = (b ‚àí ti‚àí1 )/4;
LAST = 1.
Paso 16 Llame R K 4(h, w i‚àí1 , ti‚àí1 , w i , ti , w i+1 , ti+1 , w i+2 , ti+2 );
Determine NFLAG = 1;
i = i + 3. (Rama verdadera completada. Termina paso 6.)
Pr√≥ximo paso 20
Paso 17 Determine q = (TOL/(2œÉ ))1/4 . (Rama falsa a partir del paso 6:
Resultado rechazado.)
Paso 18 Si q < 0.1 entonces determine h = 0.1h
tambi√©n determine h = qh.
Paso 19 Si h < hm√≠n entonces determine FLAG = 0;
SALIDA (‚Äòm√≠nima excedida‚Äô)
tambi√©n
si NFLAG = 1 entonces determine i = i ‚àí 3;
(Resultados previos tambi√©n rechazados.)
Llame R K 4(h, w i‚àí1 , ti‚àí1 , w i , ti , w i+1 ,
ti+1 , w i+2 , ti+2 );
determine i = i + 3;
NFLAG = 1. ( Fin del paso 6.)
Paso 20 Determine t = ti‚àí1 + h. (Fin del paso 4.)
Paso 21 PARE.

Ejemplo 1

Utilice el m√©todo indicador-corrector de tama√±o de paso variable de Adams con tama√±o
m√°ximo de paso hm√°x 5 0.2, tama√±o m√≠nimo de paso hm√≠n 5 0.01, y tolerancia TOL 5 1025
para aproximar la soluci√≥n del problema de valor inicial

y = y ‚àí t 2 + 1,

0 ‚â§ t ‚â§ 2,

y(0) = 0.5.

Soluci√≥n Comenzamos con h 5 hm√°x 5 0.2 y obtenemos w0, w1, w2, y w3 mediante RungeKutta, a continuaci√≥n encontramos wp4 y wc4 al aplicar el m√©todo indicador-corrector. Estos
c√°lculos se realizaron en el ejemplo 5 de la secci√≥n 5.6, donde se determin√≥ que las aproximaciones Runge-Kutta con

y(0) = w 0 = 0.5, y(0.2) ‚âà w 1 = 0.8292933, y(0.4) ‚âà w 2 = 1.2140762, y
y(0.6) ‚âà w 3 = 1.6489220.

240

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

El indicador y corrector dan

y(0) = w 0 = 0.5, y(0.2) ‚âà w 1 = 0.8292933, y(0.4) ‚âà w 2 = 1.2140762, y
y(0.6) ‚âà w 3 = 1.6489220
y

y(0.8) ‚âà w 4 p = w 3 +

0.2
(55 f (0.6, w 3 ) ‚àí 59 f (0.4, w 2 ) + 37 f (0.2, w 1 ) ‚àí 9 f (0, w 0 ))
24

= 2.1272892
y

y(0.8) ‚âà w 4c = w 3 +

0.2
9 f (0.8, w 4 p ) + 19 f (0.6, w 3 ) ‚àí 5 f (0.42, w 2 ) + f (0.2, w 1 )
24

= 2.1272056.
$KRUDQHFHVLWDPRVGHWHUPLQDUVLODVDSUR[LPDFLRQHVVRQVX√ÄFLHQWHPHQWHSUHFLVDVRVLVH
necesita cambiar el tama√±o de paso. Primero, encontramos

œÉ =

19
19
|w 4c ‚àí w 4 p | =
|2.1272056 ‚àí 2.1272892| = 2.941 √ó 10‚àí5 .
270h
270(0.2)

Puesto que esto excede la tolerancia de 1025, se necesita un tama√±o de paso nuevo y √©ste es

qh =

10‚àí5
2Œ¥

1/4

=

1/4

10‚àí5
2(2.941 √ó 10‚àí5 )

(0.2) = 0.642(0.2) ‚âà 0.128.

Como consecuencia, necesitamos comenzar de nuevo con el procedimiento, al calcular los
valores de Runge-Kutta con este tama√±o de paso y, a continuaci√≥n, utilizamos el m√©todo
indicador-corrector con este tama√±o de paso para calcular los valores nuevos de w4p y w4c.
'HVSXpVQHFHVLWDPRVHMHFXWDUXQDYHUL√ÄFDFLyQGHSUHFLVLyQHQHVWDVDSUR[LPDFLRQHVSDUD
ver si tuvimos √©xito. La tabla 5.15 muestra si esta segunda ejecuci√≥n es exitosa y enumera
todos los resultados obtenidos en el algoritmo 5.5.

Tabla 5.15

ti

y(ti )

wi

hi

œÉi

|y(ti ) ‚àí w i |

0
0.12841297
0.25682594
0.38523891
0.51365188
0.64206485
0.77047782
0.89889079
1.02730376
1.15571673
1.28412970
1.38980552
1.49548134
1.60115716
1.70683298
1.81250880
1.91818462
1.93863847
1.95909231
1.97954616
2.00000000

0.5
0.70480460
0.93320140
1.18390410
1.45545014
1.74617653
2.05419248
2.37734803
2.71319871
3.05896505
3.41148675
3.70413577
3.99668536
4.28663498
4.57120536
4.84730747
5.11150794
5.16095461
5.20978430
5.25796697
5.30547195

0.5
0.70480402
0.93320019
1.18390218
1.45544767
1.74617341
2.05418856
2.37734317
2.71319271
3.05895769
3.41147778
3.70412572
3.99667414
4.28662249
4.57119105
4.84729107
5.11148918
5.16093546
5.20976475
5.25794701
5.30545159

0.12841297
0.12841297
0.12841297
0.12841297
0.12841297
0.12841297
0.12841297
0.12841297
0.12841297
0.12841297
0.10567582
0.10567582
0.10567582
0.10567582
0.10567582
0.10567582
0.02045384
0.02045384
0.02045384
0.02045384

4.431680 √ó 10‚àí6
4.431680 √ó 10‚àí6
4.431680 √ó 10‚àí6
4.431680 √ó 10‚àí6
5.057497 √ó 10‚àí6
5.730989 √ó 10‚àí6
6.522850 √ó 10‚àí6
7.416639 √ó 10‚àí6
8.433180 √ó 10‚àí6
9.588365 √ó 10‚àí6
7.085927 √ó 10‚àí6
7.085927 √ó 10‚àí6
7.085927 √ó 10‚àí6
7.085927 √ó 10‚àí6
7.844396 √ó 10‚àí6
8.747367 √ó 10‚àí6
1.376200 √ó 10‚àí8
1.376200 √ó 10‚àí8
1.376200 √ó 10‚àí8
1.376200 √ó 10‚àí8

0.0000005788
0.0000012158
0.0000019190
0.0000024670
0.0000031210
0.0000039170
0.0000048660
0.0000060010
0.0000073570
0.0000089720
0.0000100440
0.0000112120
0.0000124870
0.0000143120
0.0000163960
0.0000187650
0.0000191530
0.0000195490
0.0000199540
0.0000203670

5.8 M√©todos de extrapolaci√≥n

241

La secci√≥n Conjunto de ejercicios 5.7 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

5.8 M√©todos de extrapolaci√≥n
/D H[WUDSRODFLyQ VH XVy HQ OD VHFFLyQ  SDUD DSUR[LPDU LQWHJUDOHV GH√ÄQLGDV HQ GRQGH
encontramos que al promediar de manera correcta las aproximaciones trapezoidales, relativamente imprecisas, se produc√≠an otras nuevas en extremo precisas. En esta secci√≥n aplicaremos la extrapolaci√≥n para incrementar la precisi√≥n de las aproximaciones para la soluci√≥n
de problemas de valor inicial. Como observamos antes, las aproximaciones originales deben
WHQHUXQDH[SDQVLyQGHHUURUGHXQDIRUPDHVSHFt√ÄFDSDUDTXHHOSURFHGLPLHQWRVHDH[LWRVR
Para aplicar la extrapolaci√≥n en la resoluci√≥n de problemas de valor inicial usamos la
t√©cnica con base en el m√©todo de punto medio:

w i+1 = w i‚àí1 + 2h f (ti , w i ),

para i ‚â• 1.

(5.43)

Esta t√©cnica requiere dos valores iniciales w0 y w1 que se necesitan antes de que se pueda determinar la primera aproximaci√≥n del punto medio w2. Un valor inicial es la condici√≥n inicial
para w 0 = y(a) = Œ±. Para determinar el segundo valor inicial, w1, aplicamos el m√©todo de
Euler. Las aproximaciones subsiguientes se obtienen a partir de la ecuaci√≥n (5.43). Despu√©s
de generar una serie de aproximaciones de este tipo que terminan en un valor t, se realiza una
FRUUHFFLyQGHOH[WUHPRUHODFLRQDGDFRQODVGRVDSUR[LPDFLRQHV√ÄQDOHVGHSXQWRPHGLR(VWR
produce una aproximaci√≥n w(t, h) para y(t) que tiene la forma
‚àû

y(t) = w(t, h) +

Œ¥k h 2k ,

(5.44)

k=1

donde Œ¥k son constantes relacionadas con las derivadas de la soluci√≥n y(t). El punto importante es que Œ¥k no depende del tama√±o de paso h. Los detalles de este procedimiento se
pueden encontrar en el art√≠culo de Gragg [Gr].
Para ilustrar la t√©cnica de extrapolaci√≥n para resolver

y (t) = f (t, y),

a ‚â§ t ‚â§ b,

y(a) = Œ±,

VXSRQJDTXHWHQHPRVXQWDPDxRGHSXQWR√ÄMRh. Nos gustar√≠a aproximar y(t1 ) = y(a + h).
Para el primer paso de extrapolaci√≥n, dejamos que h 0 = h/2 y usamos el m√©todo de
Euler con w 0 = Œ± para aproximar y(a + h 0 ) = y(a + h/2) como

w 1 = w 0 + h 0 f (a, w 0 ).
Aplicamos entonces el m√©todo de punto medio con ti‚àí1 = a y ti = a + h 0 = a + h/2 para
producir una primera aproximaci√≥n y(a + h) = y(a + 2h 0 ),

w 2 = w 0 + 2h 0 f (a + h 0 , w 1 ).
6HDSOLFDODFRUUHFFLyQGHOH[WUHPRSDUDREWHQHUODDSUR[LPDFLyQ√ÄQDOSDUDy(a + h) para el
tama√±o de paso h0. Esto resulta en la aproximaci√≥n O(h 20 ) para y(t1).

y1,1 =

1
[w 2 + w 1 + h 0 f (a + 2h 0 , w 2 )].
2

Guardamos la aproximaci√≥n y1,1 y descartamos los resultados intermedios w1 y w2.
Para obtener la aproximaci√≥n y2,1, para y(t1), hacemos h 1 = h/4 y usamos el m√©todo
de Euler con w 0 = Œ± para obtener una aproximaci√≥n para y(a + h 1 ) = y(a + h/4), que
llamaremos w1:

w 1 = w 0 + h 1 f (a, w 0 ).

242

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

A continuaci√≥n aproximamos y(a + 2h 1 ) = y(a + h/2) con w 2 , y(a + 3h 1 ) = y(a +
3h/4) con w3, y w4 para y(a + 4h 1 ) = y(t1 ) con el m√©todo de punto medio:

w 2 = w 0 + 2h 1 f (a + h 1 , w 1 ),
w 3 = w 1 + 2h 1 f (a + 2h 1 , w 2 ),
y

w 4 = w 2 + 2h 1 f (a + 3h 1 , w 3 ).
Ahora, se aplica la correcci√≥n del extremo para w3 y w4 para producir la aproximaci√≥n mejorada O(h 21 ) para y(t1),

y2,1 =

1
[w 4 + w 3 + h 1 f (a + 4h 1 , w 4 )].
2

Debido a la forma el error provisto en la ecuaci√≥n (5.44), las dos aproximaciones para
y(a + h) tienen la propiedad de que
2

h
2

y(a + h) = y1,1 + Œ¥1

4

h
2

+ Œ¥2

+ ¬∑ ¬∑ ¬∑ = y1,1 + Œ¥1

h2
h4
+ Œ¥2
+ ¬∑¬∑¬∑
4
16

y

y(a + h) = y2,1 + Œ¥1

h
4

2

+ Œ¥2

h
4

4

+ ¬∑ ¬∑ ¬∑ = y2,1 + Œ¥1

h2
h4
+ Œ¥2
+ ¬∑¬∑¬∑ .
16
256

Podemos eliminar la parte O(h2) de este error de truncamiento al promediar las dos f√≥rmulas
de manera adecuada. Especialmente, si restamos la primera f√≥rmula a cuatro veces la segunda
y dividimos el resultado entre tres, obtenemos

h4
1
+ ¬∑¬∑¬∑ .
y(a + h) = y2,1 + (y2,1 ‚àí y1,1 ) ‚àí Œ¥2
3
64
Por lo que la aproximaci√≥n para y(t1) dada por

1
y2,2 = y2,1 + (y2,1 ‚àí y1,1 )
3
tiene un error de orden O(h4).
A continuaci√≥n hacemos h 2 = h/6 y aplicamos el m√©todo de Euler una vez, seguido del
m√©todo de punto medio cinco veces. A continuaci√≥n, utilizamos la correcci√≥n del extremo
para determinar la aproximaci√≥n h2, y3, 1, para y(a + h) = y(t1 ). Esta aproximaci√≥n se puede promediar con y2,1 para producir una segunda aproximaci√≥n O(h4) que designamos y3, 2.
A continuaci√≥n, se promedia y3, 2 y y2, 2 para eliminar los t√©rminos de error O(h4) y producimos una aproximaci√≥n con error de orden O(h6). Se generan f√≥rmulas de orden superior al
continuar con el proceso.
/D~QLFDGLIHUHQFLDVLJQL√ÄFDWLYDHQWUHODH[WUDSRODFLyQUHDOL]DGDDTXt\ODTXHXWLOL]DLQtegraci√≥n de Romberg en la secci√≥n 4.5 resulta de la forma de seleccionar las subdivisiones.
En la integraci√≥n de Romberg, existe una f√≥rmula conveniente para representar las aproximaciones de la regla trapezoidal compuesta que usa divisiones consecutivas del tama√±o de
paso mediante los enteros 1, 2, 4, 8, 16, 32, 64,   . Este procedimiento permite el proceso
de promediado para continuar de una manera que se puede seguir f√°cilmente.
1RWHQHPRVORVPHGLRVSDUDSURGXFLUIiFLOPHQWHDSUR[LPDFLRQHVUH√ÄQDGDVSDUDSUREOHmas de valor inicial, por lo que se seleccionan las divisiones para la t√©cnica de extrapolaci√≥n para minimizar el n√∫mero de evaluaciones de funci√≥n requerido. El procedimiento de

5.8 M√©todos de extrapolaci√≥n

243

promediado que surge de esta selecci√≥n de la subdivisi√≥n, mostrado en la tabla 5.16, no es
b√°sico, pero, aparte de eso, el proceso es igual al que se usa para la integraci√≥n de Romberg.

Tabla 5.16

y1,1 = w(t, h 0 )
y2,1 = w(t, h 1 )
y3,1 = w(t, h 2 )

El algoritmo 5.6 utiliza nodos de
la forma 2n y 2n ? 3. Es posible
usar otras opciones.

h 21
(y2,1 ‚àí y1,1 )
2
h 0 ‚àí h 21
2
h
y3,2 = y3,1 + 2 2 2 (y3,1 ‚àí y2,1 )
h1 ‚àí h2
y2,2 = y2,1 +

y3,3 = y3,2 +

h 22
(y3,2 ‚àí y2,2 )
h 20 ‚àí h 22

El algoritmo 5.6 utiliza la t√©cnica de extrapolaci√≥n con la secuencia de enteros

q0 = 2, q1 = 4, q2 = 6, q3 = 8, q4 = 12, q5 = 16, q6 = 24

y

q7 = 32.

Se selecciona un tama√±o de paso b√°sico h, y el m√©todo progresa mediante h i = h/qi, para
cada i = 0, . . . , 7, para aproximar y(t + h). El error se controla al requerir que se calculen
las aproximaciones y1,1 , y2,2 , . . . hasta |yi,i ‚àí yi‚àí1,i‚àí1 | sea menor a la tolerancia provista. Si
la tolerancia no se alcanza mediante i 5 8, entonces se disminuye h y el proceso se vuelve
a aplicar.
6H HVSHFL√ÄFDQ ORV YDORUHV Pi[LPR \ PtQLPR GH h, hm√≠n, y hm√°x, respectivamente,
para garantizar el control del m√©todo. Si se encuentra que yi,i es aceptable, entonces w1 se
FRQ√ÄJXUD HQ yi, i y los c√°lculos comienzan de nuevo para determinar w2, que aproximar√°
y(t2 ) = y(a + 2h). El proceso se repite hasta que se determina la aproximaci√≥n wN para y(b).

ALGORITMO

5.6

Extrapolaci√≥n
Para aproximar la soluci√≥n del problema de valor inicial

y = f (t, y),

a ‚â§ t ‚â§ b,

y(a) = Œ±,

con error de truncamiento local dentro de una tolerancia determinada:
ENTRADA extremos a, b; condici√≥n inicial Œ±; tolerancia TOL; tama√±o m√°ximo de
paso hm√°x; tama√±o m√≠nimo de paso hm√≠n.
SALIDA
T, W, h, donde W aproxima y(t) y se usa el tama√±o de paso h o
un mensaje de que se excedi√≥ el tama√±o m√≠nimo de paso.
Paso 1 Inicialice el arreglo N K = (2, 4, 6, 8, 12, 16, 24, 32).
Paso 2 Determine T O = a;
W O = Œ±;
h = hm√°x;
FLAG = 1. (Se usa FLAG para salir del ciclo en el paso 4.)
Paso 3 Para i = 1, 2, . . . , 7
para j = 1, . . . , i
2
determine Q i, j = (N K i+1 /N K j )2 . (Nota: Q i, j = h 2j / h i+1
.)
Paso 4 Mientras (FLAG = 1) haga los pasos 5‚Äì20.
Paso 5 Determine k = 1;
NFLAG = 0. (Cuando se alcanza la precisi√≥n deseada,
NFLAG se configura en1.)

244

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

Paso 6 Mientras (k ‚â§ 8 y NFLAG = 0) haga los pasos 7‚Äì14.
Paso 7 Determine H K = h/N K k ;
T = T O;
W 2 = W O;
W 3 = W 2 + H K ¬∑ f (T, W 2);
T = T O + H K.

(Primer paso de Euler. )

Paso 8 Para j = 1, . . . , N K k ‚àí 1
determine W 1 = W 2;
W 2 = W 3;
W 3 = W 1 + 2H K ¬∑ f (T, W 2); (M√©todo de punto medio. )
T = T O + ( j + 1) ¬∑ H K .
Paso 9 Determine yk = [W 3 + W 2 + H K ¬∑ f (T, W 3)]/2.
(Correcci√≥n de extremo para calcular yk,1 .)
Paso 10 Si k ‚â• 2 haga los pasos 11‚Äì13.
(Nota: yk‚àí1 ‚â° yk‚àí1,1 , yk‚àí2 ‚â° yk‚àí2,2 , . . . , y1 ‚â° yk‚àí1,k‚àí1 ya que s√≥lo
se guarda la fila previa de la tabla.)
Paso 11 Determine j = k;
v = y1 . (Guarda y k‚àí1,k‚àí1 .)
Paso 12 Mientras ( j ‚â• 2) haga
determine y j‚àí1 = y j +

y j ‚àí y j‚àí1
;
Q k‚àí1, j‚àí1 ‚àí 1

(Extrapolaci√≥n para calcular y j‚àí1 ‚â° yk,k‚àí j+2 .)
Nota:

y j‚àí1 =

h 2j‚àí1 y j ‚àí h 2k y j‚àí1
h 2j‚àí1 ‚àí h 2k

.

j = j ‚àí 1.
Paso 13 Si|y1 ‚àí v| ‚â§ TOL entonces haga NFLAG = 1.
(y1 aceptado como w nueva.)
Paso 14 Determine k = k + 1.
Paso 15 Determine k = k ‚àí 1.

(Termina paso 6 )

(Parte del paso 4)

Paso 16 Si NFLAG = 0 haga los pasos 17 y 18 (Resultado rechazado.)
tambi√©n haga los pasos 19 y 20. ( Resultado aceptado. )
Paso 17 Determine h = h/2. (Valor nuevo para w rechazado, h disminuye.)
Paso 18 Si h < hm√≠n entonces
SALIDA (‚Äòhm√≠n excedida ‚Äô);
Determine FLAG = 0. (Termina paso 16 )
(Rama verdadera completada, el siguiente paso
regresa al paso 4.)
Paso 19 Determine W O = y1 ; (Valor de w nuevo aceptado.)
T O = T O + h;
SALIDA (T O, W O, h).

5.8 M√©todos de extrapolaci√≥n

245

Paso 20 Si T O ‚â• b entonces determine FLAG = 0
(Procedimiento completado con √©xito. )
tambi√©n si T O + h > b entonces determine h = b ‚àí T O
(Termina en t = b.)
tambi√©n si (k ‚â§ 3 y h < 0.5(hm√°x) entonces determine h = 2h.
(Incrementa tama√±o de paso, de ser posible)
(Fin del paso 4 y 16)
Paso 21 PARE.
Ejemplo 1

Use el m√©todo de extrapolaci√≥n con el tama√±o de paso m√°ximo hm√°x 5 0.2, tama√±o de paso
m√≠nimo hm√≠n 5 0.01 y tolerancia TOL 5 1029 para aproximar la soluci√≥n del problema de
valor inicial

y = y ‚àí t 2 + 1,

0 ‚â§ t ‚â§ 2,

y(0) = 0.5.

Para el primer paso del m√©todo de extrapolaci√≥n, hacemos w 0 = 0.5, t0 = 0, y
h 5 0.2. A continuaci√≥n, calculamos

Soluci√≥n

h 0 = h/2 = 0.1,
w 1 = w 0 + h 0 f (t0 , w 0 ) = 0.5 + 0.1(1.5) = 0.65,
y

w 2 = w 0 + 2h 0 f (t0 + h 0 , w 1 ) = 0.5 + 0.2(1.64) = 0.828,
y la primera aproximaci√≥n para y(0.2) es

y11 =

1
1
(w 2 + w 1 + h 0 f (t0 + 2h 0 , w 2 )) = (0.828 + 0.65 + 0.1 f (0.2, 0.828))
2
2

= 0.8284.
Para la segunda aproximaci√≥n para y(0.2), calculamos

h 1 = h/4 = 0.05,
w 1 = w 0 + h 1 f (t0 , w 0 ) = 0.5 + 0.05(1.5) = 0.575,
w 2 = w 0 + 2h 1 f (t0 + h 1 , w 1 ) = 0.5 + 0.1(1.5725) = 0.65725,
w 3 = w 1 + 2h 1 f (t0 + 2h 1 , w 2 ) = 0.575 + 0.1(1.64725) = 0.739725,
y

w 4 = w 2 + 2h 1 f (t0 + 3h 1 , w 3 ) = 0.65725 + 0.1(1.717225) = 0.8289725.
Entonces, la aproximaci√≥n de la correcci√≥n del extremo es

1
(w 4 + w 3 + h 1 f (t0 + 4h 1 , w 4 ))
2
1
= (0.8289725 + 0.739725 + 0.05 f (0.2, 0.8289725)) = 0.8290730625.
2

y21 =

Esto nos da la primera aproximaci√≥n de extrapolaci√≥n

y22 = y21 +

(1/4)2
(1/2)2 ‚àí (1/4)2

(y21 ‚àí y11 ) = 0.8292974167.

246

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

La tercera aproximaci√≥n se encuentra al calcular

h 2 = h/6 = 0.03,
w 1 = w 0 + h 2 f (t0 , w 0 ) = 0.55,
w 2 = w 0 + 2h 2 f (t0 + h 2 , w 1 ) = 0.6032592593,
w 3 = w 1 + 2h 2 f (t0 + 2h 2 , w 2 ) = 0.6565876543,
w 4 = w 2 + 2h 2 f (t0 + 3h 2 , w 3 ) = 0.7130317696,
w 5 = w 3 + 2h 2 f (t0 + 4h 2 , w 4 ) = 0.7696045871,
w 6 = w 4 + 2h 2 f (t0 + 5h 2 , w 5 ) = 0.8291535569,
y entonces la aproximaci√≥n de correcci√≥n del extremo

y31 =

1
(w 6 + w 5 + h 2 f (t0 + 6h 2 , w 6 ) = 0.8291982979.
2

Ahora podemos encontrar dos aproximaciones extrapoladas

y32 = y31 +

(1/6)2
(1/4)2 ‚àí (1/6)2

(y31 ‚àí y21 ) = 0.8292984862

y33 = y32 +

(1/6)2
(1/2)2 ‚àí (1/6)2

(y32 ‚àí y22 ) = 0.8292986199.

y

Puesto que

|y33 ‚àí y22 | = 1.2 √ó 10‚àí6
QRVDWLVIDFHODWROHUDQFLDQHFHVLWDPRVFDOFXODUSRUORPHQRVXQD√ÄODGHODWDEODGHH[WUDpolaci√≥n. Usamos h 3 = h/8 = 0.025 y calculamos w1 con el m√©todo de Euler w 2 , ¬∑ ¬∑ ¬∑ , w 8 ,
a trav√©s del m√©todo de punto medio y aplicamos la correcci√≥n del extremo. Esto nos proporcionar√° la aproximaci√≥n nueva y41TXHQRVSHUPLWHFDOFXODUODQXHYD√ÄODGHH[WUDSRODFLyQ

y41 = 0.8292421745 y42 = 0.8292985873 y43 = 0.8292986210 y44 = 0.8292986211.
Al comparar |y44 ‚àí y33 | = 1.2 √ó 10‚àí9, encontramos que la tolerancia de precisi√≥n no se
KD DOFDQ]DGR 3DUD REWHQHU ODV HQWUDGDV HQ OD VLJXLHQWH √ÄOD XVDPRV h 4 = h/12 = 0.06.
Primero, calcule w1 con el m√©todo de Euler, a continuaci√≥n w2 a trav√©s de w12 con el m√©todo
de punto medio. Finalmente, utilice la correcci√≥n del extremo para obtener y51. Las entradas
restantes en la quinta columna se obtienen por medio de extrapolaci√≥n y se muestran en la
tabla 5.17. Puesto que y55 2 0.8292986213 est√° dentro de 1029 de y44, se acepta como
la extrapolaci√≥n para y(0.2). El procedimiento provee una nueva aproximaci√≥n y(0.4). El
conjunto completo de aproximaciones precisas para los lugares listados se proporciona en
la tabla 5.18.

Tabla 5.17
y1,1 = 0.8284000000
y2,1 = 0.8290730625
y3,1 = 0.8291982979
y4,1 = 0.8292421745
y5,1 = 0.8292735291

y2,2 = 0.8292974167
y3,2 = 0.8292984862
y4,2 = 0.8292985873
y5,2 = 0.8292986128

y3,3 = 0.8292986199
y4,3 = 0.8292986210
y5,3 = 0.8292986213

y4,4 = 0.8292986211
y5,4 = 0.8292986213

y5,5 = 0.8292986213

5.9 Ecuaciones de orden superior y sistemas de ecuaciones diferenciales

Tabla 5.18

ti

yi = y(ti )

wi

hi

k

0.200
0.400
0.600
0.700
0.800
0.900
0.925
0.950
1.000
1.100
1.200
1.300
1.400
1.450
1.475
1.525
1.575
1.675
1.775
1.825
1.875
1.925
1.975
2.000

0.8292986210
1.2140876512
1.6489405998
1.8831236462
2.1272295358
2.3801984444
2.4446908698
2.5096451704
2.6408590858
2.9079169880
3.1799415386
3.4553516662
3.7324000166
3.8709427424
3.9401071136
4.0780532154
4.2152541820
4.4862274254
4.7504844318
4.8792274904
5.0052154398
5.1280506670
5.2473151731
5.3054719506

0.8292986213
1.2140876510
1.6489406000
1.8831236460
2.1272295360
2.3801984450
2.4446908710
2.5096451700
2.6408590860
2.9079169880
3.1799415380
3.4553516610
3.7324000100
3.8709427340
3.9401071050
4.0780532060
4.2152541820
4.4862274160
4.7504844210
4.8792274790
5.0052154290
5.1280506570
5.2473151660
5.3054719440

0.200
0.200
0.200
0.100
0.100
0.100
0.025
0.025
0.050
0.100
0.100
0.100
0.100
0.050
0.025
0.050
0.050
0.100
0.100
0.050
0.050
0.050
0.050
0.025

5
4
4
5
4
7
8
3
3
7
6
8
5
7
3
4
3
4
4
3
3
4
8
3

247

La prueba de que el m√©todo presentado en el algoritmo 5.6 converge implica resultados
a partir de la teor√≠a de capacidad de suma; se puede encontrar en el art√≠culo original de Gragg
[Gr]. Existen otros procedimientos de extrapolaci√≥n, algunos de los cuales utilizan t√©cnicas
de tama√±o de paso variable. Para los procedimientos adicionales con base en los procesos de
extrapolaci√≥n, consulte los art√≠culos de Bulirsch y Stoer [BS1], [BS2] y [BS3] o el texto de
Stetter [Stet]. Los m√©todos usados por Bulirsch y Stoer implican interpolaci√≥n con funciones
racionales en lugar de la interpolaci√≥n polinomial utilizada en el procedimiento de Gragg.
La secci√≥n Conjunto de ejercicios 5.8 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

5.9 Ecuaciones de orden superior y sistemas de ecuaciones
diferenciales
Esta secci√≥n contiene una introducci√≥n a la soluci√≥n num√©rica de problemas de valor inicial
de orden superior. Las t√©cnicas analizadas est√°n limitadas a aquellas que transforman una
ecuaci√≥n de orden superior en un sistema de ecuaciones diferenciales de primer orden. Antes
de analizar el procedimiento de transformaci√≥n, se necesitan algunas observaciones respecto
a los sistemas que implican ecuaciones diferenciales de primer orden.
Un sistema de m-√©simo orden de problemas de valor inicial de primer orden tiene la
forma
du 1
= f 1 (t, u 1 , u 2 , . . . , u m ),
dt
du 2
= f 2 (t, u 1 , u 2 , . . . , u m ),
(5.45)
dt
..
.

du m
= f m (t, u 1 , u 2 , . . . , u m ),
dt

248

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

para a ‚â§ t ‚â§ b, con condiciones iniciales

u 1 (a) = Œ±1 , u 2 (a) = Œ±2 , . . . , u m (a) = Œ±m .

(5.46)

El objetivo es encontrar m funciones u 1 (t), u 2 (t), . . . , u m (t) que satisfacen cada una de las
ecuaciones diferenciales junto con todas las condiciones iniciales.
Para analizar la existencia y unicidad de las soluciones para los sistemas de ecuaciones,
QHFHVLWDPRVDPSOLDUODGH√ÄQLFLyQGHODFRQGLFLyQGH/LSVFKLW]SDUDODVIXQFLRQHVGHGLVWLQWDV
variables.
DeÔ¨Ånici√≥n 5.16

Se dice que la funci√≥n f (t, y1 , . . . , ym )GH√ÄQLGDHQHOFRQMXQWR

D = {(t, u 1 , . . . , u m ) | a ‚â§ t ‚â§ b y ‚àí ‚àû < u i < ‚àû, para cada i = 1, 2, . . . , m},
satisface la condici√≥n de Lipschitz en D en las variables u 1 , u 2 , . . . , u m si existe una constante L > 0 con
m

| f (t, u 1 , . . . , u m ) ‚àí f (t, z 1 , . . . , z m )| ‚â§ L

|u j ‚àí z j |,
j=1

(5.47)

para todas las (t, u 1 , . . . , u m ) y (t, z 1 , . . . , z m ) en D.
Al usar el teorema de valor medio se puede mostrar que si f y sus primeras derivadas
parciales son continuas en D y si

‚àÇ f (t, u 1 , . . . , u m )
‚â§ L,
‚àÇu i
para cada i = 1, 2, . . . , m y todas las (t, u 1 , . . . , u m ) en D, entonces f satisface la condici√≥n
de Lipschitz en D con constante de Lipschitz L (consulte [BiR], p. 141). A continuaci√≥n se
muestra un teorema de existencia y unicidad b√°sico, su demostraci√≥n se puede encontrar en
[BiR], p. 152-154.
Teorema 5.17

Suponga que

D = { (t, u 1 , u 2 , . . . , u m ) | a ‚â§ t ‚â§ b y ‚àí ‚àû < u i < ‚àû, para cada i = 1, 2, . . . , m },
y f i (t, u 1 , . . . , u m ), para cada i = 1, 2, . . . , m y que es continua y satisface la condici√≥n
de Lipschitz en D. El sistema de ecuaciones diferenciales de primer orden (5.45), sujeto a
las condiciones iniciales (5.46), tiene una √∫nica soluci√≥n u 1 (t), . . . , u m (t), para a ‚â§ t ‚â§ b.
Los m√©todos para resolver sistemas de ecuaciones diferenciales de primer orden son generalizaciones de los m√©todos de ecuaciones de primer orden simples que se han presentado
antes en este cap√≠tulo. Por ejemplo, el m√©todo cl√°sico de Runge-Kutta de orden 4 dado por

w 0 = Œ±,
k1 = h f (ti , w i ),
k2 = h f ti +

h
1
, w i + k1 ,
2
2

k3 = h f ti +

h
1
, w i + k2 ,
2
2

k4 = h f (ti+1 , w i + k3 ),
1
w i+1 = w i + (k1 + 2k2 + 2k3 + k4 ),
6

para cada i = 0, 1, . . . , N ‚àí 1,

5.9 Ecuaciones de orden superior y sistemas de ecuaciones diferenciales

249

utilizado para resolver el problema de valor inicial de primer orden

y = f (t, y),

a ‚â§ t ‚â§ b,

y(a) = Œ±,

se generaliza de acuerdo con lo siguiente.
Si se selecciona un entero N > 0 y se establece h = (b ‚àí a)/N . La partici√≥n del intervalo [a, b] en N subintervalos con los puntos de malla

t j = a + j h,

para cada j = 0, 1, . . . , N .

Use la notaci√≥n wi j, para cada j 5 0, 1,   , N y i 5 1, 2,   , m, para denotar una aproximaci√≥n u i (t j ). Es decir, wi j aproxima la i-√©sima soluci√≥n ui(t) de (5.45) en el j-√©simo punto
de malla tj3DUDODVFRQGLFLRQHVLQLFLDOHVHVWDEOH]FD FRQVXOWHOD√ÄJXUD
w1,0 5 a1, w2,0 5 a2,   , wm,0 5 am.

(5.48)

Figura 5.6

y

y

w11
w12
w13

w23
w22
u1(a) 5 a1

u1(t)

y
wm3 um(a) 5 am
wm2

u2(t)

um(t)

wm1

w21
u2(a) 5 a2

a 5 t0 t1

t2

t3

t

a 5 t0 t1

t2

t3

t

a 5 t0 t1

t2

t3

t

Suponga que los valores w 1, j , w 2, j , . . . , w m, j se han calculado. Nosotros obtenemos
w 1, j+1 , w 2, j+1 , . . . , w m, j+1 al calcular primero

k1,i = h f i (t j , w 1, j , w 2, j , . . . , w m, j ),
k2,i = h f i

tj +

para cada i = 1, 2, . . . , m,

(5.49)

h
1
1
1
, w 1, j + k1,1 , w 2, j + k1,2 , . . . , w m, j + k1,m ,
2
2
2
2

(5.50)

h
1
1
1
, w 1, j + k2,1 , w 2, j + k2,2 , . . . , w m, j + k2,m ,
2
2
2
2

(5.51)

para cada i = 1, 2, . . . , m;

k3,i = h f i

tj +

para cada i = 1, 2, . . . , m;

k4,i = h f i (t j + h, w 1, j + k3,1 , w 2, j + k3,2 , . . . , w m, j + k3,m ),

(5.52)

para cada i = 1, 2, . . . , m; y entonces

1
w i, j+1 = w i, j + (k1,i + 2k2,i + 2k3,i + k4,i ),
6

(5.53)

para cada i = 1, 2, . . . , m . Observe que todos los valores k1,1 , k1,2 , . . . , k1,m deben calcularse antes de que cualquiera de los t√©rminos de la forma k2,i se pueda determinar. En
general, cada kl,1 , kl,2 , . . . , kl,m se debe calcular antes que cualquiera de las expresiones kl+1,i.
El algoritmo 5.7 implementa el m√©todo Runge-Kutta de cuarto orden para sistemas de problemas de valor inicial.

250

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

ALGORITMO

5.7

M√©todo Runge-Kutta para sistemas de ecuaciones diferenciales
Para aproximar la soluci√≥n del sistema de m-√©simo orden de problemas de valor inicial de
primer orden

u j = f j (t, u 1 , u 2 , . . . , u m ),

a ‚â§ t ‚â§ b,

con

u j (a) = Œ± j ,

para j = 1, 2, . . . , m en (N + 1) n√∫meros igualmente espaciados en el intervalo [a, b]:
ENTRADA extremos a, b; n√∫mero de ecuaciones m; entero N; condiciones iniciales Œ±1 , . . . , Œ±m .
SALIDA

aproximaciones w j para u j (t) en (N + 1) valores de t.

Paso 1 Determine h = (b ‚àí a)/N ;
t = a.
Paso 2 Para j = 1, 2, . . . , m determine w j = Œ± j .
Paso 3 SALIDA (t, w 1 , w 2 , . . . , w m ).
Paso 4 Para i = 1, 2, . . . , N haga los pasos 5‚Äì11.
Paso 5 Para j = 1, 2, . . . , m determine
k1, j = h f j (t, w 1 , w 2 , . . . , w m ).
Paso 6 Para j = 1, 2, . . . , m determine
k2, j = h f j t + h2 , w 1 + 12 k1,1 , w 2 + 12 k1,2 , . . . , w m + 12 k1,m .
Paso 7 Para j = 1, 2, . . . , m determine
k3, j = h f j t + h2 , w 1 + 12 k2,1 , w 2 + 12 k2,2 , . . . , w m + 12 k2,m .
Paso 8 Para j = 1, 2, . . . , m determine
k4, j = h f j (t + h, w 1 + k3,1 , w 2 + k3,2 , . . . , w m + k3,m ).
Paso 9 Para j = 1, 2, . . . , m determine
w j = w j + (k1, j + 2k2, j + 2k3, j + k4, j )/6.
Paso 10 Determine t = a + i h.
Paso 11 SALIDA (t, w 1 , w 2 , . . . , w m ).
Paso 12 PARE.

Ilustraci√≥n

La ley de Kirchhoff establece que la suma de todos los cambios de voltaje alrededor de un
circuito cerrado es cero. Esta ley implica que la corriente I(t) en un circuito cerrado que contiene una resistencia de R ohms, una capacitancia de C faradios, una inductancia de L henrios
y una fuente voltaje de E(t) volts satisface la ecuaci√≥n

L I (t) + R I (t) +

1
C

I (t) dt = E(t).

Las corrientes I1 (t) y I2 (t) en los ciclos izquierdo y derecho, respectivamente, del circuito
PRVWUDGRHQOD√ÄJXUDVRQODVVROXFLRQHVSDUDHOVLVWHPDGHHFXDFLRQHV

2I1 (t) + 6[I1 (t) ‚àí I2 (t)] + 2I1 (t) = 12,
1
0.5

I2 (t) dt + 4I2 (t) + 6[I2 (t) ‚àí I1 (t)] = 0.

5.9 Ecuaciones de orden superior y sistemas de ecuaciones diferenciales

Figura 5.7

2V

0.5 F

I1(t)

I2(t)
4V

6V

12 V

251

2H

Si el interruptor en el circuito est√° cerrado en el tiempo t 5 0, tenemos las condiciones iniciales I1(0) 5 0 y I2(0) 5 0. Resuelva para I1 (t) en la primera ecuaci√≥n, al derivar la segunda
ecuaci√≥n y sustituyendo para I1 (t) se obtiene

I1 = f 1 (t, I1 , I2 ) = ‚àí4I1 + 3I2 + 6,

I1 (0) = 0,

I2 = f 2 (t, I1 , I2 ) = 0.6I1 ‚àí 0.2I2 = ‚àí2.4I1 + 1.6I2 + 3.6,

I2 (0) = 0.

La soluci√≥n exacta para este sistema es

I1 (t) = ‚àí3.375e‚àí2t + 1.875e‚àí0.4t + 1.5,
I2 (t) = ‚àí2.25e‚àí2t + 2.25e‚àí0.4t .
Aplicaremos el m√©todo Runge-Kutta de orden 4 para este sistema con h 5 0.1. Puesto que
w 1,0 = I1 (0) = 0 y w 2,0 = I2 (0) = 0,

k1,1 = h f 1 (t0 , w 1,0 , w 2,0 ) = 0.1 f 1 (0, 0, 0) = 0.1 (‚àí4(0) + 3(0) + 6) = 0.6,
k1,2 = h f 2 (t0 , w 1,0 , w 2,0 ) = 0.1 f 2 (0, 0, 0) = 0.1 (‚àí2.4(0) + 1.6(0) + 3.6) = 0.36,
1
1
1
k2,1 = h f 1 t0 + h, w 1,0 + k1,1 , w 2,0 + k1,2
2
2
2

= 0.1 f 1 (0.05, 0.3, 0.18)

= 0.1 (‚àí4(0.3) + 3(0.18) + 6) = 0.534,
1
1
1
k2,2 = h f 2 t0 + h, w 1,0 + k1,1 , w 2,0 + k1,2
2
2
2

= 0.1 f 2 (0.05, 0.3, 0.18)

= 0.1 (‚àí2.4(0.3) + 1.6(0.18) + 3.6) = 0.3168.
Al generar las entradas restantes de manera similar obtenemos

k3,1 = (0.1) f 1 (0.05, 0.267, 0.1584) = 0.54072,
k3,2 = (0.1) f 2 (0.05, 0.267, 0.1584) = 0.321264,
k4,1 = (0.1) f 1 (0.1, 0.54072, 0.321264) = 0.4800912,
k4,2 = (0.1) f 2 (0.1, 0.54072, 0.321264) = 0.28162944.
Como consecuencia,

1
I1 (0.1) ‚âà w 1,1 = w 1,0 + (k1,1 + 2k2,1 + 2k3,1 + k4,1 )
6
1
= 0 + (0.6 + 2(0.534) + 2(0.54072) + 0.4800912) = 0.5382552
6

252

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

y

1
I2 (0.1) ‚âà w 2,1 = w 2,0 + (k1,2 + 2k2,2 + 2k3,2 + k4,2 ) = 0.3196263.
6
Las entradas restantes en la tabla 5.19 se generan de manera similar.

Tabla 5.19

tj

w 1, j

w 2, j

|I1 (t j ) ‚àí w 1, j |

|I2 (t j ) ‚àí w 2, j |

0.0

0

0

0

0

0.1
0.2
0.3
0.4
0.5

0.5382550
0.9684983
1.310717
1.581263
1.793505

0.3196263
0.5687817
0.7607328
0.9063208
1.014402

‚àí5

0.5803 √ó 10‚àí5
0.9596 √ó 10‚àí5
0.1216 √ó 10‚àí4
0.1311 √ó 10‚àí4
0.1240 √ó 10‚àí4

0.8285 √ó 10
0.1514 √ó 10‚àí4
0.1907 √ó 10‚àí4
0.2098 √ó 10‚àí4
0.2193 √ó 10‚àí4

Ecuaciones diferenciales de orden superior
Muchos problemas f√≠sicos importantes, por ejemplo, circuitos el√©ctricos y sistemas de vibraci√≥n, implican problemas de valor inicial cuyas ecuaciones tienen √≥rdenes mayores que 1.
No se requieren t√©cnicas nuevas para resolverlos. Al reetiquetar las variables podemos reducir una ecuaci√≥n diferencial de orden superior en un sistema de ecuaciones diferenciales de
primer orden y, a continuaci√≥n aplicar uno de los m√©todos que ya analizamos.
Un problema de valor inicial de m-√©simo orden general

y (m) (t) = f (t, y, y , . . . , y (m‚àí1) ),

a ‚â§ t ‚â§ b,

con condiciones iniciales y(a) = Œ±1 , y (a) = Œ±2 , . . . , y (m‚àí1) (a) = Œ±m, se pueden convertir
en un sistema de ecuaciones de la forma (5.45) y (5.46).
Si u 1 (t) = y(t), u 2 (t) = y (t), . . . , y u m (t) = y (m‚àí1) (t). Esto produce el sistema de
primer orden

du 1
dy
=
= u2,
dt
dt

du 2
dy
=
= u3,
dt
dt

du m‚àí1
dy (m‚àí2)
=
= um ,
dt
dt

¬∑¬∑¬∑ ,

y

du m
dy (m‚àí1)
=
= y (m) = f (t, y, y , . . . , y (m‚àí1) ) = f (t, u 1 , u 2 , . . . , u m ),
dt
dt
con condiciones iniciales

u 1 (a) = y(a) = Œ±1 ,
Ejemplo 1

u 2 (a) = y (a) = Œ±2 ,

...,

u m (a) = y (m‚àí1) (a) = Œ±m .

Transforme el problema de valor inicial de segundo orden

y ‚àí 2y + 2y = e2t sen t,

para 0 ‚â§ t ‚â§ 1,

con y(0) = ‚àí0.4, y (0) = ‚àí0.6

en un sistema de problemas de valor inicial de primer orden y use el m√©todo Runge-Kutta
con h 5 0.1 para aproximar la soluci√≥n.
Sea u 1 (t) = y(t) y u 2 (t) = y (t). Esto transforma la ecuaci√≥n de segundo orden
en el sistema

Soluci√≥n

u 1 (t) = u 2 (t),
u 2 (t) = e2t sen t ‚àí 2u 1 (t) + 2u 2 (t),
con condiciones iniciales u 1 (0) = ‚àí0.4, u 2 (0) = ‚àí0.6.

5.9 Ecuaciones de orden superior y sistemas de ecuaciones diferenciales

253

Las condiciones iniciales dan w 1,0 = ‚àí0.4 y w 2,0 = ‚àí0.6. Las ecuaciones (5.49) a
(5.52) en la p√°gina 249 con j 5 0 proporcionan

k1,1 = h f 1 (t0 , w 1,0 , w 2,0 ) = hw 2,0 = ‚àí0.06,
k1,2 = h f 2 (t0 , w 1,0 , w 2,0 ) = h e2t0 sen t0 ‚àí 2w 1,0 + 2w 2,0 = ‚àí0.04,
k2,1 = h f 1 t0 +

h
1
1
, w 1,0 + k1,1 , w 2,0 + k1,2
2
2
2

k2,2 = h f 2 t0 +

h
1
1
, w 1,0 + k1,1 , w 2,0 + k1,2
2
2
2

1
= h w 2,0 + k1,2 = ‚àí0.062,
2

1
= h e2(t0 +0.05) sen(t0 + 0.05) ‚àí 2 w 1,0 + k1,1
2

1
+ 2 w 2,0 + k1,2
2

= ‚àí0.03247644757,
1
k3,1 = h w 2,0 + k2,2 = ‚àí0.06162832238,
2
1
k3,2 = h e2(t0 +0.05) sen(t0 + 0.05) ‚àí 2 w 1,0 + k2,1
2

1
+ 2 w 2,0 + k2,2
2

= ‚àí0.03152409237,
k4,1 = h w 2,0 + k3,2 = ‚àí0.06315240924,
y
k4,2 = h e2(t0 +0.1) sen(t0 + 0.1) ‚àí 2(w 1,0 + k3,1 ) + 2(w 2,0 + k3,2 ) = ‚àí0.02178637298.
Por lo que,
1
w 1,1 = w 1,0 + (k1,1 + 2k2,1 + 2k3,1 + k4,1 ) = ‚àí0.4617333423
6
y
1
w 2,1 = w 2,0 + (k1,2 + 2k2,2 + 2k3,2 + k4,2 ) = ‚àí0.6316312421.
6
El valor w,1,1 aproxima u 1 (0.1) = y(0.1) = 0.2e2(0.1) (sen 0.1 ‚àí 2 cos 0.1), y w2,1 aproxima u 2 (0.1) = y (0.1) = 0.2e2(0.1) (4 sen 0.1 ‚àí 3 cos 0.1).
El conjunto de valores w 1, j y w 2, j , para j = 0, 1, . . . , 10, se presenta en la tabla 5.20
y se comparan con los valores reales de u 1 (t) = 0.2e2t (sen t ‚àí 2 cos t) y u 2 (t) = u 1 (t) =
0.2e2t (4 sen t ‚àí 3 cos t).

Tabla 5.20
tj

y(t j ) = u 1 (t j )

w 1, j

y (t j ) = u 2 (t j )

w 2, j

|y(t j ) ‚àí w 1, j |

|y (t j ) ‚àí w 2, j |

0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0

‚àí0.40000000
‚àí0.46173297
‚àí0.52555905
‚àí0.58860005
‚àí0.64661028
‚àí0.69356395
‚àí0.72114849
‚àí0.71814890
‚àí0.66970677
‚àí0.55643814
‚àí0.35339436

‚àí0.40000000
‚àí0.46173334
‚àí0.52555988
‚àí0.58860144
‚àí0.64661231
‚àí0.69356666
‚àí0.72115190
‚àí0.71815295
‚àí0.66971133
‚àí0.55644290
‚àí0.35339886

‚àí0.6000000
‚àí0.6316304
‚àí0.6401478
‚àí0.6136630
‚àí0.5365821
‚àí0.3887395
‚àí0.1443834
0.2289917
0.7719815
1.534764
2.578741

‚àí0.60000000
‚àí0.63163124
‚àí0.64014895
‚àí0.61366381
‚àí0.53658203
‚àí0.38873810
‚àí0.14438087
0.22899702
0.77199180
1.5347815
2.5787663

0
3.7 √ó 10‚àí7
8.3 √ó 10‚àí7
1.39 √ó 10‚àí6
2.03 √ó 10‚àí6
2.71 √ó 10‚àí6
3.41 √ó 10‚àí6
4.05 √ó 10‚àí6
4.56 √ó 10‚àí6
4.76 √ó 10‚àí6
4.50 √ó 10‚àí6

0
7.75 √ó 10‚àí7
1.01 √ó 10‚àí6
8.34 √ó 10‚àí7
1.79 √ó 10‚àí7
5.96 √ó 10‚àí7
7.75 √ó 10‚àí7
2.03 √ó 10‚àí6
5.30 √ó 10‚àí6
9.54 √ó 10‚àí6
1.34 √ó 10‚àí5

254

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

Los otros m√©todos de un paso se pueden extender a los sistemas de manera similar.
Cuando los m√©todos de control de error como el Runge-Kutta-Fehlberg son extendidos, a
cada componente de la soluci√≥n num√©rica (w 1 j , w 2 j , . . . , w m j ) se debe examinar su preciVLyQ6LFXDOTXLHUDGHORVFRPSRQHQWHVQRHVVX√ÄFLHQWHPHQWHSUHFLVRWRGDODVROXFLyQQXPprica (w 1 j , w 2 j , . . . , w m j ) se debe volver a calcular.
Los m√©todos multipasos y las t√©cnicas de indicador-corrector tambi√©n se pueden
ampliar a los sistemas. De nuevo, si se utiliza control de error, cada componente debe ser preciso. La extensi√≥n de la t√©cnica de extrapolaci√≥n para los sistemas tambi√©n se puede realizar,
pero la notaci√≥n se vuelve bastante implicada. Si este tema es de inter√©s, consulte [HNW1].
Los teoremas de convergencia y estimaci√≥n de error para sistemas son similares a los
considerados en la secci√≥n 5.10 para las ecuaciones individuales, excepto que las cotas est√°n
dadas en t√©rminos de normas de vectores, un tema considerado en el cap√≠tulo 7 (una buena
referencia para estos teoremas es [Ge1], p. 45‚Äì72).
La secci√≥n Conjunto de ejercicios 5.9 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

5.10 Estabilidad
En este cap√≠tulo se ha presentado una serie de m√©todos para aproximar la soluci√≥n para un
problema de valor inicial. A pesar de que existen otras t√©cnicas, seleccionamos los m√©todos
descritos aqu√≠ porque, en general, satisfacen tres criterios:
¬á 6XGHVDUUROORHVVX√ÄFLHQWHPHQWHFODURSRUORTXHVHSXHGHFRPSUHQGHUFyPR\SRUTXp
funcionan.
‚Ä¢ Uno o m√°s m√©todos proporcionar√°n resultados satisfactorios para muchos de los problemas encontrados por los estudiantes de ciencias e ingenier√≠a.
‚Ä¢ Muchas de las t√©cnicas m√°s avanzadas y complejas est√°n basadas en una o varios de los
procedimientos descritos aqu√≠.

M√©todos de un paso
En esta secci√≥n analizamos porqu√© se espera que estos m√©todos proporcionen resultados satisfactorios cuando m√©todos similares no lo hacen. Antes de empezar este an√°lisis, necesitamos presenWDUGRVGH√ÄQLFLRQHVUHODFLRQDGDVFRQODFRQYHUJHQFLDGHORVPpWRGRVGHHFXDFLyQGHGLIHUHQFLD
de un paso para la soluci√≥n de esa ecuaci√≥n diferencial conforme el tama√±o de peso disminuye.
DeÔ¨Ånici√≥n 5.18

Se dice que un m√©todo de ecuaci√≥n de diferencia de un paso con error de truncamiento local
œÑi (h) en el i-√©simo paso es consistente con la ecuaci√≥n diferencial que aproxima si

l√≠m m√°x |œÑi (h)| = 0.

h‚Üí0 1‚â§i‚â§N

Un m√©todo de un paso es
consistente si la ecuaci√≥n de
diferencia para el m√©todo se
enfoca en la ecuaci√≥n diferencial,
conforme el tama√±o de paso
tiende a cero.

2EVHUYHTXHHVWDGH√ÄQLFLyQHVXQDGH√ÄQLFLyQlocal ya que, para cada uno de los valores
œÑi (h), suponemos que la aproximaci√≥n w i‚àí1 y la soluci√≥n exacta y(ti‚àí1 ) es la misma. Un
medio m√°s realista para analizar los efectos de reducir h es determinar el efecto global del
m√©todo. √âste es el error m√°ximo del m√©todo sobre todo el rango de la aproximaci√≥n, al suponer solamente que el m√©todo da el resultado exacto en el valor inicial.

DeÔ¨Ånici√≥n 5.19

Se dice que un m√©todo de ecuaci√≥n de diferencia de un paso es convergente respecto a la
ecuaci√≥n diferencial que aproxima si

Un m√©todo es convergente
si la soluci√≥n de la ecuaci√≥n
de diferencia se enfoca en
la soluci√≥n de la ecuaci√≥n
diferencial conforme el tama√±o
de paso tiende a cero.

l√≠m m√°x |w i ‚àí y(ti )| = 0,

h‚Üí0 1‚â§i‚â§N

donde y(ti) denota el valor exacto de la soluci√≥n de la ecuaci√≥n diferencial y wi es la aproximaci√≥n obtenida a partir del m√©todo de diferencia en el i-√©simo paso.

5.10

Ejemplo 1

Estabilidad

255

Muestre que el m√©todo de Euler es convergente.
Soluci√≥n Examine la desigualdad (5.10) en la p√°gina 202, en la f√≥rmula de la cota de error
para el m√©todo de Euler, nosotros observamos que bajo la hip√≥tesis del teorema 5.9

m√°x |w i ‚àí y(ti )| ‚â§

1‚â§i‚â§N

Mh L(b‚àía)
|e
‚àí 1|.
2L

Sin embargo, M, L y b son todas las constantes, y

Mh L(b‚àía)
e
‚àí 1 = 0.
h‚Üí0 2L

l√≠m m√°x |w i ‚àí y(ti )| ‚â§ l√≠m

h‚Üí0 1‚â§i‚â§N

De modo que el m√©todo de Euler es convergente respecto a una ecuaci√≥n diferencial que
VDWLVIDFHODVFRQGLFLRQHVGHHVWDGH√ÄQLFLyQ/DUD]yQGHFRQYHUJHQFLDHVO(h).

Un m√©todo es estable cuando
los resultados dependen
continuamente de los datos
iniciales.

Teorema 5.20

Un m√©todo consistente de un paso tiene la propiedad de que la ecuaci√≥n de diferencia
para el m√©todo aproxima la ecuaci√≥n diferencial cuando el tama√±o de paso tiende a cero. Por
lo que el error de truncamiento local de un m√©todo consistente se aproxima a cero conforme
el tama√±o de paso se aproxima a cero.
El otro tipo de cota de error del problema que existe al usar m√©todos de diferencia o
soluciones aproximadas para las ecuaciones diferenciales es una consecuencia de no usar
resultados exactos. En la pr√°ctica, ni las condiciones iniciales ni la aritm√©tica que se realiza
por consiguiente se representan de manera exacta debido al error de redondeo asociado con
ODDULWPpWLFDGHGtJLWRV√ÄQLWRV(QODVHFFLyQREVHUYDPRVTXHHVWDFRQVLGHUDFLyQSXHGH
FRQGXFLUDGL√ÄFXOWDGHVLQFOXVRSDUDHOPpWRGRFRQYHUJHQWHGH(XOHU
Para analizar esta situaci√≥n, por lo menos en parte, tratamos de determinar los m√©todos
que son estables en el sentido de que cambios peque√±os o perturbaciones en las condiciones
iniciales producen proporcionalmente peque√±os cambios en las aproximaciones subsiguientes.
El concepto de estabilidad de una ecuaci√≥n de diferencia de un paso es de alguna forma
an√°loga para la condici√≥n de una ecuaci√≥n diferencial bien planteada, por lo que no es sorprendente que la condici√≥n de Lipschitz aparezca aqu√≠, como lo hizo en el teorema correspondiente para ecuaciones diferenciales, el teorema 5.6 en la secci√≥n 5.1.
La parte i) del siguiente teorema se preocupa por la estabilidad de un m√©todo de un paso.
La prueba de este resultado no es dif√≠cil y se considera en el ejercicio 1. La parte ii) del teoUHPDVHSUHRFXSDSRUFRQGLFLRQHVVX√ÄFLHQWHVSDUDTXHXQPpWRGRFRQVLVWHQWHVHDFRQYHUJHQWH/DSDUWHLLL MXVWL√ÄFDODREVHUYDFLyQUHDOL]DGDHQODVHFFLyQVREUHFRQWURODUHO
error global de un m√©todo al controlar su error de truncamiento local e implica que cuando
el error de truncamiento local tiene una raz√≥n de convergencia O(hn), el error global tendr√°
la misma raz√≥n de convergencia. Las pruebas de las partes ii) y iii) son m√°s dif√≠ciles que las
de la parte i) y se pueden encontrar dentro del material presentado en [Ge1], p. 57-58.
Suponga que el problema de valor inicial

y = f (t, y),

a ‚â§ t ‚â§ b,

y(a) = Œ±,

se aproxima por medio de un m√©todo de diferencia de un paso de la forma

w 0 = Œ±,
w i+1 = w i + hœÜ(ti , w i , h).
Tambi√©n suponga que existe un n√∫mero h 0 > 0 y que œÜ(t, w, h) es continua y que satisface
la condici√≥n de Lipschitz en la variable w con constante L de Lipschitz en el conjunto

D = { (t, w, h) | a ‚â§ t ‚â§ b y ‚àí ‚àû < w < ‚àû, 0 ‚â§ h ‚â§ h 0 }.

256

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

Entonces
i)

El m√©todo es estable;

ii)

El m√©todo de diferencia es convergente si y s√≥lo si es consistente, lo cual es equivalente a

œÜ(t, y, 0) = f (t, y),
iii)

para todas las a ‚â§ t ‚â§ b;

Si existe una funci√≥n œÑ y, para cada i = 1, 2, . . . , N , el error de truncamiento local
œÑi (h) satisface |œÑi (h)| ‚â§ œÑ (h) siempre que 0 ‚â§ h ‚â§ h 0, entonces

|y(ti ) ‚àí w i | ‚â§
Ejemplo 2

œÑ (h) L(ti ‚àía)
.
e
L

(OPpWRGRPRGL√ÄFDGRGH(XOHUHVWiGDGRSRUw 0 = Œ±,

w i+1 = w i +

h
[ f (ti , w i ) + f (ti+1 , w i + h f (ti , w i ))] ,
2

para i = 0, 1, . . . , N ‚àí 1.

9HUL√ÄTXHTXHHVWHPpWRGRHVHVWDEOHDOPRVWUDUTXHVDWLVIDFHODKLSyWHVLVGHOWHRUHPD
Soluci√≥n Para este m√©todo

œÜ(t, w, h) =

1
1
f (t, w) + f (t + h, w + h f (t, w)).
2
2

Si f satisface la condici√≥n de Lipschitz en { (t, w) | a ‚â§ t ‚â§ b y ‚àí ‚àû < w < ‚àû } en
la variable w con constante L, entonces, puesto que

œÜ(t, w, h) ‚àí œÜ(t, w, h) =

1
1
f (t, w) + f (t + h, w + h f (t, w))
2
2
1
1
‚àí f (t, w) ‚àí f (t + h, w + h f (t, w)),
2
2

la condici√≥n de Lipschitz en f conduce a

1
1
L|w ‚àí w| + L |w + h f (t, w) ‚àí w ‚àí h f (t, w)|
2
2
1
‚â§ L|w ‚àí w| + L |h f (t, w) ‚àí h f (t, w)|
2
1
‚â§ L|w ‚àí w| + h L 2 |w ‚àí w|
2
1
= L + h L 2 |w ‚àí w|.
2

|œÜ(t, w, h) ‚àí œÜ(t, w, h)| ‚â§

Por lo tanto, f satisface la condici√≥n de Lipschitz en w en el conjunto

{ (t, w, h) | a ‚â§ t ‚â§ b, ‚àí‚àû < w < ‚àû, y 0 ‚â§ h ‚â§ h 0 },
para cualquier h 0 > 0 con constante

1
L = L + h0 L 2.
2
Finalmente, si f es continua en { (t, w) | a ‚â§ t ‚â§ b, ‚àí‚àû < w < ‚àû }, entonces f
es continua en

{ (t, w, h) | a ‚â§ t ‚â§ b, ‚àí‚àû < w < ‚àû, y 0 ‚â§ h ‚â§ h 0 },

5.10

257

Estabilidad

SRUORTXHHOWHRUHPDLPSOLFDTXHHOPpWRGRPRGL√ÄFDGRGH(XOHUHVHVWDEOH$OKDFHU
h 5 0, tenemos

œÜ(t, w, 0) =

1
1
f (t, w) + f (t + 0, w + 0 ¬∑ f (t, w)) = f (t, w),
2
2

por lo que la condici√≥n de consistencia expresada en el teorema 5.20, parte ii), se mantiene. Por
lo tanto, el m√©todo es convergente. Adem√°s, hemos visto que para este m√©todo, el error de
truncamiento local es O(h2  SRU OR TXH OD FRQYHUJHQFLD GHO PpWRGR PRGL√ÄFDGR GH (XOHU
tambi√©n es O(h2).

M√©todos multipasos
Para m√©todos multipasos, los problemas relacionados con la consistencia, la convergencia y
la estabilidad son compuestos debido al n√∫mero de aproximaciones que implica cada paso.
Mientras en los m√©todos de un paso, la aproximaci√≥n w i+1 depende directamente s√≥lo de la
aproximaci√≥n previa wi, los m√©todos multipasos utilizan por lo menos dos de las aproximaciones previas y los m√©todos comunes utilizados implican m√°s.
El m√©todo general multipasos para aproximar la soluci√≥n del problema de valor inicial

y = f (t, y),

a ‚â§ t ‚â§ b,

y(a) = Œ±,

(5.54)

tiene la forma

w 0 = Œ±,

w 1 = Œ±1 ,

. . . , w m‚àí1 = Œ±m‚àí1 ,

w i+1 = am‚àí1 w i + am‚àí2 w i‚àí1 + ¬∑ ¬∑ ¬∑ + a0 w i+1‚àím + h F(ti , h, w i+1 , w i , . . . , w i+1‚àím ),
(5.55)
para cada i = m ‚àí 1, m, . . . , N ‚àí 1, donde a0 , a1 , . . . , am+1 son constantes, y como
siempre, h = (b ‚àí a)/N y ti = a + i h.
El error de truncamiento local para un m√©todo multipasos expresado en esta forma es

œÑi+1 (h) =

y(ti+1 ) ‚àí am‚àí1 y(ti ) ‚àí ¬∑ ¬∑ ¬∑ ‚àí a0 y(ti+1‚àím )
h
‚àí F(ti , h, y(ti+1 ), y(ti ), . . . , y(ti+1‚àím )),

para cada i = m ‚àí 1, m, . . . , N ‚àí 1. Como en los m√©todos de un paso, el error de truncamiento local mide la forma en la que la soluci√≥n y para la ecuaci√≥n diferencial no puede
satisfacer la ecuaci√≥n de diferencia.
Por el m√©todo Adams-Bashforth de cuatro pasos, hemos observado que

œÑi+1 (h) =

251 (5)
y (Œºi )h 4 ,
720

para algunos Œºi ‚àà (ti‚àí3 , ti+1 ),

mientras que el m√©todo Adams‚ÄìMoulton de tres pasos se tiene

œÑi+1 (h) = ‚àí

19 (5)
y (Œºi )h 4 ,
720

siempre y cuando, por supuesto, y ‚àà C 5 [a, b].

para algunas Œºi ‚àà (ti‚àí2 , ti+1 ),

258

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

A lo largo del an√°lisis, se realizan dos suposiciones respecto a la funci√≥n F:
‚Ä¢ Si f ‚â° 0 (es decir, si la ecuaci√≥n diferencial es homog√©nea), entonces F ‚â° 0 tambi√©n.
‚Ä¢ F satisface la condici√≥n de Lipschitz respecto a {w j } en el sentido de que existe una constante L, y, para cada par de secuencias {v j } Nj=0 y {vÃÉ j } Nj=0 y para i = m ‚àí 1, m, . . . , N ‚àí 1,
tenemos
m

|F(ti , h, vi+1 , . . . , vi+1‚àím ) ‚àí F(ti , h, vÃÉi+1 , . . . , vÃÉi+1‚àím )| ‚â§ L

|vi+1‚àí j ‚àí vÃÉi+1‚àí j |.
j=0

Los m√©todos Adams-Bashforth expl√≠cito y Adams-Moulton impl√≠cito satisfacen ambas
condiciones, siempre que f satisfaga la condici√≥n de Lipschitz (consulte el ejercicio 2).
El concepto de convergencia para m√©todos multipasos es el mismo que para los m√©todos
de un paso.
‚Ä¢ Un m√©todo multipasos es convergente si la soluci√≥n de la ecuaci√≥n de diferencia se aproxima a la soluci√≥n de la ecuaci√≥n diferencial conforme el tama√±o de paso se aproxima a
FHUR(VWRVLJQL√ÄFDTXHl√≠mh‚Üí0 m√°x0‚â§i‚â§N | w i ‚àí y(ti )| = 0.
Para la consistencia, sin embargo, se presenta una situaci√≥n algo diferente. De nuevo,
queremos que un m√©todo multipasos sea consistente siempre que la ecuaci√≥n de diferencia se
aproxime a la ecuaci√≥n diferencial conforme el tama√±o de paso se aproxima a cero; es decir,
el error de truncamiento local se aproxima a cero en cada paso conforme el tama√±o de paso
se aproxima a cero. La condici√≥n adicional se presenta debido al n√∫mero de valores iniciales
requerido para los m√©todos multipasos. Puesto que, normalmente, s√≥lo el primer valor inicial
w 0 = Œ±, es exacto, necesitamos requerir que los errores en todos los valores iniciales {Œ±i } se
aproximen a cero conforme el tama√±o de paso se aproxima a cero. Por lo que,

l√≠m |œÑi (h)| = 0,

para toda i = m, m + 1, . . . , N ,

l√≠m |Œ±i ‚àí y(ti )| = 0,

para toda i = 1, 2, . . . , m ‚àí 1,

h‚Üí0

h‚Üí0

y

(5.56)
(5.57)

debe ser verdadero para que un m√©todo multipasos de la forma (5.55) sea consistente. Observe que la ecuaci√≥n (5.57) implica que un m√©todo multipasos no ser√° consistente a menos
que el m√©todo de un paso que genera los valores iniciales tambi√©n sea consistente.
El siguiente teorema para los m√©todos multipasos es similar al teorema 5.20, parte iii),
y provee una relaci√≥n entre el error de truncamiento local y el error global de un m√©todo
PXOWLSDVRV(VWHSURSRUFLRQDODMXVWL√ÄFDFLyQWHyULFDSDUDLQWHQWDUFRQWURODUHOHUURUJOREDO
al controlar el error de truncamiento local. La demostraci√≥n de una forma ligeramente m√°s
general de este teorema se puede encontrar en [IK], p. 387-388.
Teorema 5.21

Suponga que se aproxima el problema de valor inicial

y = f (t, y),

a ‚â§ t ‚â§ b,

y(a) = Œ±,

con un m√©todo indicador-corrector expl√≠cito de Adams con una ecuaci√≥n indicadora de
Adams-Bashforth de m pasos

w i+1 = w i + h[bm‚àí1 f (ti , w i ) + ¬∑ ¬∑ ¬∑ + b0 f (ti+1‚àím , w i+1‚àím )],
con error de truncamiento local œÑi+1 (h), y una ecuaci√≥n correctora impl√≠cita de AdamsMoulton de (m 2 1) pasos

w i+1 = w i + h bÃÉm‚àí1 f (ti+1 , w i+1 ) + bÃÉm‚àí2 f (ti , w i ) + ¬∑ ¬∑ ¬∑ + bÃÉ0 f (ti+2‚àím , w i+2‚àím ) ,

5.10

259

Estabilidad

con error de truncamiento local œÑÃÉi+1 (h). Adem√°s, suponga que f (t, y) y f y (t, y) son continuas en D = { (t, y) | a ‚â§ t ‚â§ b y ‚àí‚àû < y < ‚àû } y fy est√° acotada. Entonces, el error de
truncamiento local œÉi+1 (h) del m√©todo indicador corrector es

œÉi+1 (h) = œÑÃÉi+1 (h) + œÑi+1 (h)bÃÉm‚àí1

‚àÇf
(ti+1 , Œ∏i+1 ),
‚àÇy

donde Œ∏i+1 es un n√∫mero entre cero y hœÑi+1 (h).
Adem√°s, existen constantes k1 y k2 tal que

|w i ‚àí y(ti )| ‚â§

m√°x

0‚â§ j‚â§m‚àí1

w j ‚àí y(t j ) + k1 œÉ (h) ek2 (ti ‚àía) ,

donde œÉ (h) = m√°xm‚â§ j‚â§N |œÉ j (h)|.
Antes de analizar las conexiones entre consistencia, convergencia y estabilidad para
los m√©todos multipasos, necesitamos considerar con m√°s detalle la ecuaci√≥n de diferencia
para un m√©todo multipasos. Al hacerlo, descubriremos la raz√≥n para seleccionar los m√©todos
Adams como nuestros m√©todos multipasos est√°ndar.
Relacionado con la ecuaci√≥n de diferencia (5.55) provisto al inicio de este an√°lisis,

w 0 = Œ±, w 1 = Œ±1 , . . . , w m‚àí1 = Œ±m‚àí1 ,
w i+1 = am‚àí1 w i + am‚àí2 w i‚àí1 + ¬∑ ¬∑ ¬∑ + a0 w i+1‚àím + h F(ti , h, w i+1 , w i , . . . , w i+1‚àím ),
es un polinomio, llamado polinomio caracter√≠stico del m√©todo, dado por

P(Œª) = Œªm ‚àí am‚àí1 Œªm‚àí1 ‚àí am‚àí2 Œªm‚àí2 ‚àí ¬∑ ¬∑ ¬∑ ‚àí a1 Œª ‚àí a0 .

(5.58)

La estabilidad de un m√©todo multipasos respecto al error de redondeo est√° dictada por
las magnitudes de los ceros del polinomio caracter√≠stico. Para observar esto, considere aplicar el m√©todo multipasos est√°ndar (5.55) al problema trivial de valor inicial

y ‚â° 0,

y(a) = Œ±,

donde Œ± = 0.

(5.59)

Este problema tiene soluci√≥n exacta y(t) ‚â° Œ±. Al examinar las ecuaciones (5.27) y (5.28)
en la secci√≥n 5.6 (consulte las p√°ginas 226 y 227), podemos observar que cualquier m√©todo
multipasos producir√°, en teor√≠a, la soluci√≥n exacta w n = Œ± para todas las n. La √∫nica desviaci√≥n de la soluci√≥n exacta se debe al error de redondeo del m√©todo.
El lado derecho de la ecuaci√≥n diferencial en (5.59) tiene f (t, y) ‚â° 0, por lo que mediante la suposici√≥n (1), tenemos F(ti , h, w i+1 , w i+2 , . . . , w i+1‚àím ) = 0 en la ecuaci√≥n de
diferencia (5.55). Como consecuencia, la forma est√°ndar de esta ecuaci√≥n se convierte en

w i+1 = am‚àí1 w i + am‚àí2 w i‚àí1 + ¬∑ ¬∑ ¬∑ + a0 w i+1‚àím .

(5.60)

Suponiendo que l es uno de los ceros del polinomio caracter√≠stico relacionado con la
ecuaci√≥n (5.55). Entonces w n = Œªn para cada n es una soluci√≥n para la ecuaci√≥n (5.59) ya
que

Œªi+1 ‚àí am‚àí1 Œªi ‚àí am‚àí2 Œªi‚àí1 ‚àí ¬∑ ¬∑ ¬∑ ‚àí a0 Œªi+1‚àím = Œªi+1‚àím [Œªm ‚àí am‚àí1 Œªm‚àí1 ‚àí ¬∑ ¬∑ ¬∑ ‚àí a0 ] = 0.
En efecto, si Œª1 , Œª2 , . . . , Œªm son ra√≠ces distintas del polinomio caracter√≠stico para la ecuaci√≥n
(5.55) se puede mostrar de que todas las soluciones de la ecuaci√≥n (5.60) se pueden expresar
en la forma
m

wn =

ci Œªin ,

i=1

para alg√∫n conjunto √∫nico de constantes c1 , c2 , . . . , cm .

(5.61)

260

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

Puesto que la soluci√≥n exacta para la ecuaci√≥n (5.59) es y(t) 5 a, la selecci√≥n wn 5 a,
para todas las n, es una soluci√≥n para la ecuaci√≥n (5.60). Usando este hecho en la ecuaci√≥n
(5.60) obtenemos

0 = Œ± ‚àí Œ±am‚àí1 ‚àí Œ±am‚àí2 ‚àí ¬∑ ¬∑ ¬∑ ‚àí Œ±a0 = Œ±[1 ‚àí am‚àí1 ‚àí am‚àí2 ‚àí ¬∑ ¬∑ ¬∑ ‚àí a0 ].
Esto implica que Œª = 1 es uno de los ceros del polinomio caracter√≠stico (5.58). Supondremos que en la representaci√≥n (5.61), esta soluci√≥n est√° descrita por Œª1 = 1 y c1 = Œ±, por lo
que todas las soluciones de la ecuaci√≥n (5.59) se expresan como
m

wn = Œ± +

ci Œªin .

(5.62)

i=2

Si todos los c√°lculos fueran exactos, todas las constantes c2 , c3 , . . . , cm ser√≠an cero. En la
pr√°ctica, las constantes c2 , c3 , . . . , cm no son cero debido al error de redondeo. De hecho,
el error de redondeo crece de manera exponencial a menos que |Œªi | ‚â§ 1 para cada una de
las ra√≠ces Œª2 , Œª3 , . . . , Œªm. Mientras m√°s peque√±a sea la magnitud de estas ra√≠ces, m√°s estable
ser√° el m√©todo respecto al crecimiento del error de redondeo.
$OGHULYDUODHFXDFLyQ  UHDOL]DPRVODVXSRVLFLyQVLPSOL√ÄFDGRUDGHTXHORVFHURV
del polinomio caracter√≠stico son distintos. La situaci√≥n es similar cuando se presentan m√∫ltiples ceros. Por ejemplo, si Œªk = Œªk+1 = ¬∑ ¬∑ ¬∑ = Œªk+ p para algunas k y p, simplemente requiere
reemplazar la suma

ck Œªnk + ck+1 Œªnk+1 + ¬∑ ¬∑ ¬∑ + ck+ p Œªnk+ p
en (5.62) con
n‚àí p

ck Œªnk + ck+1 nŒªn‚àí1
+ ck+2 n(n ‚àí 1)Œªn‚àí2
+ ¬∑ ¬∑ ¬∑ + ck+ p [n(n ‚àí 1) ¬∑ ¬∑ ¬∑ (n ‚àí p + 1)]Œªk
k
k

.

(5.63)
&RQVXOWH>+H@S¬≤ $SHVDUGHTXHVHPRGL√ÄFDODIRUPDGHODVROXFLyQHOHUURUGH
redondeo |Œªk | > 1 sigue creciendo exponencialmente.
Aunque s√≥lo hemos considerado el caso especial de aproximaci√≥n de los problemas
de valor inicial de la forma (5.59), las caracter√≠sticas de estabilidad para esta ecuaci√≥n determinan la estabilidad de la situaci√≥n cuando f(t, y) no es id√©nticamente cero. Esto porque
la soluci√≥n para la ecuaci√≥n homog√©nea (5.59) est√° integrada en la soluci√≥n de cualquier
HFXDFLyQ/DVVLJXLHQWHVGH√ÄQLFLRQHVHVWiQPRWLYDGDVSRUHVWHDQiOLVLV
DeÔ¨Ånici√≥n 5.22

Si Œª1 , Œª2 , . . . , Œªm denota las ra√≠ces (no necesariamente distintas) de la ecuaci√≥n caracter√≠stica

P(Œª) = Œªm ‚àí am‚àí1 Œªm‚àí1 ‚àí ¬∑ ¬∑ ¬∑ ‚àí a1 Œª ‚àí a0 = 0
relacionada con el m√©todo de diferencias multipasos

w 0 = Œ±,

w 1 = Œ±1 ,

... ,

w m‚àí1 = Œ±m‚àí1

w i+1 = am‚àí1 w i + am‚àí2 w i‚àí1 + ¬∑ ¬∑ ¬∑ + a0 w i+1‚àím + h F(ti , h, w i+1 , w i , . . . , w i+1‚àím ).
Si |Œªi | ‚â§ 1, para cada i = 1, 2, . . . , m, y todas las ra√≠ces con valor absoluto 1 son ra√≠ces
simples, entonces se dice que el m√©todo de diferencia satisface la condici√≥n de ra√≠z.
DeÔ¨Ånici√≥n 5.23

i)

Los m√©todos que satisfacen la condici√≥n de ra√≠z y tienen Œª = 1 como la √∫nica ra√≠z
de la ecuaci√≥n caracter√≠stica con magnitud uno reciben el nombre de √ÄUPHPHQWH
estables.

ii)

Los m√©todos que satisfacen la condici√≥n de ra√≠z y tienen m√°s de una ra√≠z distinta con
magnitud uno reciben el nombre de d√©bilmente estables.

iii)

Los m√©todos que no satisfacen la condici√≥n de ra√≠z reciben el nombre de inestables.

5.10

Estabilidad

261

La consistencia y convergencia de un m√©todo multipasos est√°n relacionadas de cerca con la
estabilidad del redondeo del m√©todo. El siguiente teorema detalla estas conexiones. Para
la demostraci√≥n de este resultado y la teor√≠a en la que est√° basado, consulte [IK] p. 410‚Äì417
DeÔ¨Ånici√≥n 5.24

Un m√©todo multipasos de la forma

w 0 = Œ±,

w 1 = Œ±1 ,

... ,

w m‚àí1 = Œ±m‚àí1 ,

w i+1 = am‚àí1 w i + am‚àí2 w i‚àí1 + ¬∑ ¬∑ ¬∑ + a0 w i+1‚àím + h F(ti , h, w i+1 , w i , . . . , w i+1‚àím )
es estable si y s√≥lo si satisface la condici√≥n ra√≠z. Adem√°s, si el m√©todo de diferencia es consistente con la ecuaci√≥n diferencial, entonces el m√©todo es estable si y s√≥lo si es convergente.
Ejemplo 3

El m√©todo Adams‚ÄìBashforth de cuarto orden se puede expresar como

w i+1 = w i + h F(ti , h, w i+1 , w i , . . . , w i‚àí3 ),
donde

F(ti , h, w i+1 , , . . . , w i‚àí3 ) =

h
[55 f (ti , w i ) ‚àí 59 f (ti‚àí1 , w i‚àí1 )
24
+ 37 f (ti‚àí2 , w i‚àí2 ) ‚àí 9 f (ti‚àí3 , w i‚àí3 )].

0XHVWUHTXHHVWHPpWRGRHV√ÄUPHPHQWHHVWDEOH
Soluci√≥n En este caso, tenemos m = 4, a0 = 0, a1 = 0, a2 = 0 y a3 = 1, por lo que
la ecuaci√≥n caracter√≠stica para este m√©todo Adams-Bashforth es

0 = P(Œª) = Œª4 ‚àí Œª3 = Œª3 (Œª ‚àí 1).
Este polinomio tiene ra√≠ces Œª1 = 1, Œª2 = 0, Œª3 = 0 y Œª4 = 0. Por lo tanto, satisface la conGLFLyQGHUDt]\HV√ÄUPHPHQWHHVWDEOH
El m√©todo Adams‚ÄìMoulton tiene un polinomio caracter√≠stico similar, P(Œª) = Œª3 ‚àí Œª2 ,
con ceros Œª1 = 1, Œª2 = 0 y Œª3 = 0\WDPELpQHV√ÄUPHPHQWHHVWDEOH
Ejemplo 4

Muestre que el m√©todo Milne de cuarto orden, el m√©todo expl√≠cito multipasos provisto por

w i+1 = w i‚àí3 +

4h
[2 f (ti , w i ) ‚àí f (ti‚àí1 , w i‚àí1 ) + 2 f (ti‚àí2 , w i‚àí2 )] ,
3

satisface la condici√≥n ra√≠z, pero s√≥lo es d√©bilmente estable.
Soluci√≥n La ecuaci√≥n caracter√≠stica para este m√©todo 0 = P(Œª) = Œª4 ‚àí 1, tiene cuatro

ra√≠ces con magnitud 1: Œª1 = 1, Œª2 = ‚àí1, Œª3 = i, y Œª4 = ‚àíi. Puesto que todas las ra√≠ces
tienen magnitud 1, el m√©todo satisface la condici√≥n de ra√≠z. Sin embargo, existen ra√≠ces m√∫ltiples con magnitud 1, por lo que el m√©todo es d√©bilmente estable.
Ejemplo 5

$SOLTXHHOPpWRGR$GDPV%DVKIRUWKGHFXDUWRRUGHQ√ÄUPHPHQWHHVWDEOH\HOPpWRGR0LOQH
d√©bilmente estable con h 5 0.1 para el problema de valor inicial

y = ‚àí6y + 6,

0 ‚â§ t ‚â§ 1,

y(0) = 2,

que tiene la soluci√≥n exacta y(t) = 1 + e‚àí6t .
Soluci√≥n Los resultados en la tabla 5.21 muestran los efectos de un m√©todo d√©bilmente
estable versusXQR√ÄUPHPHQWHHVWDEOHSDUDHVWHSUREOHPD

262

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

Tabla 5.21
ti
0.10000000
0.20000000
0.30000000
0.40000000
0.50000000
0.60000000
0.70000000
0.80000000
0.90000000
1.00000000

Exacto
y(ti )

M√©todo
Adams‚ÄìBashforth
wi

1.0907180
1.0497871
1.0273237
1.0149956
1.0082297
1.0045166
1.0024788

1.5488116
1.3011942
1.1652989
1.0996236
1.0513350
1.0425614
1.0047990
1.0359090
0.9657936
1.0709304

Error
|yi ‚àí w i |

M√©todo de
Milne
wi

Error
|yi ‚àí w i |

8.906 √ó 10‚àí3
1.548 √ó 10‚àí3
1.524 √ó 10‚àí2
1.020 √ó 10‚àí2
2.768 √ó 10‚àí2
3.872 √ó 10‚àí2
6.845 √ó 10‚àí2

1.5488116
1.3011942
1.1652989
1.0983785
1.0417344
1.0486438
0.9634506
1.1289977
0.7282684
1.6450917

7.661 √ó 10‚àí3
8.053 √ó 10‚àí3
2.132 √ó 10‚àí2
5.154 √ó 10‚àí2
1.208 √ó 10‚àí1
2.762 √ó 10‚àí1
6.426 √ó 10‚àí1

La raz√≥n para seleccionar el m√©todo Adams‚ÄìBashforth-Moulton como nuestra t√©cnica
indicador-corrector de cuarto orden en la secci√≥n 5.6 sobre el m√©todo del Milne-Simpson
GHOPLVPRRUGHQHVTXHWDQWRHOPpWRGR$GDPV%DVKIRUWKFRPRHO$GDPV0RXOWRQVRQ√ÄUmemente estables. Estos tienen m√°s posibilidades de proporcionar aproximaciones precisas
para una clase m√°s amplia de problemas que el m√©todo indicador-corrector con base en las
t√©cnicas de Milne y Simpson, ambos d√©bilmente estables.
La secci√≥n Conjunto de ejercicios est√° disponible en l√≠nea. Encuentre la ruta de acceso
en las p√°ginas preliminares.

5.11 Ecuaciones diferenciales r√≠gidas

Los sistemas r√≠gidos derivan
su nombre del movimiento de
sistemas de masa-resorte que
tienen grandes constantes de
resorte.

Todos los m√©todos para aproximar la soluci√≥n de los problemas de valor inicial tienen t√©rminos de error que implican una derivada superior de la soluci√≥n de la ecuaci√≥n. Si se puede
acotar de manera razonable la derivada, entonces el m√©todo tendr√° una cota de error previsible que se puede utilizar para calcular la precisi√≥n de la aproximaci√≥n. Incluso si la derivada
aumenta conforme los pasos aumentan, el error se puede mantener en control relativo, siempre y cuando la soluci√≥n tambi√©n crezca en magnitud. Sin embargo, con frecuencia surgen
problemas cuando la magnitud de la derivada aumenta, pero la soluci√≥n no lo hace. En esta
situaci√≥n, el error puede aumentar tanto que domina los c√°lculos. Los problemas de valor inicial para los que es probable que esto se presente reciben el nombre de ecuaciones r√≠gidas y
son bastante comunes, de modo especial en el estudio de vibraciones, reacciones qu√≠micas
y circuitos el√©ctricos.
Las ecuaciones diferenciales r√≠gidas se caracterizan como aquellas cuya soluci√≥n exacta
tiene un t√©rmino de la forma e‚àíct, donde c es una constante positiva grande. Normalmente,
esto s√≥lo es una parte de la soluci√≥n, llamada soluci√≥n transitoria. La parte m√°s importante
de la soluci√≥n se llama soluci√≥n de estado estable. La parte transitoria de una ecuaci√≥n r√≠gida
tiende r√°pidamente a cero conforme t aumenta, pero debido a que la en√©sima derivada de
este t√©rmino tiene magnitud cn e‚àíct, la derivada no decae tan r√°pido. De hecho, puesto que
la derivada en el t√©rmino de error se eval√∫a no en t sino en un n√∫mero entre cero y t, los
t√©rminos derivados pueden incrementar conforme t aumenta, y por cierto, de manera muy
r√°pida. Afortunadamente, en general, las ecuaciones r√≠gidas se pueden predecir a partir de
un problema f√≠sico, desde el que se derivan las ecuaciones, y, con cuidado, el error se puede
mantener bajo control. La manera en la que esto se hace se considera en esta secci√≥n.

5.11 Ecuaciones diferenciales r√≠gidas

Ilustraci√≥n

263

El sistema de problemas de valor inicial

1
4
sen t, u 1 (0) =
3
3
1
2
u 2 = ‚àí24u 1 ‚àí 51u 2 ‚àí 9 cos t + sen t, u 2 (0) =
3
3

u 1 = 9u 1 + 24u 2 + 5 cos t ‚àí

tiene la √∫nica soluci√≥n

u 1 (t) = 2e‚àí3t ‚àí e‚àí39t +

1
cos t,
3

u 2 (t) = ‚àíe‚àí3t + 2e‚àí39t ‚àí

1
cos t.
3

El t√©rmino transitorio e‚àí39t en la soluci√≥n causa que este sistema sea r√≠gido. Al aplicar el
algoritmo 5.7, el m√©todo Runge-Kutta de cuarto orden para sistemas, da los resultados listados en la tabla 5.22. Cuando h 5 0.05, los resultados de estabilidad y las aproximaciones son
precisas. Al incrementar el tama√±o de paso a h 5 0.1, sin embargo, conduce a los resultados
desastrosos mostrados en la tabla.

Tabla 5.22
t

u 1 (t)

w 1 (t)
h = 0.05

w 1 (t)
h = 0.1

u 2 (t)

w 2 (t)
h = 0.05

w 2 (t)
h = 0.1

0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0

1.793061
1.423901
1.131575
0.9094086
0.7387877
0.6057094
0.4998603
0.4136714
0.3416143
0.2796748

1.712219
1.414070
1.130523
0.9092763
9.7387506
0.6056833
0.4998361
0.4136490
0.3415939
0.2796568

‚àí2.645169
‚àí18.45158
‚àí87.47221
‚àí934.0722
‚àí1760.016
‚àí7848.550
‚àí34989.63
‚àí155979.4
‚àí695332.0
‚àí3099671.

‚àí1.032001
‚àí0.8746809
‚àí0.7249984
‚àí0.6082141
‚àí0.5156575
‚àí0.4404108
‚àí0.3774038
‚àí0.3229535
‚àí0.2744088
‚àí0.2298877

‚àí0.8703152
‚àí0.8550148
‚àí0.7228910
‚àí0.6079475
‚àí0.5155810
‚àí0.4403558
‚àí0.3773540
‚àí0.3229078
‚àí0.2743673
‚àí0.2298511

7.844527
38.87631
176.4828
789.3540
3520.00
15697.84
69979.87
311959.5
1390664.
6199352.

Aunque a menudo la rigidez se relaciona con los sistemas de ecuaciones diferenciales,
las caracter√≠sticas de la aproximaci√≥n de un m√©todo num√©rico particular aplicado a un sistema r√≠gido se pueden predecir al examinar el error producido cuando se aplica el m√©todo a
una ecuaci√≥n de prueba simple,
y = Œªy, y(0) = Œ±, donde Œª < 0.
(5.64)
La soluci√≥n para esta ecuaci√≥n es y(t) = Œ±eŒªt, que contiene la soluci√≥n transitoria eŒªt. La
soluci√≥n de estado estable es cero, por lo que las caracter√≠sticas de un m√©todo son f√°ciles
de determinar. (Un an√°lisis m√°s completo del error de redondeo relacionado con sistemas
r√≠gidos requiere examinar la ecuaci√≥n de prueba cuando l es un n√∫mero complejo con parte
real negativa; consulte [Ge1], p. 222.)
Primero, considere el m√©todo de Euler aplicado a la ecuaci√≥n de prueba. Si h = (b ‚àí a)/N
y t j = j h, para j = 0, 1, 2, . . . , N , la ecuaci√≥n (5.8) en la p√°gina 266 implica que

w 0 = Œ±,

y

w j+1 = w j + h(Œªw j ) = (1 + hŒª)w j ,

por lo que

w j+1 = (1 + hŒª) j+1 w 0 = (1 + hŒª) j+1 Œ±,

para j = 0, 1, . . . , N ‚àí 1.

Puesto que la soluci√≥n exacta es y(t) = Œ±eŒªt, el error absoluto es

|y(t j ) ‚àí w j | = e j hŒª ‚àí (1 + hŒª) j |Œ±| = (ehŒª ) j ‚àí (1 + hŒª) j |Œ±|,

(5.65)

264

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

y la precisi√≥n se determina por qu√© tan bien el t√©rmino 1+ hŒª aproxima ehŒª. Cuando Œª < 0, la
soluci√≥n exacta (ehŒª ) j tiende a cero conforme j aumenta, pero con la ecuaci√≥n (5.65), la aproximaci√≥n tendr√° esta propiedad s√≥lo si |1 + hŒª| < 1 , lo cual implica que ‚àí2 < hŒª < 0.
Esto restringe efectivamente el tama√±o de paso h para el m√©todo de Euler para satisfacer
h < 2/|Œª|.
Ahora, suponga que se introduce un error de redondeo Œ¥0 en la condici√≥n inicial para el
m√©todo de Euler,

w 0 = Œ± + Œ¥0 .
En el j-√©simo paso, el error de redondeo es

Œ¥ j = (1 + hŒª) j Œ¥0 .
Puesto que Œª < 0, la condici√≥n para el control del crecimiento del error de redondeo es
la misma que la condici√≥n para controlar el error absoluto |1 + hŒª| < 1, que implica que
h < 2/|Œª|. Por lo que
‚Ä¢ Se espera que el m√©todo de Euler sea estable para

y = Œªy, y(0) = Œ±,

en donde Œª < 0,

s√≥lo si el tama√±o de paso h es menor a 2 /|Œª|.
La situaci√≥n es similar para otros m√©todos de un paso. En general, existe una funci√≥n Q
con la propiedad de que el m√©todo de diferencia, cuando se aplica a la ecuaci√≥n de prueba, da

w i+1 = Q(hŒª)w i .

(5.66)

La precisi√≥n del m√©todo depende de qu√© tan bien Q(hŒª) aproxima ehŒª, y el error crecer√° sin
cota si |Q(hŒª)| > 1. El m√©todo de Taylor de en√©simo orden, por ejemplo, tendr√° estabilidad
respecto tanto al crecimiento del error de redondeo como al error absoluto, siempre y cuando
se seleccione h para satisfacer
1
1
1 + hŒª + h 2 Œª2 + ¬∑ ¬∑ ¬∑ + h n Œªn < 1.
2
n!
(OHMHUFLFLRH[DPLQDHOFDVRHVSHFt√ÄFRFXDQGRHOPpWRGRHVHOFOiVLFR5XQJH.XWWDGH
cuarto orden, que es, fundamentalmente, el m√©todo de Taylor de cuarto orden.
Cuando se aplica un m√©todo multipasos de la forma (5.54) a la ecuaci√≥n de prueba, el
resultado es

w j+1 = am‚àí1 w j + ¬∑ ¬∑ ¬∑ + a0 w j+1‚àím + hŒª(bm w j+1 + bm‚àí1 w j + ¬∑ ¬∑ ¬∑ + b0 w j+1‚àím ),
para j = m ‚àí 1, . . . , N ‚àí 1, o

(1 ‚àí hŒªbm )w j+1 ‚àí (am‚àí1 + hŒªbm‚àí1 )w j ‚àí ¬∑ ¬∑ ¬∑ ‚àí (a0 + hŒªb0 )w j+1‚àím = 0.
Relacionado con esta ecuaci√≥n de diferencia homog√©nea se encuentra el polinomio
caracter√≠stico

Q(z, hŒª) = (1 ‚àí hŒªbm )z m ‚àí (am‚àí1 + hŒªbm‚àí1 )z m‚àí1 ‚àí ¬∑ ¬∑ ¬∑ ‚àí (a0 + hŒªb0 ).
Este polinomio es similar al polinomio caracter√≠stico (5.58), pero tambi√©n incorpora la ecuaci√≥n de prueba. La teor√≠a aqu√≠ iguala el an√°lisis de estabilidad en la secci√≥n 5.10.
Suponga que w 0 , . . . , w m‚àí1 est√°n determinados y, para hŒª, sean Œ≤1 , . . . , Œ≤m los ceros
del polinomio Q(z, hŒª). Si Œ≤1 , . . . , Œ≤m son distintos, entonces existe c1 , . . . , cm con
m

wj =
k=1

ck (Œ≤k ) j ,

para j = 0, . . . , N .

(5.67)

5.11 Ecuaciones diferenciales r√≠gidas

265

Si Q(z, hŒª) tiene m√∫ltiples ceros, wj VH GH√ÄQH GH PDQHUD VLPLODU &RQVXOWH OD HFXDFLyQ
(5.63) en la secci√≥n 5.10.) Si wj aproxima con precisi√≥n y(t j ) = e j hŒª = (ehŒª ) j, entonces
todos los ceros Œ≤k deben satisfacer |Œ≤k | < 1; de lo contrario, ciertas opciones de a resultar√°n
en ck = 0, y el t√©rmino ck (Œ≤k ) j no decaer√° a cero.
Ilustraci√≥n

La ecuaci√≥n diferencial de prueba

y = ‚àí30y,

0 ‚â§ t ‚â§ 1.5,

y(0) =

1
3

tiene la soluci√≥n exacta y = 13 e‚àí30t. Al utilizar h 5 0.1 para el algoritmo 5.1 de Euler, el
algoritmo 5.2 de Runge-Kutta de cuarto orden y el algoritmo 5.4 de Adams indicadorcorrector da los resultados en t 5 1.5 en la tabla 5.23.

Tabla 5.23

Soluci√≥n exacta
M√©todo de Euler
M√©todo Runge-Kutta
M√©todo indicador corrector

9.54173 √ó 10‚àí21
‚àí1.09225 √ó 104
3.95730 √ó 101
8.03840 √ó 105

Las imprecisiones en la ilustraci√≥n se deben al hecho de que |Q(hŒª)| > 1 para el m√©todo de Euler y el m√©todo Runge-Kutta y que Q(z, hŒª) tiene ceros con m√≥dulos que exceden
a uno para el m√©todo indicador-corrector. Para aplicar estos m√©todos a este problema, se
GHEHUHGXFLUHOWDPDxRGHSDVR/DVLJXLHQWHGH√ÄQLFLyQVHXVDSDUDGHVFULELUODFDQWLGDGGH
reducci√≥n de tama√±o de paso requerida
DeÔ¨Ånici√≥n 5.25

Este m√©todo es impl√≠cito porque
involucra wj11 en ambos lados
de la ecuaci√≥n.

La regi√≥n R de estabilidad absoluta para un m√©todo de un paso es R = { hŒª ‚àà C | |Q(hŒª)|
< 1 }, y para un m√©todo multipasos es R = { hŒª ‚àà C | |Œ≤k | < 1, para todos los ceros Œ≤k de
Q(z, hŒª) }.
Las ecuaciones (5.66) y (5.67) implican que se puede aplicar un m√©todo de manera
efectiva a una ecuaci√≥n r√≠gida s√≥lo si hŒª se encuentra en la regi√≥n de estabilidad absoluta del
m√©todo, lo cual, para un problema determinado, establece una restricci√≥n en el tama√±o de h.
Aunque el t√©rmino exponencial en la soluci√≥n exacta decae r√°pidamente a cero, Œªh debe
permanecer dentro de la regi√≥n de estabilidad absoluta a lo largo del intervalo de t valores
para que la aproximaci√≥n tienda a cero y el crecimiento del error est√© bajo control. Esto sigQL√ÄFDTXHDSHVDUGHTXHh se podr√≠a incrementar normalmente debido a las consideraciones
del error de truncamiento, el criterio absoluto de estabilidad fuerza h a continuar siendo
peque√±o. Los m√©todos de tama√±o de paso variable son especialmente vulnerables a este
problema ya que una revisi√≥n del error de truncamiento local podr√≠a indicar que el tama√±o
de paso puede aumentar. Esto podr√≠a resultar inadvertidamente en hŒª fuera de la regi√≥n de
estabilidad absoluta.
En general, la regi√≥n de estabilidad absoluta de un m√©todo es el factor cr√≠tico al producir
aproximaciones precisas para sistemas r√≠gidos, por lo que los m√©todos num√©ricos se buscan
con una regi√≥n de estabilidad absoluta tan grande como sea posible. Se dice que un m√©todo
num√©rico es A-estable si su regi√≥n R de estabilidad absoluta contiene todo el plano medio
izquierdo.
El m√©todo trapezoidal impl√≠cito, determinado por

w 0 = Œ±,
w j+1 = w j +

(5.68)

h
f (t j+1 , w j+1 ) + f (t j , w j ) ,
2

0 ‚â§ j ‚â§ N ‚àí 1,

es un m√©todo A-estable (consulte el ejercicio 14) y es el √∫nico m√©todo multipasos A-estable.
A pesar de que el m√©todo trapezoidal no brinda aproximaciones precisas para los tama√±os de
paso grandes, su error no aumentar√° de manera exponencial.

266

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

Las t√©cnicas que se usan con mayor frecuencia para los sistemas r√≠gidos son m√©todos
multipasos impl√≠citos. En general, w i+1 se obtiene al resolver una ecuaci√≥n no lineal o un
sistema no lineal de manera iterativa, a menudo con el m√©todo de Newton. Considere, por
ejemplo, el m√©todo trapezoidal impl√≠cito

w j+1 = w j +

h
[ f (t j+1 , w j+1 ) + f (t j , w j )].
2

Al calcular t j , t j+1 , y w j, necesitamos determinar w i+1, la soluci√≥n para

F(w) = w ‚àí w j ‚àí

h
[ f (t j+1 , w) + f (t j , w j )] = 0.
2

(5.69)

(k)
Para aproximar esta soluci√≥n, seleccione w (0)
j+1, normalmente como w j, y genere w j+1 al
aplicar el m√©todo de Newton a la ecuaci√≥n (5.69)

(k‚àí1)
w (k)
j+1 = w j+1 ‚àí

F w (k‚àí1)
j+1

F w (k‚àí1)
j+1

(k‚àí1)
h
w (k‚àí1)
j+1 ‚àí w j ‚àí 2 f (t j , w j ) + f t j+1 , w j+1
(k‚àí1)
= w j+1 ‚àí
1 ‚àí h2 f y t j+1 , w (k‚àí1)
j+1
(k‚àí1)
hasta que |w (k)
j+1 ‚àí w j+1 | VHD VX√ÄFLHQWHPHQWH SHTXHxD eVWH HV HO SURFHGLPLHQWR TXH VH
aplica en el algoritmo 5.8. Normalmente, s√≥lo se requieren tres o cuatro operaciones por paso
debido a la convergencia cuadr√°tica del m√©todo de Newton.
El m√©todo de la secante se puede utilizar como una alternativa para el m√©todo de Newton
en la ecuaci√≥n (5.69) pero entonces, se requieren dos aproximaciones iniciales diferentes para
(0)
(1)
w j+1. Para usar el m√©todo de la secante, la pr√°ctica usual es hacer w j+1 = w j y obtener w j+1
a partir de alg√∫n m√©todo multipasos expl√≠cito. Cuando un sistema de ecuaciones r√≠gidas participa, se requiere una generalizaci√≥n ya sea para el m√©todo de Newton o de la secante. Estos
temas se consideran en el cap√≠tulo 10.

ALGORITMO

5.8

Iteraci√≥n trapezoidal con Newton
Para aproximar la soluci√≥n del problema de valor inicial

y = f (t, y),

para a ‚â§ t ‚â§ b, con y(a) = Œ±

en (N + 1) n√∫meros igualmente espaciados en el intervalo [ a, b]:
ENTRADA extremos a, b; entero N; condici√≥n inicial Œ±; tolerancia TOL; n√∫mero m√°ximo
de iteraciones M en cualquier paso uno.
SALIDA

aproximaci√≥n w para y en los valores (N + 1) de t o un mensaje de falla.

Paso 1 Determine h = (b ‚àí a)/N ;
t = a;
w = Œ±;
SALIDA (t, w).
Paso 2 Para i = 1, 2, . . . , N haga los pasos 3‚Äì7.
Paso 3 Determine k1 = w + h2 f (t, w);
w 0 = k1 ;
j = 1;
FLAG = 0.

5.11 Ecuaciones diferenciales r√≠gidas

267

Paso 4 Mientras FLAG = 0 haga los pasos 5‚Äì6.
h
f (t + h, w 0 ) ‚àí k1
2
Paso 5 Determine w = w 0 ‚àí
.
h
1 ‚àí f y (t + h, w 0 )
2
Paso 6 Si|w ‚àí w 0 | < TOL entonces determine FLAG = 1
entonces determine j = j + 1;
w 0 = w;
si j > M entonces
SALIDA (‚ÄòEl n√∫mero m√°ximo de
iteraciones excedido‚Äô);
PARE.
w0 ‚àí

Paso 7 Determine t = a + i h;
SALIDA (t, w). Fin del paso 2
Paso 8 PARE.
Ilustraci√≥n

El problema de valor inicial r√≠gido

y = 5e5t (y ‚àí t)2 + 1,

0 ‚â§ t ‚â§ 1,

y(0) = ‚àí1

tiene como soluci√≥n y(t) = t ‚àí e‚àí5t. Para mostrar los efectos de rigidez, el m√©todo trapezoidal impl√≠cito y el m√©todo Runge-Kutta de cuarto orden se aplican con N 5 4, al hacer
h 5 0.25, y con N 5 5, que da h 5 0.20.
El m√©todo trapezoidal se desempe√±a correctamente en ambos casos, usando M 5 10 y
TOL 5 1026, al igual que Runge-Kutta con h 5 0.2. Sin embargo, h 5 0.25 est√° fuera de
la regi√≥n de estabilidad absoluta del m√©todo Runge-Kutta, que es evidente a partir de los
resultados en la tabla 5.24.

Tabla 5.24

M√©todo Runge-Kutta
h = 0.2

ti

wi

0.0
0.2
0.4
0.6
0.8
1.0

‚àí1.0000000
‚àí0.1488521
0.2684884
0.5519927
0.7822857
0.9934905

ti

wi

0.0
0.25
0.5
0.75
1.0

‚àí1.0000000
0.4014315
3.4374753
1.44639 √ó 1023
Desbordado

M√©todo trapezoidal
h = 0.2

|y(ti ) ‚àí w i |

wi

0
1.9027 √ó 10‚àí2
3.8237 √ó 10‚àí3
1.7798 √ó 10‚àí3
6.0131 √ó 10‚àí4
2.2845 √ó 10‚àí4

‚àí1.0000000
‚àí0.1414969
0.2748614
0.5539828
0.7830720
0.9937726

h = 0.25

0
2.6383 √ó 10‚àí2
1.0197 √ó 10‚àí2
3.7700 √ó 10‚àí3
1.3876 √ó 10‚àí3
5.1050 √ó 10‚àí4

h = 0.25

|y(ti ) ‚àí w i |

wi

0
4.37936 √ó 10‚àí1
3.01956 √ó 100
1.44639 √ó 1023

‚àí1.0000000
0.0054557
0.4267572
0.7291528
0.9940199

|y(ti ) ‚àí w i |

|y(ti ) ‚àí w i |

0
4.1961 √ó 10‚àí2
8.8422 √ó 10‚àí3
2.6706 √ó 10‚àí3
7.5790 √ó 10‚àí4

Aqu√≠ hemos presentado s√≥lo una breve introducci√≥n para lo que el lector, que con frecuencia encuentra ecuaciones diferenciales r√≠gidas, deber√≠a saber. Para mayores detalles consulte [Ge2], [Lam] o [SGe].
La secci√≥n Conjunto de ejercicios 5.11 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

268

CAP√çTULO 5

Problemas de valor inicial para ecuaciones de diferenciales ordinarias

5.12 Software num√©rico
La biblioteca IMSL incluye dos subrutinas para aproximar las soluciones de los problemas
de valor inicial. Cada uno de los m√©todos resuelve un sistema de m ecuaciones de primer
orden en m variables. Las ecuaciones son de la forma

du i
= f i (t, u 1 , u 2 , . . . , u m ),
dt

para i = 1, 2, . . . , m,

donde u i (t0 ) se da para cada i. Una subrutina de tama√±o de paso variable est√° basada en los
m√©todos Runge-Kutta de cuarto y quinto orden descritos en el ejercicio 7 de la secci√≥n 5.5.
Una subrutina de tipo Adams tambi√©n est√° disponible para utilizarse para ecuaciones r√≠gidas
con base en un m√©todo de C. William Gear. Este m√©todo utiliza m√©todos multipasos impl√≠citos de orden hasta 12 y f√≥rmulas de diferenciaci√≥n regresiva de orden hasta 5.
Los procedimientos tipo Runge-Kutta contenidos en la biblioteca NAG est√°n basados en
la forma Merson del m√©todo Runge-Kutta. Un m√©todo Adams de una variable y de tama√±o
de paso variable para sistemas r√≠gidos. Otras rutinas incluyen los mismos m√©todos, pero se
iteran hasta que un componente de la soluci√≥n logra un valor determinado o hasta que una
funci√≥n de la soluci√≥n es cero.
La biblioteca netlib incluye varias subrutinas para aproximar las soluciones de los
problemas de valor inicial en el paquete EDO. Una subrutina est√° basada en los m√©todos
Runge-Kutta-Verner de quinto y sexto orden, otra en los m√©todos Runge-Kutta-Fehlberg de
cuarto y quinto orden, como se describe en la p√°gina 220 de la secci√≥n 5.5. Una subrutina
para problemas de valor inicial de ecuaci√≥n diferencial ordinaria r√≠gida est√° basada en una
IyUPXODGHGLIHUHQFLDFLyQUHJUHVLYDGHFRH√ÄFLHQWHYDULDEOH
Las secciones Preguntas de an√°lisis, Conceptos clave y Revisi√≥n del cap√≠tulo est√°n disponibles en l√≠nea. Encuentre la ruta de acceso en las p√°ginas preliminares.

CAP√çTULO

6

M√©todos directos para resolver
sistemas lineales
Introducci√≥n
/DVOH\HVGH.LUFKKRIIGHFLUFXLWRVHOpFWULFRVHVWDEOHFHQTXHWDQWRHO√ÅXMRGHFRUULHQWHGHOD
UHGKDFLDFDGDXQLyQFRPRODFDtGDGHYROWDMHDOUHGHGRUGHFDGDFLFORFHUUDGRGHXQFLUFXLWR
son cero. Suponga que se aplica un potencial de V volts entre los puntos A y G en el circuito y
que i1, i2, i3, i4 y i5UHSUHVHQWDQHO√ÅXMRGHFRUULHQWHFRPRVHPXHVWUDHQHOGLDJUDPD8VDQGR
G como punto de referencia, las leyes de Kirchhoff implican que las corrientes satisfacen el
siguiente sistema de ecuaciones lineales:

5i 1 + 5i 2 = V,
i 3 ‚àí i 4 ‚àí i 5 = 0,
2i 4 ‚àí 3i 5 = 0,
i 1 ‚àí i 2 ‚àí i 3 = 0,
5i 2 ‚àí 7i 3 ‚àí 2i 4 = 0.
2V

A

3V

B

i1

i2

i3

C
2V

i4
i5

5V

V volts

2V

i1
G

3V

1V

i3
F

4V

D

i5

E

En este cap√≠tulo se considerar√° la soluci√≥n de sistemas de este tipo. Esta aplicaci√≥n se
DQDOL]DHQHOHMHUFLFLRGHODVHFFLyQ
Los sistemas de ecuaciones lineales est√°n relacionados con muchos problemas en
ingenier√≠a y ciencia, as√≠ como con aplicaciones de matem√°ticas para las ciencias sociales y
el estudio cuantitativo de problemas de negocios y econom√≠a.
En este cap√≠tulo, consideramos m√©todos directos para resolver un sistema de n ecuaciones lineales en n variables. Este sistema tiene la forma

E1 :

a11 x1 + a12 x2 + ¬∑ ¬∑ ¬∑ + a1n xn = b1 ,

E2 :

a21 x1 + a22 x2 + ¬∑ ¬∑ ¬∑ + a2n xn = b2 ,
..
.



E n : an1 x1 + an2 x2 + ¬∑ ¬∑ ¬∑ + ann xn = bn .
En este sistema nos proporcionan las constantes ai j, para cada i, j = 1, 2, . . . , n, y bi, para
cada i = 1, 2, . . . , n y necesitamos determinar las inc√≥gnitas x1 , . . . , xn .
269

270

CAP√çTULO 6

M√©todos directos para resolver sistemas lineales

Las t√©cnicas directas son m√©todos que proporcionan te√≥ricamente la soluci√≥n exacta
GHOVLVWHPDHQXQQ~PHUR√ÄQLWRGHSDVRV(QODSUiFWLFDSRUVXSXHVWRODVROXFLyQREWHQLGD
estar√° contaminada por el error de redondeo relacionado con la aritm√©tica utilizada. Analizar
HOHIHFWRGHHVWHHUURUGHUHGRQGHR\GHWHUPLQDUIRUPDVSDUDPDQWHQHUOREDMRFRQWUROVHUiXQ
componente muy importante de este cap√≠tulo.
No se supone que un curso de √°lgebra lineal sea un prerrequisito para este cap√≠tulo, por
lo que incluiremos una serie de nociones b√°sicas sobre el tema. Estos resultados tambi√©n se
utilizar√°n en el cap√≠tulo 7, donde consideraremos los m√©todos para aproximar la soluci√≥n de
los sistemas lineales con m√©todos iterativos.

6.1 Sistemas de ecuaciones lineales
8WLOL]DPRVWUHVRSHUDFLRQHVSDUDVLPSOL√ÄFDUHOVLVWHPDOLQHDOGDGRHQODHFXDFLyQ  
1.

2.

3.

La ecuaci√≥n Ei puede multiplicarse por cualquier constante l diferente de cero y
la ecuaci√≥n resultante puede usarse en lugar de Ei. Esta operaci√≥n se denota como
(ŒªE i ) ‚Üí (E i ).
La ecuaci√≥n Ej puede multiplicarse por cualquier constante l diferente de cero y
sumarse con la ecuaci√≥n Ei y la ecuaci√≥n resultante puede usarse en lugar de Ei. Esta
operaci√≥n se denota como (E i + ŒªE j ) ‚Üí (Ei 
El orden de las ecuaciones Ei y Ej puede intercambiarse. Esta operaci√≥n se denota
(E i ) ‚Üî (Ej 

Mediante una secuencia de estas operaciones, un sistema lineal se transformar√° de manera
FRQVWDQWHHQXQRQXHYRPiVIiFLOGHUHVROYHU\FRQODVPLVPDVVROXFLRQHV FRQVXOWHHOHMHUFLFLR /DVHFXHQFLDGHODVRSHUDFLRQHVVHLOXVWUDDFRQWLQXDFLyQ
Ilustraci√≥n

Las cuatro ecuaciones

E1 :

x1 + x2

E2 :
E3 :

2x1 + x2 ‚àí x3 + x4 = 1,
3x1 ‚àí x2 ‚àí x3 + 2x4 = ‚àí3,

+ 3x4 =

E4 :

‚àíx1 + 2x2 + 3x3 ‚àí x4 =

4,


4,

se resolver√°n para x1, x2, x3 y x4. Primero usaremos la ecuaci√≥n E1 para eliminar el valor desconocido x1 de las ecuaciones E2, E3 y E4 DOUHDOL]DU(E 2 ‚àí 2E 1 ) ‚Üí (E 2 ), (E 3 ‚àí 3E 1 ) ‚Üí (E 3 ), y
(E 4 + E 1 ) ‚Üí (E 4 )3RUHMHPSORHQODVHJXQGDHFXDFLyQ

(E 2 ‚àí 2E 1 ) ‚Üí (E 2 )
produce

(2x1 + x2 ‚àí x3 + x4 ) ‚àí 2(x1 + x2 + 3x4 ) = 1 ‚àí 2(4),
TXHVHVLPSOL√ÄFDSDUDHOUHVXOWDGRPRVWUDGRFRPRE2 en

E1 :
E2 :
E3 :
E4 :

x1 + x2

+ 3x4 =

4,

‚àí x2 ‚àí x3 ‚àí 5x4 = ‚àí7,
‚àí 4x2 ‚àí x3 ‚àí 7x4 = ‚àí15,
3x2 + 3x3 + 2x4 =

8.

Por simplicidad, las ecuaciones nuevas se etiquetan otra vez como E1, E2, E3 y E4.

6.1 Sistemas de ecuaciones lineales

271

En el nuevo sistema, E2 se usa para eliminar la inc√≥gnita x2 de E3 y E4 al realizar
(E 3 ‚àí 4E 2 ) ‚Üí (E 3 ) y (E 4 + 3E 2 ) ‚Üí (E 4 ). Esto resulta en

E1 :
E2 :

x1 + x2
+ 3x4 =
4,
‚àí x2 ‚àí x3 ‚àí 5x4 = ‚àí7,

E3 :

3x3 + 13x4 =

E4 :

13,



‚àí 13x4 = ‚àí13.

$KRUDHOVLVWHPDGHHFXDFLRQHV  WLHQHXQDforma triangular (o reducida \VHSXHGH
resolver para las inc√≥gnitas mediante un proceso de sustituci√≥n hacia atr√°s. Ya que E4 implica x4 5 1, podemos resolver E3 para x3 para obtener

x3 =

1
1
(13 ‚àí 13x4 ) = (13 ‚àí 13) = 0.
3
3

Al continuar, E2 nos da

x2 = ‚àí(‚àí7 + 5x4 + x3 ) = ‚àí(‚àí7 + 5 + 0) = 2,
y E1 da

x1 = 4 ‚àí 3x4 ‚àí x2 = 4 ‚àí 3 ‚àí 2 = ‚àí1.
/DVROXFLyQSDUDHOVLVWHPD  \SRUFRQVLJXLHQWHSDUDHOVLVWHPD  HVSRUORWDQWR
x1 5 21, x2 5 2, x3 5 0, y x4 5 1.

Matrices y vectores
Al realizar los c√°lculos en la ilustraci√≥n, no es necesario escribir las ecuaciones completas
en cada paso o retener las variables x1, x2, x3, y x4 a lo largo de los c√°lculos, si siempre permanecen en la misma columna. La √∫nica variaci√≥n de sistema a sistema se presenta en los
FRH√ÄFLHQWHVGHODVLQFyJQLWDV\HQORVYDORUHVGHOODGRGHUHFKRGHODVHFXDFLRQHV'HELGRD
esto, a menudo, se reemplaza un sistema lineal con una matriz que contiene toda la informaci√≥n sobre el sistema necesaria para determinar su soluci√≥n, pero de manera compacta y una
que se representa f√°cilmente en una computadora.
DeÔ¨Ånici√≥n 6.1

8QD matriz n 3 m (n por m HVXQDUUHJORUHFWDQJXODUGHHOHPHQWRVFRQn√ÄODV\m columnas en las que no s√≥lo se encuentra el valor de un elemento importante, sino tambi√©n su
posici√≥n en el arreglo.
La notaci√≥n para una matriz n 3 m ser√° una may√∫scula, como A, para la matriz y min√∫sculas con sub√≠ndices dobles, como ai j, para referirse a la entrada en la intersecci√≥n de la
ipVLPD√ÄOD\ODj-√©sima columna; es decir
‚é°
‚é§
a11 a12 ¬∑ ¬∑ ¬∑ a1m
‚é¢ a21 a22 ¬∑ ¬∑ ¬∑ a2m ‚é•
‚é¢
‚é•
A = [ai j ] = ‚é¢ .
..
.. ‚é• .
‚é£ ..
.
. ‚é¶
an1 an2 ¬∑ ¬∑ ¬∑ anm

Ejemplo 1

Determine el tama√±o y las entradas respectivas de la matriz.

A=

2
3

‚àí1
1

7
0

.

Soluci√≥n /DPDWUL]WLHQHGRV√ÄODV\WUHVFROXPQDVSRUORTXHVXWDPDxRHV3 3. Sus en-

tradas se describen con a11 = 2, a12 = ‚àí1, a13 = 7, a21 = 3, a22 = 1, y a23 = 0.

272

CAP√çTULO 6

M√©todos directos para resolver sistemas lineales

La matriz 1 5 n

A = [a11 a12 ¬∑ ¬∑ ¬∑ a1n ]
recibe el nombre de YHFWRU√ÄODn-dimensional y una matriz n 3 1
‚é°
‚é§
a11
‚é¢ a21 ‚é•
‚é¢
‚é•
A=‚é¢ . ‚é•
‚é£ .. ‚é¶

an1

recibe el nombre de vector columna n-dimensional. Normalmente, para los vectores se
omiten los sub√≠ndices y se utilizan letras min√∫sculas negritas para denotarlos. Por lo tanto,
‚é°
‚é§
x1
‚é¢ x2 ‚é•
‚é¢
‚é•
x=‚é¢ . ‚é•
‚é£ .. ‚é¶

xn

denota un vector de columna y

y = [y1 y2 . . . yn ]
XQ YHFWRU GH √ÄOD $GHPiV D PHQXGR ORV YHFWRUHV √ÄOD WLHQHQ FRPDV HQWUH ODV HQWUDdas para hacer que la separaci√≥n sea clara. Por lo que usted podr√≠a ver y escrita como
y = [y1 , y2 , . . . , yn ].
8QDPDWUL]n 3 (n 1 VHSXHGHXVDUSDUDUHSUHVHQWDUHOVLVWHPDOLQHDO

a11 x1 + a12 x2 + ¬∑ ¬∑ ¬∑ + a1n xn = b1 ,
a21 x1 + a22 x2 + ¬∑ ¬∑ ¬∑ + a2n xn = b2 ,
..
.

..
.

an1 x1 + an2 x2 + ¬∑ ¬∑ ¬∑ + ann xn = bn ,
al construir primero

‚é°

a12
a22
..
.

¬∑¬∑¬∑
¬∑¬∑¬∑

a11
‚é¢a21
‚é¢
[A, b] = ‚é¢ .
‚é£ ..

a12
a22
..
.

a11
‚é¢ a21
‚é¢
A = [ai j ] = ‚é¢ .
‚é£ ..

an2

an1
‚é°

an1

$XPHQWDGDVHUH√ÄHUHDOKHFKRGH
que el lado derecho del sistema
se ha incluido en la matriz.

‚é§
a1n
a2n ‚é•
‚é•
.. ‚é•
. ‚é¶

¬∑ ¬∑ ¬∑ ann

an2

¬∑¬∑¬∑
¬∑¬∑¬∑

a1n
a2n
..
.

¬∑ ¬∑ ¬∑ ann

‚é°

y

‚é§
b1
‚é¢ b2 ‚é•
‚é¢
‚é•
b=‚é¢ . ‚é•
‚é£ .. ‚é¶
bn

.. b1 ‚é§
..
.. b2 ‚é•
.. . ‚é•
‚é•,
.. .. ‚é¶
..
. bn

GRQGHODOtQHDSXQWHDGDYHUWLFDOVHXVDSDUDVHSDUDUORVYDORUHVGHORVFRH√ÄFLHQWHVGHODV
inc√≥gnitas con los del lado derecho de las ecuaciones. El arreglo [A, b] recibe el nombre de
matriz aumentada.
La repetici√≥n de las operaciones implicadas en la ilustraci√≥n de la p√°gina 270 con la
notaci√≥n resulta en considerar primero la matriz aumentada:
‚é°
‚é§
1
1
0
3 ... 4
‚é¢ 2
1 ‚àí1
1 ... 1 ‚é•
‚é¢
‚é•.
‚é£ 3 ‚àí1 ‚àí1
2 ... ‚àí3 ‚é¶
‚àí1
2
3 ‚àí1 .. 4

6.1 Sistemas de ecuaciones lineales

273

5HDOL]DUODVRSHUDFLRQHVGHDFXHUGRFRQORGHVFULWRHQHOHMHPSORSURGXFHPDWULFHVDXPHQtadas
‚é°
‚é§
‚é°
‚é§
1
1
0
3 ...
4
1
1
0
3 ...
4
‚é¢ 0 ‚àí1 ‚àí1 ‚àí5 .. ‚àí7 ‚é•
‚é¢ 0 ‚àí1 ‚àí1 ‚àí5 .. ‚àí7 ‚é•
‚é¢
‚é•
‚é¢
‚é•.
.
..
‚é£ 0 ‚àí4 ‚àí1 ‚àí7 .. ‚àí15 ‚é¶ y ‚é£ 0
‚é¶
0
3
13
13
.
.
.
0
3
3
2 ..
8
0
0
0 ‚àí13 .. ‚àí13
8QDWpFQLFDVLPLODUDOD
eliminaci√≥n gaussiana apareci√≥
por primera vez durante la
dinast√≠a Han en China, en el texto
Chapters on the Mathematical
Art (Cap√≠tulos sobre el arte
matem√°tico), escrito alrededor
del a√±o 200 a.C. Joseph
/RXLV/DJUDQJH ¬≤ 
describi√≥ una t√©cnica similar
DHVWHSURFHGLPLHQWRHQ
para el caso en que el valor
de cada ecuaci√≥n sea 0. Gauss
proporcion√≥ una descripci√≥n
m√°s general en Theoria Motus
corporum coelestium sectionibus
solem ambientium, que describ√≠a
la t√©cnica de m√≠nimos cuadrados
TXHXVyHQSDUDGHWHUPLQDU
la √≥rbita del planeta menor Ceres.

$KRUDODPDWUL]√ÄQDOVHSXHGHWUDQVIRUPDUHQVXVLVWHPDOLQHDOFRUUHVSRQGLHQWH\HV
posible obtener soluciones para x1, x2, x3 y x4. Este procedimiento recibe el nombre de
eliminaci√≥n gaussiana con sustituci√≥n hacia atr√°s.
El procedimiento general de eliminaci√≥n gaussiana aplicado al sistema lineal

E1 :
E2 :

a11 x1 + a12 x2 + ¬∑ ¬∑ ¬∑ + a1n xn = b1 ,
a21 x1 + a22 x2 + ¬∑ ¬∑ ¬∑ + a2n xn = b2 ,
..
..
.
.



E n : an1 x1 + an2 x2 + ¬∑ ¬∑ ¬∑ + ann xn = bn ,
VHPDQHMDGHPDQHUDVLPLODU3ULPHURIRUPHODPDWUL]DXPHQWDGDAÃÉ,
‚é°
‚é§
.
a11 a12 ¬∑ ¬∑ ¬∑ a1n .. a1,n+1
‚é¢ a21 a22 ¬∑ ¬∑ ¬∑ a2n ... a2,n+1 ‚é•
‚é¢
‚é•
..
AÃÉ = [A, b] = ‚é¢ .
‚é•,
..
..
..
..
‚é£ ..
‚é¶
.
.
.
..
an1 an2 ¬∑ ¬∑ ¬∑ ann .. an,n+1



donde AGHQRWDODPDWUL]IRUPDGDSRUORVFRH√ÄFLHQWHV/DVHQWUDGDVHQOD n 1 FROXPQD
son los valores de b; es decir, ai,n+1 = bi para cada i = 1, 2, . . . , n.
Siempre y cuando a11 = 0, realizamos las operaciones correspondientes para

(E j ‚àí (a j1 /a11 )E 1 ) ‚Üí (E j )

para cada j = 2, 3, . . . , n

SDUDHOLPLQDUHOFRH√ÄFLHQWHGHx1HQFDGDXQDGHHVWDV√ÄODV$SHVDUGHTXHVHHVSHUDTXHFDPELHQODVHQWUDGDVHQODV√ÄODV2, 3, . . . , n, para facilidad de notaci√≥n, nuevamente denotamos la
entrada en la ipVLPD√ÄOD\ODj-√©sima columna mediante aij. Con esto en mente, seguimos el
procedimiento secuencial para i = 2, 3, . . . , n ‚àí 1 y realizamos la operaci√≥n

(E j ‚àí (a ji /aii )E i ) ‚Üí (E j )

para cada j = i + 1, i + 2, . . . , n,

siempre y cuando aii = 0(VWRHOLPLQD FDPELDHOFRH√ÄFLHQWHDFHUR xiHQFDGD√ÄODGHEDMR
del i-√©simo para todos los valores de i = 1, 2, . . . , n ‚àí 1. La matriz resultante tiene la forma
‚é§
‚é°
.
a11 a12 ¬∑ ¬∑ ¬∑ a1n ... a1,n+1
.
‚é•
‚é¢ 0 . . a.22
. . . ¬∑ ¬∑ ¬∑ a2n ... a2,n+1 ‚é•
Àú =‚é¢
AÃÉ
‚é•,
‚é¢ .. . . . . . . . . . .
..
.
..
..
... ... .
‚é¶
‚é£ .
..
. ..
0 . . . . . . . . . 0 ann .. an,n+1
donde, excepto en la primera columna, no se espera que los valores de ai j concuerden con
Àú representa un sistema lineal con la misma soluci√≥n
los de la matriz original AÃÉ. La matriz AÃÉ
establecida como el sistema original.
El sistema lineal nuevo es triangular,

a11 x1 + a12 x2 + ¬∑ ¬∑ ¬∑ + a1n xn = a1,n+1 ,
a22 x2 + ¬∑ ¬∑ ¬∑ + a2n xn = a2,n+1 ,
..
..
.
.
ann xn = an,n+1 ,

6.1 Sistemas de ecuaciones lineales

Ejemplo 2

275

Represente el sistema lineal

E1 :
E2 :
E3 :

x1 ‚àí x2 + 2x3 ‚àí x4 = ‚àí8,
2x1 ‚àí 2x2 + 3x3 ‚àí 3x4 = ‚àí20,
x1 + x2 + x3
= ‚àí2,

E4 :

x1 ‚àí x2 + 4x3 + 3x4 =

4,

como una matriz aumentada y utilice la eliminaci√≥n gaussiana para encontrar su soluci√≥n.
Soluci√≥n

La matriz aumentada es

‚é°

1
‚é¢
2
AÃÉ = AÃÉ(1) = ‚é¢
‚é£ 1
1

‚àí1 2
‚àí2 3
1 1
‚àí1 4

‚àí1
‚àí3
0
3

‚é§
..
.. ‚àí8
.. ‚àí20 ‚é•
‚é•.
..
.. ‚àí2 ‚é¶
..
4

Al realizar las operaciones

(E 2 ‚àí 2E 1 ) ‚Üí (E 2 ), (E 3 ‚àí E 1 ) ‚Üí (E 3 ),

y

(E 4 ‚àí E 1 ) ‚Üí (E 4 ),

obtenemos

‚é°

1
‚é¢ 0
(2)
AÃÉ = ‚é¢
‚é£ 0
0
El elemento pivote para una
FROXPQDHVSHFt√ÄFDHVODHQWUDGD
utilizada para colocar ceros
en las otras entradas en esa
columna.

‚àí1
0
2
0

2
‚àí1
‚àí1
2

‚àí1
‚àí1
1
4

‚é§
..
.. ‚àí8
.. ‚àí4 ‚é•
‚é•.
..
6 ‚é¶
..
.. 12

(2)
, conocida como elemento pivote, es 0, por lo que el proceLa entrada diagonal a22
dimiento no puede continuar en su forma actual. Sin embargo se permiten las operaciones
(2)
(2)
(E i ) ‚Üî (E j ), por lo que se realiza una b√∫squeda de elementos a32
para el primer
y a42
(2)
elemento diferente de cero. Puesto que a32 = 0, se realiza la operaci√≥n (E 2 ) ‚Üî (E 3 ) para
obtener una matriz nueva
‚é°
‚é§
.
1 ‚àí1
2 ‚àí1 ... ‚àí8
‚é¢ 0
2 ‚àí1
1 ...
6 ‚é•
‚é•.
AÃÉ(2) = ‚é¢
.
‚é£ 0
0 ‚àí1 ‚àí1 . ‚àí4 ‚é¶
..
0
0
2
4 . 12

Puesto que x2 ya se ha eliminado de E3 y AÃÉ(3) ser√° AÃÉ(2) , y los c√°lculos contin√∫an con la operaci√≥n (E 4 + 2E 3 ) ‚Üí (E 4 ), lo que nos da
‚é°
‚é§
.
1 ‚àí1
2 ‚àí1 ... ‚àí8
‚é¢ 0
2 ‚àí1
1 ...
6 ‚é•
‚é•.
AÃÉ(4) = ‚é¢
.
‚é£ 0
0 ‚àí1 ‚àí1 .. ‚àí4 ‚é¶
0
0
0
2 ..
4
Finalmente, la matriz se vuelve a convertir en un sistema lineal que tiene una soluci√≥n equivalente a la soluci√≥n del sistema original y se aplica la sustituci√≥n hacia atr√°s:

4
= 2,
2
[‚àí4 ‚àí (‚àí1)x4 ]
= 2,
x3 =
‚àí1
[6 ‚àí [(‚àí1)x3 + x4 ]]
x2 =
= 3,
2
[‚àí8 ‚àí [(‚àí1)x2 + 2x3 + (‚àí1)x4 ]]
= ‚àí7.
x1 =
1
x4 =

276

CAP√çTULO 6

M√©todos directos para resolver sistemas lineales
(k)
= 0 para algunas k = 1, 2, . . . , n ‚àí 1. Para
(OHMHPSORLOXVWUDORTXHVHKDFHVL akk
(k‚àí1)
la k-√©sima columna de AÃÉ
desde la kpVLPD√ÄODKDVWDODHQpVLPD√ÄODVHEXVFDODSULPHUD
entrada diferente a cero. Si a (k)
pk = 0 para algunas p, con k + 1 ‚â§ p ‚â§ n, entonces se realiza
la operaci√≥n (E k ) ‚Üî (E p ) para obtener AÃÉ(k‚àí1) . El procedimiento puede continuar para formar AÃÉ(k) y as√≠ sucesivamente. Si a (k)
pk = 0 para cada p, se puede mostrar (consulte el teorema
HQODSiJLQD TXHHOVLVWHPDOLQHDOQRWLHQHXQDVROXFLyQ~QLFD\HOSURFHGLPLHQWR
(n)
= 0, el sistema lineal no tiene una soluci√≥n √∫nica y, de nuevo,
se detiene. Finalmente, si ann
el procedimiento se detiene.
(ODOJRULWPRUHVXPHODHOLPLQDFLyQJDXVVLDQDFRQVXVWLWXFLyQKDFLDDWUiV(ODOJRULW(k)
es 0 al intercambiar la kpVLPD√ÄODFRQ
mo incorpora el pivote cuando uno de los pivotes akk
la ppVLPD√ÄODGRQGHp es el entero m√°s peque√±o superior a k para el que a (k)
pk = 0.

ALGORITMO

6.1

Eliminaci√≥n gaussiana con sustituci√≥n hacia atr√°s
Para resolver el sistema lineal n 3 n

E1 :
E2 :
..
.

a11 x1 + a12 x2 + ¬∑ ¬∑ ¬∑ + a1n xn = a1,n+1
a21 x1 + a22 x2 + ¬∑ ¬∑ ¬∑ + a2n xn = a2,n+1
..
..
..
..
.
.
.
.

E n : an1 x1 + an2 x2 + ¬∑ ¬∑ ¬∑ + ann xn = an,n+1
ENTRADA n√∫mero de inc√≥gnitas y ecuaciones n; matriz aumentada A = [ai j ], donde
1 ‚â§ i ‚â§ n y 1 ‚â§ j ‚â§ n + 1.
SALIDA
√∫nica.

Para x1 , x2 , . . . , xn o mensaje de que el sistema lineal no tiene soluci√≥n

Paso 1 Para i = 1, . . . , n ‚àí 1 haga los pasos 2‚Äì4.

(Proceso de eliminaci√≥n. )

Paso 2 Si p es el entero m√°s peque√±o con i ‚â§ p ‚â§ n y a pi = 0.
si no es posible encontrar un entero p
entonces SALIDA (‚Äòno existe una soluci√≥n √∫nica‚Äô);
PARE.
Paso 3 Si p = i entonces realice (E p ) ‚Üî (E i ).
Paso 4 Para j = i + 1, . . . , n haga los pasos 5 y 6.
Paso 5 Determine m ji = a ji /aii .
Paso 6 Ejecute (E j ‚àí m ji E i ) ‚Üí (E j );
Paso 7 Si ann = 0 entonces SALIDA (‚Äòno existe una soluci√≥n √∫nica‚Äô);
PARE.
Paso 8 Determine xn = an,n+1 /ann . (Inicia sustituci√≥n hacia atr√°s.)
Paso 9 Para i = n ‚àí 1, . . . , 1 determine xi = ai,n+1 ‚àí

n
j=i+1 ai j x j

Paso 10 SALIDA ( x1 , . . . , xn ); (Procedimiento completado con √©xito.)
PARE.

aii .

6.1 Sistemas de ecuaciones lineales

Ilustraci√≥n

277

(OREMHWLYRGHHVWDLOXVWUDFLyQHVPRVWUDUORTXHSDVDVLHODOJRULWPRIDOOD/RVFiOFXORVVH
realizar√°n de manera simult√°nea en dos sistemas lineales

x1 + x2 + x3 = 4,
2x1 + 2x2 + x3 = 6,
x1 + x2 + 2x3 = 6,

y

x1 + x2 + x3 = 4,
2x1 + 2x2 + x3 = 4,
x1 + x2 + 2x3 = 6.

Estos sistemas producen las matrices aumentadas
‚é°
‚é§
‚é°
.
1 1 1 .. 4
1 1 1
..
AÃÉ = ‚é£ 2 2 1 .. 6 ‚é¶
y
AÃÉ = ‚é£ 2 2 1
1 1 2 .. 6
1 1 2

‚é§
..
.. 4
.. 4 ‚é¶ .
..
. 6

Puesto que a11 = 1, realizamos (E 2 ‚àí 2E 1 ) ‚Üí (E 2 ) y (E 3 ‚àí E 1 ) ‚Üí (E 3 ) para producir
‚é°
‚é§
‚é°
‚é§
.
.
1 1
1 ..
4
1 1
1 ..
4
..
..
AÃÉ = ‚é£ 0 0 ‚àí1 .. ‚àí2 ‚é¶
y
AÃÉ = ‚é£ 0 0 ‚àí1 .. ‚àí4 ‚é¶ .
0 0
1 ..
2
0 0
1 ..
2
En este punto, a22 = a32 = 0. El algoritmo requiere que el procedimiento se detenga y no
se obtiene ninguna soluci√≥n para el sistema. Al escribir las ecuaciones para cada sistema
obtenemos

x1 + x2 +

x3 = 4,
‚àíx3 = ‚àí2,
x3 = 2,

x1 + x2 +
y

x3 = 4,
‚àíx3 = ‚àí4,
x3 = 2.

(OSULPHUVLVWHPDOLQHDOWLHQHXQQ~PHURLQ√ÄQLWRGHVROXFLRQHVTXHVHSXHGHQGHVFULELUPHdiante x3 = 2, x2 = 2 ‚àí x1 , y x1 de manera arbitraria.
El segundo sistema conduce a la contradicci√≥n x3 = 2 y x3 = 4, por lo que no existe
soluci√≥n. Sin embargo, en este caso, no hay soluci√≥n √∫nica, como concluimos a partir del
DOJRULWPR
$ SHVDU GH TXH HO DOJRULWPR  SXHGH VHU REVHUYDGR FRPR ODV FRQVWUXFFLRQHV GH ODV
matrices aumentadas AÃÉ(1) , . . . , AÃÉ(n), los c√°lculos pueden realizarse con s√≥lo un arreglo
n √ó(n +1). En cada paso, simplemente reemplazamos el valor anterior de ai j por el nuevo.
Adem√°s, podemos almacenar multiplicadores de mji en las ubicaciones de aji porque aji tiene
el valor 0 para cada i = 1, 2, . . . , n ‚àí 1 y j = i + 1, i + 2, . . . , n. Por lo tanto, A se puede
VREUHHVFULELUPHGLDQWHORVPXOWLSOLFDGRUHVHQODVHQWUDGDVGHEDMRGHODGLDJRQDOSULQFLSDO HV
decir, las entradas de la forma aji, con j > i \PHGLDQWHODVHQWUDGDVUHFLHQWHPHQWHFDOFXladas de AÃÉ(n) en y sobre la diagonal principal (las entradas de la forma ai j con j ‚â§ i (VWRV
valores se pueden obtener para resolver otros sistemas lineales relacionados con la matriz
original AFRPRREVHUYDUHPRVHQODVHFFLyQ

Conteo de operaciones
Tanto el tiempo requerido para completar los c√°lculos como el error de redondeo subseFXHQWHGHSHQGHQGHOQ~PHURGHRSHUDFLRQHVDULWPpWLFDVGHSXQWR√ÅRWDQWHQHFHVDULDVSDUD
resolver un problema de rutina. En general, la cantidad de tiempo requerido para realizar
una multiplicaci√≥n o divisi√≥n en una computadora es aproximadamente el mismo y es muy
superior al requerido para realizar una suma o una resta. Las diferencias reales en el tiempo
GHHMHFXFLyQVLQHPEDUJRGHSHQGHQGHOVLVWHPDFRPSXWDFLRQDOSDUWLFXODU3DUDGHPRVWUDU
las operaciones de conteo para un m√©todo determinado, contaremos las operaciones requeridas para resolver un sistema lineal com√∫n de n ecuaciones con n inc√≥gnitas mediante el
DOJRULWPR0DQWHQGUHPRVHOFRQWHRGHODVVXPDVUHVWDVVHSDUDGRGHOFRQWHRGHODVPXOWLSOLFDFLRQHVGLYLVLRQHVGHELGRDOGLIHUHQFLDOGHWLHPSR

278

CAP√çTULO 6

M√©todos directos para resolver sistemas lineales

1RVHUHDOL]DQRSHUDFLRQHVDULWPpWLFDVKDVWDORVSDVRV\HQHODOJRULWPR(OSDVR
requiere realizar (n 2 i GLYLVLRQHV(OUHHPSOD]RGHODHFXDFLyQEj mediante (E j ‚àí m ji E i )
HQ HO SDVR  UHTXLHUH PXOWLSOLFDU mji por cada t√©rmino en Ei, lo cual resulta en un total de
(n ‚àí i)(n ‚àí i + 1) multiplicaciones. Despu√©s de completar esto, cada t√©rmino de la ecuaci√≥n
resultante se resta del t√©rmino correspondiente en Ej. Esto requiere (n ‚àí i)(n ‚àí i + 1) restas.
Para cada i = 1, 2, . . . , n ‚àí 1, ODVRSHUDFLRQHVUHTXHULGDVHQORVSDVRV\VRQORVVLJXLHQWHV
Multiplicaciones/divisiones:

(n ‚àí i) + (n ‚àí i)(n ‚àí i + 1) = (n ‚àí i)(n ‚àí i + 2).
Sumas/restas:

(n ‚àí i)(n ‚àí i + 1).
(OQ~PHURWRWDOGHRSHUDFLRQHVUHTXHULGDVSRUORVSDVRV\VHREWLHQHDOVXPDUHO
conteo de operaciones para cada i. Al recordar, a partir del c√°lculo que
m

m

1 = m,
j=1

j=
j=1

m(m + 1)
,
2

m

j2 =

y
j=1

m(m + 1)(2m + 1)
,
6

tenemos el siguiente conteo de operaciones.
Multiplicaciones/divisiones:
n‚àí1

n‚àí1

(n ‚àí i)(n ‚àí i + 2) =
i=1

(n 2 ‚àí 2ni + i 2 + 2n ‚àí 2i)
i=1
n‚àí1

n‚àí1

(n ‚àí i)2 + 2

=
i=1

=

n‚àí1

(n ‚àí i) =
i=1

n‚àí1

i2 + 2
i=1

i
i=1

(n ‚àí 1)n
2n 3 + 3n 2 ‚àí 5n
(n ‚àí 1)n(2n ‚àí 1)
+2
=
.
6
2
6

Sumas/restas:
n‚àí1

n‚àí1

(n ‚àí i)(n ‚àí i + 1) =
i=1

(n 2 ‚àí 2ni + i 2 + n ‚àí i)
i=1
n‚àí1
i=1

=

n‚àí1

(n ‚àí i)2 +

=

n‚àí1

(n ‚àí i) =
i=1

n‚àí1

i2 +
i=1

i
i=1

n3 ‚àí n
(n ‚àí 1)n(2n ‚àí 1) (n ‚àí 1)n
+
=
.
6
2
3

/RV~QLFRVRWURVSDVRVHQHODOJRULWPRTXHLPSOLFDQRSHUDFLRQHVDULWPpWLFDVVRQORV
UHTXHULGRVSDUDODVXVWLWXFLyQKDFLDDWUiVORVSDVRV\UHTXLHUHQXQDGLYLVLyQ(OSDVR
requiere (n ‚àí i) multiplicaciones y (n ‚àí i ‚àí 1) sumas para cada t√©rmino de suma y, despu√©s,
XQDUHVWD\XQDGLYLVLyQ(OQ~PHURWRWDOGHRSHUDFLRQHVHQORVSDVRV\HVODVLJXLHQWH

6.2 Estrategias de pivoteo

279

Multiplicaciones/divisiones:
n‚àí1

1+

n‚àí1

((n ‚àí i) + 1) = 1 +
i=1

(n ‚àí i)

+n‚àí1

i=1
n‚àí1

n‚àí1

(n ‚àí i) = n +

=n+
i=1

i=
i=1

n2 + n
.
2

Sumas/restas:
n‚àí1

n‚àí1

((n ‚àí i ‚àí 1) + 1) =
i=1

n‚àí1

(n ‚àí i) =
i=1

i=
i=1

n2 ‚àí n
2

(OQ~PHURWRWDOGHRSHUDFLRQHVDULWPpWLFDVHQHODOJRULWPRHVSRUORWDQWR
Multiplicaciones/divisiones:

n2 + n
n3
2n 3 + 3n 2 ‚àí 5n
n
+
=
+ n2 ‚àí .
6
2
3
3
Sumas/restas:

n2 ‚àí n
n3
n2
5n
n3 ‚àí n
+
=
+
‚àí
.
3
2
3
2
6
Para n grande, el n√∫mero total de multiplicaciones y divisiones es aproximadamente
n 3 /3, ya que es el n√∫mero total de sumas y restas. Por lo tanto, la cantidad de c√°lculos y el
tiempo requerido aumenta con n en proporci√≥n a n3FRPRVHPXHVWUDHQODWDEOD

Tabla 6.1

n

Multiplicaciones/divisiones

Sumas/restas

3
10
50
100

17
430
44150
343300

11
375
42 875
338 250

La secci√≥n Conjunto de ejercicios 6.1 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

6.2 Estrategias de pivoteo
$OGHULYDUHODOJRULWPRHQFRQWUDPRVTXHVHQHFHVLWDEDXQLQWHUFDPELRGH√ÄODVFXDQGRXQR
(k)
HVFHUR(VWHLQWHUFDPELRGH√ÄODVWLHQHODIRUPD(E k ) ‚Üî (E p ),
de los elementos pivote akk
donde p es el entero m√°s peque√±o superior a k con a (k)
pk = 0. Para reducir el error de redondeo,
DPHQXGRHVQHFHVDULRUHDOL]DULQWHUFDPELRVGH√ÄODVLQFOXVRFXDQGRORVHOHPHQWRVSLYRWH
no son cero.
(k)
Si akk
es de magnitud peque√±a, en comparaci√≥n con a (k)
jk , entonces la magnitud del
multiplicador

m jk =

a (k)
jk

(k)
akk

280

CAP√çTULO 6

M√©todos directos para resolver sistemas lineales

ser√° mucho m√°s grande que 1. El error de redondeo introducido en el c√°lculo de uno de los
, que compone el error original.
t√©rminos akl(k) se multiplica mediante mjk al calcular a (k+1)
jl
Adem√°s, al realizar la sustituci√≥n hacia atr√°s para

xk =

(k)
‚àí
ak,n+1

(k)
n
j=k+1 ak j
,
(k)
akk

(k)
, cualquier error en el numerador puede incrementar dr√°sticacon un valor peque√±o de akk
(k)
(QQXHVWURHMHPSORREVHUYDUHPRVTXHLQFOXVRSDUDVLVWHmente debido a la divisi√≥n de akk
mas peque√±os, el error de redondeo puede dominar los c√°lculos.

Ejemplo 1

Aplique la eliminaci√≥n gaussiana para el sistema

E1 :

0.003000x1 + 59.14x2 = 59.17

E2 :

5.291x1 ‚àí 6.130x2 = 46.78,

por medio de aritm√©tica de cuatro d√≠gitos con redondeo y compare los resultados con la soluci√≥n exacta x1 = 10.00 y x2 = 1.000.
(1)
El primer elemento pivote a11
= 0.003000 , es peque√±o y est√° relacionado con el
multiplicador,

Soluci√≥n

m 21 =

5.291
= 1763.66,
0.003000

VHUHGRQGHDDOQ~PHURPiVJUDQGH$OUHDOL]DU (E 2 ‚àí m 21 E 1 ) ‚Üí (E 2 ) y el redondeo
adecuado da el sistema

0.003000x1 + 59.14x2 ‚âà 59.17
‚àí104300x2 ‚âà ‚àí104400,
en lugar del sistema exacto, que es

0.003000x1 + 59.14x2 = 59.17
‚àí104309.376x2 = ‚àí104309.376.
La disparidad en las magnitudes de m 21 a13 y a23 ha introducido error de redondeo, pero el
error de redondeo a√∫n no se ha propagado. La sustituci√≥n hacia atr√°s produce

x2 ‚âà 1.001,
que es una aproximaci√≥n cercana al valor real x2 5 1.000. Sin embargo, debido al pivote
peque√±o a11 5 0.003000,
59.17 ‚àí (59.14)(1.001)
= ‚àí10.00
x1 ‚âà
0.003000
contiene el error peque√±o de 0.001 multiplicado por

59.14
‚âà 20000.
0.003000
Esto arruina la aproximaci√≥n del valor real x1 5 10.00.
(VWRHVFODUDPHQWHXQHMHPSORDUWL√ÄFLDO\ODJUi√ÄFDHQOD√ÄJXUDPXHVWUDODUD]yQSRU
la que el error se puede presentar f√°cilmente. Para sistemas m√°s grandes es mucho m√°s dif√≠cil
predecir cu√°ndo se presentar√° un error de redondeo devastador.

6.2 Estrategias de pivoteo

Figura 6.1

281

x2
E2

Aproximaci√≥n
(210, 1.001)

Soluci√≥n exacta
(10, 1)

210

10

E1
x1

Pivoteo parcial
(OHMHPSORPXHVWUDODIRUPDHQODTXHSXHGHQVXUJLUGL√ÄFXOWDGHVFXDQGRHOHOHPHQWRSLYR(k)
es peque√±o relativo a las entradas ai(k)
te akk
j , para k ‚â§ i ‚â§ n y k ‚â§ j ‚â§ n. Para evitar este
problema se realiza pivoteo al seleccionar un elemento a (k)
pq con una magnitud m√°s grande
como el pivote y al intercambiar las k-√©sima y ppVLPD√ÄODV(VWRSXHGHLUVHJXLGRGHXQ
intercambio de la k-√©sima y q-√©sima columnas, en caso necesario.
La estrategia m√°s simple, llamada pivoteo parcial, es seleccionar un elemento en la
PLVPDFROXPQDTXHHVWiGHEDMRGHODGLDJRQDO\TXHWLHQHHOPi[LPRYDORUDEVROXWRHVSHFt√ÄFDPHQWHGHWHUPLQDPRVOD p ‚â• k m√°s peque√±a de tal forma que
(k)
|a (k)
pk | = m√°x |aik |
k‚â§i‚â§n

y realizamos (E k ) ‚Üî (E p ). En este caso, no se usa intercambio de columnas.
Ejemplo 2

Aplique la eliminaci√≥n gaussiana para el sistema

E 1 : 0.003000x1 + 59.14x2 = 59.17
E2 :

5.291x1 ‚àí 6.130x2 = 46.78,

con pivoteo parcial y aritm√©tica de cuatro d√≠gitos con redondeo y compare los resultados con
la soluci√≥n exacta x1 = 10.00 y x2 = 1.000.
Soluci√≥n

El procedimiento pivotal parcial requiere encontrar primero
(1)
(1)
(1)
m√°x |a11
|, |a21
| = m√°x {|0.003000|, |5.291|} = |5.291| = |a21
|.

Esto requiere realizar la operaci√≥n (E 2 ) ‚Üî (E 1 ) para producir el sistema equivalente

5.291x1 ‚àí 6.130x2 = 46.78,
E1 :
E 2 : 0.003000x1 + 59.14x2 = 59.17.
El multiplicador para este sistema es

m 21 =

(1)
a21

(1)
a11

= 0.0005670,

y la operaci√≥n (E 2 ‚àí m 21 E 1 ) ‚Üí (E 2 ) reduce el sistema a

5.291x1 ‚àí 6.130x2 ‚âà 46.78,
59.14x2 ‚âà 59.14.
La respuesta de cuatro d√≠gitos resultante a partir de la sustituci√≥n hacia atr√°s son los
valores correctos de x1 = 10.00 y x2 = 1.000.

282

CAP√çTULO 6

M√©todos directos para resolver sistemas lineales

La t√©cnica recientemente descrita recibe el nombre de pivoteo parcial (o pivoteo de
columna m√°xima \VHGHVFULEHFRQGHWDOOHHQHODOJRULWPR(OLQWHUFDPELRGH√ÄODUHDOVH
simula en el algoritmo al intercambiar los valores de NROW en el paso 5.

ALGORITMO

6.2

Eliminaci√≥n gaussiana con pivoteo parcial
Para resolver el sistema lineal n 3 n

E1 :
E2 :

a11 x1 + a12 x2 + ¬∑ ¬∑ ¬∑ + a1n xn = a1,n+1
a21 x1 + a22 x2 + ¬∑ ¬∑ ¬∑ + a2n xn = a2,n+1
..
..
.
.

E n : an1 x1 + an2 x2 + ¬∑ ¬∑ ¬∑ + ann xn = an,n+1
ENTRADA n√∫mero de inc√≥gnitas y ecuaciones n; matriz aumentada A = [ai j ] donde
1 ‚â§ i ‚â§ n y 1 ‚â§ j ‚â§ n + 1.
SALIDA

soluci√≥n x1 , . . . , xn o mensaje de que el sistema lineal no tiene soluci√≥n √∫nica.

Paso 1 Para i = 1, . . . , n determine NROW(i) = i.

(Inicialice indicador de fila.)

Paso 2 Para i = 1, . . . , n ‚àí 1 haga los pasos 3‚Äì6.

( Proceso de eliminaci√≥n.)

Paso 3 Sea p el entero m√°s peque√±o con i ‚â§ p ‚â§ n y
|a(NROW( p), i)| = m√°xi‚â§ j‚â§n |a(NROW( j), i)|.
(Notaci√≥n: a(NROW(i), j) ‚â° aNROWi , j .)
Paso 4 Si a(NROW( p), i) = 0 entonces SALIDA (‚Äòno existe soluci√≥n √∫nica‚Äô);
PARE.
Paso 5 Si NROW(i) = NROW( p) entonces determine NCOPY = NROW(i);
NROW(i) = NROW( p);
NROW( p) = NCOPY.
(Intercambio de fila simulado.)
Paso 6 Para j = i + 1, . . . , n haga los pasos 7 y 8.
Paso 7 Determine m(NROW( j), i) = a(NROW( j), i)/a(NROW(i), i).
Paso 8 Realice (E NROW( j) ‚àí m(NROW( j), i) ¬∑ E NROW(i) ) ‚Üí (E NROW( j) ).
Paso 9 Si a(NROW(n), n) = 0 entonces SALIDA (‚Äòno existe soluci√≥n √∫nica‚Äô);
PARE.
Paso 10 Determine xn = a(NROW(n), n + 1)/a(NROW(n), n).
(Inicie sustituci√≥n hacia atr√°s.)
Paso 11 Para i = n ‚àí 1, . . . , 1
determine xi =

a(NROW(i), n + 1) ‚àí

n
j=i+1 a(NROW(i), j) ¬∑ x j

a(NROW(i), i)

.

Paso 12 SALIDA (x1 , . . . , xn ); (Procedimiento completado con √©xito.)
PARE.

Cada multiplicador mji en el algoritmo de pivoteo parcial tiene una magnitud menor o
LJXDOTXH$SHVDUGHTXHHVWDHVWUDWHJLDHVVX√ÄFLHQWHSDUDPXFKRVVLVWHPDVOLQHDOHVVXUJHQ
situaciones en las que es inadecuada.

6.2 Estrategias de pivoteo

Ilustraci√≥n

283

El sistema lineal

E1 :
E2 :

30.00x1 + 591400x2 = 591700,
5.291x1 ‚àí 6.130x2 = 46.78,

HVLJXDODOGHORVHMHPSORV\H[FHSWRTXHWRGDVODVHQWUDGDVHQODSULPHUDHFXDFLyQVH
han multiplicado por 104(OSURFHGLPLHQWRGHSLYRWHRSDUFLDOGHVFULWRHQHODOJRULWPR
FRQDULWPpWLFDGHFXDWURGtJLWRVFRQGXFHDORVPLVPRVUHVXOWDGRVREWHQLGRVHQHOHMHPSOR
El valor m√°ximo en la primera columna es 30.00 y el multiplicador

m 21 =

5.291
= 0.1764
30.00

conduce al sistema

30.00x1 + 591400x2 ‚âà 591700,
‚àí104300x2 ‚âà ‚àí104400,
TXHWLHQHODVPLVPDVVROXFLRQHVLQDGHFXDGDVTXHHOHMHPSORx2 ‚âà 1.001 y x1 ‚âà ‚àí10.00.

Pivoteo parcial escalado
El pivoteo parcial escalado (o pivoteo de columna escalado HVHODGHFXDGRSDUDHOVLVWHPD
en la ilustraci√≥n. √âste coloca al elemento en la posici√≥n pivote que es la m√°s grande en relaFLyQFRQODVHQWUDGDVHQVX√ÄOD(OSULPHUSDVRHQHVWHSURFHGLPLHQWRHVGH√ÄQLUXQIDFWRUGH
escala siSDUDFDGD√ÄODFRPR

si = m√°x |ai j |.
1‚â§ j‚â§n

Si tenemos si 5 0 para algunas i, entonces el sistema no tiene soluci√≥n √∫nica ya que todas
las entradas en la i-√©sLPD√ÄODVRQ$OVXSRQHUTXHpVWHQRHVHOFDVRHOLQWHUFDPELRGH√ÄOD
adecuado para colocar los ceros en la primera columna se determina al seleccionar el √∫ltimo
entero p con

|ak1 |
|a p1 |
= m√°x
1‚â§k‚â§n
sp
sk
y realizar (E 1 ) ‚Üî (E p ). El efecto de escalamiento es garantizar que el elemento m√°s grande
en cada columna tenga una magnitud relativa de 1 antes de la comparaci√≥n realizada para
GHWHUPLQDUVLVHUHDOL]DHOLQWHUFDPELRGH√ÄOD
De manera similar, antes de eliminar la variable xi mediante las operaciones

E k ‚àí m ki E i ,

para k = i + 1, . . . , n,

seleccionamos el entero m√°s peque√±o p ‚â• i con

|aki |
|a pi |
= m√°x
i‚â§k‚â§n sk
sp
\UHDOL]DPRVHOLQWHUFDPELRGH√ÄOD(E i ) ‚Üî (E p ) si i = p. Los factores de escala s1 , . . . , sn
VHFDOFXODQXQDVRODYH]DOLQLFLRGHOSURFHGLPLHQWR$KRUDGHSHQGHQGHOD√ÄODSRUORTXH
WDPELpQVHGHEHQLQWHUFDPELDUFXDQGRVHUHDOL]DQORVLQWHUFDPELRVGH√ÄOD

284

CAP√çTULO 6

M√©todos directos para resolver sistemas lineales

Ilustraci√≥n

Al aplicar el pivoteo parcial escalado para la ilustraci√≥n previa obtenemos

s1 = m√°x{|30.00|, |591400|} = 591400
y

s2 = m√°x{|5.291|, |‚àí6.130|} = 6.130.
Por consiguiente,

30.00
|a11 |
= 0.5073 √ó 10‚àí4 ,
=
s1
591400

|a21 |
5.291
= 0.8631,
=
s2
6.130

y se realiza el intercambio (E 1 ) ‚Üî (E 2 ).
Al aplicar la eliminaci√≥n gaussiana al nuevo sistema

5.291x1 ‚àí 6.130x2 = 46.78
30.00x1 + 591400x2 = 591700
produce los resultados correctos x1 = 10.00 y x2 = 1.000.
$OJRULWPRLPSOHPHQWDHOSLYRWHRSDUFLDOHVFDODGR

ALGORITMO

6.3

Eliminaci√≥n gaussiana con pivoteo parcial escalado.
/RV~QLFRVSDVRVHQHVWHDOJRULWPRTXHGL√ÄHUHQGHORVGHODOJRULWPRVRQ

Paso 1 Para i = 1, . . . , n determine si = m√°x1‚â§ j‚â§n |ai j |;
si si = 0 entonces SALIDA (‚Äòno existe soluci√≥n √∫nica‚Äô);
PARE.
tambi√©n determine NROW(i) = i.
Paso 2 Para i = 1, . . . , n ‚àí 1 haga los pasos 3‚Äì6. (Proceso de eliminaci√≥n.)
Paso 3 Si p es el entero m√°s peque√±o con i ‚â§ p ‚â§ n y
|a(NROW( j), i)|
|a(NROW( p), i)|
= m√°x
.
i‚â§ j‚â§n
s(NROW( p))
s(NROW( j))

Ejemplo 3

Resuelva el sistema lineal con aritm√©tica de redondeo de tres d√≠gitos.

2.11x1 ‚àí 4.21x2 + 0.921x3 = 2.01,
4.01x1 + 10.2x2 ‚àí 1.12x3 = ‚àí3.09,
1.09x1 + 0.987x2 + 0.832x3 = 4.21.
Soluci√≥n

Tenemos s1 = 4.21, s2 = 10.2, y s3 = 1.09. Por lo que,

2.11
|a11 |
= 0.501,
=
s1
4.21

|a21 |
4.01
= 0.393,
=
s1
10.2

La matriz aumentada A AVHGH√ÄQHPHGLDQWH
‚é°
2.11
‚àí4.21
‚é£ 4.01
10.2
1.09
.987

.921
‚àí1.12
.832

y

|a31 |
1.09
= 1.
=
s3
1.09

‚é§
..
2.01
..
.. ‚àí3.09 ‚é¶.
..
4.21
.

6.2 Estrategias de pivoteo

285

Puesto que |a31 |/s3 es el m√°s grande, realizamos (E 1 ) ‚Üî (E 3 ) para obtener
‚é°
‚é§
.
1.09
.987
.832 ...
4.21
‚é£ 4.01
10.2
‚àí1.12 .. ‚àí3.09 ‚é¶.
.
2.11 ‚àí4.21
.921 ..
2.01
Calcule los multiplicadores

m 21 =

a21
= 3.68;
a11

m 31 =

a31
= 1.94.
a11

Realice las primeras dos eliminaciones para producir
‚é°
‚é§
.
1.09
.987
.832 ...
4.21
.. ‚àí18.6 ‚é¶.
‚é£ 0
6.57
‚àí4.18
.
0
‚àí6.12
‚àí.689 .. ‚àí6.16
Puesto que

|a22 |
6.57
= 0.644 y
=
s2
10.2
realizamos E 2 ‚Üî E 3, lo cual nos da
‚é°
1.09
.987
‚é£ 0
‚àí6.12
0
6.57

|a32 |
6.12
= 1.45,
=
s3
4.21
‚é§
..
4.21
..
.. ‚àí6.16 ‚é¶.
..
. ‚àí18.6

.832
‚àí.689
‚àí4.18

El multiplicador m32 se calcula a trav√©s de

m 32 =

a32
= ‚àí1.07,
a22

y el siguiente paso de eliminaci√≥n en la matriz
‚é°
1.09
.987
.832
‚é£ 0
‚àí6.12
‚àí.689
0
0
‚àí4.92

..
..
..
..
.

‚é§
4.21
‚àí6.16 ‚é¶.
‚àí25.2

Finalmente, la sustituci√≥n hacia atr√°s da la soluci√≥n x, la cual, para tres d√≠gitos decimales, es
x1 = ‚àí0.431, x2 = 0.430, y x3 = 5.12.
Los primeros c√°lculos adicionales requeridos para pivoteo parcial escalado resultan de la
determinaci√≥n de los factores de escala; existen (n 2 FRPSDUDFLRQHVSDUDFDGDXQDGHODV
n√ÄODVSDUDXQWRWDOGH

n(n ‚àí 1) comparaciones.
Para determinar el primer intercambio correcto, se realizan n divisiones, seguidas de n 2 1
comparaciones. Por lo que, la primera determinaci√≥n de intercambio a√±ade
n divisiones y (n 2 FRPSDUDFLRQHV
Los factores de escalamiento se calculan solamente una vez, por lo que el segundo paso
requiere
(n 2 divisiones y (n 2 FRPSDUDFLRQHV

286

CAP√çTULO 6

M√©todos directos para resolver sistemas lineales

6HJXLPRVGHPDQHUDVLPLODUKDVWDREWHQHUFHURVSRUGHEDMRGHODGLDJRQDOSULQFLSDOHQ
WRGDVODV√ÄODVH[FHSWRHQpVLPD(OSDVR√ÄQDOUHTXLHUHUHDOL]DU
2 divisiones y 1 comparaci√≥n.
En consecuencia, el pivoteo parcial escalado a√±ade un total de
n‚àí1

n(n ‚àí 1) +

k = n(n ‚àí 1) +
k=1

3
(n ‚àí 1)n
= n(n ‚àí 1)
2
2

comparaciones



y
n

n

k=
k=2

k

‚àí1=

k=1

1
n(n + 1)
‚àí 1 = (n ‚àí 1)(n + 2)
2
2

divisiones

al procedimiento de eliminaci√≥n gaussiana. El tiempo requerido para realizar una comparaFLyQHVDSUR[LPDGDPHQWHHOPLVPRTXHHOGHXQDVXPDUHVWD3XHVWRTXHHOWLHPSRWRWDOSDUD
realizar el procedimiento b√°sico de eliminaci√≥n gaussiana es O(n 3 /3)PXOWLSOLFDFLRQHVGLYLsiones y O(n 3 /3)VXPDVUHVWDVHOSLYRWHRSDUFLDOHVFDODGRQRDxDGHWLHPSRFRPSXWDFLRQDO
VLJQL√ÄFDWLYRUHTXHULGRSDUDUHVROYHUXQVLVWHPDSDUDYDORUHVJUDQGHVGHn.
Para enfatizar la importancia de seleccionar una sola vez los factores de escala, consideUHODFDQWLGDGGHFiOFXORVDGLFLRQDOHVTXHVHUHTXHULUtDQVLVHPRGL√ÄFDUDHOSURFHGLPLHQWRGH
tal forma que se determinaran factores de escala nuevos cada vez que se realice una decisi√≥n
GHLQWHUFDPELRGH√ÄOD(QHVWHFDVRHOWpUPLQRn(n 2 HQODHFXDFLyQ  VHUtDUHHPSODzado por
n

k(k ‚àí 1) =
k=2

1
n(n 2 ‚àí 1).
3

En consecuencia, esta t√©cnica de pivoteo a√±adir√≠a O(n 3 /3) comparaciones, adem√°s de las
[n(n 1 @2 1 divisiones.

Pivoteo completo
(OSLYRWHRSXHGHLQFOXLUHOLQWHUFDPELRWDQWRGH√ÄODVFRPRGHFROXPQDV(Opivoteo completo (o m√°ximo HQHOk-√©simo paso busca todas las entradas ai j , para i = k, k + 1, . . . , n y
j 5 k, k +1, . . . , nSDUDHQFRQWUDUODHQWUDGDFRQPD\RUPDJQLWXG/RVLQWHUFDPELRVGH√ÄOD
y de columna se realizan para colocar esta entrada en la posici√≥n pivote. El primer paso del
pivoteo total requiere realizar n2 2 1 comparaciones, el segundo paso requiere (n ‚àí 1)2 ‚àí 1
comparaciones, y as√≠ sucesivamente. El tiempo adicional total requerido para incorporar el
pivoteo completo a la eliminaci√≥n gaussiana es
n

(k 2 ‚àí 1) =
k=2

n(n ‚àí 1)(2n + 5)
6

comparaciones. El pivoteo completo es, por consiguiente, la estrategia recomendada solaPHQWHSDUDVLVWHPDVHQORVTXHODSUHFLVLyQHVIXQGDPHQWDO\ODFDQWLGDGGHWLHPSRGHHMHFXFLyQQHFHVDULRSDUDHVWHPpWRGRVHSXHGHMXVWL√ÄFDU
La secci√≥n Conjunto de ejercicios 6.2 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

6.3 √Ålgebra lineal e inversi√≥n de matriz

287

6.3 √Ålgebra lineal e inversi√≥n de matriz
/DVPDWULFHVVHLQWURGXMHURQHQODVHFFLyQFRPRXQPpWRGRFRQYHQLHQWHSDUDH[SUHVDU
y manipular sistemas lineales. En esta secci√≥n consideramos cierta √°lgebra relacionada con
matrices y muestra c√≥mo se puede utilizar para resolver problemas asociados con sistemas
lineales.
DeÔ¨Ånici√≥n 6.2

Dos matrices A y B son igualesVLWLHQHQHOPLVPRQ~PHURGH√ÄODV\FROXPQDVGLJDPRV
n 3 m, y s√≠ ai j = bi j , para cada i = 1, 2, . . . , n y j = 1, 2, . . . , m.
(VWDGH√ÄQLFLyQVLJQL√ÄFDSRUHMHPSORTXH

‚é°

2
3

‚àí1
1

7
0

2
= ‚é£ ‚àí1
7

‚é§
3
1 ‚é¶
0

SXHVWRTXHGL√ÄHUHQHQGLPHQVLyQ

Aritm√©tica de matriz
Dos operaciones importantes en matrices son la suma de dos matrices y la multiplicaci√≥n de
una matriz por un n√∫mero real.
DeÔ¨Ånici√≥n 6.3

Si A y B son matrices, ambas de n 3 m, la suma de A y B, denotada A 1 B, es la matriz n 3 m
cuyas entradas son ai j + bi j , para cada i = 1, 2, . . . , n y j = 1, 2, . . . , m.

DeÔ¨Ånici√≥n 6.4

Si A es una matriz n 3 m y l es un n√∫mero real, entonces la multiplicaci√≥n escalar de l y
A, denotada lA, es la matriz n 3 m cuyas entradas son laij, para cada i = 1, 2, . . . , n
y j = 1, 2, . . . , m.

Ejemplo 1

Determine A 1 B y lA, cuando

A=
Soluci√≥n

2
3

‚àí1
1

7
0

2
1

‚àí8
6

,

y Œª = ‚àí2.

‚àí1 + 2 7 ‚àí 8
1+1 0+6

=

6
3

1
2

‚àí1
6

2
‚àí2

‚àí14
0

,

B=

4
0

Tenemos

A+B =

2+4
3+0

y

ŒªA =

‚àí2(2)
‚àí2(3)

‚àí2(‚àí1)
‚àí2(1)

‚àí2(7)
‚àí2(0)

=

‚àí4
‚àí6

.

Tenemos las siguientes propiedades generales para suma de matriz y multiplicaci√≥n
HVFDODU(VWDVSURSLHGDGHVVRQVX√ÄFLHQWHVSDUDFODVL√ÄFDUHOFRQMXQWRGHWRGDVODVPDWULFHV
n 3 m con entradas reales como espacio vectorial sobre el campo de n√∫meros reales.
¬á 6LGHMDPRVTXHO denote una matriz, donde todas sus entradas son 0 y 2A denota la matriz
cuyas entradas son 2aij.

288

CAP√çTULO 6

M√©todos directos para resolver sistemas lineales

Teorema 6.5

Si A, B y C son matrices n 3 m y l y m son n√∫meros reales. Se cumplen las siguientes propiedades de suma y multiplicaci√≥n escalar:

i)
iii)
v)
vii)

A + B = B + A,
A + O = O + A = A,
Œª(A + B) = ŒªA + ŒªB,
Œª(ŒºA) = (ŒªŒº)A,

ii)
iv)
vi)
viii)

(A + B) + C = A + (B + C),
A + (‚àíA) = ‚àí A + A = O,
(Œª + Œº)A = ŒªA + ŒºA,
1A = A.

7RGDVHVWDVSURSLHGDGHVVLJXHQUHVXOWDGRVVLPLODUHVUHVSHFWRDORVQ~PHURVFRPSOHMRV

Productos matriz-vector
(OSURGXFWRGHPDWULFHVWDPELpQVHSXHGHGH√ÄQLUHQFLHUWDVLQVWDQFLDV3ULPHURFRQVLGHUDUHmos el producto de una matriz n 3 m y un vector columna m 3 1.
DeÔ¨Ånici√≥n 6.6

Sea A una matriz n 3 m y b un vector columna m-dimensional. El producto matriz-vector
de A y b, denotado Ab, es un vector columna n-dimensional dado por
‚é°
‚é§‚é° ‚é§ ‚é° m
‚é§
a11 a12 ¬∑ ¬∑ ¬∑ a1m
b1
i=1 a1i bi
m
‚é¢a21 a22 ¬∑ ¬∑ ¬∑ a2m ‚é• ‚é¢ b2 ‚é• ‚é¢ i=1
a2i bi ‚é•
‚é¢
‚é•‚é¢ ‚é• ‚é¢
‚é•
Ab = ‚é¢ .
=
‚é•
‚é¢
‚é•
‚é¢
‚é•.
..
..
..
..
‚é£ ..
‚é¶
‚é£
‚é¶
‚é£
‚é¶
.
.
.
.

an1

Ejemplo 2

an2

m
i=1 ani bi

bm

¬∑ ¬∑ ¬∑ anm

3DUDGH√ÄQLUHVWHSURGXFWRHOQ~PHURGHFROXPQDVGHODPDWUL]A debe concordar con el
Q~PHURGH√ÄODVGHOYHFWRUb\HOUHVXOWDGRHVRWURYHFWRUFROXPQDFRQHOQ~PHURGH√ÄODVTXH
FRQFXHUGDFRQHOQ~PHURGH√ÄODVHQODPDWUL]
‚é°
‚é§
3 2
3
.
Determine el producto Ab si A = ‚é£ ‚àí1 1 ‚é¶ y b =
‚àí1
6 4
Soluci√≥n Puesto que A tiene dimensi√≥n 3 3 2 y b tiene dimensi√≥n 2 3 1, el producto est√°
GH√ÄQLGR\HVXQYHFWRUFRQWUHV√ÄODVeVWDVVRQ

3(3) + 2(‚àí1) = 7,

(‚àí1)(3) + 1(‚àí1) = ‚àí4,

y

6(3) + 4(‚àí1) = 14.

Es decir,

‚é°

3
Ab = ‚é£ ‚àí1
6

‚é§
2
1 ‚é¶
4

‚é°

3
‚àí1

‚é§
7
= ‚é£ ‚àí4 ‚é¶.
14

La introducci√≥n del producto matriz-vector nos permite ver el sistema lineal

a11 x1 + a12 x2 + ¬∑ ¬∑ ¬∑ + a1n xn = b1 ,
a21 x1 + a22 x2 + ¬∑ ¬∑ ¬∑ + a2n xn = b2 ,
..
..
.
.
an1 x1 + an2 x2 + ¬∑ ¬∑ ¬∑ + ann xn = bn ,
como la ecuaci√≥n matricial

Ax = b,

6.3 √Ålgebra lineal e inversi√≥n de matriz

289

donde

‚é°

a11
‚é¢ a21
‚é¢
A=‚é¢ .
‚é£ ..

an1

a12
a22
..
.

an2

¬∑¬∑¬∑
¬∑¬∑¬∑

‚é§
a1n
a2n ‚é•
‚é•
.. ‚é• ,
. ‚é¶

¬∑ ¬∑ ¬∑ ann

‚é°

‚é§
x1
‚é¢ x2 ‚é•
‚é¢
‚é•
x = ‚é¢ . ‚é•,
‚é£ .. ‚é¶

‚é°

y

xn

‚é§
b1
‚é¢ b2 ‚é•
‚é¢
‚é•
b = ‚é¢ . ‚é•,
‚é£ .. ‚é¶
bn

puesto que todas las entradas en el producto Ax deben corresponder con las entradas en el
vector b. Por lo tanto, la matriz n 3 m se puede ver como una funci√≥n con dominio en conMXQWRGHYHFWRUHVFROXPQDmGLPHQVLRQDO\UDQJRHQXQVXEFRQMXQWRGHYHFWRUHVFROXPQD
n-dimensional.

Productos de matriz-matriz
3RGHPRV XWLOL]DU OD PXOWLSOLFDFLyQ PDWUL]YHFWRU SDUD GH√ÄQLU OD PXOWLSOLFDFLyQ JHQHUDO
matriz-matriz.
DeÔ¨Ånici√≥n 6.7

Si A es una matriz n 3 m y B una matriz m 3 p. El producto de la matriz de A y B, denotado
AB es una matriz C n 3 p cuyas entradas cij son
m

ci j =

aik bk j = ai1 b1 j + ai2 b2 j + ¬∑ ¬∑ ¬∑ + aim bm j ,
k=1

para cada i = 1, 2, . . . n, y j = 1, 2, . . . , p.
El c√°lculo de cij se puede observar como la multiplicaci√≥n de las entradas de la i-√©sima
√ÄODGHA con entradas correspondientes en la j-√©sima columna de B, seguida por la sumatoria;
es decir,
‚é§
‚é°
b1 j
‚é¢ b2 j ‚é•
‚é•
‚é¢
[ai1 , ai2 , . . . , aim ] ‚é¢ . ‚é• = ci j ,
‚é£ .. ‚é¶
bm j
donde
m

ci j = ai1 b1 j + ai2 b2 j + ¬∑ ¬∑ ¬∑ + aim bm j =

aik bk j .
k=1

Esto explica porqu√© el n√∫mero de columnas de AGHEHVHULJXDODOQ~PHURGH√ÄODVGHB para
el producto ABTXHVHYDDGH√ÄQLU
(OVLJXLHQWHHMHPSORGHEHUtDVHUYLUSDUDDFODUDUHOSURFHVRGHPXOWLSOLFDFLyQGHPDWULFHV
Ejemplo 3

Determine todos los productos posibles de las matrices.
‚é°
‚é§
3 2
2 1 ‚àí1
A = ‚é£ ‚àí1 1 ‚é¶ , B =
,
3 1
2
1 4
‚é°
‚é§
2 1 0 1
1 ‚àí1
C = ‚é£ ‚àí1 3 2 1 ‚é¶ , y D =
2 ‚àí1
1 1 2 0
Soluci√≥n

.

El tama√±o de las matrices es

A : 3 √ó 2,

B : 2 √ó 3,

C : 3 √ó 4,

y

D : 2 √ó 2.

290

CAP√çTULO 6

M√©todos directos para resolver sistemas lineales

/RVSURGXFWRVTXHVHSXHGHQGH√ÄQLU\VXVGLPHQVLRQHVVRQ

AB : 3 √ó 3,

BA : 2 √ó 2,

AD : 3 √ó 2,

Estos productos son
‚é°
‚é§
12 5 1
AB = ‚é£ 1 0 3 ‚é¶ ,
14 5 7

2
7

BC =

4
8

0
6

3
4

BC : 2 √ó 4,

DB : 2 √ó 3,
‚é°

,

y DD : 2 √ó 2.

7
AD = ‚é£ 1
9

BA =

4
10

1
15

,

DB =

‚àí1
1

0
1

‚àí3
,
‚àí4

‚é§
‚àí5
0 ‚é¶,
‚àí5

DD =

y

‚àí1
0

0
.
‚àí1

Observe que a pesar de que ambos productos de la matriz AB y BA HVWiQGH√ÄQLGRVVXV
UHVXOWDGRVVRQPX\GLIHUHQWHVHOORVQLVLTXLHUDWLHQHQODPLVPDGLPHQVLyQ(QOHQJXDMHPDtem√°tico, decimos que la operaci√≥n del producto de la matriz no es conmutativo; es decir, los
productos en orden inverso pueden diferir. √âste es el caso incluso cuando ambos productos
VHGH√ÄQHQ\WLHQHQODPLVPDGLPHQVLyQ&DVLFXDOTXLHUHMHPSORPRVWUDUiHVWRSRUHMHPSOR

1
1

1
0

0
1

1
1

=

1
0

2
1

0
1

mientras

1
1

1
1

1
0

=

1 0
2 1

.

Ciertas operaciones importantes relacionadas con el producto de matrices se mantienen,
como se indica en el siguiente resultado.
Teorema 6.8

Si A es una matriz n 3 m, B es una matriz m 3 k, C es una matriz k 3 p, D es una matriz
m 3 k y l es un n√∫mero real. Las siguientes propiedades se mantienen:
a) A(BC) = (AB)C; b) A(B + D) = AB + AD; c) Œª(AB) = (ŒªA)B = A(ŒªB).
Demostraci√≥n /DYHUL√ÄFDFLyQGHODSURSLHGDGHQODSDUWHD VHSUHVHQWDSDUDPRVWUDUHOPptodo involucrado. Las otras partes se pueden demostrar de manera similar.
Para mostrar que A(BC) = (AB)C, calcule la i j-entrada de cada lado de la ecuaci√≥n. BC
es una matriz m 3 p con i j-entrada
k

(BC)s j =

bsl cl j .
l=1

Por lo tanto, A(BC HVXQDPDWUL]n 3 p con entradas
m

[A(BC)]i j =

m

ais (BC)s j =
s=1

k

ais

m

bsl cl j

s=1

k

=

l=1

ais bsl cl j .
s=1 l=1

De igual forma, AB es una matriz n 3 k con entradas
m

(AB)il =

ais bsl ,
s=1

por lo que (AB C es una matriz n 3 p con entradas
k

[(AB)C]i j =

k

m

(AB)il cl j =
l=1

k

ais bsl
l=1

s=1

m

cl j =

ais bsl cl j .
l=1 s=1

6.3 √Ålgebra lineal e inversi√≥n de matriz

291

Al intercambiar el orden de la suma del lado derecho nos da
m

k

[(AB)C]i j =

ais bsl cl j = [A(BC)]i j ,
s=1 l=1

para cada i = 1, 2, . . . , n y j = 1, 2, . . . , p. Por lo que A(BC) = (AB)C.

Matrices cuadradas
/DVPDWULFHVTXHWLHQHQHOPLVPRQ~PHURGH√ÄODVFRPRFROXPQDVVRQHVSHFLDOPHQWHLPSRUtantes en aplicaciones.
DeÔ¨Ånici√≥n 6.9

El t√©rmino diagonal aplicado
DXQDPDWUL]VHUH√ÄHUHDODV
entradas en la diagonal que
van desde la entrada superior
izquierda hasta la entrada inferior
derecha.

i)

8QDPDWUL]FXDGUDGDWLHQHHOPLVPRQ~PHURGH√ÄODVTXHGHFROXPQDV

ii)

8QD PDWUL] GLDJRQDO D = [di j ] es una matriz cuadrada con di j = 0 siempre que
i = j.

iii)

La matriz identidad de orden n, In = [Œ¥i j ], es una matriz diagonal cuyas entradas
diagonales son todas 1. Cuando el tama√±o de In es claro, en general, la matriz se
escribe simplemente como I.

3RUHMHPSORODPDWUL]LGHQWLGDGGHRUGHQHV
‚é°
‚é§
1 0 0
I = ‚é£ 0 1 0 ‚é¶.
0 0 1
DeÔ¨Ånici√≥n 6.10

8QD PDWUL] n 3 n triangular superior U = [u i j ] tiene, para cada j = 1, 2, . . . , n, las
entradas

u i j = 0,
8QDPDWUL]WULDQJXODUHVDTXHOOD
que tiene todas las entradas cero,
H[FHSWR\DVHDHQFLPD VXSHULRU 
RGHEDMR LQIHULRU GHODGLDJRQDO
principal.

Ilustraci√≥n

para cada i = j + 1, j + 2, . . . , n;

y una matriz triangular inferior L = [li j ] tiene, para cada j = 1, 2, . . . , n las entradas

li j = 0,

para cada i = 1, 2, . . . , j ‚àí 1.

8QDPDWUL]GLDJRQDOHQWRQFHVHVWDQWRWULDQJXODUVXSHULRUFRPRWULDQJXODULQIHULRUGHbido a que sus entradas diferentes de cero deben estar en la diagonal principal.
Considere la matriz identidad de orden 3,
‚é°

‚é§
1 0 0
I3 = ‚é£ 0 1 0 ‚é¶ .
0 0 1

Si A es cualquier matriz 3 3 3, entonces
‚é§‚é°
‚é§ ‚é°
‚é°
1 0 0
a11
a11 a12 a13
AI3 = ‚é£ a21 a22 a23 ‚é¶ ‚é£ 0 1 0 ‚é¶ = ‚é£ a21
a31 a32 a33
a31
0 0 1

a12
a22
a32

‚é§
a13
a23 ‚é¶ = A.
a33

La matriz identidad In conmuta con cualquier matriz n 3 n A; es decir, el orden de multiplicaci√≥n no importa,

In A = A = AIn .
Considere que esta propiedad, en general, no es cierta, incluso para las matrices cuadradas.

292

CAP√çTULO 6

M√©todos directos para resolver sistemas lineales

Matrices inversas
La inversa de una matriz est√° relacionada con los sistemas lineales.
DeÔ¨Ånici√≥n 6.11
/DSDODEUD¬¥VLQJXODU¬µVLJQL√ÄFD
que algo se desv√≠a de lo
ordinario. Por lo tanto, una
matriz singular no tiene inversa.

Teorema 6.12

Ejemplo 4

Se dice que una matriz A n 3 n es no singular (o invertible VLH[LVWHXQDPDWUL] A‚àí1 n 3 n
con A A‚àí1 = A‚àí1 A = I . La matriz A‚àí1 recibe el nombre de inversa de A8QDPDWUL]TXH
no tenga inversa recibe el nombre de singular (o no invertible 
/DVVLJXLHQWHVSURSLHGDGHVUHVSHFWRDODVLQYHUVDVVXUJHQDSDUWLUGHODGH√ÄQLFLyQ
/DVSUXHEDVGHHVWRVUHVXOWDGRVVHFRQVLGHUDQHQHOHMHUFLFLR
Para cualquier matriz n 3 n no singular A:

i)

A‚àí1 es √∫nica.

ii)

A‚àí1 es no singular y (A‚àí1 )‚àí1 = A.

iii)

Si B tambi√©n es una matriz no singular n √ó n, entonces (AB)‚àí1 = B ‚àí1 A‚àí1 .

Si

‚é°

1
A=‚é£ 2
‚àí1

2
1
1

‚é°

‚é§
‚àí1
0 ‚é¶
2

‚àí 29

‚é¢
B=‚é£

y

5
9
‚àí 19
1
3

4
9
‚àí 13

‚àí 19
2
9
1
3

‚é§
‚é•
‚é¶.

Muestre que B = A‚àí1 y que la soluci√≥n para el sistema lineal descrito mediante

x1 + 2x2 ‚àí x3 = 2,
2x1 + x2
= 3,
‚àíx1 + x2 + 2x3 = 4,
est√° dado por las entradas en Bb, donde b es el vector columna con entradas 2, 3 y 4, respectivamente.
Soluci√≥n

Primero note que

‚é°

1 2
AB = ‚é£ 2 1
‚àí1 1

‚é§ ‚é° ‚àí2
‚àí1
9
‚é¢
0 ‚é¶ ¬∑ ‚é£ 94
2
‚àí1

5
9
1
‚àí9
1
3

3

‚é§

‚é§
1 0 0
2 ‚é•
= ‚é£ 0 1 0 ‚é¶ = I3 .
9 ‚é¶
1
0 0 1

‚àí 19

‚é°

3

De forma similar, B A = I3, por lo que A y B son no singulares con B = A‚àí1 y A = B ‚àí1 .
Ahora convierta el sistema lineal dado en una ecuaci√≥n matricial
‚é°
‚é§‚é°
‚é§ ‚é° ‚é§
1 2 ‚àí1
x1
2
‚é£ 2 1
0 ‚é¶ ‚é£ x2 ‚é¶ = ‚é£ 3 ‚é¶
‚àí1 1
2
4
x3
y multiplique ambos lados por B, la inversa de A. Como

B(Ax) = (B A)x = I3 x = x

y

B(Ax) = b,

tenemos

‚éõ‚é°
‚éú‚é¢
B Ax = ‚éù‚é£

‚àí 29
4
9
‚àí 39

5
9
‚àí 19
3
9

‚àí 19

‚é§‚é°

1
2 ‚é•‚é£
2
‚é¶
9
3
‚àí1
9

2
1
1

‚é§‚éû
‚àí1
‚éü
0 ‚é¶‚é† x = x
2

6.3 √Ålgebra lineal e inversi√≥n de matriz

293

y

‚é°

‚àí 29

‚é¢
B Ax = B(b) = ‚é£

4
9
‚àí 13

‚é§ ‚é° 7 ‚é§
2
9
13 ‚é•
2 ‚é•‚é£
‚é¶=‚é¢
3
.
‚é¶
‚é£
9
9 ‚é¶
5
1
4

‚àí 19

5
9
‚àí 19
1
3

‚é§‚é°

3

3

Esto implica que x = Bb y obtenemos la soluci√≥n x1 = 7/9, x2 = 13/9, y x3 = 5/3.
Aunque es f√°cil resolver un sistema lineal de la forma Ax = b si A‚àí1, se conoce, no es
FRPSXWDFLRQDOPHQWHH√ÄFLHQWHGHWHUPLQDU A‚àí1FRQHO√ÄQGHUHVROYHUHOVLVWHPD &RQVXOWHHO
HMHUFLFLR $XQDVtHV~WLOGHVGHXQSXQWRGHYLVWDFRQFHSWXDOGHVFULELUXQPpWRGRSDUD
determinar la inversa de una matriz.
Para encontrar un m√©todo para calcular A‚àí1 al suponer que A es no singular, observemos
nuevamente la multiplicaci√≥n de matrices. Si Bj es la j-√©sima columna de la matriz n 3 n B,

‚é°

‚é§
b1 j
‚é¢ b2 j ‚é•
‚é¢
‚é•
Bj = ‚é¢ . ‚é• .
‚é£ .. ‚é¶
bn j

Si AB 5 C, entonces la j-√©sima columna de C est√° dada por el producto

‚é°

‚é°
‚é§
c1 j
a11
‚é¢ c2 j ‚é•
‚é¢ a21
‚é¢
‚é¢
‚é•
‚é¢ .. ‚é• = C j = AB j = ‚é¢ ..
‚é£ . ‚é¶
‚é£ .
cn j

an1

a12
a22
..
.

an2

¬∑¬∑¬∑
¬∑¬∑¬∑

‚é§‚é°
‚é§ ‚é°
a1n
b1 j
‚é¢
‚é• ‚é¢
a2n ‚é•
‚é• ‚é¢ b2 j ‚é• ‚é¢
.. ‚é• ‚é¢ .. ‚é• = ‚é¢
‚é¢
. ‚é¶‚é£ . ‚é¶ ‚é£

¬∑ ¬∑ ¬∑ ann

bn j

n
k=1 a1k bk j
n
k=1 a2k bk j

..
.

n
k=1 ank bk j

‚é§
‚é•
‚é•
‚é•.
‚é•
‚é¶

Suponga que A‚àí1 existe y que A‚àí1 = B = (bi j ). Entonces AB 5 I y

‚é°

‚é§
0
‚é¢ .. ‚é•
‚é¢ . ‚é•
‚é¢ ‚é•
‚é¢ 0 ‚é•
‚é¢ ‚é•
‚é•
AB j = ‚é¢
‚é¢ 1 ‚é• , donde el valor 1 aparece en la j-√©sima√ÄOD
‚é¢ 0 ‚é•
‚é¢ ‚é•
‚é¢ .. ‚é•
‚é£ . ‚é¶
0
Para encontrar B, necesitamos resolver n sistemas lineales en los que la j-√©sima columna
de la inversa es la soluci√≥n del sistema lineal con el lado derecho de la j-√©sima columna de I.
La siguiente ilustraci√≥n demuestra este m√©todo.

Ilustraci√≥n

Para determinar la inversa de la matriz

‚é°

1
A=‚é£ 2
‚àí1

2
1
1

‚é§
‚àí1
0 ‚é¶,
2

294

CAP√çTULO 6

M√©todos directos para resolver sistemas lineales

consideremos primero el producto A B, donde B es una matriz arbitraria 3 3 3:
‚é§
‚é°
‚é§‚é°
1 2 ‚àí1
b11 b12 b13
0 ‚é¶ ‚é£ b21 b22 b23 ‚é¶
AB = ‚é£ 2 1
‚àí1 1
2
b31 b32 b33
‚é°
‚é§
b11 + 2b21 ‚àí b31
b12 + 2b22 ‚àí b32
b13 + 2b23 ‚àí b33
‚é¶.
2b11 + b21
2b12 + b22
2b13 + b23
=‚é£
‚àíb11 + b21 + 2b31 ‚àíb12 + b22 + 2b32 ‚àíb13 + b23 + 2b33
Si B = A‚àí1 , entonces, AB = I , de manera que

b11 + 2b21 ‚àí b31 = 1,
= 0,
2b11 + b21
‚àíb11 + b21 + 2b31 = 0,

b12 + 2b22 ‚àí b32 = 0,
2b12 + b22
= 1,
‚àíb12 + b22 + 2b32 = 0,

y

b13 + 2b23 ‚àí b33 = 0,
2b13 + b23
= 0,
‚àíb13 + b23 + 2b33 = 1.

2EVHUYHTXHORVFRH√ÄFLHQWHVHQFDGDXQRGHORVVLVWHPDVGHHFXDFLRQHVVRQORVPLVPRVHO
√∫nico cambio en los sistemas se presenta en el lado derecho de las ecuaciones. Como consecuencia, la eliminaci√≥n gaussiana se puede realizar en una matriz aumentada formada al
combinar las matrices de cada uno de los sistemas:
‚é°
‚é§
.
1 2 ‚àí1 .. 1 0 0
.
‚é£ 2 1
0 .. 0 1 0 ‚é¶ .
.
‚àí1 1
2 .. 0 0 1
Primero, realice (E 2 ‚àí 2E 1 ) ‚Üí (E 2 ) y (E 3 + E 1 ) ‚Üí (E 3 ), seguido de (E 3 + E 2 ) ‚Üí (E 3 ),
produce

‚é°

1
‚é£ 0
0

2 ‚àí1
‚àí3
2
3
1

‚é§
..
1 0 0
..
.. ‚àí2 1 0 ‚é¶
..
1 0 1
.

‚é°
y

1
‚é£ 0
0

2
‚àí3
0

‚àí1
2
3

..
1
..
.. ‚àí2
..
. ‚àí1

0
1
1

‚é§
0
0 ‚é¶.
1

La sustituci√≥n hacia atr√°s se realiza en cada una de las tres matrices aumentadas,
‚é°
‚é§ ‚é°
‚é§ ‚é°
‚é§
.
.
.
1
2 ‚àí1 ..
1
1
2 ‚àí1 .. 0
1
2 ‚àí1 .. 0
.
.
.
‚é£ 0 ‚àí3
2 ... ‚àí2 ‚é¶ , ‚é£ 0 ‚àí3
2 ... 1 ‚é¶ , ‚é£ 0 ‚àí3
2 ... 0 ‚é¶ ,
.
.
0
0
3 . ‚àí1
0
0
3 . 1
0
0
3 .. 1
SDUDDO√ÄQDOSURSRUFLRQDU

b11 = ‚àí 29 ,

b12 = 59 ,

b21 = 49 ,

b22 = ‚àí 19 ,

b31 = ‚àí 13 ,

b32 = 13 ,

b13 = ‚àí 19 ,
b23 = 29 ,

y

b32 = 13 .

&RPRVHPXHVWUDHQHOHMHPSORpVWDVVRQODVHQWUDGDVGHA‚àí1:
‚é§
‚é° 2
5
‚àí 19
‚àí9
9
‚é¢
2 ‚é•
B = A‚àí1 = ‚é£ 94 ‚àí 19
.
9 ‚é¶

‚àí 13

1
3

1
3

&RPRREVHUYDPRVHQODLOXVWUDFLyQFRQHO√ÄQGHFDOFXODUA‚àí1HVFRQYHQLHQWHFRQ√ÄJXUDU
la matriz aumentada m√°s grande,
.
A .. I .

6.3 √Ålgebra lineal e inversi√≥n de matriz

295

$OUHDOL]DUODHOLPLQDFLyQGHDFXHUGRFRQHODOJRULWPRREWHQHPRVXQDPDWUL]DXPHQWDGD
de la forma

U

..
.

Y

,

donde U es una matriz triangular superior y Y es la matriz obtenida al realizar las mismas
operaciones en la identidad I que se realizaron para llevar A a U.
La eliminaci√≥n gaussiana con sustituci√≥n hacia atr√°s requiere

4 3 1
n ‚àí n multiplicaciones/divisiones
3
3

y

4 3 3 2 n
n ‚àí n + sumas/restas
3
2
6

para resolver los nVLVWHPDVOLQHDOHV FRQVXOWHHOHMHUFLFLRD 6HGHEHWHQHUPXFKRFXLGDGR
en la implementaci√≥n para observar las operaciones que noVHGHEHQUHDOL]DUSRUHMHPSOR
una multiplicaci√≥n cuando se sabe que uno de los multiplicadores es la unidad o una resta
FXDQGRVHVDEHTXHHOVXVWUDHQGRHV(OQ~PHURGHPXOWLSOLFDFLRQHVGLYLVLRQHVUHTXHULGDV
se puede reducir a n3\HOQ~PHURGHVXPDVUHVWDVVHSXHGHUHGXFLUDn 3 ‚àí 2n 2 + n (consulte
HOHMHUFLFLRG 

Transpuesta de una matriz
Otra matriz importante relacionada con una matriz dada A es su transpuesta, denotada At.
DeÔ¨Ånici√≥n 6.13

Ilustraci√≥n

La transpuesta de una matriz A 5 [aij] n 3 m es la matriz At 5 [aij], m 3 n, donde para
cada i, la i-√©sima columna de At es la misma que la i-√©sima√ÄODGHA8QDPDWUL]FXDGUDGDA
recibe el nombre de sim√©trica si A 5 At.
Las matrices
‚é°

7
A=‚é£ 3
0

‚é§
2
0
5 ‚àí1 ‚é¶ ,
5 ‚àí6

tienen transpuestas
‚é°
‚é§
7
3
0
5
5 ‚é¶,
At = ‚é£ 2
0 ‚àí1 ‚àí6

‚é°

B=

2
3

4
‚àí5

‚é°

2
Bt = ‚é£ 4
7

7
‚àí1

‚é§
3
‚àí5 ‚é¶ ,
‚àí1

,

6
C =‚é£ 4
‚àí3

‚é°

6
Ct = ‚é£ 4
‚àí3

4
‚àí2
0

‚é§
‚àí3
0 ‚é¶
1

4
‚àí2
0

‚é§
‚àí3
0 ‚é¶.
1

La matriz C es sim√©trica porque C t 5 C. Las matrices A y B no son sim√©tricas.
/DSUXHEDGHOVLJXLHQWHUHVXOWDGRVHVLJXHGLUHFWDPHQWHGHODGH√ÄQLFLyQGHODWUDQVSXHVWD
Teorema 6.14

Las siguientes operaciones relacionadas con la transpuesta de una matriz se mantienen siempre que la operaci√≥n sea posible

i)

(At )t = A,

iii)

(AB)t = B t At ,

ii)

(A + B)t = At + B t ,

iv)

si A‚àí1 existe, entonces (A‚àí1 )t = (At )‚àí1 .

La secci√≥n Conjunto de ejercicios 6.3 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

296

CAP√çTULO 6

M√©todos directos para resolver sistemas lineales

6.4 Determinante de una matriz
El determinante de una matriz proporciona resultados de existencia y unicidad para sistemas
lineales que tienen el mismo n√∫mero de inc√≥gnitas y de ecuaciones. Nosotros denotaremos
el determinante de una matriz cuadrada A mediante detA, pero tambi√©n es com√∫n utilizar la
notaci√≥n |A|.
DeÔ¨Ånici√≥n 6.15

La noci√≥n de determinante
apareci√≥ de manera
LQGHSHQGLHQWHHQWDQWRHQ
Jap√≥n como en Europa, a pesar
de que ni Takakazu Seki Kowa
¬≤ QL*RWWIULHG/HLEQL]
¬≤ SDUHFtDQKDEHU
usado el t√©rmino ‚Äúdeterminante‚Äù.

Suponga que A es una matriz cuadrada.

i)

Si A = [a] es una matriz 1√ó 1, entonces det A = a.

ii)

Si A es una matriz n √ó n, con n > 1, el menor Mi j es el determinante de la submatriz (n ‚àí 1) √ó (n ‚àí 1) de A obtenida al quitar la i-√©sima fila y la j-√©sima columna de
la matriz A.

iii)

El cofactor Ai j asociado con Mi j est√° definido por Ai j = (‚àí1)i+ j Mi j .

iv)

El determinante de la matriz A n √ó n, cuando n > 1, est√° dado ya sea por
n

det A =

n

ai j Ai j =
j=1

(‚àí1)i+ j ai j Mi j ,

para i = 1, 2, . . . , n,

(‚àí1)i+ j ai j Mi j ,

para cualquier j = 1, 2, . . . , n.

j=1

o mediante
n

n

ai j Ai j =

det A =
i=1

i=1

6HSXHGHPRVWUDU FRQVXOWHHOHMHUFLFLR TXHFDOFXODUHOGHWHUPLQDQWHGHXQDPDWUL]
general n 3 nPHGLDQWHHVWDGH√ÄQLFLyQUHTXLHUHO(n!)PXOWLSOLFDFLRQHVGLYLVLRQHV\VXPDV
restas. Incluso para valores relativamente peque√±os de n, el n√∫mero de c√°lculos se vuelve
GLItFLOGHPDQHMDU
A pesar de que parece que existen 2nGH√ÄQLFLRQHVGLIHUHQWHVGHGHWA, dependiendo de la
√ÄODRFROXPQDVHOHFFLRQDGDWRGDVODVGH√ÄQLFLRQHVOOHYDQDOPLVPRUHVXOWDGRQXPpULFR/D
√ÅH[LELOLGDGHQODGH√ÄQLFLyQVHXWLOL]DHQHOVLJXLHQWHHMHPSOR(VPiVFRQYHQLHQWHFDOFXODU
detADORODUJRGHOD√ÄODRGHODFROXPQDFRQODPD\RUFDQWLGDGGHFHURV
Ejemplo 1

Encuentre el determinante de la matriz
‚é°

2
‚é¢ 4
A=‚é¢
‚é£ ‚àí3
6

‚àí1
‚àí2
‚àí4
‚àí6

‚é§
0
0 ‚é•
‚é•
5 ‚é¶
0

3
7
1
8

PHGLDQWHOD√ÄODRFROXPQDFRQODPD\RUFDQWLGDGGHHQWUDGDVFHUR
Soluci√≥n

Para calcular detA, es m√°s f√°cil utilizar la cuarta columna:

det A = a14 A14 + a24 A24 + a34 A34 + a44 A44 = 5A34 = ‚àí5M34 .
$OHOLPLQDUODWHUFHUD√ÄOD\ODFXDUWDFROXPQDREWHQHPRV
‚é°
‚é§
2 ‚àí1 3
det A = ‚àí5 det ‚é£ 4 ‚àí2 7 ‚é¶
6 ‚àí6 8

= ‚àí5 2 det

‚àí2
‚àí6

7
8

‚àí (‚àí1) det

4
6

7
8

+ 3 det

4
6

‚àí2
‚àí6

= ‚àí30.

6 .4

Determinante de una matriz

297

Las siguientes propiedades son √∫tiles para relacionar sistemas lineales y eliminaci√≥n gaussiana para determinantes. √âstas est√°n probadas en cualquier texto de √°lgebra lineal est√°ndar.
Teorema 6.16

Suponga que A es una matriz n 3 n:
i)

6LFXDOTXLHU√ÄODRFROXPQD A s√≥lo tiene entradas cero, entonces det A 5 0.

ii)

Si AWLHQHGRV√ÄODVRGRVFROXPQDVLJXDOHVHQWRQFHVGHWA 5 0.

iii)

Si AÃÉ se obtiene a partir de A mediante la operaci√≥n (E i ) ‚Üî (E j ), con i = j,
entonces det AÃÉ = ‚àí det A.

iv)

Si AÃÉ se obtiene a partir de A mediante la operaci√≥n (ŒªE i ) ‚Üí (E i ), entonces
AÃÉ = Œª det A.

v)

Si AÃÉ se obtiene a partir de A mediante la operaci√≥n (E i + ŒªE j ) ‚Üí (E i ) con i = j,
entonces det AÃÉ = det A.

vi)

Si B tambi√©n es una matriz n 3 n, entonces det A B 5 det A det B.

vii)

det At 5 det A.

viii) Cuando A21 existe, det A21 5 (det A)21.
ix)

Si A es una matriz triangular superior, triangular inferior o diagonal, entonces
n
A 5 i=1
aii .

La parte ix) del teorema 6.16 indica que el determinante de una matriz triangular es
VLPSOHPHQWHHOSURGXFWRGHVXVHOHPHQWRVGLDJRQDOHV$OHPSOHDUODVRSHUDFLRQHVGH√ÄOD
dadas en las partes iii), iv) y v), podemos reducir una matriz cuadrada determinada a la forma
triangular para encontrar su determinante.
Ejemplo 2

Calcule el determinante de la matriz
‚é°

2
‚é¢ 1
‚é¢
A=‚é£
‚àí1
3

1
1
2
‚àí1

‚àí1
0
3
‚àí1

‚é§
1
3 ‚é•
‚é•
‚àí1 ‚é¶
2

por medio de las partes iii), iv) y v) del teorema 6.16.
Soluci√≥n

La secuencia de las operaciones en la tabla 6.2 produce la matriz
‚é°
‚é§
1
1 12 ‚àí 12
2
‚é¢ 0 1 1
5 ‚é•
‚é•.
AÃÉ = ‚é¢
‚é£ 0 0 3
13 ‚é¶
0 0 0 ‚àí13

Por medio de la parte ix), det AÃÉ 5 239, luego det A 5 39.

Tabla 6.2

Operaci√≥n

Efecto

1
E ‚Üí E1
2 1

det A1 = 12 det A

E2 ‚àí E1 ‚Üí E2

det A2 = det A1 = 12 det A

E3 + E1 ‚Üí E3

det A3 = det A2 = 12 det A

E 4 ‚àí 3E 1 ‚Üí E 4

det A4 = det A3 = 12 det A

2E 2 ‚Üí E 2

det A5 = 2 det A4 = det A

E 3 ‚àí 52 E 2 ‚Üí E 3

det A6 = det A5 = det A

E 4 + 52 E 2 ‚Üí E 4

det A7 = det A6 = det A

E3 ‚Üî E4

det A8 = ‚àí det A7 = ‚àí det A

298

CAP√çTULO 6

M√©todos directos para resolver sistemas lineales

El resultado clave relacionado con no singularidad, eliminaci√≥n gaussiana, sistemas
lineales y determinantes es que las siguientes declaraciones son equivalentes.
Teorema 6.17

Las siguientes declaraciones son equivalentes para cualquier matriz n 3 n A:
i)

La ecuaci√≥n Ax 5 0 tiene la soluci√≥n √∫nica x 5 0.

ii)

El sistema Ax 5 b tiene una soluci√≥n √∫nica para cualquier vector de columna n-dimensional b.

iii)

La matriz A es no singular, es decir, existe A21.

iv)

det A = 0.

v)

/DHOLPLQDFLyQJDXVVLDQDFRQLQWHUFDPELRVGH√ÄODVHSXHGHUHDOL]DUHQHOVLVWHPD
Ax 5 b para cualquier vector de columna n-dimensional b.

El siguiente corolario para el teorema 6.17 ilustra c√≥mo se puede utilizar el determinante
para mostrar propiedades importantes sobre matrices cuadradas.
Corolario 6.18

Suponga que tanto A como B son matrices n 3 n ya sea con AB = I o B A = I . Entonces
B = A‚àí1 (y A = B ‚àí1 ).
Demostraci√≥n

Suponga que A B 5 I. Entonces, por medio de la parte vi) del teorema 6.16

1 = det(I ) = det(AB) = det(A) ¬∑ det(B),

por lo que

det(A) = 0 y det(B) = 0.

La equivalencia de las partes iii) y iv) del teorema 6.17 implica que existe tanto A21 como
B21. Por lo tanto,

A‚àí1 = A‚àí1 ¬∑ I = A‚àí1 ¬∑ (AB) = A‚àí1 A ¬∑ B = I ¬∑ B = B.
Los papeles de A y B son similares, por lo que esto tambi√©n establece que B A 5 I. Por lo
que B 5 A21.
La secci√≥n Conjunto de ejercicios 6.4 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

6.5 Factorizaci√≥n de matriz
La eliminaci√≥n gaussiana es la herramienta principal en la soluci√≥n directa de sistemas de
ecuaciones lineales, por lo que no deber√≠a ser sorprendente que aparezca de otras formas. En
esta secci√≥n, observaremos que los pasos para resolver un sistema de la forma Ax = b se
pueden usar para factorizar una matriz. La factorizaci√≥n es especialmente √∫til cuando tiene
la forma A 5 LU, donde L es triangular inferior y U es triangular superior. A pesar de que no
todas las matrices tienen este tipo de representaci√≥n, muchas se presentan con frecuencia en
la aplicaci√≥n de t√©cnicas num√©ricas.
En la secci√≥n 6.1 encontramos que la eliminaci√≥n gaussiana aplicada a un sistema lineal
arbitrario Ax = b requiere O(n3/3) operaciones aritm√©ticas para determinar x. Sin embargo,
resolver un sistema lineal que implica un sistema triangular superior s√≥lo requiere sustituci√≥n hacia atr√°s, que toma O(n2) operaciones. El n√∫mero de operaciones requeridas para
resolver un sistema triangular inferior es similar.
Suponga que A se ha factorizado en la forma triangular A 5 LU, donde L es triangular
inferior y U es triangular superior. Entonces podemos resolver para x con mayor facilidad a
trav√©s de un proceso de dos pasos.

6.5 Factorizaci√≥n de matriz

299

‚Ä¢ Primero, hacemos y = U x y resolvemos el sistema triangular superior Ly = b para y.
Puesto que L es triangular, determinar y a partir de esta ecuaci√≥n s√≥lo requiere O(n2) operaciones.
‚Ä¢ Una vez que conocemos y, el sistema triangular superior U x = y solamente requiere una
operaci√≥n O(n2) adicional para determinar la soluci√≥n de x.
La resoluci√≥n de un sistema lineal Ax = b HQ IRUPD IDFWRUL]DGD VLJQL√ÄFD TXH HO Q~mero de operaciones necesario para resolver el sistema Ax = b se reduce a partir de
O(n 3 /3) a O(2n 2 ).
Ejemplo 1

Compare el n√∫mero aproximado de operaciones requeridas para determinar la soluci√≥n para
un sistema lineal mediante una t√©cnica que requiera O(n 3 /3) operaciones y una que necesite
O(2n 2 ) cuando n = 20, n = 100 y n = 1000.
Soluci√≥n

Tabla 6.3

La tabla 6.3 muestra los resultados de estos c√°lculos.

n

n 3 /3

10
100
1000

3.3 √ó 10
3.3 √ó 105
3.3 √ó 108

2n 2
2

% de reducci√≥n

2 √ó 10
2 √ó 104
2 √ó 106
2

40
94
99.4

Como ilustra el ejemplo, el factor de reducci√≥n aumenta dr√°sticamente con el tama√±o de la
matriz. No es sorprendente que las reducciones a partir de la factorizaci√≥n tengan un costo;
ODGHWHUPLQDFLyQGHODVPDWULFHVHVSHFt√ÄFDVL y U requiere O(n3/3) operaciones. Pero, una
vez que se determina la factorizaci√≥n, los sistemas relacionados con la matriz A se pueden
UHVROYHUGHHVWDIRUPDVLPSOL√ÄFDGDSDUDFXDOTXLHUQ~PHURGHYHFWRUHVb.
Para observar qu√© matrices tienen factorizaci√≥n LU y encontrar c√≥mo se determina,
primero suponga que la eliminaci√≥n gaussiana se puede realizar en el sistema Ax 5 b sin
LQWHUFDPELRVGH√ÄOD&RQODQRWDFLyQHQODVHFFLyQHVWRHVHTXLYDOHQWHDWHQHUaii(i) , elementos pivote diferentes de cero, para cada i = 1, 2, . . . , n.
El primer paso en el proceso de eliminaci√≥n gaussiana consiste en realizar, para cada
j = 2, 3, . . . , n, las operaciones

(E j ‚àí m j,1 E 1 ) ‚Üí (E j ),

La factorizaci√≥n de matrices
es otra de las t√©cnicas
importantes que Gauss parece
haber descubierto primero.
Est√° incluida en su tratado de
dos vol√∫menes sobre mec√°nica
celeste Theoria motus corporum
coelestium in sectionibus conicis
Solem ambientium, publicado
en 1809.

donde m j,1 =

a (1)
j1

(1)
a11

.

(6.8)

Estas operaciones transforman el sistema en uno en el que todas las entradas en la primera
columna por debajo de la diagonal son cero.
El sistema de operaciones en la ecuaci√≥n (6.8) se puede observar de otra manera. Se
logra simult√°neamente al multiplicar la matriz original A a la izquierda de la matriz
‚é°
‚é§
1
0.. .. .. . . . . . . . . . 0.
.. ‚é•
‚é¢ ‚àí m. 21 1 . . . . .
.. ‚é•
... ...
‚é¢
.
(1)
.
..
0. . . . . . . . . . . .. ‚é•
M =‚é¢
‚é¢
‚é•.
..
.. . . . . . .
‚é£
‚é¶
0
.
..
.
.. ...
‚àím n1 0 . . . . . . . . 0 1
Esto recibe el nombre de primera matriz de transformaci√≥n gaussiana. Nosotros denotamos el producto de esta matriz con A(1) ‚â° A por A(2) y con b y b(2), por lo que

A(2) x = M (1) Ax = M (1) b = b(2) .

300

CAP√çTULO 6

M√©todos directos para resolver sistemas lineales

De manera similar, construimos M(2), la matriz identidad con entradas por debajo de la
diagonal en la segunda columna reemplazadas por los negativos de los multiplicadores

m j,2 =

a (2)
j2

(2)
a22

.

El producto de esta matriz con A(2) tiene ceros por debajo de la diagonal en las primeras dos
columnas y hacemos

A(3) x = M (2) A(2) x = M (2) M (1) Ax = M (2) M (1) b = b(3) .
En general, con A(k) x = b(k) ya formada, multiplicamos por la k-√©sima matriz de
transformaci√≥n gaussiana
‚é§
‚é°
.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 0.
1
0
.
... ...
‚é¢
.. ‚é•
‚é¢
... ...
.. ‚é•
‚é•
‚é¢
... ...
‚é¢ 0. . .
.. ‚é•
.
...
...
‚é¢ .. . .
.. ‚é•
...
‚é¢ . ..
...
.. ‚é•
..
‚é•
‚é¢ ..
.
... ...
..
‚é¢ .
.. ‚é•
.
.
...
...
..
‚é¢ ..
.. ‚é•
...
‚é¢ .
...
.. ‚é•
‚é•
‚é¢ ..
.
...
0.
...
‚é¢ .
.. ‚é• ,
(k)
.
.
...
M =‚é¢ .
.
.
...
.. ‚é•
..
‚é¢ ..
.
...
.. ‚é•
‚é•
‚é¢ ..
‚àí m. k+1,k . . . .
..
...
‚é¢ .
.. ‚é•
...
..
.
.
‚é¢ ..
..
..
...
.. ‚é•
..
‚é¢ .
..
...
0. . . . . . .
. ‚é•
..
‚é¢ ..
. . . .. ‚é•
..
.. . . . . . .
‚é•
‚é¢ .
..
... ..
. . ‚é•
‚é¢ ..
..
..
..
...
‚é•
‚é¢ .
.
0
.
.
.
..
... ..
‚é•
‚é¢ ..
..
..
‚é¶
‚é£ .
.. ...
..
.
.
.
0 ......... 0
‚àím
0 . . . . . . . . . . . .0
1
n,k
para obtener

A(k+1) x = M (k) A(k) x = M (k) ¬∑ ¬∑ ¬∑ M (1) Ax = M (k) b(k) = b(k+1) = M (k) ¬∑ ¬∑ ¬∑ M (1) b. (6.9)
El proceso termina con la formaci√≥n de A(n) x = b(n) , donde A(n), es la matriz triangular
superior
‚é§
‚é° (1)
(1) . . . . . . . . . (1)
a12
a. 1n
a11
...
..
...
‚é•
‚é¢
(2)
‚é•
‚é¢ 0. . . a22
. . . ..
.
(n)
.
‚é•,
‚é¢
... .. .
A = ‚é¢ .. . . . .
‚é•
(n‚àí1)
.
.
.
.
.
a
‚é£ .
. . . n‚àí1,n ‚é¶
...
.
.
.
(n)
0 . . . . . . . . . . .. 0
ann
dada por

A(n) = M (n‚àí1) M (n‚àí2) ¬∑ ¬∑ ¬∑ M (1) A.
Este proceso forma la parte U = A(n) de la factorizaci√≥n de la matriz A 5 LU. Para
determinar la matriz L triangular inferior complementaria, primero recuerde la multiplicaci√≥n de A(k) x = b(k) por la transformaci√≥n gaussiana de M (k) utilizada para obtener la
ecuaci√≥n (6.9):

A(k+1) x = M (k) A(k) x = M (k) b(k) = b(k+1) ,
donde M (k)JHQHUDODVRSHUDFLRQHVGHOD√ÄOD

(E j ‚àí m j,k E k ) ‚Üí (E j ),

para j = k + 1, . . . , n.

6.5 Factorizaci√≥n de matriz

301

Invertir los efectos de esta transformaci√≥n y regresar a A(k) requiere realizar las operaciones
(E j + m j,k E k ) ‚Üí (E j ) para cada j = k + 1, . . . , n. esto es equivalente a multiplicar por
la inversa de la matriz M (k), la matriz

‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
(k)
(k) ‚àí1
L = [M ] = ‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é£

1 . . . 0.. .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 0.
..
... ...
..
0. . . . . . . . . . . . .
.
.. . . .
..
... ...
.
.
.
..
..
...
... ...
.
.
.
..
..
.
... ....
.
..
..
... ....
0.
...
..
..
..
...
.
.
...
..
..
..
...
m k+1,k
.
.
.
...
..
..
..
..
...
.
.
..
..
..
0. . . . . . . . . . . . ..
..
..
.. . . . . . .
..
. .
... ... . 0
..
..
..
..
.
.
.
.
..
.
.
.
. ..
0 . . . . . . . . . .0
1
0 ......... 0
m n,k

‚é§
‚é•
‚é•
‚é•
‚é•
‚é•
‚é•
‚é•
‚é•.
‚é•
‚é•
‚é•
‚é•
‚é•
‚é•
‚é¶

La matriz L triangular inferior en la factorizaci√≥n de A, entonces, es el producto de las
matrices L(k):
‚é°
‚é§
1
0.. .. .. . . . . . . . . 0.
.. ‚é•
‚é¢ m. 21 1 . . . . . . .
...
... .... . ‚é•,
L = L (1) L (2) ¬∑ ¬∑ ¬∑ L (n‚àí1) = ‚é¢
.
...
.
‚é£ ..
.. ..... 0 ‚é¶
.
m n1 . . . . . m n,n‚àí1 . 1
puesto que el producto de L con la matriz triangular superior U = M (n‚àí1) ¬∑ ¬∑ ¬∑ M (2) M (1) A da

LU = L (1) L (2) ¬∑ ¬∑ ¬∑ L (n‚àí3) L (n‚àí2) L (n‚àí1) ¬∑ M (n‚àí1) M (n‚àí2) M (n‚àí3) ¬∑ ¬∑ ¬∑ M (2) M (1) A
= [M (1) ]‚àí1 [M (2) ]‚àí1 ¬∑ ¬∑ ¬∑ [M (n‚àí2) ]‚àí1 [M (n‚àí1) ]‚àí1 ¬∑ M (n‚àí1) M (n‚àí2) ¬∑ ¬∑ ¬∑ M (2) M (1) A = A.
El teorema 6.19 sigue estas observaciones.
Teorema 6.19

Si la eliminaci√≥n gaussiana se puede realizar en el sistema lineal Ax 5 b sin intercambios de
√ÄODHQWRQFHVODPDWUL]A se puede factorizar en el producto de una matriz triangular inferior
(i)
L y una matriz triangular superior U, es decir, A 5 LU, donde m ji = a (i)
ji /aii ,

‚é°
‚é¢
‚é¢
U =‚é¢
‚é¢
‚é£

Ejemplo 2

(1)
a11

(1) . . . . . . . . . (1)
a. 1n
a12
...
.
...
(2)
.
. . . ...
0. . . . a22 . . .
.
.. . . .
...
...
. . . a (n‚àí1)
..
.
. .n‚àí1,n
.
..
.
(n)
0 ...........0
ann

‚é§
‚é•
‚é•
‚é•,
‚é•
‚é¶

‚é§
1
0.. .. .. . . . . . . . . 0.
...
.
‚é¢ m
.. 21. . . .1. . . . . . . . . . . . .. ‚é•
‚é•.
L=‚é¢
.
.
.
‚é£ ..
... ...
0 ‚é¶
.
.
.. ...
.
1
m n1 . . . . . . m n,n‚àí1
‚é°

y

a) Determine la factorizaci√≥n LU para la matriz A en el sistema lineal Ax 5 b, donde
‚é°
‚é§
‚é°
‚é§
1
1
0
3
4
‚é¢ 2
‚é¢
‚é•
1 ‚àí1
1 ‚é•
‚é• y b = ‚é¢ 1 ‚é•.
A=‚é¢
‚é£ 3 ‚àí1 ‚àí1
‚é£ ‚àí3 ‚é¶
2 ‚é¶
‚àí1
2
3 ‚àí1
4
b) Entonces utilice la factorizaci√≥n para resolver el sistema

+ 3x4 = 8,
x1 + x2
2x1 + x2 ‚àí x3 + x4 = 7,
3x1 ‚àí x2 ‚àí x3 + 2x4 = 14,
‚àíx1 + 2x2 + 3x3 ‚àí x4 = ‚àí7.

302

CAP√çTULO 6

M√©todos directos para resolver sistemas lineales
Soluci√≥n

a) El sistema original se consider√≥ en la secci√≥n 6.1, donde observamos que la
secuencia de operaciones (E 2 ‚àí 2E 1 ) ‚Üí (E 2 ), (E 3 ‚àí 3E 1 ) ‚Üí (E 3 ), (E 4 ‚àí (‚àí1)E 1 ) ‚Üí (E 4 ),
(E 3 ‚àí 4E 2 ) ‚Üí (E 3 ), (E 4 ‚àí (‚àí3)E 2 ) ‚Üí (E 4 ) convierte el sistema en el sistema triangular

x1 + x2
+ 3x4 =
4,
‚àí x2 ‚àí x3 ‚àí 5x4 = ‚àí7,
3x3 + 13x4 = 13,
‚àí 13x4 = ‚àí13.
Los multiplicadores mij y la matriz triangular superior producen la factorizaci√≥n

‚é°

1
1
‚é¢ 2
1
‚é¢
A=‚é£
3 ‚àí1
‚àí1
2

0
‚àí1
‚àí1
3

‚é§ ‚é°
3
1
‚é¢
1 ‚é•
‚é•=‚é¢ 2
2 ‚é¶ ‚é£ 3
‚àí1
‚àí1

0
1
4
‚àí3

0
0
1
0

‚é§‚é°
0
1
‚é¢
0 ‚é•
‚é•‚é¢ 0
0 ‚é¶‚é£ 0
1
0

1
‚àí1
0
0

0
‚àí1
3
0

‚é§
3
‚àí5 ‚é•
‚é• = LU.
13 ‚é¶
‚àí13

b) Para resolver

‚é°

1
‚é¢ 2
Ax = LU x = ‚é¢
‚é£ 3
‚àí1

‚é§‚é°
1
0
‚é¢ 0
0 ‚é•
‚é•‚é¢
0 ‚é¶‚é£ 0
0
1

0 0
1 0
4 1
‚àí3 0

1
‚àí1
0
0

0
‚àí1
3
0

‚é§ ‚é°
‚é§
‚é§‚é°
8
3
x1
‚é• ‚é¢
‚é•
‚é¢
‚àí5 ‚é•
‚é• ‚é¢ x2 ‚é• = ‚é¢ 7 ‚é• ,
‚é¶
‚é£
‚é¶
‚é£
14 ‚é¶
x3
13
‚àí7
x4
‚àí13

primero introducimos la sustituci√≥n y = U x. Entonces b = L(U x) = Ly. Es decir,

‚é°

1
‚é¢ 2
Ly = ‚é¢
‚é£ 3
‚àí1

0
1
4
‚àí3

0
0
1
0

‚é§‚é°
‚é§ ‚é°
‚é§
0
y1
8
‚é¢
‚é• ‚é¢
‚é•
0 ‚é•
‚é• ‚é¢ y2 ‚é• = ‚é¢ 7 ‚é• .
‚é¶
‚é£
‚é¶
‚é£
0
y3
14 ‚é¶
1
y4
‚àí7

Este sistema se resuelve para y mediante un proceso de sustituci√≥n hacia adelante:

y1 = 8;
2y1 + y2 = 7,

por lo que y2 = 7 ‚àí 2y1 = ‚àí9;

3y1 + 4y2 + y3 = 14,

por lo que y3 = 14 ‚àí 3y1 ‚àí 4y2 = 26;

‚àíy1 ‚àí 3y2 + y4 = ‚àí7,

por lo que y4 = ‚àí7 + y1 + 3y2 = ‚àí26.

Entonces, resolvemos U x 5 y para x, la soluci√≥n del sistema original; es decir,
‚é§ ‚é°
‚é§
‚é°
‚é§‚é°
8
1
1
0
3
x1
‚é• ‚é¢
‚é•
‚é¢ 0 ‚àí1 ‚àí1
‚é¢
‚àí5 ‚é•
‚é¢
‚é• ‚é¢ x2 ‚é• = ‚é¢ ‚àí9 ‚é• .
‚é£ 0
0
3
13 ‚é¶ ‚é£ x3 ‚é¶ ‚é£ 26 ‚é¶
‚àí26
x4
0
0
0 ‚àí13
Utilizando la sustituci√≥n hacia atr√°s, obtenemos x4 = 2, x3 = 0, x2 = ‚àí1, x1 = 3.
La factorizaci√≥n utilizada en el ejemplo 2 recibe el nombre de m√©todo de Doolittle y
requiere que los 1 est√©n en la diagonal de L, lo cual resulta en la factorizaci√≥n descrita en el
teorema 6.19. En la secci√≥n 6.6, consideramos el m√©todo de Crout, factorizaci√≥n que requiere que los elementos en la diagonal de U sean 1, y el m√©todo de Choleski, que requiere que
lii = u ii, para cada i.
Un procedimiento general para factorizaci√≥n de matrices como un producto de matrices
triangulares se encuentra en el algoritmo 6.4. A pesar de que se construyen matrices nuevas
L y U, los valores generados pueden reemplazar las entradas correspondientes de A que ya
no son necesarias.
(ODOJRULWPRSHUPLWHHVSHFL√ÄFDUODGLDJRQDOGHL o la diagonal de U.

6.5 Factorizaci√≥n de matriz

ALGORITMO

6.4

303

Factorizaci√≥n LU
Para factorizar la matriz n 3 n A = [ai j ] en el producto de la matriz triangular inferior
L = [li j ] y la matriz triangular superior U = [u i j ], es decir A 5 LU, donde la diagonal
principal ya sea de L o U consta s√≥lo de unos:

ENTRADA dimensi√≥n n; las entradas ai j , 1 ‚â§ i, j ‚â§ n de A; la diagonal l11 = ¬∑ ¬∑ ¬∑ = lnn = 1
de L o la diagonal u 11 = ¬∑ ¬∑ ¬∑ = u nn = 1 de U .
SALIDA las entradas li j , 1 ‚â§ j ‚â§ i, 1 ‚â§ i ‚â§ n de L y las entradas u i j , i ‚â§ j ‚â§ n,
1 ‚â§ i ‚â§ n de U .
Paso 1 Seleccione l11 y u 11 al satisfacer l11 u 11 =a11 .
Si l11 u 11 = 0 entonces SALIDA (‚ÄòFactorizaci√≥n imposible‚Äô);
PARE.
Paso 2 Para j = 2, . . . , n determine u 1 j = a1 j /l11 ; (Primera fila de U.)
l j1 = a j1 /u 11 . (Primera columna de L.)
Paso 3 Para i = 2, . . . , n ‚àí 1 haga los pasos 4 y 5.
Paso 4 Seleccione lii y u ii al satisfacer lii u ii = aii ‚àí

i‚àí1
k=1 lik u ki .

Si lii u ii = 0 entonces SALIDA (‚ÄòFactorizaci√≥n imposible‚Äô);
PARE.
Paso 5 Para j = i + 1, . . . , n
Determine u i j = l1ii ai j ‚àí

i‚àí1
k=1 lik u k j

;

(i-√©sima fila de U.)

l ji = u1ii a ji ‚àí

i‚àí1
k=1 l jk u ki

.

(i-√©sima columna de L.)

Paso 6 Seleccione lnn y u nn al satisfacer lnn u nn = ann ‚àí

n‚àí1
k=1 l nk u kn .

(Nota: Si lnn u nn = 0, entonces A = LU pero A es singular.)
Paso 7 SALIDA (li j para j = 1, . . . , i y i = 1, . . . , n);
SALIDA (u i j para j = i, . . . , n y i = 1, . . . , n);
PARE.

Una vez que se completa la factorizaci√≥n de la matriz, la soluci√≥n de un sistema lineal
de la forma Ax = LU x = b se obtiene primero, haciendo y = U x y resolver Ly = b para
y. Puesto que L es triangular inferior, tenemos

y1 =
y, para cada i = 2, 3, . . . , n,

b1
,
l11

‚é°
‚é§
i‚àí1
1 ‚é£
bi ‚àí
yi =
li j y j ‚é¶ .
lii
j=1

Despu√©s de encontrar y mediante este proceso de sustituci√≥n hacia adelante, el sistema triangular superior U x 5 y se resuelve para x mediante sustituci√≥n hacia atr√°s con las ecuaciones
‚é°
‚é§
n
yn
1 ‚é£
yi ‚àí
xn =
y xi =
ui j x j ‚é¶ .
u nn
u ii
j=i+1

304

CAP√çTULO 6

M√©todos directos para resolver sistemas lineales

Matrices de permutaci√≥n
En el an√°lisis previo suponemos que A x 5 b se puede resolver por medio de eliminaci√≥n
JDXVVLDQDVLQLQWHUFDPELRVGH√ÄOD'HVGHXQSXQWRGHYLVWDSUiFWLFRHVWDIDFWRUL]DFLyQHV
~WLOVyORFXDQGRQRVHUHTXLHUHQLQWHUFDPELRVGH√ÄODSDUDFRQWURODUHOHUURUGHUHGRQGHRTXH
UHVXOWDDSDUWLUGHOXVRGHDULWPpWLFDGHGtJLWRV√ÄQLWRV3RUIRUWXQDPXFKRVVLVWHPDVTXHHQcontramos al utilizar m√©todos de aproximaci√≥n son de este tipo, pero ahora consideraremos
ODVPRGL√ÄFDFLRQHVTXHVHGHEHQKDFHUFXDQGRVHUHTXLHUHQFDPELRVGH√ÄOD&RPHQ]DPRVHO
an√°lisis con la introducci√≥n de una clase de matrices que se usan para reordenar, o permutar,
√ÄODVGHXQDPDWUL]GHWHUPLQDGD
Una matriz n 3 n de permutaci√≥n P = [ pi j ] es una matriz obtenida al reordenar las
√ÄODVGHIn, la matriz identidad. Esto produce una matriz con precisamente una entrada difeUHQWHDFHURHQFDGD√ÄOD\HQFDGDFROXPQD\FDGDHQWUDGDGLIHUHQWHDFHURHVXQ
Ilustraci√≥n

La matriz

‚é°

‚é§
1 0 0
P=‚é£ 0 0 1 ‚é¶
0 1 0
es una matriz de permutaci√≥n 3 3 3. Para cualquier matriz A 3 3 3, multiplicar a la izquierda
por P WLHQHHOHIHFWRGHLQWHUFDPELDUODVHJXQGD\WHUFHUD√ÄODVGHA:
‚é§ ‚é°
‚é§
‚é°
‚é§‚é°
a11 a12 a13
a11 a12 a13
1 0 0
P A = ‚é£ 0 0 1 ‚é¶ ‚é£ a21 a22 a23 ‚é¶ = ‚é£ a31 a32 a33 ‚é¶ .
a31 a32 a33
a21 a22 a23
0 1 0
De igual forma, multiplicar A a la derecha por P intercambia la segunda y tercera columnas
de A.
Dos propiedades √∫tiles de las matrices de permutaci√≥n se relacionan con la eliminaci√≥n
gaussiana, la primera de las cuales se ilustra en el ejemplo previo. Suponga que k1 , . . . , kn
es una permutaci√≥n de los enteros 1, . . . , n y la matriz de permutaci√≥n P = ( pi j )VHGH√ÄQH
mediante

pi j =

1, si j = ki ,
0, en otro caso.

Entonces
La multiplicaci√≥n de la matriz
AP permuta las columnas de A.

‚Ä¢ P ASHUPXWDODV√ÄODVGHA; es decir,
‚é°
ak 1 1
‚é¢ ak 1
‚é¢ 2
PA = ‚é¢ .
‚é£ ..

ak n 1

ak 1 2
ak 2 2
..
.

ak n 2

¬∑¬∑¬∑
¬∑¬∑¬∑
..
.

‚é§
ak 1 n
ak 2 n ‚é•
‚é•
.. ‚é• .
. ‚é¶

¬∑ ¬∑ ¬∑ ak n n

‚Ä¢ P21 existe y P21 5 P t.
$O√ÄQDOGHODVHFFLyQREVHUYDPRVTXHSDUDFXDOTXLHUPDWUL]QRVLQJXODUA, el sistema lineal A x 5 b se puede resolver mediante la eliminaci√≥n gaussiana, sin la posibilidad
GH LQWHUFDPELRV GH √ÄODV 6L FRQRFHPRV ORV LQWHUFDPELRV GH √ÄODV UHTXHULGRV SDUD UHVROYHU
el sistema con eliminaci√≥n gaussiana, podemos acomodar las ecuaciones originales en un
RUGHQTXHJDUDQWL]DUtDTXHQRVHQHFHVLWHQLQWHUFDPELRVGH√ÄOD3RUORWDQWRH[LVWHXQDUHorganizaci√≥n de ecuaciones en el sistema que permite eliminaci√≥n gaussiana para proceder

6.5 Factorizaci√≥n de matriz

305

sinLQWHUFDPELRVGH√ÄOD(VWRLPSOLFDTXHSDUDXQDPDWUL]QRVLQJXODUA, existe una matriz de
permutaci√≥n P para la que el sistema

P Ax = Pb
VH SXHGH UHVROYHU VLQ LQWHUFDPELRV GH √ÄOD &RPR FRQVHFXHQFLD HVWD PDWUL] P A se puede
factorizar en

P A = LU,
donde L es triangular inferior y U es triangular superior. Puesto que P21 5 P t, esto produce
la factorizaci√≥n

A = P ‚àí1 LU = (P t L)U.
La matriz U sigue siendo triangular superior, pero P t L no es triangular inferior a menos que
P 5 I.
Ejemplo 3

Determine una factorizaci√≥n en la forma A = (P t L)U para la matriz
‚é°
‚é§
0
0 ‚àí1 1
‚é¢ 1
1 ‚àí1 2 ‚é•
‚é•.
A=‚é¢
‚é£ ‚àí1 ‚àí1
2 0 ‚é¶
1
2
0 2
Soluci√≥n La matriz A no puede tener una factorizaci√≥n LU porque a11 5 0. Sin embargo, utili]DUXQLQWHUFDPELRGH√ÄOD(E 1 ) ‚Üî (E 2 ), seguido por (E 3 + E 1 ) ‚Üí (E 3 ) y (E 4 ‚àí E 1 ) ‚Üí (E 4 ),
produce
‚é°
‚é§
1 1 ‚àí1 2
‚é¢ 0 0 ‚àí1 1 ‚é•
‚é¢
‚é•.
‚é£ 0 0
1 2 ‚é¶
0 1
1 0

(QWRQFHV HO LQWHUFDPELR GH √ÄOD (E 2 ) ‚Üî (E 4 ), seguido de (E 4 + E 3 ) ‚Üí (E 4 ), produce la
matriz
‚é°
‚é§
1 1 ‚àí1 2
‚é¢ 0 1
1 0 ‚é•
‚é•.
U =‚é¢
‚é£ 0 0
1 2 ‚é¶
0 0
0 3
/DPDWUL]GHSHUPXWDFLyQUHODFLRQDGDFRQORVLQWHUFDPELRVGH√ÄOD(E 1 ) ‚Üî (E 2 ) y (E 2 ) ‚Üî
(E 4 ) es
‚é°
‚é§
0 1 0 0
‚é¢ 0 0 0 1 ‚é•
‚é•
P=‚é¢
‚é£ 0 0 1 0 ‚é¶
1 0 0 0
y

‚é°

1
‚é¢ 1
PA = ‚é¢
‚é£ ‚àí1
0

1
2
‚àí1
0

‚àí1
0
2
‚àí1

‚é§
2
2 ‚é•
‚é•.
0 ‚é¶
1

306

CAP√çTULO 6

M√©todos directos para resolver sistemas lineales

La eliminaci√≥n gaussiana se realiza en P A mediante las mismas operaciones que en A,
H[FHSWRVLQLQWHUFDPELRVGH√ÄOD(VGHFLU(E 2 ‚àí E 1 ) ‚Üí (E 2 ), (E 3 + E 1 ) ‚Üí (E 3 ), seguido de (E 4 + E 3 ) ‚Üí (E 4 ). Los multiplicadores diferentes a cero para P A son, por consiguiente,

m 21 = 1,
y la factorizaci√≥n LU de P A es
‚é°
1
‚é¢ 1
PA = ‚é¢
‚é£ ‚àí1
0

m 31 = ‚àí1,

y

m 43 = ‚àí1,

‚é§‚é°
0 0
1 1
‚é¢ 0 1
0 0 ‚é•
‚é•‚é¢
1 0 ‚é¶‚é£ 0 0
‚àí1 1
0 0

‚é§
‚àí1 2
1 0 ‚é•
‚é• = LU.
1 2 ‚é¶
0 3

Multiplicar por P21 5 P t produce la factorizaci√≥n
‚é°
0 0
‚é¢ 1 0
‚àí1
t
t
A = P (LU ) = P (LU ) = (P L)U = ‚é¢
‚é£ ‚àí1 0
1 1

‚é§‚é°
‚àí1 1
1 1
‚é¢ 0 1
0 0 ‚é•
‚é•‚é¢
1 0 ‚é¶‚é£ 0 0
0 0
0 0

0
1
0
0

‚àí1
1
1
0

‚é§
2
0 ‚é•
‚é•.
2 ‚é¶
3

La secci√≥n Conjunto de ejercicios 6.5 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

6.6 Tipos especiales de matrices
Ahora nos enfocaremos en dos clases de matrices para las que la eliminaci√≥n gaussiana se
SXHGHUHDOL]DUGHPDQHUDHIHFWLYDVLQLQWHUFDPELRVGH√ÄOD

Matrices diagonalmente dominantes
/DSULPHUDFODVHVHGHVFULEHHQODVLJXLHQWHGH√ÄQLFLyQ
DeÔ¨Ånici√≥n 6.20

Se dice que la matriz A n 3 n es diagonalmente dominante cuando
n

|aii | ‚â•

|ai j |

se mantiene para cada i = 1, 2, . . . , n.

(6.10)

j=1,
j=i

Cada entrada de la diagonal
principal en una matriz
estrictamente diagonalmente
dominante tiene una magnitud
que es estrictamente superior a la
suma de las magnitudes de todas
ODVRWUDVHQWUDGDVHQHVD√ÄOD

Ilustraci√≥n

Una matriz diagonalmente dominante es estrictamente diagonalmente dominante
cuando la desigualdad en la ecuaci√≥n (6.10) es estricta para cada n, es decir, cuando
n

|aii | >

|ai j |

se mantiene para cada i = 1, 2, . . . , n.

2
5
5

‚é§
0
‚àí1 ‚é¶
‚àí6

j=1,
j=i

Considere las matrices

‚é°

7
A=‚é£ 3
0

‚é°

y

6
B=‚é£ 4
‚àí3

4
‚àí2
0

‚é§
‚àí3
0 ‚é¶.
1

6.6 Tipos especiales de matrices

307

La matriz no sim√©trica A es estrictamente diagonalmente dominante porque

|7| > |2| + |0|,

|5| > |3| + |‚àí1|,

|‚àí6| > |0| + |5|.

y

La matriz sim√©trica B no es estrictamente diagonalmente dominante porque, por ejemplo,
HQODSULPHUD√ÄODHOYDORUDEVROXWRGHOHOHPHQWRGLDJRQDOHV|6| < |4| + |‚àí3| = 7. Es interesante observar que AtQRHVHVWULFWDPHQWHGLDJRQDOPHQWHGRPLQDQWHSRUTXHOD√ÄODGHOPHGLR
de At es [2 5 5], ni, por supuesto es B t porque B t = B.
El siguiente teorema se utiliz√≥ en la secci√≥n 3.5 para garantizar que existen soluciones
√∫nicas para los sistemas lineales necesarios para determinar spline c√∫bicos interpolantes.
Teorema 6.21

Una matriz estrictamente diagonalmente dominante A es no singular. Adem√°s, en este caso,
la eliminaci√≥n gaussiana se puede realizar en cualquier sistema lineal de la forma Ax = b
SDUDREWHQHUVX~QLFDVROXFLyQVLQLQWHUFDPELRVGH√ÄODRFROXPQD\ORVFiOFXORVVHUiQHVWDbles respecto al crecimiento de errores de redondeo.
Demostraci√≥n Primero aplicamos la demostraci√≥n por contradicci√≥n para mostrar que A es

no singular. Considere el sistema lineal descrito por Ax = 0 y suponga que existe una soluci√≥n distinta a cero x = (xi ) para este sistema. Si k es un √≠ndice para el que

0 < |xk | = m√°x |x j |.
1‚â§ j‚â§n

Puesto que

n
j=1 ai j x j = 0 para cada i = 1, 2, . . . , n, tenemos, cuando i 5 k,
n

akk xk = ‚àí

ak j x j .
j=1,
j=k

A partir de la desigualdad triangular, tenemos
n

n

|akk ||xk | ‚â§

|ak j ||x j |,

por lo que

|akk | ‚â§

n

|ak j |

j=1,
j=k

j=1,
j=k

|x j |
‚â§
|ak j |.
|xk |
j=1,
j=k

Esta desigualdad contradice la dominancia diagonal estricta de A. Por consiguiente, la √∫nica
soluci√≥n para Ax = 0 es x = 0. Esto muestra, en el teorema 6.17 en la p√°gina 298, ser equivalente a la no singularidad de A.
3DUD SUREDU TXH OD HOLPLQDFLyQ JDXVVLDQD VH SXHGH UHDOL]DU VLQ LQWHUFDPELRV GH √ÄOD
mostramos que cada una de las matrices A(2) , A(3) , . . . , A(n) generadas por el proceso de
eliminaci√≥n gaussiana (y descrito en la secci√≥n 6.5) es estrictamente diagonalmente dominante. Eso garantizar√≠a que en cada etapa del proceso de eliminaci√≥n gaussiana, el elemento
pivote es distinto a cero.
Puesto que A es estrictamente diagonalmente dominante, a11 = 0 y A(2) se puede realizar. Por lo tanto, para cada i = 2, 3, . . . , n,
(1)
ai(2)
j = ai j ‚àí

(1)
a1(1)j ai1
(1)
a11

,

para

2 ‚â§ j ‚â§ n.

(2)
= 0. La desigualdad triangular implica que
Primero, ai1
n
j=2
j =i

|ai(2)
j |=

n
j=2
j =i

ai(1)
j ‚àí

(1)
a1(1)j ai1
(1)
a11

n

‚â§
j=2
j =i

|ai(1)
j |+

n
j=2
j =i

(1)
a1(1)j ai1
(1)
a11

.

308

CAP√çTULO 6

M√©todos directos para resolver sistemas lineales

Pero puesto que A es estrictamente diagonalmente dominante,
n

n

(1)
(1)
|ai(1)
j | < |aii | ‚àí |ai1 |

(1)
|a1(1)j | < |a11
| ‚àí |a1i(1) |,

y

j=2
j =i

j=2
j =i

por lo que
n

(1)
(1)
|ai(2)
j | < |aii | ‚àí |ai1 | +

j=2
j =i

(1)
|
|ai1

(1)
(|a11
| ‚àí |a1i(1) |) = |aii(1) | ‚àí
(1)
|a11 |

(1)
||a1i(1) |
|ai1
(1)
|a11
|

.

La desigualdad triangular tambi√©n implica que

|aii(1) | ‚àí

(1)
||a1i(1) |
|ai1
(1)
|a11
|

‚â§ aii(1) ‚àí

(1)
||a1i(1) |
|ai1
(1)
|a11
|

= |aii(2) |,

lo cual nos da
n

(2)
|ai(2)
j | < |aii |.

j=2
j =i

(VWRHVWDEOHFHHOGRPLQLRGLDJRQDOHVWULFWRSDUDODV√ÄODV2, . . . , n3HURODSULPHUD√ÄODGHA(2)
y A son la misma, por lo que A(2) es estrictamente diagonalmente dominante.
Este proceso contin√∫a de manera inductiva hasta que se obtiene A(n) triangular superior y
estrictamente diagonalmente dominante. Esto implica que todos los elementos diagonales son
GLIHUHQWHVDFHURSRUORTXHVHSXHGHUHDOL]DUODHOLPLQDFLyQJDXVVLDQDVLQLQWHUFDPELRVGH√ÄOD
La demostraci√≥n de estabilidad para este procedimiento se puede encontrar en [We].

Matrices deÔ¨Ånidas positivas
La siguiente clase especial de matrices recibe el nombre de GH√ÄQLGDSRVLWLYD.
DeÔ¨Ånici√≥n 6.22

Una matriz A es GH√ÄQLGDSRVLWLYD si es sim√©trica y si xt Ax > 0 para cada vector n-dimensional x = 0.

(OQRPEUHGH√ÄQLGDSRVLWLYDVH
UH√ÄHUHDOKHFKRGHTXHHOQ~PHUR
xt Ax debe ser positivo siempre
que x 5
/ 0.

1RWRGRVORVDXWRUHVUHTXLHUHQVLPHWUtDGHXQDPDWUL]GH√ÄQLGDSRVLWLYD3RUHMHPSOR
Golub y van Loan (GV), una referencia est√°ndar en m√©todos matriciales s√≥lo requieren que
xt Ax > 0 para cada x = 0. /DVPDWULFHVTXHOODPDPRVGH√ÄQLGDVSRVLWLYDVUHFLEHQHOQRPEUHGHGH√ÄQLGDSRVLWLYDVLPpWULFDHQ>*9@0DQWHQJDHVWDGLVFUHSDQFLDHQPHQWHVLXWLOL]D
material de otras fuentes.
3DUDVHUSUHFLVRODGH√ÄQLFLyQGHEHUtDHVSHFL√ÄFDUTXHODPDWUL]3 1 generada por
la operaci√≥n xt Ax tiene un valor positivo para su √∫nica entrada ya que la operaci√≥n se realiza
de acuerdo con lo siguiente:
‚é°
‚é§‚é°
‚é§
a11 a12 ¬∑ ¬∑ ¬∑ a1n
x1
‚é¢ a21 a22 ¬∑ ¬∑ ¬∑ a2n ‚é• ‚é¢ x2 ‚é•
‚é¢
‚é•‚é¢
‚é•
xt Ax = [x1 , x2 , . . . , xn ] ‚é¢ .
..
.. ‚é• ‚é¢ .. ‚é•
‚é£ ..
.
. ‚é¶‚é£ . ‚é¶

an1

‚é°
‚é¢
‚é¢
= [x1 , x2 , . . . , xn ] ‚é¢
‚é£

an2

¬∑ ¬∑ ¬∑ ann

n
j=1 a1 j x j
n
j=1 a2 j x j

..
.

n
j=1 an j x j

‚é§

xn

‚é§
‚é°
n
n
‚é•
‚é• ‚é£
ai j xi x j ‚é¶ .
‚é•=
‚é¶
i=1 j=1

6.6 Tipos especiales de matrices

Ejemplo 1

309

Muestre que la matriz

‚é°

2 ‚àí1
2
A = ‚é£ ‚àí1
0 ‚àí1

‚é§
0
‚àí1 ‚é¶
2

HVGH√ÄQLGDSRVLWLYD
Soluci√≥n

Suponga que x es cualquier vector columna tridimensional. Entonces

‚é§
‚é§‚é°
2 ‚àí1
0
x1
2 ‚àí1 ‚é¶ ‚é£ x2 ‚é¶
xt Ax = [x1 , x2 , x3 ] ‚é£ ‚àí1
x3
0 ‚àí1
2
‚é§
‚é°
x2
2x1 ‚àí
= [x1 , x2 , x3 ] ‚é£ ‚àíx1 + 2x2 ‚àí x3 ‚é¶
‚àíx2 + 2x3
‚é°

= 2x12 ‚àí 2x1 x2 + 2x22 ‚àí 2x2 x3 + 2x32 .
Al reorganizar los t√©rminos obtenemos

xt Ax = x12 + (x12 ‚àí 2x1 x2 + x22 ) + (x22 ‚àí 2x2 x3 + x32 ) + x32
= x12 + (x1 ‚àí x2 )2 + (x2 ‚àí x3 )2 + x32 ,
lo cual implica que

x12 + (x1 ‚àí x2 )2 + (x2 ‚àí x3 )2 + x32 > 0
a menos que x1 = x2 = x3 = 0.
$SDUWLUGHOHMHPSORGHEHUtDTXHGDUFODURTXHXWLOL]DUODGH√ÄQLFLyQSDUDGHWHUPLQDUVL
XQDPDWUL]HVGH√ÄQLGDSRVLWLYDSXHGHVHUGLItFLO$IRUWXQDGDPHQWHH[LVWHQFULWHULRVGHYHUL√ÄFDFLyQPiVIiFLOHVSUHVHQWDGRVHQHOFDStWXORSDUDLGHQWL√ÄFDUDORVPLHPEURVGHHVWD
importante clase. El siguiente resultado proporciona algunas condiciones necesarias que se
pueden usar para determinar ciertas matrices de la consideraci√≥n.
Teorema 6.23

Si A es una matriz n 3 nGH√ÄQLGDSRVLWLYDHQWRQFHV

i) A tiene una inversa;
iii) m√°x1‚â§k, j‚â§n |ak j | ‚â§ m√°x1‚â§i‚â§n |aii |;

ii) aii > 0, para cada i = 1, 2, . . . , n;
iv) (ai j )2 < aii a j j para cada i = j.

Demostraci√≥n

i)

Si x satisface Ax 5 0, entonces xt Ax 5 0. Ya que AHVGH√ÄQLGDSRVLWLYDHVWRLPSOLFD
x 5 0. Por consiguiente, Ax 5 0 s√≥lo tiene la soluci√≥n cero. Mediante el teorema
6.17 en la p√°gina 298, esto es equivalente a que A no sea singular.

ii)

Para una i determinada, si x = (x j ) se puede definir mediante xi = 1 y x j = 0, si
j = i. Puesto que x = 0,
0 < xt Ax = aii .

iii)

Para k = j, deÔ¨Åna x = (xi ) mediante
‚éß
‚é™
‚é® 0, si i = j y i = k,
xi =
1, si i = j,
‚é™
‚é©
‚àí1, si i = k.

310

CAP√çTULO 6

M√©todos directos para resolver sistemas lineales

Puesto que x = 0,
0 < xt Ax = a j j + akk ‚àí a jk ‚àí ak j .
Pero At = A, por lo que a jk = ak j , lo cual implica que
2ak j < a j j + akk .

(6.11)

Ahora defina z = (z i ) mediante
0, si i = j y i = k,
1, si i = j o i = k.

zi =
Entonces zt Az > 0, por lo que

‚àí2ak j < akk + a j j .

(6.12)

Las ecuaciones (6.11) y (6.12) implican que para cada k = j,
|ak j | <
iv)

akk + a j j
‚â§ m√°x |aii |,
1‚â§i‚â§n
2

por lo que

m√°x |ak j | ‚â§ m√°x |aii |.

1‚â§k, j‚â§n

1‚â§i‚â§n

Para i = j, deÔ¨Åna x = (xk ) mediante
‚éß
‚é™
‚é®0, si k = j y k = i,
xk = Œ±, si k = i,
‚é™
‚é©
1, si k = j,
donde Œ± representa un n√∫mero real arbitrario. Puesto que x = 0,
0 < xt Ax = aii Œ± 2 + 2ai j Œ± + a j j .

Como polinomio cuadr√°tico en Œ± sin ra√≠ces reales, el discriminante de P(Œ±) =
aii Œ± 2 + 2ai j Œ± + a j j debe ser negativo. Por lo tanto,

4ai2j ‚àí 4aii a j j < 0

y ai2j < aii a j j .

A pesar de que el teorema 6.23 provee algunas condiciones importantes que deben ser
YHUGDGHUDVSDUDPDWULFHVGH√ÄQLGDVSRVLWLYDVQRJDUDQWL]DTXHXQDPDWUL]TXHVDWLVIDJDHVWDV
FRQGLFLRQHVVHDGH√ÄQLGDSRVLWLYD
/DVLJXLHQWHQRFLyQVHXVDUiSDUDSURSRUFLRQDUXQDFRQGLFLyQQHFHVDULD\VX√ÄFLHQWH
DeÔ¨Ånici√≥n 6.24

Una primera submatriz principal de una matriz A es una matriz de la forma
‚é§
‚é°
a11 a12 ¬∑ ¬∑ ¬∑ a1k
‚é¢ a21 a22 ¬∑ ¬∑ ¬∑ a2k ‚é•
‚é•
‚é¢
Ak = ‚é¢ .
..
.. ‚é• ,
‚é£ ..
.
. ‚é¶

ak1

ak2

¬∑ ¬∑ ¬∑ akk

para algunas 1 ‚â§ k ‚â§ n.
Una demostraci√≥n del siguiente resultado se puede encontrar en [Stew1], p. 250.
Teorema 6.25
Ejemplo 2

Una matriz sim√©trica AHVGH√ÄQLGDSRVLWLYDVL\VyORVLFDGDXQDGHVXVSULPHUDVVXEPDWULFHV
principales tiene un determinante positivo.
(QHOHMHPSORXWLOL]DPRVODGH√ÄQLFLyQSDUDPRVWUDUTXHODPDWUL]VLPpWULFD
‚é°
‚é§
2 ‚àí1
0
2 ‚àí1 ‚é¶
A = ‚é£ ‚àí1
0 ‚àí1
2

6.6 Tipos especiales de matrices

311

HVGH√ÄQLGDSRVLWLYD&RQ√ÄUPHHVWRFRQHOWHRUHPD
Soluci√≥n

Observe que

det A1 = det[2] = 2 > 0,
det A2 = det

2
‚àí1

‚àí1
2

= 4 ‚àí 1 = 3 > 0,

y

‚é°

2
det A3 = det ‚é£ ‚àí1
0

‚àí1
2
‚àí1

‚é§
0
‚àí1 ‚é¶ = 2 det
2

2
‚àí1

‚àí1
2

‚àí (‚àí1) det

‚àí1
0

‚àí1
2

=2(4 ‚àí 1) + (‚àí2 + 0) = 4 > 0,
de acuerdo con el teorema 6.25.
El siguiente resultado ampl√≠a la parte i) del teorema 6.23 e iguala los resultados estrictamente diagonalmente dominantes presentados en el teorema 6.21 en la p√°gina 307. No
probaremos este teorema porque requiere introducir terminolog√≠a y resultados que no son
necesarios para cualquier otro prop√≥sito. El desarrollo y la demostraci√≥n se pueden encontrar
en [We], p. 120 ff.
Teorema 6.26

Una matriz sim√©trica AHVXQDGH√ÄQLGDSRVLWLYDVL\VyORVLVHSXHGHUHDOL]DUHOLPLQDFLyQ
JDXVVLDQDVLQLQWHUFDPELRVGH√ÄODHQHOVLVWHPDOLQHDOA x 5 b con todos los elementos pivote
positivos. Adem√°s, en este caso, los c√°lculos son estables respecto al crecimiento de errores
de redondeo.
Algunos hechos interesantes no cubiertos al construir la demostraci√≥n del teorema 6.26
se presentan en los siguientes corolarios.

Corolario 6.27

La matriz A HVGH√ÄQLGDSRVLWLYDVL\VyORVLA se puede factorizar en la forma LDL t, donde L
es triangular inferior con n√∫meros 1 en su diagonal y D es una matriz diagonal con entradas
diagonales positivas.

Corolario 6.28

La matriz AHVGH√ÄQLGDSRVLWLYDVL\VyORVLA se puede factorizar en la forma LL t, donde L es
triangular inferior con entradas diagonales diferentes a cero.
La matriz L en el corolario 6.28 no es igual a la matriz L en el corolario 6.27. Una relaci√≥n entre ellas se presenta en el ejercicio 32.
El algoritmo 6.25 est√° basado en el algoritmo de factorizaci√≥n LU 6.4 y obtiene la factorizaci√≥n LDL t descrita en el corolario 6.27.

ALGORITMO

6.5

Factorizaci√≥n LDLt
Para factorizar la matriz A n 3 nGH√ÄQLGDSRVLWLYDHQODIRUPDLDL t, donde L es una matriz
triangular inferior con 1 a lo largo de la diagonal y D es una matriz diagonal con entradas
positivas en la diagonal:

ENTRADA la dimensi√≥n n; entradas ai j , para 1 ‚â§ i, j ‚â§ n de A.
SALIDA las entradas li j , para 1 ‚â§ j < i y 1 ‚â§ i ‚â§ n de L, y di , para 1 ‚â§ i ‚â§ n de D.
Paso 1 Para i = 1, . . . , n haga los pasos 2‚Äì4.
Paso 2 Para j = 1, . . . , i ‚àí 1, determine v j = li j d j .

312

CAP√çTULO 6

M√©todos directos para resolver sistemas lineales

Paso 3 Determine di = aii ‚àí

i‚àí1
j=1 li j v j .

Paso 4 Para j = i + 1, . . . , n determine l ji = (a ji ‚àí

i‚àí1
k=1 l jk vk )/di .

Paso 5 SALIDA (li j para j = 1, . . . , i ‚àí 1 y i = 1, . . . , n);
SALIDA (di para i = 1, . . . , n);
PARE.

El corolario 6.27 tiene una contraparte cuando A es sim√©trica, pero no necesariamente
GH√ÄQLGDSRVLWLYD(VWHUHVXOWDGRVHDSOLFDGHPDQHUDDPSOLDSRUTXHODVPDWULFHVVLPpWULFDV
son comunes y se reorganizan con facilidad.
Corolario 6.29

Ejemplo 3

Si A es una matriz sim√©trica n 3 n para el que se puede aplicar eliminaci√≥n gaussiana sin
LQWHUFDPELRVGH√ÄOD(QWRQFHVA se puede factorizar en LDL t, donde L es triangular inferior
(1)
(n)
con n√∫meros 1 en su diagonal y D es la matriz diagonal con a11 , . . . , ann
en su diagonal.
Determine la factorizaci√≥n LDL tGHODPDWUL]GH√ÄQLGDSRVLWLYD
‚é°
‚é§
4 ‚àí1
1
4.25 2.75 ‚é¶ .
A = ‚é£ ‚àí1
1
2.75 3.5
La factorizaci√≥n LDL t tiene n√∫meros 1 en la diagonal de la matriz triangular inferior L, por lo que necesitamos tener
‚é§‚é°
‚é§
‚é§ ‚é°
‚é§‚é°
‚é°
d1 0 0
1 0
0
1 l21 l31
a11 a21 a31
0 ‚é¶ ‚é£ 0 d2 0 ‚é¶ ‚é£ 0 1
l32 ‚é¶
A = ‚é£ a21 a22 a32 ‚é¶ = ‚é£ l21 1
a31 a32 a33
0 0
1
l31 l32 1
0 0 d3

Soluci√≥n

‚é°

d1
= ‚é£ d1l21
d1l31

d1l21
2
d2 + d1l21
d1l21l31 + d2l32

‚é§
d1l31
‚é¶.
d2l32 + d1l21l31
2
2
d1l31 + d2l32 + d3

Por lo tanto,

a11 : 4 = d1
a31 : 1 = d1l31

d1 = 4,
l31 = 0.25,

a32 : 2.75 = d1l21l31 + d2l32

a21 : ‚àí 1 = d1l21

l21 = ‚àí0.25

2
a22 : 4.25 = d2 + d1l21

d2 = 4

2
2
l32 = 0.75, a33 : 3.5 = d1l31
+ d2l32
+ d3

d3 = 1,

y tenemos

‚é°

Andr√©-Louis Cholesky
¬≤ IXHXQR√ÄFLDO
militar franc√©s involucrado en
la geodesia y la topograf√≠a a
principios de 1900. √âl desarroll√≥
este m√©todo de factorizaci√≥n
para calcular soluciones a
los problemas de m√≠nimos
cuadrados.

1
A = LDLt = ‚é£ ‚àí0.25
0.25

‚é§‚é°
‚é§‚é°
0
0
4 0 0
1
1
0 ‚é¶‚é£ 0 4 0 ‚é¶‚é£ 0
0.75 1
0 0 1
0

‚àí0.25
1
0

‚é§
0.25
0.75 ‚é¶ .
1

(ODOJRULWPRVHPRGL√ÄFDIiFLOPHQWHSDUDIDFWRUL]DUODVPDWULFHVVLPpWULFDVGHVFULWDV
HQHOFRURODULR6LPSOHPHQWHUHTXLHUHDxDGLUXQDYHUL√ÄFDFLyQSDUDJDUDQWL]DUTXHORV
elementos diagonales sean diferentes a cero. El algoritmo de Cholesky 6.6 produce la factorizaci√≥n LL t, descrita en el corolario 6.28.

6.6 Tipos especiales de matrices

ALGORITMO

6.6

313

Factorizaci√≥n de Cholesky
3DUDIDFWRUL]DUODPDWUL]GH√ÄQLGDSRVLWLYDA n 3 n en LL t, donde L es triangular inferior:

ENTRADA la dimensi√≥n n; entradas ai j , para ‚â§ i, j ‚â§ n de A.
SALIDA las entradas li j, para 1 ‚â§ j ‚â§ i y 1 ‚â§ i ‚â§ n de L. (Las entradas de U = L t son
u i j = l ji , para i ‚â§ j ‚â§ y 1 ‚â§ i ‚â§ n.)
‚àö
Paso 1 Determine l11 = a11 .
Paso 2 Para j = 2, . . . , n, determine l j1 = a j1 /l11 .
Paso 3 Para i = 2, . . . , n ‚àí 1 haga los pasos 4 y 5.
Paso 4 Determine lii = aii ‚àí

i‚àí1 2
k=1 lik

1/2

.

Paso 5 Para j = i + 1, . . . , n
determine l ji = a ji ‚àí
n‚àí1 2
k=1 l nk

Paso 6 Determine lnn = ann ‚àí

i‚àí1
k=1 l jk lik

/lii .

1/2

.

Paso 7 SALIDA (li j para j = 1, . . . , i y i = 1, . . . , n);
PARE.
Ejemplo 4

Determine la factorizaci√≥n LL tGH&KROHVN\GHODPDWUL]GH√ÄQLGDSRVLWLYD

‚é°

4
A = ‚é£ ‚àí1
1

‚àí1
4.25
2.75

‚é§
1
2.75 ‚é¶ .
3.5

La factorizaci√≥n LL t no necesariamente tiene n√∫meros 1 en la diagonal de la matriz triangular inferior L, por lo que necesitamos tener
‚é§ ‚é°
‚é§‚é°
‚é§
‚é°
0
l11 0
l11 l21 l31
a11 a21 a31
A = ‚é£ a21 a22 a32 ‚é¶ = ‚é£ l21 l22 0 ‚é¶ ‚é£ 0 l22 l32 ‚é¶
l31 l32 l33
a31 a32 a33
0 0
l33
‚é§
‚é°
2
l11l21
l11l31
l11
2
2
+ l22
l21l31 + l22l32 ‚é¶ .
= ‚é£ l11l21 l21
2
2
2
l11l31 l21l31 + l22l32 l31
+ l32
+ l33
Soluci√≥n

Por lo tanto,

a11 :

2
4 = l11

l11 = 2,

a31 :

1 = l11l31

l31 = 0.5,

a32 :

2.75 = l21l31 + l22l32

a21 :

‚àí 1 = l11l21

a22 :

2
2
4.25 = l21
+ l22

l21 = ‚àí0.5

2
2
2
l32 = 1.5, a33 : 3.5 = l31
+ l32
+ l33

l22 = 2
l33 = 1,

y tenemos

‚é°

2
A = L L t = ‚é£ ‚àí0.5
0.5

0
2
1.5

‚é§‚é°
0
2
0 ‚é¶‚é£ 0
1
0

‚é§
‚àí0.5 0.5
2
1.5 ‚é¶ .
0
1

La factorizaci√≥n LDL t descrita en el algoritmo 6.5 requiere

7
1 3
n + n 2 ‚àí n multiplicaciones/divisiones
6
6

y

1 3 1
n ‚àí n sumas/restas.
6
6

314

CAP√çTULO 6

M√©todos directos para resolver sistemas lineales

La factorizaci√≥n LL tGH&KROHVN\GHXQDPDWUL]GH√ÄQLGDSRVLWLYDVyORUHTXLHUH

1 3 1 2 2
n + n ‚àí n multiplicaciones/divisiones
6
2
3

1 3 1
n ‚àí n sumas/restas.
6
6

y

Esta ventaja computacional de la factorizaci√≥n de Cholesky es enga√±osa porque requiere
extraer n ra√≠ces cuadradas. Sin embargo, el n√∫mero de operaciones que se necesitan para
calcular las n ra√≠ces cuadradas es un factor lineal de n y disminuir√° su importancia conforme
n aumenta.
(ODOJRULWPRSURYHHXQPpWRGRHVWDEOHSDUDIDFWRUL]DUXQDPDWUL]GH√ÄQLGDSRVLWLYD
de la forma A = LDLtSHURVHGHEHPRGL√ÄFDUSDUDUHVROYHUHOVLVWHPDOLQHDO Ax = b. Para
hacerlo, borramos la declaraci√≥n PARE del paso 5 en el algoritmo y a√±adimos los siguientes
pasos para resolver el sistema triangular inferior Ly = b:

Paso 6 Determine y1 = b1 .
Paso 7 Para i = 2, . . . , n determine yi = bi ‚àí

i‚àí1
j=1 li j y j .

El sistema lineal Dz = y se puede resolver por medio de

Paso 8 Para i = 1, . . . , n determine z i = yi /di .
Finalmente, el sistema triangular superior L t x = z se resuelve con los pasos dados por

Paso 9 Determine xn = z n .
n
j=i+1 l ji x j .

Paso 10 Para i = n ‚àí 1, . . . , determine xi = z i ‚àí
Paso 11 SALIDA (xi para i = 1, . . . , n);
PARE.

La tabla 6.4 muestra las operaciones adicionales requeridas para resolver el sistema
lineal.

Tabla 6.4

Paso

Multiplicaciones/divisiones

Sumas/restas

6
7
8
9
10
Total

0
n(n ‚àí 1)/2
n
0
n(n ‚àí 1)/2
n2

0
n(n ‚àí 1)/2
0
0
n(n ‚àí 1)/2
n2 ‚àí n

6LVHSUH√ÄHUHODIDFWRUL]DFLyQGH&KROHVN\GDGDHQHODOJRULWPRORVSDVRVDGLFLRQDles para resolver el sistema Ax = b son los siguientes. Primero borre la instrucci√≥n PARE
del paso 7. A continuaci√≥n, a√±ada:

Paso 8 Determine y1 = b1 /l11 .
Paso 9 Para i = 2, . . . , n determine yi = bi ‚àí

i‚àí1
j=1 li j y j

lii .

Paso 10 Determine xn = yn /lnn .
Paso 11 Para i = n ‚àí 1, . . . , determine xi = yi ‚àí

n
j=i+1 l ji x j

lii .

Paso 12 SALIDA (xi para i = 1, . . . , n);
PARE.
Los pasos 8-12 requieren n2 1 n multiplicaciones/divisiones y n2 2 n sumas/restas.

6.6 Tipos especiales de matrices

315

Matrices de banda
La √∫ltima clase de matrices considerada son las matrices de banda. En muchas aplicacioQHVODVPDWULFHVGHEDQGDWDPELpQVRQHVWULFWDPHQWHGLDJRQDOPHQWHGRPLQDQWHVRGH√ÄQLGDV
positivas.
DeÔ¨Ånici√≥n 6.30
El nombre para una matriz de
banda proviene del hecho de que
todas las entradas diferentes a
cero se encuentran en una banda
centrada en la diagonal principal.

Una matriz n 3 n recibe el nombre de matriz de banda si existen los enteros p y q con
1< p, q < n, con la propiedad de que ai j = 0 siempre que p ‚â§ j ‚àí i o q ‚â§ i ‚àí j. El
ancho de bandaREDQGDGHXQDPDWUL]VHGH√ÄQHFRPRw = p + q ‚àí 1.
El n√∫mero p describe el n√∫mero de diagonales sobre la diagonal principal, incluyendo
la diagonal principal, en la que pueden encontrar entradas diferentes a cero. El n√∫mero q
describe el n√∫mero de diagonales debajo de la diagonal principal, incluyendo la diagonal
principal, en la que pueden encontrar entradas diferentes a cero. Por ejemplo, la matriz
‚é°
‚é§
7
2
0
5 ‚àí1 ‚é¶
A=‚é£ 3
0 ‚àí5 ‚àí6
es una matriz de banda con p 5 q 5 2 y ancho de banda 2 1 2 2 1 5 3.
/DGH√ÄQLFLyQGHPDWUL]GHEDQGDREOLJDDHVWDVPDWULFHVDFRQFHQWUDUWRGDVVXVHQWUDGDV
diferentes a cero alrededor de la diagonal. Dos casos especiales de matrices de banda que se
presentan con frecuencia tienen p 5 q 5 2 y p 5 q 5 4.

Matrices tridiagonales
Las matrices de ancho de banda 3 se presentan cuando p 5 q 5 2, reciben el nombre de
tridiagonales porque tienen la forma
‚é§
‚é°
0.. .. .. . . . . . . . . . . . . . . . . 0.
a11 a12
...
..
...
‚é•
‚é¢ a21 a22 a23
..
...
‚é•
‚é¢
..
...
‚é•
‚é¢ 0 a
‚é•
‚é¢ .. . . . 32 . . .a33 . . .a. 34 . . . . . . . . ..
...
A = ‚é¢ . ...
‚é•.
.
.
.
.
.
... .... ....
...
‚é•
‚é¢ ..
0
..
... ...
‚é•
‚é¢ .
...
. . . . . . an‚àí1,n ‚é¶
‚é£ ..
...
.
.
.
.
.
.
..
.
0 . . . . . . . . . . . . . . . 0 an,n‚àí1
ann
Las matrices tridiagonales tambi√©n se consideran en el cap√≠tulo 11 junto con el estudio de las
aproximaciones lineales por tramos para problemas de valor en la frontera. El caso p 5 q 5 4,
se utilizar√° para solucionar problemas de valor en la frontera al aproximar funciones que
asumen la forma de splines c√∫bicos.
/RVDOJRULWPRVGHIDFWRUL]DFLyQVHSXHGHQVLPSOL√ÄFDUFRQVLGHUDEOHPHQWHHQHOFDVRGHPDWULces de banda debido al gran n√∫mero de ceros que aparecen en estas matrices en patrones regulares.
Es especialmente interesante observar la forma que el m√©todo Crout o Doolittle asume en este caso.
Para ilustrar la situaci√≥n, suponga que una matriz tridiagonal A se puede factorizar en las
matrices triangulares L y U. Entonces A tiene m√°ximo (3n 2 2) entradas diferentes a cero.
Por lo que existen solamente (3n 2 2) condiciones a aplicar para determinar las entradas de
L y U, siempre y cuando, por supuesto, tambi√©n se obtengan las entradas cero de A.
Suponga que las matrices L y U tambi√©n tienen forma tridiagonal; es decir,

‚é§
l11 0.. .. .. . . . . . . . . . . . . 0.
.. ‚é•
‚é¢ l21 . l22 . . . . . .
.. ‚é•
‚é¢
... ... ...
. ‚é•
‚é¢
L = ‚é¢ 0. . . . . . . . . . . . . . . . . . .. ‚é•
... ... . . ‚é•
‚é¢ .. . . .
...
... ... 0 ‚é¶
‚é£ .
.
.
.. .
0 . . . . . . . 0 ln,n‚àí1 lnn

‚é°

‚é°

y

‚é§
1 u 12 . . 0.. .. . . . . . . .0.
... ...
.
‚é¢ 0. . 1 .
‚é•
. .
‚é¢ . . . . . . . . . . . . . . ..
‚é•
‚é¢ .. . . . . . . . . 0 ‚é•
U =‚é¢ .
‚é•.
.
.
..
‚é¢ ..
‚é•
.. .... ...
..
‚é£ .
u
.
. . . . .n‚àí1,n ‚é¶
0 .......... 0
1

316

CAP√çTULO 6

M√©todos directos para resolver sistemas lineales

Hay (2n 2 1) entradas indeterminadas de L y (n 2 1) entradas determinadas de U, que
suman (3n 2 2), el n√∫mero de posibles entradas diferentes a cero de A. Las entradas 0 de A
se obtienen autom√°ticamente.
La multiplicaci√≥n relacionada con A 5 LU nos da, adem√°s de las entradas 0,

a11 = l11 ;
ai,i‚àí1 = li,i‚àí1 ,

para cada i = 2, 3, . . . , n;

aii = li,i‚àí1 u i‚àí1,i + lii ,

(6.13)

para cada i = 2, 3, . . . , n;

(6.14)

y

ai,i+1 = lii u i,i+1 ,

para cada i = 1, 2, . . . , n ‚àí 1.

(6.15)

Una soluci√≥n para este sistema se encuentra al utilizar primero la ecuaci√≥n (6.13) para
obtener los t√©rminos fuera de la diagonal diferentes a cero en L y despu√©s, con las ecuaciones
(6.14) y (6.15) para obtener de manera alternativa el resto de las entradas en U y L. Una vez
que se calcula una entrada L o U, la entrada correspondiente en A no es necesaria. Por lo que,
las entradas en A se pueden sobrescribir mediante las entradas en L y U con el resultado de
que no se requiere almacenamiento nuevo.
El algoritmo 6.7 resuelve un sistema n 3 nGHHFXDFLRQHVOLQHDOHVFX\DPDWUL]GHFRH√Äciente es tridiagonal. Este algoritmo solamente requiere (5n 2 4) multiplicaciones/divisiones y (3n 2 3) sumas/restas. Por consiguiente, tiene una ventaja computacional considerable
sobre los m√©todos que no consideran la tridiagonalidad de la matriz.

ALGORITMO

6.7

Factorizaci√≥n de Crout para sistemas lineales tridiagonales
Para resolver el sistema lineal n 3 n

E1 :
E2 :
..
.
E n‚àí1 :
En :

a11 x1 + a12 x2

= a1,n+1 ,

a21 x1 + a22 x2 + a23 x3
..
.

= a2,n+1 ,
..
.

an‚àí1,n‚àí2 xn‚àí2 + an‚àí1,n‚àí1 xn‚àí1 + an‚àí1,n xn = an‚àí1,n+1 ,
an,n‚àí1 xn‚àí1

+ ann xn

= an,n+1 ,

que se supone que tiene una soluci√≥n √∫nica:
ENTRADA la dimensi√≥n n; las entradas de A.
SALIDA la soluci√≥n x1 , . . . , xn .
(Los pasos 1-3 configuran y resuelven L z = b.)
Paso 1 Determine l11 = a11 ;
u 12 = a12 /l11 ;
z 1 = a1,n+1 /l11 .
Paso 2 Para i = 2, . . . , n ‚àí 1 determine li,i‚àí1 = ai,i‚àí1 ; (i-√©sima fila de L.)
lii = aii ‚àí li,i‚àí1 u i‚àí1,i ;
u i,i+1 = ai,i+1 /lii ; ((i + 1)-√©sima columna de U.)
z i = (ai,n+1 ‚àí li,i‚àí1 z i‚àí1 )/lii .
Paso 3 Determine ln,n‚àí1 = an,n‚àí1 ; (en√©sima fila de L .)
lnn = ann ‚àí ln,n‚àí1 u n‚àí1,n .
z n = (an,n+1 ‚àí ln,n‚àí1 z n‚àí1 )/lnn .

6.6 Tipos especiales de matrices

317

(Pasos 4 y 5 resuelven U x = z.)
Paso 4 Determine xn = z n .
Paso 5 Para i = n ‚àí 1, . . . , 1 determine xi = z i ‚àí u i,i+1 xi+1 .
Paso 6 SALIDA (x1 , . . . , xn );
PARE.
Ejemplo 5

Determine la factorizaci√≥n de Crout de la matriz tridiagonal sim√©trica
‚é°
‚é§
2 ‚àí1
0
0
‚é¢ ‚àí1
2 ‚àí1
0 ‚é•
‚é•
‚é¢
‚é£ 0 ‚àí1
2 ‚àí1 ‚é¶
0
0 ‚àí1
2
y utilice esta factorizaci√≥n para resolver el sistema lineal

2x1 ‚àí x2
= 1,
= 0,
‚àíx1 + 2x2 ‚àí x3
‚àí x2 + 2x3 ‚àí x4 = 0,
‚àí x3 + 2x4 = 1.
Soluci√≥n

La factorizaci√≥n LU de A tiene la forma
‚é§ ‚é°
‚é§‚é°
‚é§
l11
1 u 12
a11
0
0
0
0
0
0
0
0
‚é¢
‚é¢
‚é¢ a21 a22 a23
0 ‚é•
0
0 ‚é•
0 ‚é•
1 u 23
‚é• ‚é¢ l21 l22
‚é•‚é¢ 0
‚é•
A=‚é¢
‚é£ 0 a32 a33 a34 ‚é¶ = ‚é£ 0 l32 l33
0 ‚é¶‚é£ 0
0
1 u 34 ‚é¶
0
0 a43 a44
0
0
0
1
0
0 l43 l44
‚é°
‚é§
l11
l11 u 12
0
0
‚é¢ l21 l22 + l21 u 12
l22 u 23
0 ‚é•
‚é•.
=‚é¢
‚é£ 0
l33 u 34 ‚é¶
l32 l33 + l32 u 23
0
0
l43 l44 + l43 u 34

‚é°

Por lo tanto,

a11 :

2 = l11

l11 = 2,

a12 :

a21 :

‚àí 1 = l21

a23 :

‚àí 1 = l22 u 23

u 23 = ‚àí 23 ,

a32 :

‚àí 1 = l32

a33 :

2 = l33 + l32 u 23

l33 = 43 ,

a34 :

‚àí 1 = l33 u 34

a43 :

‚àí 1 = l43

l21 = ‚àí1,

l43 = ‚àí1,

u 12 = ‚àí 12 ,

a22 : 2 = l22 + l21 u 12

l22 = 32 ,
l32 = ‚àí1,
u 34 = ‚àí 34 ,

a44 : 2 = l44 + l43 u 34

Esto nos da la factorizaci√≥n de Crout
‚é°
‚é§ ‚é°
2
0
0
2 ‚àí1
0
0
3
‚é¢ ‚àí1
‚é• ‚é¢ ‚àí1
0
2
‚àí1
0
2
‚é•=‚é¢
A=‚é¢
4
‚é£ 0 ‚àí1
2 ‚àí1 ‚é¶ ‚é£ 0 ‚àí1
3
0
0 ‚àí1
2
0
0 ‚àí1
Al resolver el sistema
‚é°
2
0
0
3
‚é¢ ‚àí1
0
2
Lz = ‚é¢
4
‚é£ 0 ‚àí1
3
0
0 ‚àí1

‚àí 1 = l11 u 12

‚é§‚é°
0
1 ‚àí 12
‚é•
‚é¢
0 ‚é•‚é¢ 0
1
0 ‚é¶‚é£ 0
0
5
0
0
4

‚é§‚é°
‚é§ ‚é° ‚é§
0
z1
1
‚é¢ z2 ‚é• ‚é¢ 0 ‚é•
0 ‚é•
‚é•‚é¢
‚é•=‚é¢ ‚é•
0 ‚é¶ ‚é£ z3 ‚é¶ ‚é£ 0 ‚é¶
5
1
z4
4

l44 = 54 .

0
‚àí 23
1
0

‚é§
0
0‚é•
‚é• = LU.
‚àí 34 ‚é¶
1

‚é§ ‚é° 1 ‚é§
2
z1
1 ‚é•
‚é¢ z2 ‚é• ‚é¢
3 ‚é•
‚é•=‚é¢
nos da ‚é¢
,
‚é¢
‚é£ z3 ‚é¶ ‚é£ 1 ‚é•
‚é¶
4
z4
1
‚é°

318

CAP√çTULO 6

M√©todos directos para resolver sistemas lineales

y al resolver

‚é°

0
1 ‚àí 12
‚é¢ 0
1 ‚àí 23
‚é¢
Ux = ‚é£
0
0
1
0
0
0

‚é§ ‚é° 1 ‚é§
‚é§‚é°
2
0
x1
‚é¢ 1 ‚é•
‚é•
‚é•
‚é¢
0 ‚é• ‚é¢ x2 ‚é• ‚é¢ 3 ‚é•
=‚é¢
‚é•
‚àí 34 ‚é¶ ‚é£ x3 ‚é¶ ‚é£ 1 ‚é¶
4
x4
1
1

‚é§ ‚é° ‚é§
1
x1
‚é¢ x2 ‚é• ‚é¢ 1 ‚é•
‚é•
‚é¢
‚é•.
‚é¢
=
nos da ‚é£
x3 ‚é¶ ‚é£ 1 ‚é¶
1
x4
‚é°

El algoritmo de factorizaci√≥n Crout se puede aplicar siempre que lii = 0 para cada
i = 1, 2, . . . , n. Dos condiciones, cualquiera que garantice que esto es verdad, son que la
PDWUL]GHFRH√ÄFLHQWHVGHOVLVWHPDHVGH√ÄQLGDSRVLWLYDRTXHHVHVWULFWDPHQWHGLDJRQDOPHQWH
dominante. Una condici√≥n adicional que garantiza la aplicaci√≥n de este algoritmo se proporciona en el siguiente teorema, cuya demostraci√≥n se considera en el ejercicio 30.
Teorema 6.31

Suponga que A = [ai j ] es tridiagonal con ai,i‚àí1 ai,i+1 = 0, para cada i = 2, 3, . . . , n ‚àí 1.
Si |a11 | > |a12 |, |aii | ‚â• |ai,i‚àí1 | + |ai,i+1 |, para cada i = 2, 3, . . . , n ‚àí 1, y |ann | > |an,n‚àí1 |,
entonces A es no singular y los valores de li i descritos en el algoritmo de factorizaci√≥n Crout
son diferentes a cero para cada i = 1, 2, . . . , n.
La secci√≥n Conjunto de ejercicios 6.6 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

6.7 Software num√©rico
El software para operaciones de matriz y la soluci√≥n directa de sistemas lineales implementados en IMSL y NAG est√° basado en LAPACK, un paquete de subrutinas de dominio
p√∫blico. Existe excelente documentaci√≥n y libros disponible acerca de ellos. Nosotros nos
enfocaremos en varias subrutinas disponibles en las tres fuentes.
LAPACK de acompa√±amiento es un conjunto de operaciones de nivel inferior llamadas
Subprogramas B√°sicos de √Ålgebra Lineal (BLAS). En general, el nivel 1 de BLAS consiste
en operaciones vector-vector, como sumas de vector con datos de entrada y conteo de operaciones de O(n). El nivel 2 consiste en operaciones matriz-vector, como el producto de una
matriz y un vector con datos de entrada y conteo de operaciones de O(n2). El nivel 3 consiste
en operaciones matriz-matriz, como productos de matriz con datos de entrada y conteo de
operaciones de O(n3).
Las subrutinas en LAPACK para resolver sistemas lineales primero factorizan la matriz A.
La factorizaci√≥n depende del tipo de matriz de la siguiente forma:
1.

Matriz general P A 5 LU

2.

0DWUL]GH√ÄQLGDSRVLWLYDA 5 LLt

3.

Matriz sim√©trica A 5 LDLt

4.

Matriz tridiagonal A 5 LU (en forma de banda)

Adem√°s, se pueden calcular inversas y determinantes.
La Biblioteca IMSL incluye contrapartes para casi todas las subrutinas LAPACK y tambi√©n algunas extensiones. La Biblioteca NAG tiene numerosas subrutinas para m√©todos directos para resolver sistemas lineales similares a los de LAPACK e IMSL.
Las secciones Preguntas de an√°lisis, Conceptos clave y Revisi√≥n del cap√≠tulo est√°n disponibles en l√≠nea. Encuentre la ruta de acceso en las p√°ginas preliminares.

CAP√çTULO

7

T√©cnicas iterativas en √°lgebra de matrices

Introducci√≥n
Las vigas son estructuras livianas capaces de sostener cargas pesadas. En el dise√±o de puentes, las partes individuales de las vigas est√°n conectadas con uniones orientables que permiWHQWUDQVIHULUODVIXHU]DVGHVGHXQDSDUWHGHODYLJDKDVWDRWUD/D√ÄJXUDDGMXQWDPXHVWUDXQD
viga sostenida de manera estacionaria en el extremo izquierdo inferior permite movimiento horizontal en el extremo derecho inferior y tiene uniones en , , y . Una carga de
10 000 newtons (N) se coloca en la uni√≥n y las fuerzas resultantes en las uniones provistas
por f1, f2, f3, f4 y f5, como se muestra. Cuando son positivas, estas fuerzas indican tensi√≥n en
los elementos de la viga y, cuando son negativas, compresi√≥n. El miembro de soporte estacionario podr√≠a tener tanto un componente de fuerza horizontal F1 como un componente de
fuerza vertical F2, pero el miembro de soporte movible s√≥lo tiene un componente de fuerza
vertical F3.
f1

2

f4
f3

F1

f1
1

f3
f2

f2

3

l1
f5

f4
f5

4

10 000 N
F2

F3

Si la viga est√° en equilibrio est√°tico, las fuerzas en cada uni√≥n se deben sumar al vector
cero, por lo que la suma de los componentes horizontales y verticales en cada uni√≥n debe
VHU(VWRSURGXFHHOVLVWHPDOLQHDOGHHFXDFLRQHVTXHVHPXHVWUDHQODWDEODDGMXQWD8QD
matriz de 8 √ó 8 que describe este sistema tiene 47 entradas cero y s√≥lo 17 entradas diferentes
DFHUR/DVPDWULFHVFRQXQDOWRSRUFHQWDMHGHHQWUDGDVFHURUHFLEHQHOQRPEUHGHdispersas
y, a menudo, se resuelven utilizando m√©todos iterativos en lugar de t√©cnicas directas. La soOXFLyQLWHUDWLYDDHVWHVLVWHPDVHFRQVLGHUDHQHOHMHUFLFLRGHODVHFFLyQ\HQHOHMHUFLFLR
10 en la secci√≥n 7.4.
Uni√≥n

Componente horizontal

Componente vertical

‚àö
‚àíF1 + 22 f 1 + f 2 = 0
‚àö
‚àö
‚àí 22 f 1 + 23 f 4 = 0

2
f ‚àí F2 = 0
2 1
‚àö
2
‚àí 2 f 1 ‚àí f 3 ‚àí 12 f 4 = 0

‚àí f2 + f5 = 0

f 3 ‚àí 10,000 = 0

‚àí 23 f 4 ‚àí f 5 = 0

1
f ‚àí F3 = 0
2 4

‚àö

‚àö

319

320

CAP√çTULO 7

T√©cnicas iterativas en √°lgebra de matrices

Los m√©todos que se presentan en el cap√≠tulo 6 usaban t√©cnicas directas para resolver
un sistema de n 3 n ecuaciones lineales de la forma Ax 5 b. En este cap√≠tulo presentamos
m√©todos iterativos para resolver un sistema de este tipo.

7.1 Normas de vectores y matrices

Un escalar es un n√∫mero real
RFRPSOHMR TXHSRUORJHQHUDO
se denota con letras it√°licas
o griegas. Los vectores se
representan por medio de letras
negritas.

En el cap√≠tulo 2 describimos las t√©cnicas iterativas para encontrar ra√≠ces de ecuaciones de
la forma f (x) = 0. Se encontr√≥ una aproximaci√≥n inicial (o aproximaciones) y, despu√©s,
se determinaron aproximaciones nuevas con base en qu√© tan bien satisfac√≠an la ecuaci√≥n las
DSUR[LPDFLRQHVSUHYLDV(OREMHWLYRHVHQFRQWUDUXQDIRUPDGHPLQLPL]DUODGLIHUHQFLDHQWUH
las aproximaciones y la soluci√≥n exacta.
Para analizar los m√©todos iterativos que resuelvan sistemas lineales, primero necesitamos
determinar una forma para medir la distancia entre vectores columna n-dimensionales. Esto
nos permitir√° determinar si una sucesi√≥n de vectores converge a una soluci√≥n del sistema.
En la actualidad, esta medida tambi√©n se necesita cuando la soluci√≥n se obtiene con los
m√©todos directos presentados en el cap√≠tulo 6. Estos m√©todos requer√≠an un gran n√∫mero de
RSHUDFLRQHVDULWPpWLFDV\XWLOL]DUDULWPpWLFDGHGtJLWRV√ÄQLWRVVyORFRQGXFHDXQDDSUR[LPDci√≥n para una soluci√≥n real del sistema.

Normas de vector
Sea que RnGHQRWDHOFRQMXQWRGHWRGRVORVYHFWRUHVFROXPQDn-dimensionales con compoQHQWHVGHQ~PHURVUHDOHV3DUDGH√ÄQLUODGLVWDQFLDHQRn, usamos la noci√≥n de una norma,
que es la generalizaci√≥n del valor absoluto en RHOFRQMXQWRGHQ~PHURVUHDOHV
DeÔ¨Ånici√≥n 7.1

Una norma vectorial en Rn, es una funci√≥n, ¬∑ , de Rn, a R, con las siguientes propiedades:

i)

x

0 para toda x ‚àà Rn ,

ii)

x

0 si y s√≥lo si x = 0,

iii)

Œ±x

iv)

x+y

Œ± x para toda Œ± ‚àà R y x ‚àà Rn ,
x

y para toda x, y ‚àà Rn .

Los vectores en Rn, son vectores columna y es conveniente usar la notaci√≥n de la transpuesta presentada en la secci√≥n 6.3 cuando un vector se representa en t√©rminos de sus comSRQHQWHV3RUHMHPSORHOYHFWRU
‚é°
‚é§
x1
‚é¢ x2 ‚é•
‚é¢
‚é•
x=‚é¢ . ‚é•
‚é£ .. ‚é¶

xn

se escribir√° x = (x1 , x2 , . . . , xn )t .
6yORQHFHVLWDUHPRVGRVQRUPDVHVSHFt√ÄFDVHQRn, a pesar de que se presenta una tercera
norma en RnHQHOHMHUFLFLR
DeÔ¨Ånici√≥n 7.2

Las normas l2 y l‚àû para el vector x = (x1 , x2 , . . . , xn )tVHGH√ÄQHQPHGLDQWH
n

x 2=

1/2

xi2
i=1

y

x ‚àû = m√°x |xi |.
1‚â§i‚â§n

7.1 Normas de vectores y matrices

321

Observe que cada una de estas normas se reduce al valor absoluto para el caso n 5 1.
La norma l2 recibe el nombre de norma euclidiana del vector x porque representa la noci√≥n com√∫n de distancia desde el origen cuando x est√° en R1 ‚â° R, R2 o R33RUHMHPSOR
la norma l2 del vector x = (x1 , x2 , x3 )t provee la longitud del segmento recto que une los
puntos (0, 0, 0) y (x1 , x2 , x3 )/D√ÄJXUDPXHVWUDODIURQWHUDGHHVRVYHFWRUHVHQR2 y R3
que tienen una norma l2PHQRUD/D√ÄJXUDHVXQDLOXVWUDFLyQVLPLODUGHODQRUPDl‚àû.

Figura 7.1
x3

x2

Los vectores en el
primer octante de ‚∫¢3
con la norma l2 menor
a 1 est√°n dentro de esta
figura.

Los vectores en ‚∫¢2

con la norma l2 menor
a 1 se encuentran dentro (0, 1)
de esta figura.

(0, 0, 1)

(1, 0)

(21, 0)

x1
(0, 1, 0)

(1, 0, 0)

x2

x1
(0, 21)

Figura 7.2

x2
(21, 1)

x3
(0, 1)

(1, 1)

(0, 0, 1)
(1, 0, 1)

(21, 0)

(1, 0)

(0, 1, 1)
(1, 1, 1)

x1
(1, 0, 0)

(0, 1, 0)

x1
(21, 21)

(0, 21)

(1, 21)

Los vectores en ‚∫¢2 con la norma
l ` menor a 1 se encuentran
dentro de esta figura.

(1, 1, 0)

Los vectores en el primer octante de
‚∫¢3 con la norma l ` menor a 1 se
encuentran dentro de esta figura.

x2

322

CAP√çTULO 7

T√©cnicas iterativas en √°lgebra de matrices

Ejemplo 1

Determine la norma l2 y la norma l‚àû del vector x = (‚àí1, 1, ‚àí2)t .
Soluci√≥n El vector x = (‚àí1, 1, ‚àí2)t en R3 tiene las normas

x 2=

‚àö
6

(‚àí1)2 + (1)2 + (‚àí2)2 =

y
x ‚àû = m√°x{| ‚àí 1|, |1|, | ‚àí 2|} = 2.
(VIiFLOPRVWUDUTXHODVSURSLHGDGHVHQODGH√ÄQLFLyQPDQWLHQHQODQRUPDl‚àû porque
siguen resultados similares para valores absolutos. La √∫nica propiedad que requiere m√°s demostraci√≥n es iv) y, en este caso, si x = (x1 , x2 , . . . , xn )t y y = (y1 , y2 , . . . , yn )t , entonces

x + y ‚àû = m√°x |xi + yi | ‚â§ m√°x (|xi | + |yi |) ‚â§ m√°x |xi | + m√°x |yi
1‚â§i‚â§n

1‚â§i‚â§n

1‚â§i‚â§n

x ‚àû

1‚â§i‚â§n

y ‚àû.

Las primeras tres condiciones tambi√©n son f√°ciles de demostrar para la norma l2. Pero
para mostrar eso

x+y 2

y 2,

x 2

para cada x, y ‚àà Rn ,

necesitamos una desigualdad famosa.
Teorema 7.3

(Desigualdad Cauchy-Bunyakovsky-Schwarz para sumas)

Por cada x = (x1 , x2 , . . . , xn )t y y = (y1 , y2 , . . . , yn )t en Rn ,
n

xt y =

1/2

n

xi2

i=1

Existen muchas formas de esta
desigualdad, por lo tanto, muchos
descubridores. Augustin Louis
&DXFK\ ¬≤ ODGHVFULELy
en 1821 en Cours d‚ÄôAnalyse
Alg√©brique (Curso de an√°lisis
algebraico), el primer libro sobre
c√°lculo riguroso. Una integral
de la forma de la igualdad
DSDUHFHHQHOWUDEDMRGH9LNWRU
<DNRYOHYLFK%XQ\DNRYVN\
¬≤ HQ\+HUPDQQ
$PDQGXV6FKZDU] ¬≤ 
us√≥ una forma de integral doble
de esta desigualdad en 1885.
M√°s detalles sobre la historia se
pueden encontrar en [Stee].

1/2

n

xi yi ‚â§

yi2

i=1

x 2 ¬∑ y 2.

(7.1)

i=1

Demostraci√≥n Si y = 0 o x = 0, el resultado es inmediato porque ambos lados de la
desigualdad son cero.
Suponga que y = 0 y x = 0. Observe que para cada l ‚àà R tenemos
n

n

0 ‚â§ ||x ‚àí Œªy||22 =

(xi ‚àí Œªyi )2 =
i=1

n

n

xi2 ‚àí 2Œª
i=1

xi yi + Œª2
i=1

yi2 ,
i=1

por lo que
n

n

n

xi yi ‚â§

2Œª
i=1

xi2 + Œª2
i=1

yi2

x 22 + Œª2 y 22 .

i=1

Sin embargo x 2 > 0 y y 2 > 0, por lo que podemos hacer Œª
n

x 2
2
y 2

xi yi
i=1

x 22 +

x 22
y 22 = 2 x 22 .
y 22

Por lo tanto,
n

xi yi ‚â§ 2 x 22

2
i=1

x 2 / y 2 para obtener

y 2
= 2 x 2 y 2,
x 2

7.1 Normas de vectores y matrices

323

y

xt y =

n

n

xi yi

x 2 y 2=

i=1

1/2

n

xi2

1/2

yi2

i=1

.

i=1

Con este resultado vemos que para cada x, y ‚àà Rn ,
n

n

x + y 22 =

(xi + yi )2 =

n

xi2 + 2

i=1

i=1

n

xi yi +
i=1

yi2

x 22 + 2 x 2 y 2

y 22 ,

i=1

que proporciona la propiedad iv) de la norma:

x+y 2 ‚â§

x 22 + 2 x 2 y 2

y 22

1/2

x 2

y 2.

Distancia entre vectores en Rn
La norma de un vector proporciona una medida para la distancia entre un vector arbitrario
y el vector cero, de la misma forma en la que el valor absoluto de un n√∫mero real describe
su distancia desde 0. De igual forma, la distancia entre dos vectoresHVWiGH√ÄQLGDFRPROD
norma de la diferencia de los vectores al igual que la distancia entre dos n√∫meros reales es el
valor absoluto de su diferencia.
DeÔ¨Ånici√≥n 7.4

Si x = (x1 , x2 , . . . , xn )t y y = (y1 , y2 , . . . , yn )t son vectores en Rn, las distancias l2 y l‚àû
entre x y yVHGH√ÄQHQPHGLDQWH
n

x‚àíy 2 =

1/2

(xi ‚àí yi )2

y x ‚àí y ‚àû = m√°x |xi ‚àí yi |.

i=1

Ejemplo 2

1‚â§i‚â§n

El sistema lineal

3.3330x1 + 15920x2 ‚àí 10.333x3 = 15913,
2.2220x1 + 16.710x2 + 9.6120x3 = 28.544,
1.5611x1 + 5.1791x2 + 1.6852x3 = 8.4254,
tiene la soluci√≥n exacta x = (x1 , x2 , x3 )t = (1, 1, 1)t . La eliminaci√≥n gaussiana realizada
mediante aritm√©tica de redondeo de cinco d√≠gitos y pivoteo parcial (algoritmo 6.2) produce
la soluci√≥n aproximada

xÃÉ = (xÃÉ1 , xÃÉ2 , xÃÉ3 )t = (1.2001, 0.99991, 0.92538)t .
Determine las distancias l2 y l‚àû entre las soluciones exactas y aproximadas.
Soluci√≥n

Las mediciones de x ‚àí xÃÉ est√°n dadas por

x ‚àí xÃÉ ‚àû = m√°x{|1 ‚àí 1.2001|, |1 ‚àí 0.99991|, |1 ‚àí 0.92538|}
= m√°x{0.2001, 0.00009, 0.07462} = 0.2001
y

x ‚àí xÃÉ 2 = (1 ‚àí 1.2001)2 + (1 ‚àí 0.99991)2 + (1 ‚àí 0.92538)2

1/2

= [(0.2001)2 + (0.00009)2 + (0.07462)2 ]1/2 = 0.21356.
A pesar de que los componentes xÃÉ2 y xÃÉ3 son buenas aproximaciones para x2 y x3, el componente xÃÉ1 es una aproximaci√≥n d√©bil para x1 y |x1 ‚àí xÃÉ1 | domina ambas normas.

324

CAP√çTULO 7

T√©cnicas iterativas en √°lgebra de matrices

El concepto de distancia en RnWDPELpQVHXVDSDUDGH√ÄQLUXQDFRWDGHXQDVXFHVLyQGH
vectores en este espacio.
DeÔ¨Ånici√≥n 7.5

n
¬∑
Se dice que una sucesi√≥n {x(k) }‚àû
k=1 de vectores en R converge a x respecto a la norma
si, dado cualquier Œµ > 0, existe un entero N (Œµ) tal que

x(k) ‚àí x < Œµ,
Teorema 7.6

para toda k ‚â• N (Œµ).

La sucesi√≥n de vectores {x(k) } converge a x en Rn respecto a la norma l‚àû si y s√≥lo si
l√≠m k‚Üí‚àû xi(k) = xi , para cada i = 1, 2, . . . , n.
Demostraci√≥n Suponga que {x(k) } converge a x respecto a la norma l‚àû. Dado cualquier

Œµ > 0, existe un entero N (Œµ) tal que para toda k ‚â• N (Œµ),
m√°x |xi(k) ‚àí xi

x(k) ‚àí x ‚àû < Œµ.

i=1,2,... ,n

Este resultado implica que |xi(k) ‚àí xi | < Œµ, para cada i = 1, 2, . . . , n, por lo que
l√≠mk‚Üí‚àû xi(k) = xi para cada i.
Por el contrario, suponga que l√≠mk‚Üí‚àû xi(k) = xi, para cada i = 1, 2, . . . , n. Para un
Œµ > 0, dado, sea Ni (Œµ) para cada i representa un entero con la propiedad de que

|xi(k) ‚àí xi | < Œµ,
siempre que k ‚â• Ni (Œµ).
DeÔ¨Åna N (Œµ) = m√°xi=1,2,... ,n Ni (Œµ). Si k ‚â• N (Œµ), entonces
m√°x |xi(k) ‚àí xi

x(k) ‚àí x ‚àû < Œµ.

i=1,2,... ,n

Esto implica que {x(k) } converge a x respecto a la norma l‚àû.
Ejemplo 3

Muestre que

x(k) = (x1(k) , x2(k) , x3(k) , x4(k) )t =

1 3
1, 2 + , 2 , e‚àík sen k
k k

t

.

converge a x = (1, 2, 0, 0)t respecto a la norma l‚àû.
Soluci√≥n Puesto que

l√≠m 1 = 1,

k‚Üí‚àû

l√≠m (2 + 1/k) = 2,

k‚Üí‚àû

l√≠m 3/k 2 = 0

k‚Üí‚àû

y

l√≠m e‚àík sen k = 0,

k‚Üí‚àû

el teorema 7.6 implica que la sucesi√≥n {x(k) } converge a (1, 2, 0, 0) t respecto a la norma l‚àû.
0RVWUDUGLUHFWDPHQWHTXHODVXFHVLyQHQHOHMHPSORFRQYHUJHD(1, 2, 0, 0) t respecto a
la norma l2HVEDVWDQWHFRPSOLFDGR(VPHMRUSUREDUHOVLJXLHQWHUHVXOWDGR\DSOLFDUORDHVWH
caso especial
Teorema 7.7

Para x ‚àà Rn ,

x ‚àû

x 2‚â§

‚àö
n x ‚àû.

7.1 Normas de vectores y matrices
Demostraci√≥n

325

Si x j es una coordenada de x de tal forma que x ‚àû = m√°x1‚â§i‚â§n |xi | = |x j |.

Entonces
n

x 2‚àû = |x j |2 = x 2j ‚â§

xi2

x 22 ,

i=1

y
x ‚àû

x 2.

Por lo que,
n

x
y x 2‚â§

2
2 =

n

xi2 ‚â§
i=1

‚àö
n x ‚àû.

x 2j = nx 2j = n||x||2‚àû ,
i=1

/D√ÄJXUDLOXVWUDHVWHUHVXOWDGRFXDQGRn 5 2.
Figura 7.3

x2
ix i ` < 1
1
ix i 2 < 1

21

1

x1
2
ix i ` < ‚àö
2

21

Ejemplo 4

(QHOHMHPSORHQFRQWUDPRVTXHODVXFHVLyQ{x(k) }, GH√ÄQLGDSRU

x(k) =

1 3
1, 2 + , 2 , e‚àík sen k
k k

t

,

converge a x = (1, 2, 0, 0)t respecto a la norma l‚àû. Muestre que esta sucesi√≥n tambi√©n
converge a x respecto a la norma l2.
Soluci√≥n

Dado cualquier Œµ > 0, existe un entero N (Œµ/2) con la propiedad de que

x(k) ‚àí x ‚àû <

Œµ
,
2

siempre que k ‚â• N (Œµ/2). Mediante el teorema 7.7, esto implica que
‚àö
x(k) ‚àí x 2 ‚â§ 4 x(k) ‚àí x ‚àû ‚â§ 2(Œµ/2) = Œµ,
cuando k ‚â• N (Œµ/2). Por lo que {x(k) } tambi√©n converge a x respecto a la norma l2.

326

CAP√çTULO 7

T√©cnicas iterativas en √°lgebra de matrices

Se puede mostrar que todas las normas en Rn son equivalentes respecto a la convergencia; es decir, si ¬∑ y ¬∑ son dos normas cualquiera sobre Rn y {x(k) }‚àû
k=1 tiene el l√≠mite x
¬∑
respecto a ¬∑ , entonces {x(k) }‚àû
tambi√©n
tiene
el
l√≠mite
x
respecto
a
. La prueba de este
k=1
hecho para el caso general se puede encontrar en [Or2], p. 8. El caso para las normas l2 y l‚àû
se siguen el teorema 7.7.

Normas matriciales y distancias
En las siguientes secciones de este cap√≠tulo y en cap√≠tulos posteriores, necesitaremos m√©todos para determinar la distancia entre matrices n 3 n. Para ello se requiere el concepto de
norma.
DeÔ¨Ånici√≥n 7.8

Una norma matricialVREUHHOFRQMXQWRGHODVPDWULFHVn 3 n es una funci√≥n de valor real
¬∑ GH√ÄQLGDHQHVWHFRQMXQWRTXHVHFXPSOHSDUDWRGDVODVPDWULFHVA y B n 3 n y todos los
n√∫meros reales Œ±:

i)

A

0;

ii)

A

0, si y s√≥lo si A es O, la matriz con todas las entradas 0;

iii)

Œ±A

iv)

A+B

v)

AB

Œ± A ;
A
A

B ;

B .

La distancia entre matrices A y B n 3 n respecto a esta norma matricial es A ‚àí B .
A pesar de que las normas matriciales se pueden obtener de diversas maneras, las que se
consideran con mayor frecuencia son las que son consecuencias naturales de las normas de
vectores l2 y l‚àû.
(VWDVQRUPDVVHGH√ÄQHQDOXWLOL]DUHOVLJXLHQWHWHRUHPDFX\DSUXHEDVHFRQVLGHUDHQHO
HMHUFLFLR
Teorema 7.9

Si ¬∑ , es una norma vectorial en Rn, entonces

m√°x Ax

A

(7.2)

1

x

es una norma matricial.
Cada norma vectorial produce
una norma matricial natural
asociada.

/DVQRUPDVPDWULFLDOHVGH√ÄQLGDVSRUQRUPDVYHFWRULDOHVUHFLEHQHOQRPEUHGHnormas
matriciales naturales, o inducidas, asociadas con la norma del vector. En este texto, se asuPLUiTXHWRGDVODVQRUPDVPDWULFLDOHVVRQQDWXUDOHVDPHQRVTXHVHHVSHFL√ÄTXHORFRQWUDULR
Para cualquier z = 0, el vector x = z/ z es un vector unitario. Por lo tanto,

m√°x Ax
x

1

z
z

m√°x A
z=0

= m√°x
z=0

Az
,
z

y, alternativamente, podemos escribir

A

m√°x
z=0

Az
.
z

(OVLJXLHQWHFRURODULRSDUDHOWHRUHPDVLJXHHVWDUHSUHVHQWDFLyQGH A .
Corolario 7.10

para cualquier vector z = 0, matriz A y cualquier norma natural ¬∑ , tenemos

Az

A ¬∑ z .

(7.3)

7.1 Normas de vectores y matrices

327

/DPHGLGDGDGDDXQDPDWUL]EDMRODQRUPDQDWXUDOGHVFULEHODIRUPDHQODTXHODPDWUL]
extiende los vectores unitarios relativos a esa norma. La extensi√≥n m√°xima es la norma de la
matriz. Las normas de la matriz que consideraremos tienen las formas

A ‚àû = m√°x

Ax ‚àû ,

la norma l‚àû,

A 2 = m√°x

Ax 2 ,

la norma l2.

x ‚àû =1

y
x 2 =1

Una ilustraci√≥n de estas normas cuando n 5VHPXHVWUDHQODV√ÄJXUDV\SDUD
la matriz

A=

0
2

‚àí2
.
0

Figura 7.4
x2

x2
ÂÇ®x ÂÇ® ` 5 1
1

ÂÇ®AÂÇ® `

x

Ax

Ax para
ÂÇ®x ÂÇ® ` 5 1

1

1
21

2

x1

22

21

1

2

x1

21

21

22

Figura 7.5
x2
3
x2
Ax

ÂÇ®x ÂÇ® 2 5 1
1
21

Ax para
ÂÇ® x ÂÇ®2 5 1

ÂÇ®AÂÇ® 2

1

x
1
21

x1

22

21

1
21

23

2

x1

328

CAP√çTULO 7

T√©cnicas iterativas en √°lgebra de matrices

La norma l‚àû de una matriz se puede calcular f√°cilmente a partir de las entradas de la
matriz.
n

Teorema 7.11

Si A = (ai j ) es una matriz n 3 n, entonces A ‚àû = m√°x

|ai j |.

1‚â§i‚â§n

j=1

n

Demostraci√≥n

Primero, mostramos que A ‚àû ‚â§ m√°x

1‚â§i‚â§n

|ai j |.
j=1

x ‚àû = m√°x1‚â§i‚â§n |xi |. Puesto que Ax tambi√©n

Sea x un vector n-dimensional con 1
es un vector n-dimensional

n
1‚â§i‚â§n

Pero m√°x1‚â§ j‚â§n |x j

n

ai j x j ‚â§ m√°x

Ax ‚àû = m√°x |(Ax)i | = m√°x

1‚â§i‚â§n

1‚â§i‚â§n

j=1

|ai j | m√°x |x j |.
1‚â§ j‚â§n

j=1

x ‚àû = 1, por lo que
n

Ax ‚àû ‚â§ m√°x

1‚â§i‚â§n

|ai j |,
j=1

y, por consiguiente,
n

A ‚àû = m√°x

Ax ‚àû ‚â§ m√°x

x ‚àû =1

1‚â§i‚â§n

|ai j |.

(7.4)

j=1

Ahora mostraremos la desigualdad opuesta. Si p es un entero con
n

n

|a pj | = m√°x

1‚â§i‚â§n

j=1

|ai j |
j=1

y x es el vector con componentes

xj =

1, si a pj ‚â• 0,
‚àí1, si a pj < 0.

Entonces x ‚àû = 1 y a pj x j = |a pj |, para todas las j = 1, 2, . . . , n, por lo que
n

Ax ‚àû = m√°x

n

ai j x j ‚â•

1‚â§i‚â§n

j=1

n

n

a pj x j =
j=1

|a pj | = m√°x

1‚â§i‚â§n

j=1

|ai j |.
j=1

Este resultado implica que
n

A ‚àû = m√°x

x ‚àû =1

Ax ‚àû ‚â• m√°x

1‚â§i‚â§n

|ai j |.
j=1

n

Al unir esto con la desigualdad (7.4) obtenemos

A ‚àû = m√°x

1‚â§i‚â§n

|ai j |.
j=1

7.2 Eigenvalores y eigenvectores

Ejemplo 5

329

Determine A ‚àû para la matriz

‚é°

1
A=‚é£ 0
5
Soluci√≥n

‚é§
‚àí1
‚àí1 ‚é¶ .
1

2
3
‚àí1

Tenemos
3

3

|a1 j | = |1| + |2| + | ‚àí 1| = 4,
j=1

|a2 j | = |0| + |3| + | ‚àí 1| = 4,
j=1

y
3

|a3 j | = |5| + | ‚àí 1| + |1| = 7.
j=1

Por lo que el teorema 7.11 implica que A ‚àû = m√°x{4, 4, 7} = 7.
En la siguiente secci√≥n, veremos un m√©todo alternativo para encontrar la norma l2 de
una matriz.
La secci√≥n Conjunto de ejercicios 7.1 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

7.2 Eigenvalores y eigenvectores
Una matriz n 3 m se puede considerar como una funci√≥n que utiliza multiplicaci√≥n de matrices para tomar vectores columna m-dimensionales en vectores columna n-dimensionales. Por
lo que, una matriz n 3 m es, en realidad, una funci√≥n lineal de Rm a Rn. Una matriz cuadrada
$ WRPD HO FRQMXQWR GH YHFWRUHV n-dimensionales en s√≠ misma, lo cual provee una funci√≥n
lineal de Rm a Rn. En este caso, ciertos vectores x diferentes de cero podr√≠an ser paralelos a
AxORTXHVLJQL√ÄFDTXHH[LVWHXQDFRQVWDQWHl con Ax = Œªx. Para estos vectores, tenemos
(A ‚àí ŒªI )x = 0. Existe una conexi√≥n cercana entre los valores de l y la probabilidad de que
un m√©todo iterativo converger√°. Nosotros consideraremos la conexi√≥n en esta secci√≥n.
DeÔ¨Ånici√≥n 7.12

Si A es una matriz cuadrada, el polinomio caracter√≠stico de AHVWiGH√ÄQLGRSRU

p(Œª) = det(A ‚àí ŒªI ).
1RHVGLItFLOGHPRVWUDU FRQVXOWHHOHMHUFLFLR TXHp es un polinomio de en√©simo grado
y, por consiguiente, tiene por lo menos n ceros diferentes, algunos de los cuales podr√≠an ser
FRPSOHMRV6Ll es un cero de p, entonces, puesto que det(A ‚àí ŒªI ) = 0, el teorema 6.17 en
ODSiJLQDLPSOLFDTXHHOVLVWHPDOLQHDOGH√ÄQLGRSRU (A ‚àí ŒªI )x = 0 tiene una soluci√≥n
con x = 0. Nos gustar√≠a estudiar los ceros de p y las soluciones diferentes a cero correspondientes a estos sistemas.
DeÔ¨Ånici√≥n 7.13
(OSUH√ÄMReigen proviene del
DGMHWLYRDOHPiQTXHVLJQL√ÄFD
‚Äúpropio‚Äù y en ingl√©s es sin√≥nimo
de la palabra caracter√≠stico. Cada
matriz tiene una eigenecuaci√≥n
o caracter√≠stica propia, con
eigenvalores o funciones
caracter√≠sticos correspondientes.

Si p es el polinomio caracter√≠stico de la matriz A, los ceros de p reciben el nombre de
eigenvalores, o valores caracter√≠sticos, de la matriz A. Si l es un eigenvalor de A y x = 0
satisface (A ‚àí ŒªI )x = 0, entonces x es un eigenvector, o vector caracter√≠stico, de A correspondiente al eigenvalor l.
Para determinar los eigenvalores de una matriz, podemos utilizar el hecho de que
‚Ä¢ l es un eigenvalor de A si y s√≥lo si det(A ‚àí ŒªI ) = 0.

330

CAP√çTULO 7

T√©cnicas iterativas en √°lgebra de matrices

Una vez que se ha encontrado el eigenvalor l, un eigenvector correspondiente x = 0 se
determina al resolver el sistema
‚Ä¢ (A ‚àí ŒªI )x = 0.
Ejemplo 1

Muestre que no hay vectores x diferentes a cero en R2 con Ax paralelo a x si

A=
Soluci√≥n

0
‚àí1

1
0

.

Los eigenvalores de A son los ceros del polinomio caracter√≠stico

0 = det(A ‚àí ŒªI ) = det

‚àíŒª
‚àí1

1
‚àíŒª

= Œª2 + 1,

por lo que los eigenvalores de AVRQORVQ~PHURVFRPSOHMRV Œª1 = i y Œª2 = ‚àíi. Un eigenvector x correspondiente a Œª1 necesita satisfacer

0
0

=

‚àíi
‚àí1

1
‚àíi

x1
x2

=

‚àíi x1 + x2
‚àíx1 ‚àí i x2

,

es decir, 0 = ‚àíi x 1 + x2 , por lo que x2 = i x1 , y 0 = ‚àíx1 ‚àí i x2 . Por lo tanto, si x es un eigenvector de AHQWRQFHVH[DFWDPHQWHXQRGHVXVFRPSRQHQWHVHVUHDO\HORWURHVFRPSOHMR
Por consiguiente, no hay vectores x diferentes de cero en R2 con Ax paralela a x.
Si x es un eigenvector asociado con el eigenvalor real l, entonces Ax = Œªx, por lo que
la matriz A transforma el vector x en un m√∫ltiplo escalar de s√≠ mismo.
‚Ä¢ Si l es real y Œª > 1, entonces A tiene el efecto de expandir x en un factor de l, como se
LOXVWUDHQOD√ÄJXUDD 
‚Ä¢ Si 0 < Œª < 1, entonces A comprime x en un factor de l FRQVXOWHOD√ÄJXUDE 
‚Ä¢ Si Œª < 0, ORVHIHFWRVVRQVLPLODUHV FRQVXOWHOD√ÄJXUDF \G DSHVDUGHTXHODGLUHFci√≥n de Ax est√° invertida.
Figura 7.6
a) l . 1

b) 1 . l . 0

c) l , 21

d) 21 , l , 0

Ax
x

x

x

x

Ax

Ax
Ax
Ax 5 lx

Tambi√©n observe que si x es un eigenvector de A asociado con el eigenvalor l y a es
cualquier constante diferente a cero, entonces ax tambi√©n es un eigenvector ya que

A(Œ±x) = Œ±( Ax) = Œ±(Œªx) = Œª(Œ±x).
Una consecuencia importante de esto es que para cualquier norma vectorial || ¬∑ ||, podr√≠amos
seleccionar Œ± = ¬±||x||‚àí1 , lo cual resultar√≠a en ax como el eigenvector con norma 1. Por lo que,
‚Ä¢ Para todos los eigenvalores y cualquier norma vectorial, existen eigenvectores con norma 1.

7.2 Eigenvalores y eigenvectores

Ejemplo 2

331

Determine los eigenvalores y eigenvectores para la matriz
‚é°
‚é§
2
0 0
1 2 ‚é¶.
A=‚é£ 1
1 ‚àí1 4
Soluci√≥n

El polinomio caracter√≠stico de A es
‚é°

2‚àíŒª
p(Œª) = det(A ‚àí ŒªI ) = det ‚é£ 1
1

0
1‚àíŒª
‚àí1

‚é§
0
2 ‚é¶
4‚àíŒª

= ‚àí (Œª3 ‚àí 7Œª2 + 16Œª ‚àí 12) = ‚àí(Œª ‚àí 3)(Œª ‚àí 2)2 ,
por lo que existen dos eigenvalores de A: Œª1 = 3 y Œª2 = 2.
Un eigenvector x1 correspondiente al eigenvalor Œª1 = 3 es una soluci√≥n para la ecuaci√≥n de vector-matriz (A ‚àí 3 ¬∑ I )x1 = 0, por lo que
‚é° ‚é§ ‚é°
‚é§ ‚é°
‚é§
0
‚àí1
0 0
x1
‚é£ 0 ‚é¶ = ‚é£ 1 ‚àí2 2 ‚é¶ ¬∑ ‚é£ x2 ‚é¶ ,
0
1 ‚àí1 1
x3
lo cual implica que x1 = 0 y x2 = x3 .
Cualquier valor diferente de cero de x3 produce un eigenvector para el eigenvalor
Œª1 = 3. 3RUHMHPSORFXDQGR x3 = 1, tenemos el eigenvector x1 = (0, 1, 1)t , y cualquier
eigenvector de A correspondiente a Œª = 3 es un m√∫ltiplo diferente a cero de x1.
Un eigenvector x = 0 de A asociado con Œª2 = 2 es una soluci√≥n del sistema
(A ‚àí 2 ¬∑ I )x = 0, por lo que

‚é°

‚é§ ‚é°
0
0
‚é£ 0 ‚é¶=‚é£ 1
0
1

‚é§ ‚é°
‚é§
0 0
x1
‚àí1 2 ‚é¶ ¬∑ ‚é£ x2 ‚é¶ .
‚àí1 2
x3

En este caso, el eigenvector s√≥lo tiene que satisfacer la ecuaci√≥n

x1 ‚àí x2 + 2x3 = 0,
OR FXDO VH SXHGH UHDOL]DU GH GLIHUHQWHV IRUPDV 3RU HMHPSOR FXDQGR x1 = 0, tenemos
x2 = 2x3 , por lo que una elecci√≥n ser√≠a x2 = (0, 2, 1)t . Tambi√©n podr√≠amos seleccionar x2 = 0, lo cual requiere que x1 = ‚àí2x3 . Por lo tanto, x3 = (‚àí2, 0, 1)t da un segundo
eigenvector para el eigenvalor Œª2 = 2 que no es un m√∫ltiplo de x2. Los eigenvectores de A
correspondientes al eigenvalor Œª2 = 2 generan un plano entero. Este plano se describe
mediante todos los vectores de la forma

Œ±x2 + Œ≤x3 = (‚àí2Œ≤, 2Œ±, Œ± + Œ≤)t ,
para constantes arbitrarias a y b, siempre y cuando al menos una de las constantes sea
diferente de cero.
Las nociones de los eigenvalores y los eigenvectores se introducen aqu√≠ para convenienFLDFRPSXWDFLRQDOHVSHFt√ÄFDSHURHVWRVFRQFHSWRVVXUJHQFRQIUHFXHQFLDHQHOHVWXGLRGH
VLVWHPDVItVLFRV'HKHFKRSXHVWRTXHVRQEDVWDQWHLQWHUHVDQWHVHOFDStWXORHVWiGHGLFDGR
a su aproximaci√≥n num√©rica.

Radio espectral
DeÔ¨Ånici√≥n 7.14

El radio espectral œÅ( A) de una matriz AHVWiGH√ÄQLGRSRU

œÅ(A) = m√°x |Œª|, donde l es un eigenvalor de A.

332

CAP√çTULO 7

T√©cnicas iterativas en √°lgebra de matrices

(Para Œª = Œ± + Œ≤i, FRPSOHMRGH√ÄQLPRV|Œª| = (Œ± 2 + Œ≤ 2 )1/2 .)
3DUDODPDWUL]FRQVLGHUDGDHQHOHMHPSORœÅ( A) = m√°x{2, 3} = 3.
El radio espectral est√° estrechamente relacionado con la norma de una matriz, como se
muestra en el siguiente teorema.
Teorema 7.15

Si A es una matriz n 3 n, entonces

i)

A 2 = [œÅ(At A)]1/2 ,

ii)

œÅ(A)

A , para cualquier norma natural ¬∑ .

Demostraci√≥n La demostraci√≥n de la parte i) requiere m√°s informaci√≥n respecto a los eigenvalores actualmente disponibles. Para los detalles relacionados con la demostraci√≥n, consulte [Or2], p. 21.
Para probar la parte ii), suponga que l es un eigenvalor de A con eigenvector x y
x
1. Entonces Ax = Œªx y

|Œª| = |Œª| ¬∑ x

Œªx

Ax

A

œÅ(A) = m√°x |Œª

A .

A .

x

Por lo tanto,

La parte i) del teorema 7.15 implica que si A es sim√©trica, entonces A 2 = œÅ( A) (conVXOWHHOHMHUFLFLR
Un resultado interesante y √∫til, que es similar a la parte ii) del teorema 7.15, es que para
cualquier matriz A y cualquier Œµ > 0, existe una norma natural ¬∑ con la propiedad de que
œÅ(A) < A < œÅ( A) + Œµ. Por consiguiente, œÅ( A) es la cota inferior m√°s grande para las
normas naturales en A. La prueba de este resultado se puede encontrar en [Or2], p. 23.
Ejemplo 3

Determine la norma l2 de

‚é°

‚é§
1 1 0
A = ‚é£ 1 2 1 ‚é¶.
‚àí1 1 2
Para aplicar el teorema 7.15, necesitamos calcular œÅ(At A), por lo que primero
necesitamos los eigenvalores de At A:
‚é°
‚é§‚é°
‚é§ ‚é°
‚é§
1 1 ‚àí1
1 1 0
3 2 ‚àí1
1 ‚é¶‚é£ 1 2 1 ‚é¶ = ‚é£ 2 6
4 ‚é¶.
At A = ‚é£ 1 2
0 1
2
‚àí1 1 2
‚àí1 4
5
Soluci√≥n

Si

‚é°

‚é§
3‚àíŒª
2
‚àí1
6‚àíŒª
4 ‚é¶
0 = det(At A ‚àí ŒªI ) = det ‚é£ 2
‚àí1
4
5‚àíŒª
= ‚àí Œª3 + 14Œª2 ‚àí 42Œª = ‚àíŒª(Œª2 ‚àí 14Œª + 42),
mediante Œª = 0 o Œª = 7 ¬±
||A||2 =

‚àö
7. Mediante el teorema 7.15, tenemos

œÅ(At A) =

m√°x{0, 7 ‚àí

‚àö

7, 7 +

‚àö

7} =

7+

‚àö

7 ‚âà 3.106.

7.2 Eigenvalores y eigenvectores

333

Matrices convergentes
Al estudiar t√©cnicas de matrices iterativas, es especialmente importante saber cu√°ndo las potencias de una matriz se vuelven peque√±as (es decir, cuando todas las entradas se aproximan
a cero). Las matrices de este tipo reciben el nombre de convergentes.
DeÔ¨Ånici√≥n 7.16

Llamamos convergente a una matriz A n 3 n si

l√≠m (Ak )i j = 0,

para cada i = 1, 2, . . . , n y j = 1, 2, . . . , n.

k‚Üí‚àû

Ejemplo 4

Muestre que
1
2
1
4

A=

0
1
2

es una matriz convergente.
Soluci√≥n

Al calcular las potencias de A, obtenemos

A2 =

1
4
1
4

0

,

1
4

A3 =

1
8
3
16

0

,

1
8

A4 =

1
16
1
8

0
1
16

,

y, en general,

Ak =

( 12 )k

0

k

( 12 )k

2k+1

.

Por lo que A es una matriz convergente porque

l√≠m

k‚Üí‚àû

1
2

k

=0

y

l√≠m

k

k‚Üí‚àû 2k+1

= 0.

Observe que la matriz convergente AHQHOHMHPSORWLHQHœÅ(A) = 12 porque 12 es el √∫nico
eigenvalor de A. Esto ilustra una conexi√≥n importante que existe entre el radio espectral de
una matriz y la convergencia de la matriz, como se detalla en el siguiente resultado.
Teorema 7.17

Las siguientes declaraciones son equivalentes

i)

A es una matriz convergente

ii)

l√≠mn‚Üí‚àû An

iii)

l√≠mn‚Üí‚àû A

n

iv)

œÅ(A) < 1.

v)

l√≠mn‚Üí‚àû An x = 0, para cada x.

0, para alguna norma natural.
0, para todas las normas naturales.

La demostraci√≥n de este teorema se puede encontrar en [IK], p. 14.
La secci√≥n Conjunto de ejercicios 7.2 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

334

CAP√çTULO 7

T√©cnicas iterativas en √°lgebra de matrices

7.3 T√©cnicas iterativas de Jacobi y Gauss-Siedel
En esta secci√≥n describimos los m√©todos iterativos de Jacobi y Gauss-Siedel, m√©todos cl√°VLFRV TXH GDWDQ GH √ÄQDOHV GHO VLJOR XVIII. Las t√©cnicas iterativas casi nunca se usan para
UHVROYHUVLVWHPDVOLQHDOHVGHGLPHQVLRQHVSHTXHxDV\DTXHHOWLHPSRUHTXHULGRSDUDVX√ÄFLHQte precisi√≥n excede el requerido para las t√©cnicas directas, como la eliminaci√≥n gaussiana.
3DUDJUDQGHVVLVWHPDVFRQXQDOWRSRUFHQWDMHGHHQWUDGDVVLQHPEDUJRHVWDVWpFQLFDVVRQ
H√ÄFLHQWHVHQW√©rminos tanto de almacenamiento como de c√°lculo computacional. Los sistemas de este tipo surgen con frecuencia en an√°lisis de circuitos y en la soluci√≥n num√©rica de
problemas de valor en la frontera y ecuaciones diferenciales parciales.
Una t√©cnica iterativa para resolver el sistema lineal n 3 n Ax = b inicia con una aproximaci√≥n x(0) para la soluci√≥n x y genera una sucesi√≥n de vectores {x(k) }‚àû
k=0 que convergen a x.

M√©todo de Jacobi
El m√©todo iterativo de Jacobi se obtiene al resolver la i-√©sima ecuaci√≥n en Ax = b para xi
para obtener (siempre que aii = 0)
n

xi =

‚àí
j=1
j=i

ai j x j
aii

+

bi
,
aii
(k)

Para cada k ‚â• 1, genere los componentes xi
por medio de
‚é°

xi(k) =

para i = 1, 2, . . . , n.

de x(k) a partir de los componentes de x(k‚àí1)

‚é§

‚é•
1 ‚é¢
‚é¢
‚àí ai j x (k‚àí1)
+ bi ‚é•
j
‚é£
‚é¶,
aii j=1
n

para i = 1, 2, . . . , n.

(7.5)

j=i

Ejemplo 1

El sistema lineal Ax = b dado por

E 1 : 10x1 ‚àí x2 + 2x3
= 6,
E 2 : ‚àíx1 + 11x2 ‚àí x3 + 3x4 = 25,
E 3 : 2x1 ‚àí x2 + 10x3 ‚àí x4 = ‚àí11,
E4 :
Carl Gustav Jacob Jacobi
¬≤ IXHUHFRQRFLGRHQ
SULPHUOXJDUSRUVXWUDEDMRHQ
el √°rea de la teor√≠a de n√∫meros
y funciones el√≠pticas, pero
sus intereses matem√°ticos y
capacidades eran muy amplias.
Ten√≠a una personalidad fuerte
TXHLQ√ÅX\yHQHOHVWDEOHFLPLHQWR
de una actitud orientada hacia la
investigaci√≥n que se convirti√≥ en
el centro del resurgimiento de las
matem√°ticas en las universidades
alemanas en el siglo XIX.

3x2 ‚àí

x3 + 8x4 = 15

tiene la soluci√≥n √∫nica x = (1, 2, ‚àí1, 1)t. Utilice la t√©cnica iterativa de Jacobi para encontrar
aproximaciones x(k) para x, que inicia con x(0) = (0, 0, 0, 0)t hasta

x(k) ‚àí x(k‚àí1) ‚àû
< 10‚àí3 .
x(k) ‚àû
Soluci√≥n

Primero resolvemos la ecuaci√≥n Ei para xi, para cada i 5D√ÄQGHREWHQHU

1
1
3
x2 ‚àí x3
+ ,
10
5
5
1
1
3
25
x2 =
x1
+ x3 ‚àí x4 + ,
11
11
11
11
1
1
1
11
x3 = ‚àí x1 + x2
+ x4 ‚àí ,
5
10
10
10
3
1
15
‚àí x2 + x3
+ .
x4 =
8
8
8

x1 =

7.3 T√©cnicas iterativas de Jacobi y Gauss-Siedel

335

A partir de la aproximaci√≥n inicial x(0) = (0, 0, 0, 0)t tenemos x(1) provista por

1 (0)
1
3
+
x2 ‚àí x3(0)
= 0.6000,
10
5
5
1 (0)
1
3
25
x
= 2.2727,
+ x3(0) ‚àí x4(0) +
x2(1) =
11 1
11
11
11
1
1
1
11
= ‚àí1.1000,
+ x4(0) ‚àí
x3(1) = ‚àí x1(0) + x2(0)
5
10
10
10
3
1
15
x4(1) =
‚àí x2(0) + x3(0)
+
= 1.8750.
8
8
8

x1(1) =

Las iteraciones adicionales, x(k) = (x1(k) , x2(k) , x3(k) , x4(k) )t , se generan de manera similar y se
presentan en la tabla 7.1.

Tabla 7.1
k

0

1

2

3

4

5

6

7

8

9

10

x1(k)
x2(k)
x3(k)
x4(k)

0.000
0.0000
0.0000
0.0000

0.6000
2.2727
‚àí1.1000
1.8750

1.0473
1.7159
‚àí0.8052
0.8852

0.9326
2.053
‚àí1.0493
1.1309

1.0152
1.9537
‚àí0.9681
0.9739

0.9890
2.0114
‚àí1.0103
1.0214

1.0032
1.9922
‚àí0.9945
0.9944

0.9981
2.0023
‚àí1.0020
1.0036

1.0006
1.9987
‚àí0.9990
0.9989

0.9997
2.0004
‚àí1.0004
1.0006

1.0001
1.9998
‚àí0.9998
0.9998

Nos detuvimos despu√©s de 10 iteraciones porque

x(10) ‚àí x(9) ‚àû
8.0 √ó 10‚àí4
< 10‚àí3 .
=
x(10) ‚àû
1.9998
De hecho, x(10) ‚àí x ‚àû = 0.0002.
En general, las t√©cnicas iterativas para resolver sistemas lineales implican un proceso
que convierte el sistema Ax = b en un sistema equivalente de la forma x = T x + c para una
PDWUL]√ÄMDT y vector c. Despu√©s de seleccionar el vector inicial x(0), la sucesi√≥n de los vectores soluci√≥n aproximados se generan al calcular

x(k) = T x(k‚àí1) + c,
para cada k = 1, 2, 3, . . . (VWR GHEHUtD UHFRUGDU OD LWHUDFLyQ GH SXQWR √ÄMR HVWXGLDGD HQ HO
cap√≠tulo 2.
El m√©todo de Jacobi se puede escribir en la forma x(k) = T x(k‚àí1) + c al dividir A en sus
partes diagonal o fuera de la diagonal. Para observar esto, permita que D sea la matriz diagonal cuyas entradas diagonales sean las de A, 2L es la parte estrictamente triangular inferior
de A y 2U es la parte estrictamente triangular superior de A. Con esta notaci√≥n,

‚é°

a11
‚é¢ a21
‚é¢
A=‚é¢ .
‚é£ ..

a12
a22
..
.

an1

an2

¬∑¬∑¬∑
¬∑¬∑¬∑

‚é§
a1n
a2n ‚é•
‚é•
.. ‚é•
. ‚é¶

¬∑ ¬∑ ¬∑ ann

336

CAP√çTULO 7

T√©cnicas iterativas en √°lgebra de matrices

se divide en
‚é§ ‚é° .
‚é§
‚é°
‚é§ ‚é°
0. . ‚àía
a11 0 .. .. .. . . . . . 0.
0 .. .. .. . . . . . . . . . . . 0.
. . 12 .. .... . . ‚àía.. 1n
.
.
.
.
.
.. .
..
.. ‚é• ‚é¢ ..
‚é•
‚é¢ 0 . . a . . . .. ‚é• ‚é¢ ‚àí a . . .
. . ..
22 .
..
.
‚é¢ .
‚é•
‚é¢ .
‚é• ‚é¢
.. 21 . . . .
.. ‚é•
.
.
.
‚àí
A = ‚é¢ .. . . . . . . . ... ‚é• ‚àí ‚é¢
.
.
‚é•
‚é¢
‚é•
.
..
..
.. ‚é¶ ‚é£ ..
.. ..
. . ‚àían‚àí1,n
..
‚é¶
‚é£ ..
‚é¶ ‚é£
0
.
.
.
.
.
.
.
.
.
.
.. ..
..
.
.
..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
0
0
0 ann
‚àían1 ‚àían,n‚àí1 0

= D ‚àí L ‚àí U.
Entonces, la ecuaci√≥n Ax = b, o (D ‚àí L ‚àí U )x = b, se transforma en

Dx = (L + U )x + b,
21

y, si existe D , es decir, si aii = 0 para cada i, entonces

x = D ‚àí1 (L + U )x + D ‚àí1 b.
Esto resulta en forma matricial de la t√©cnica iterativa de Jacobi:

x(k) = D ‚àí1 (L + U )x(k‚àí1) + D ‚àí1 b,

k = 1, 2, . . . .

(7.6)

Al introducir la notaci√≥n T j = D ‚àí1 (L + U ) y c j = D ‚àí1 b obtenemos la forma de la t√©cnica
de Jacobi

x(k) = T j x(k‚àí1) + c j .

(7.7)

En la pr√°ctica, la ecuaci√≥n (7.5) se usa en el c√°lculo y la ecuaci√≥n (7.7) para prop√≥sitos
te√≥ricos.
Ejemplo 2

Exprese el m√©todo de iteraci√≥n de Jacobi para el sistema lineal Ax = b dado por

E 1 : 10x1 ‚àí x2 + 2x3
= 6,
E 2 : ‚àíx1 + 11x2 ‚àí x3 + 3x4 = 25,
E 3 : 2x1 ‚àí x2 + 10x3 ‚àí x4 = ‚àí11,
3x2 ‚àí x3 + 8x4 = 15,
E4 :
en la forma x(k) = T x(k‚àí1) + c.
Soluci√≥n (QHOHMHPSORREVHUYDPRVTXHHOPpWRGRGH-DFRELSDUDHVWHVLVWHPDWLHQHOD

forma

1
1
3
x2 ‚àí x3
+ ,
10
5
5
1
1
3
25
x2 =
+ x3 ‚àí x4 + ,
x1
11
11
11
11
1
1
1
11
+ x4 ‚àí ,
x3 = ‚àí x1 + x2
5
10
10
10
3
1
15
x4 =
‚àí x2 + x3
+ .
8
8
8
x1 =

Por lo tanto, tenemos

‚é°

0

‚é¢
1
‚é¢
‚é¢ 11
T =‚é¢
‚é¢ ‚àí1
5
‚é£
0

1
10

‚àí 15

0

1
11

1
10

0

‚àí 38

1
8

0

‚é§

‚é•
3 ‚é•
‚àí 11
‚é•
‚é•
1 ‚é•
10 ‚é¶
0

‚é°
y

3
5

‚é§

‚é¢ 25 ‚é•
‚é¢
‚é•
‚é¢ 11 ‚é•
c=‚é¢
‚é•.
‚é¢ ‚àí 11 ‚é•
‚é£ 10 ‚é¶

El algoritmo 7.1 implementa la t√©cnica iterativa de Jacobi.

15
8

7.3 T√©cnicas iterativas de Jacobi y Gauss-Siedel

ALGORITMO

7.1

337

T√©cnica iterativa de Jacobi
Para resolver Ax = b dada una aproximaci√≥n inicial x(0) :

ENTRADA el n√∫mero de ecuaciones y valores desconocidos n; las entradas ai j , 1 ‚â§ i, j ‚â§ n
de la matriz A; las entradas bi , 1 ‚â§ i ‚â§ n de b; las entradas X Oi , 1 ‚â§ i ‚â§ n de XO = x(0) ;
tolerancia TOL; n√∫mero m√°ximo de iteraciones N.
SALIDA
la soluci√≥n aproximada x1 , . . . , xn o un mensaje que indica que se excedi√≥ el
n√∫mero de iteraciones.
Paso 1 Determine k = 1.
Paso 2 Mientras (k ‚â§ N ) haga los pasos 3‚Äì6.
Paso 3 Para i = 1, . . . , n
determine xi =

1
‚àí
aii

n
j=1 (ai j X O j ) + bi
j=i

.

Paso 4 Si||x ‚àí XO|| < TOL entonces SALIDA (x1 , . . . , xn );
(El procedimiento fue exitoso.)
PARE.
Paso 5 Determine k = k + 1.
Paso 6 Para i = 1, . . . , n determine X Oi = xi .
Paso 7 SALIDA (‚Äòn√∫mero m√°ximo de iteraciones excedido‚Äô);
(El procedimiento no fue exitoso.)
PARE.

El paso 3 del algoritmo requiere que aii = 0, para cada i = 1, 2, . . . , n. Si una de las
entradas ai i es 0 y el sistema es no singular, se puede realizar una reorganizaci√≥n de las ecuaciones de tal forma que aii = 0. Para acelerar la convergencia, las ecuaciones se deber√≠an
reordenar de tal forma que ai i resulte tan grande como sea posible. Este tema se analiza con
mayor detalle m√°s adelante en este cap√≠tulo.
Otro posible criterio para detenerse en el paso 4 es iterar hasta que

x(k) ‚àí x(k‚àí1)
x(k)
3KLOOLS/XGZLJ6LHGHO ¬≤
 WUDEDMyFRPRDVLVWHQWHGH
Jacobi resolviendo problemas
sobre sistemas de ecuaciones
lineales que derivaron del
WUDEDMRGH*DXVVVREUHPtQLPRV
cuadrados. En general, estas
ecuaciones tienen elementos
fuera de la diagonal que son
mucho m√°s peque√±os que los
de la diagonal, por lo
que los m√©todos iterativos son
especialmente efectivos. Las
t√©cnicas iterativas que en la
actualidad se conocen como
de Jacobi y Gauss-Siedel eran
previamente conocidas por
Gauss antes de aplicarse en
esta situaci√≥n; sin embargo, era
frecuente que los resultados
de Gauss no se difundieran
ampliamente.

es m√°s peque√±a que parte de la tolerancia prescrita. Para este prop√≥sito, se puede utilizar
cualquier norma conveniente, la m√°s com√∫n es la norma l‚àû.

El m√©todo Gauss-Siedel
8QDSRVLEOHPHMRUDHQHODOJRULWPRVHSXHGHREVHUYDUDOUHFRQVLGHUDUODHFXDFLyQ  
Los componentes de x(k‚àí1) se usaban para calcular los componentes xi(k) de x(k) . Pero, para
(k)
i > 1, los componentes x1(k) , . . . , xi‚àí1
de x(k) ya se han calculado y se espera que sean me(k‚àí1)
(k‚àí1)
, . . . , xi‚àí1
. Por
MRUHV DSUR[LPDFLRQHV SDUD ODV VROXFLRQHV UHDOHV x1 , . . . , xi‚àí1 que x1
(k)
lo tanto, parece razonable calcular xi usando estos valores calculados recientemente. Es
decir, utilizar
‚é°
‚é§
i‚àí1
n
1 ‚é£
‚àí
xi(k) =
ai j x (k)
‚àí
ai j x (k‚àí1)
+ bi ‚é¶ ,
(7.8)
j
j
aii
j=1
j=i+1
para cada i = 1, 2, . . . , n, HQOXJDUGHODHFXDFLyQ  (VWDPRGL√ÄFDFLyQUHFLEHHOQRPEUH
de t√©cnica iterativa Gauss-Siedel\VHLOXVWUDHQHOVLJXLHQWHHMHPSOR

338

CAP√çTULO 7

T√©cnicas iterativas en √°lgebra de matrices

Ejemplo 3

Utilice la t√©cnica iterativa Gauss-Siedel para encontrar soluciones aproximadas para

10x1 ‚àí

x2 + 2x3
= 6,
‚àíx1 + 11x2 ‚àí x3 + 3x4 = 25,
2x1 ‚àí x2 + 10x3 ‚àí x4 = ‚àí11,
3x2 ‚àí x3 + 8x4 = 15,

que inicia con x = (0, 0, 0, 0)t e iterando hasta que

x(k) ‚àí x(k‚àí1) ‚àû
< 10‚àí3 .
x(k) ‚àû
La soluci√≥n x = (1, 2, ‚àí1, 1)t se aproxim√≥ usando el m√©todo de Jacobi en el
HMHPSOR3DUDHOPpWRGR*DXVV6LHGHOHVFULELPRVHOVLVWHPDSDUDFDGDk = 1, 2, . . . como

Soluci√≥n

1 (k‚àí1)
1
3
‚àí x3(k‚àí1)
+ ,
x2
10
5
5
1 (k)
1 (k‚àí1)
3 (k‚àí1) 25
(k)
x
+ x3
‚àí x4
+ ,
x2 =
11 1
11
11
11
1
1
1
11
x3(k) = ‚àí x1(k) + x2(k)
+ x4(k‚àí1) ‚àí ,
5
10
10
10
3
1
15
‚àí x2(k) + x3(k)
+ .
x4(k) =
8
8
8

x1(k) =

Cuando x(0) = (0, 0, 0, 0)t , tenemos x(1) = (0.6000, 2.3272, ‚àí0.9873, 0.8789)t. Los valores de las iteraciones subsiguientes se muestran en la tabla 7.2.

Tabla 7.2

k

0

1

2

3

4

5

x1(k)
x2(k)
x3(k)
x4(k)

0.0000
0.0000
0.0000
0.0000

0.6000
2.3272
‚àí0.9873
0.8789

1.030
2.037
‚àí1.014
0.9844

1.0065
2.0036
‚àí1.0025
0.9983

1.0009
2.0003
‚àí1.0003
0.9999

1.0001
2.0000
‚àí1.0000
1.0000

Puesto que

x(5) ‚àí x(4) ‚àû
0.0008
= 4 √ó 10‚àí4 ,
=
(5)
x ‚àû
2.000
x(5) se acepta como aproximaci√≥n razonable para la soluci√≥n. Observe que el m√©todo de
-DFRELHQHOHMHPSORUHTXHUtDHOGREOHGHLWHUDFLRQHVSDUDODPLVPDSUHFLVLyQ

Para escribir el m√©todo de Gauss-Siedel en forma matricial, multiplique ambos lados de
la ecuaci√≥n (7.8) por ai i y recopile los k-√©simos t√©rminos iterados para obtener
(k‚àí1)
ai1 x1(k) + ai2 x2(k) + ¬∑ ¬∑ ¬∑ + aii xi(k) = ‚àíai,i+1 xi+1
‚àí ¬∑ ¬∑ ¬∑ ‚àí ain xn(k‚àí1) + bi ,

7.3 T√©cnicas iterativas de Jacobi y Gauss-Siedel

339

para cada i = 1, 2, . . . , n. Al escribir todas las n ecuaciones nos da

a11 x1(k)

= ‚àía12 x2(k‚àí1) ‚àí a13 x3(k‚àí1) ‚àí ¬∑ ¬∑ ¬∑ ‚àí a1n xn(k‚àí1) + b1 ,

a21 x1(k) + a22 x2(k)
..
.

=

‚àía23 x3(k‚àí1) ‚àí ¬∑ ¬∑ ¬∑ ‚àí a2n xn(k‚àí1) + b2 ,

an1 x1(k) + an2 x2(k) + ¬∑ ¬∑ ¬∑ + ann xn(k) =

bn ;

FRQODVGH√ÄQLFLRQHVGHD, L, y U proporcionadas previamente, tenemos el m√©todo GaussSiedel representado por

(D ‚àí L)x(k) = U x(k‚àí1) + b
y

x(k) = (D ‚àí L)‚àí1 U x(k‚àí1) + (D ‚àí L)‚àí1 b,

para cada k = 1, 2, . . . .



Si permitimos que Tg = (D ‚àí L)‚àí1 U y cg = (D ‚àí L)‚àí1 b, obtenemos la t√©cnica GaussSiedel de la forma

x(k) = Tg x(k‚àí1) + cg .

(7.10)

Para la matriz triangular inferior D 2 LQRVLQJXODUHVQHFHVDULR\VX√ÄFLHQWHTXHaii = 0, para
cada i = 1, 2, . . . , n.
El algoritmo 7.2 implementa el m√©todo Gauss-Siedel.

ALGORITMO

7.2

M√©todo iterativo Gauss-Siedel
Para resolver Ax = b dada una aproximaci√≥n inicial x(0):

ENTRADA el n√∫mero de ecuaciones y valores desconocidos n; las entradas ai j , 1 ‚â§ i, j ‚â§ n
de la matriz A; las entradas bi , 1 ‚â§ i ‚â§ n deb; las entradas X Oi , 1 ‚â§ i ‚â§ n de XO = x(0) ;
tolerancia TOL; n√∫mero m√°ximo de iteraciones N.
SALIDA la soluci√≥n aproximada x1 , . . . , xn o un mensaje que indica que se super√≥ el n√∫mero
de iteraciones.
Paso 1 Determine k = 1.
Paso 2 Mientras (k ‚â§ N ) haga los pasos 3‚Äì6.
Paso 3 Para i = 1, . . . , n

‚é°
‚é§
i‚àí1
n
1 ‚é£
Determine xi =
‚àí
ai j x j ‚àí
ai j X O j + bi ‚é¶.
aii
j=1
j=i+1

Paso 4 Si||x ‚àí XO|| < TOL entonces SALIDA (x1 , . . . , xn );
(El procedimiento fue exitoso.)
PARE.
Paso 5 Determine k = k + 1.
Paso 6 Para i = 1, . . . , n determine X Oi = xi .
Paso 7 SALIDA (‚ÄòN√∫mero m√°ximo de interacciones excedido‚Äô);
(El procedimiento no fue exitoso.)
PARE.

340

CAP√çTULO 7

T√©cnicas iterativas en √°lgebra de matrices

Los comentarios que siguen al algoritmo 7.1 respecto a los criterios de reorganizaci√≥n e
interrupci√≥n tambi√©n se aplican al algoritmo 7.2 de Gauss-Siedel.
/RVUHVXOWDGRVGHORVHMHPSORV\SDUHFHQLPSOLFDUTXHHOPpWRGR*DXVV6LHGHOHV
superior al m√©todo de Jacobi. Esto casi siempre es verdad, pero existen sistemas lineales para
ORVTXHHOPpWRGRGH-DFRELFRQYHUJH\HOPpWRGRGH*DXVV6LHGHOQR FRQVXOWHHOHMHUFLFLR
\ 

M√©todos de iteraci√≥n general
Para estudiar la convergencia de t√©cnicas de iteraci√≥n general, necesitamos analizar la
f√≥rmula

x(k) = T x(k‚àí1) + c,

para cada k = 1, 2, . . . ,

donde x(0) es arbitraria. El siguiente lema y el teorema 7.17 en la p√°gina 333 dan la clave
para este estudio.
Lema 7.18

Si el radio espectral satisface œÅ(T ) < 1, entonces (I ‚àí T )‚àí1 existe, y

(I ‚àí T )‚àí1 = I + T + T 2 + ¬∑ ¬∑ ¬∑ =

‚àû

T j.
j=0

Demostraci√≥n Puesto que T x = Œªx es verdad precisamente cuando (I ‚àí T )x = (1 ‚àí Œª)x,
tenemos l como un eigenvalor de T precisamente cuando 1‚àí Œª es un eigenvalor de I 2 T.
Pero |Œª| ‚â§ œÅ(T ) < 1, por lo que Œª = 1 no es un eigenvalor de T y 0 no puede ser un eigenvalor de I 2 T. Por lo tanto, (I ‚àí T )‚àí1 existe.
Sea Sm = I + T + T 2 + ¬∑ ¬∑ ¬∑ + T m . Entonces

(I ‚àí T )Sm = (1 + T + T 2 + ¬∑ ¬∑ ¬∑ + T m ) ‚àí (T + T 2 + ¬∑ ¬∑ ¬∑ + T m+1 ) = I ‚àí T m+1 ,
y puesto que T es convergente, el teorema 7.17 implica que

l√≠m (I ‚àí T )Sm = l√≠m (I ‚àí T m+1 ) = I.

m‚Üí‚àû

m‚Üí‚àû

Por lo tanto, (I ‚àí T )‚àí1 = l√≠mm‚Üí‚àû Sm = I + T + T 2 + ¬∑ ¬∑ ¬∑ =
Teorema 7.19

‚àû
j
j=0 T .

Para cualquier x(0) ‚àà Rn , la sucesi√≥n {x(k) }‚àû
k=0GH√ÄQLGDSRU

x(k) = T x(k‚àí1) + c,

para cada k ‚â• 1,

converge a la soluci√≥n √∫nica de x = T x + c si y s√≥lo si œÅ(T ) < 1.
Demostraci√≥n Primero suponga que œÅ(T ) < 1. Entonces,

x(k) = T x(k‚àí1) + c
= T (T x(k‚àí2) + c) + c
= T 2 x(k‚àí2) + (T + I )c
..
.
= T k x(0) + (T k‚àí1 + ¬∑ ¬∑ ¬∑ + T + I )c.
Puesto que œÅ(T ) < 1, el teorema 7.17 implica que T es convergente y

l√≠m T k x(0) = 0.

k‚Üí‚àû

(7.11)

7.3 T√©cnicas iterativas de Jacobi y Gauss-Siedel

341

El lema 7.18 implica que

‚éõ
l√≠m x(k) = l√≠m T k x(0) + ‚éù

k‚Üí‚àû

k‚Üí‚àû

‚àû

‚éû
T j ‚é† c = 0 + (I ‚àí T )‚àí1 c = (I ‚àí T )‚àí1 c.

j=0

Por lo tanto, la sucesi√≥n {x(k) } converge al vector x ‚â° (I ‚àí T )‚àí1 c y x = T x + c.
Para probar lo contrario, mostraremos que para cualquier z ‚àà Rn , tenemos l√≠m k‚Üí‚àû
k
T z = 0. Con el teorema 7.17, esto es equivalente a œÅ(T ) < 1.
Sea z un vector arbitrario y x la √∫nica soluci√≥n para x = T x + c. 'H√ÄQDx(0) = x ‚àí z,
y, para k ‚â• 1, x(k) = T x(k‚àí1) + c. Entonces {x(k) } converge a x. Tambi√©n,

x ‚àí x(k) = (T x + c) ‚àí T x(k‚àí1) + c = T x ‚àí x(k‚àí1) ,
por lo que

x ‚àí x(k) = T x ‚àí x(k‚àí1) = T 2 x ‚àí x(k‚àí2) = ¬∑ ¬∑ ¬∑ = T k x ‚àí x(0) = T k z.
Por lo tanto, l√≠m k‚Üí‚àû T k z = l√≠mk‚Üí‚àû T k x ‚àí x(0) = l√≠mk‚Üí‚àû x ‚àí x(k) = 0.
Pero z ‚àà Rn era arbitrario, por lo que mediante el teorema 7.17, T es convergente y
œÅ(T ) < 1.
La prueba del siguiente corolario es similar a las pruebas en el corolario 2.5 en la p√°gina 47.
Se considera en el eMHUFLFLR
Corolario 7.20

Si T < 1 para cualquier norma matricial normal y c es un vector determinado, entonces
(k)
= T x(k‚àí1) + c converge, para cualquier x(0) ‚àà Rn ,
la sucesi√≥n {x(k) }‚àû
k=0GH√ÄQLGDSRU x
n
para un vector x ‚àà R , con x 5 T x 1 c, y las siguientes cotas de error se mantienen:

i)

x ‚àí x(k)

T k x(0) ‚àí x ;

ii)

x ‚àí x(k)

1

T k
T

x(1) ‚àí x(0) .

+HPRVREVHUYDGRTXHODVWpFQLFDVGH-DFREL\*DXVV6LHGHOVHSXHGHQHVFULELUFRPR

x(k) = T j x(k‚àí1) + c j

y x(k) = Tg x(k‚àí1) + cg

T j = D ‚àí1 (L + U )

y

usando las matrices

Tg = (D ‚àí L)‚àí1 U.

Si œÅ(T j ) o œÅ(Tg ) es menor a 1, entonces la sucesi√≥n correspondiente {x(k) }‚àû
k=0 converger√° a
la soluci√≥n x de Ax 5 b3RUHMHPSORHOHVTXHPDGH-DFRELWLHQH

x(k) = D ‚àí1 (L + U )x(k‚àí1) + D ‚àí1 b,
y si {x(k) }‚àû
k=0 converge a x, entonces

x = D ‚àí1 (L + U )x + D ‚àí1 b.
Esto implica que

Dx = (L + U )x + b y

(D ‚àí L ‚àí U )x = b.

Puesto que D 2 L 2 U 5 A, la soluci√≥n x satisface Ax 5 b.
$KRUDSRGHPRVSURSRUFLRQDUFRQGLFLRQHVGHVX√ÄFLHQFLDYHUL√ÄFDGDVIiFLOPHQWHSDUDFRQvergencia de los m√©todos de Jacobi y de Gauss-Siedel. (Para probar la convergencia para el
HVTXHPDGH-DFRELFRQVXOWHHOHMHUFLFLR\SDUDHOHVTXHPDGH*DXVV6LHGHOFRQVXOWH
[Or2], p. 120).

342

CAP√çTULO 7

T√©cnicas iterativas en √°lgebra de matrices

Teorema 7.21

Si A es estrictamente diagonalmente dominante, entonces para cualquier selecci√≥n de x(0),
tanto los m√©todos de Gauss-Siedel y de Jacobi dan sucesiones {x(k) }‚àû
k=0 que convergen a la
soluci√≥n √∫nica de Ax 5 b.
La relaci√≥n de la rapidez de convergencia para el radio espectral de la matriz de iteraci√≥n T se puede observar a partir del corolario 7.20. Las desigualdades se mantienen para
cualquier norma matricial natural, por lo que sigue la declaraci√≥n despu√©s del teorema 7.15
en la p√°gina 332 que

x(k) ‚àí x

œÅ(T )k x(0) ‚àí x .

(7.12)

Por lo tanto, nos gustar√≠a seleccionar la t√©cnica iterativa con œÅ(T ) < 1 m√≠nima para un
sistema particular Ax 5 b. No existen resultados generales para decir cu√°l de las dos t√©cnicas, Jacobi o Gauss-Siedel, ser√° m√°s exitosa para cualquier sistema lineal arbitraria. En
casos especiales, sin embargo, la respuesta es conocida, como se demuestra en el siguiente
WHRUHPD/DSUXHEDGHHVWHUHVXOWDGRVHSXHGHHQFRQWUDUHQ>\@S¬≤
Teorema 7.22

(Stein-Rosenberg)
Si ai j ‚â§ 0, para cada i = j, y aii > 0, para cada i 5 1, 2,   , n, entonces una y s√≥lo una de
las siguientes declaraciones es v√°lida:

i)
iii)

0 ‚â§ œÅ(Tg ) < œÅ(T j ) < 1;
œÅ(T j ) = œÅ(Tg ) = 0;

ii)
iv)

1 < œÅ(T j ) < œÅ(Tg );
œÅ(T j ) = œÅ(Tg ) = 1.

Para el caso especial descrito en el teorema 7.22, observamos, a partir de la parte i), que
cuando un m√©todo proporciona convergencia, entonces provee convergencia y el m√©todo de
Gauss-Siedel converge m√°s r√°pido que el m√©todo de Jacobi. La parte ii) indica que cuando
un m√©todo diverge, entonces ambos divergen y la divergencia es m√°s pronunciada para el
m√©todo Gauss-Siedel.
La secci√≥n Conjunto de ejercicios 7.3 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

7.4 T√©cnicas de relajaci√≥n para resolver sistemas lineales
En la secci√≥n 7.3 observamos que la tasa de convergencia de una t√©cnica iterativa depende
del radio espectral de la matriz relacionada con el m√©todo. Una forma de seleccionar un procedimiento para convergencia acelerada es seleccionar un m√©todo cuya matriz relacionada
tiene un radio espectral m√≠nimo. Antes de describir un procedimiento para seleccionar dicho
m√©todo, necesitamos introducir medios nuevos para medir la cantidad por la que una apro[LPDFLyQDODVROXFLyQGHXQVLVWHPDOLQHDOGL√ÄHUHGHODYHUGDGHUDVROXFLyQGHOVLVWHPD(O
PpWRGRXWLOL]DHOYHFWRUGHVFULWRHQODVLJXLHQWHGH√ÄQLFLyQ
DeÔ¨Ånici√≥n 7.23
/DSDODEUD¬¥UHVLGXDO¬µVLJQL√ÄFD
lo que sobra, por lo que es un
nombre adecuado para este
vector.

Suponga que xÃÉ ‚àà RnHVXQDDSUR[LPDFLyQDODVROXFLyQGHOVLVWHPDOLQHDOGH√ÄQLGRSRUAx 5 b.
El vector residual para xÃÉ respecto a este sistema es r = b ‚àí AxÃÉ.
En procedimientos como los m√©todos de Jacobi o Gauss-Siedel un vector residual est√°
relacionado con cada c√°lculo de un componente aproximado para el vector soluci√≥n. El verGDGHURREMHWLYRHVJHQHUDUXQDVXFHVLyQGHDSUR[LPDFLRQHVTXHFDXVDUiQTXHORVYHFWRUHV
UHVLGXDOHVFRQYHUMDQUiSLGDPHQWHDFHUR6XSRQJDTXHWHQHPRV

ri(k) = r1i(k) , r2i(k) , . . . , rni(k)

t

7.4

T√©cnicas de relajaci√≥n para resolver sistemas lineales

343

denota el vector residual para el m√©todo Gauss-Siedel correspondiente al vector soluci√≥n
aproximado xi(k)GH√ÄQLGRSRU
(k)
xi(k) = x1(k) , x2(k) , . . . , xi‚àí1
, xi(k‚àí1) , . . . , xn(k‚àí1) .
t

El m-√©simo componente de ri(k) es
(k)
rmi
= bm ‚àí

i‚àí1

am j x (k)
j ‚àí

n

am j x (k‚àí1)
,
j

(7.13)

j=i

j=1

o, de manera equivalente,
(k)
rmi
= bm ‚àí

i‚àí1

n

am j x (k)
j ‚àí

j=1

am j x (k‚àí1)
‚àí ami xi(k‚àí1) ,
j

j=i+1

para cada m = 1, 2, . . . , n.
En particular, el i-√©simo componente de ri(k) es

rii(k) = bi ‚àí

i‚àí1

n

ai j x (k)
j ‚àí

j=1

ai j x (k‚àí1)
‚àí aii xi(k‚àí1) ,
j

j=i+1

por lo que

aii xi(k‚àí1) + rii(k) = bi ‚àí

i‚àí1

n

ai j x (k)
j ‚àí

j=1

ai j x (k‚àí1)
.
j

(7.14)

j=i+1

Sin embargo, recuerde que el m√©todo Gauss-Siedel, xi(k) se selecciona como

‚é°
‚é§
i‚àí1
n
1
‚é£bi ‚àí
‚é¶,
xi(k) =
ai j x (k)
ai j x (k‚àí1)
j ‚àí
j
aii
j=1
j=i+1

(7.15)

por lo que la ecuaci√≥n (7.14) se puede reescribir como

aii xi(k‚àí1) + rii(k) = aii xi(k) .
Por consiguiente, el m√©todo Gauss-Siedel se puede caracterizar seleccionando xi(k) para satisfacer

xi(k) = xi(k‚àí1) +

rii(k)
.
aii

(7.16)

Podemos derivar otra conexi√≥n entre los vectores residuales y la t√©cnica de
(k)
(k)
= (x1(k) ,. . . ,
Gauss-Siedel. Considere el vector residual ri+1 , asociado con el vector xi+1
(k)
(k‚àí1)
xi(k) , xi+1
, . . . , xn(k‚àí1) )t . Mediante la ecuaci√≥n (7.13), el i-√©simo componente de ri+1 es
(k)
ri,i+1
= bi ‚àí

i

ai j x (k)
j ‚àí

j=1
i‚àí1

= bi ‚àí
j=1

n

ai j x (k‚àí1)
j

j=i+1

ai j x (k)
j ‚àí

n

j=i+1

ai j x (k‚àí1)
‚àí aii xi(k) .
j

7.4
Soluci√≥n

345

T√©cnicas de relajaci√≥n para resolver sistemas lineales

para cada k = 1, 2, . . . , las ecuaciones para el m√©todo de Gauss-Siedel son

x1(k) = ‚àí0.75x2(k‚àí1) + 6,
x2(k) = ‚àí0.75x1(k) + 0.25x3(k‚àí1) + 7.5,
x3(k) = 0.25x2(k) ‚àí 6,
y las ecuaciones para el m√©todo SOR con v 5 1.25 son

x1(k) = ‚àí0.25x1(k‚àí1) ‚àí 0.9375x2(k‚àí1) + 7.5,
x2(k) = ‚àí0.9375x1(k) ‚àí 0.25x2(k‚àí1) + 0.3125x3(k‚àí1) + 9.375,
x3(k) = 0.3125x2(k) ‚àí 0.25x3(k‚àí1) ‚àí 7.5.
Las primeras siete iteraciones para cada m√©todo se listan en las tablas 7.3 y 7.4. Para que
las iteraciones sean precisas para siete lugares decimales, el m√©todo de Gauss-Siedel requiere 34 iteraciones, en comparaci√≥n con las 14 del m√©todo SOR con v 5 1.25.

Tabla 7.3
k

0

1

2

3

4

5

6

7

x1(k)
x2(k)
x3(k)

1
1
1

5.250000
3.812500
‚àí5.046875

3.1406250
3.8828125
‚àí5.0292969

3.0878906
3.9267578
‚àí5.0183105

3.0549316
3.9542236
‚àí5.0114441

3.0343323
3.9713898
‚àí5.0071526

3.0214577
3.9821186
‚àí5.0044703

3.0134110
3.9888241
‚àí5.0027940

Tabla 7.4
k

0

1

2

3

4

5

6

7

x1(k)
x2(k)
x3(k)

1
1
1

6.3125000
3.5195313
‚àí6.6501465

2.6223145
3.9585266
‚àí4.6004238

3.1333027
4.0102646
‚àí5.0966863

2.9570512
4.0074838
‚àí4.9734897

3.0037211
4.0029250
‚àí5.0057135

2.9963276
4.0009262
‚àí4.9982822

3.0000498
4.0002586
‚àí5.0003486

Una pregunta obvia es c√≥mo se selecciona el valor adecuado de v cuando se usa el m√©todo SOR. A pesar de que no se conoce una respuesta completa a esta pregunta para el sistema
lineal n 3 n, los siguientes resultados se pueden utilizar en ciertas situaciones importantes.
Teorema 7.24

(Kahan)
Si aii = 0, para cada i = 1, 2, . . . , n, entonces œÅ(Tœâ ) ‚â• |œâ ‚àí 1|. Esto implica que el
m√©todo SOR puede converger s√≥lo si 0 < œâ < 2.
La demostraci√≥n de este teorema se considera en el ejercicio 13. La de los siguientes dos
resultados se puede encontrar en [Or2], p. 123-133. Estos resultados se usar√°n en el cap√≠tulo 12.

Teorema 7.25

(Ostrowski-Reich)
Si AHVXQDPDWUL]GH√ÄQLGDSRVLWLYD\ < œâ < 2 entonces el m√©todo SOR converge para
cualquier opci√≥n de vector aproximado inicial x(0).

Teorema 7.26

Si AHVGH√ÄQLGDSRVLWLYD\WULGLDJRQDOHQWRQFHVœÅ(Tg ) = [œÅ(T j )]2 < 1, y la selecci√≥n √≥ptima de v, para el m√©todo SOR es

œâ=

2
1+

1 ‚àí [œÅ(T j )]2

Con esta selecci√≥n de v, tenemos œÅ(Tœâ ) = œâ ‚àí 1.

.

346

CAP√çTULO 7

T√©cnicas iterativas en √°lgebra de matrices

Ejemplo 2

Encuentre la selecci√≥n √≥ptima de v, para el m√©todo SOR para la matriz

‚é°

4
A=‚é£ 3
0

3
4
‚àí1

‚é§
0
‚àí1 ‚é¶ .
4

Soluci√≥n Esta matriz es claramente triangular, por lo que podemos aplicar el resultado del
WHRUHPDVLWDPELpQSRGHPRVPRVWUDUTXHHVGH√ÄQLGDSRVLWLYD3XHVWRTXHODPDWUL]HV
VLPpWULFDHOWHRUHPDHQODSiJLQDHVWDEOHFHTXHHVGH√ÄQLGDSRVLWLYDVL\VyORVL
todas sus primeras submatrices principales tienen determinantes positivos. Esto se observa
f√°cilmente es este caso porque

4
3

det(A) = 24, det
Puesto que

‚é° 1
4

‚é¢
T j = D ‚àí1 (L + U ) = ‚é£ 0
0

0
1
4

0

0

3
4

‚é§‚é°

= 7, y det ([4]) = 4.

‚é§ ‚é°
‚àí3 0
0
0 1 ‚é¶ = ‚é£ ‚àí0.75
1 0
0

0
‚é•
0 ‚é¶ ‚é£ ‚àí3
1
0

‚é§
‚àí0.75 0
0
0.25 ‚é¶ ,
0.25 0

4

tenemos

‚é°

‚àíŒª
T j ‚àí ŒªI = ‚é£ ‚àí0.75
0

‚àí0.75
‚àíŒª
0.25

‚é§
0
0.25 ‚é¶ ,
‚àíŒª

por lo que
det(T j ‚àí ŒªI ) = ‚àíŒª(Œª2 ‚àí 0.625).
Por lo tanto,
œÅ(T j ) =

‚àö
0.625

y

œâ=

2
1+

1 ‚àí [œÅ(T j )]2

=

1+

‚àö

2
‚âà 1.24.
1 ‚àí 0.625

Esto explica la r√°pida convergencia obtenida en el ejemplo 1 cuando se utiliza v 5 1.25.
Cerramos esta secci√≥n con el algoritmo 7.3 para el m√©todo SOR.

ALGORITMO

7.3

SOR
Para resolver Ax 5 b dado el par√°metro v y una aproximaci√≥n inicial x(0):

ENTRADA el n√∫mero de ecuaciones y valores desconocidos n; las entradas ai j , 1 ‚â§ i, j ‚â§ n
de la matriz A; las entradas bi , 1 ‚â§ i ‚â§ n, de b; las entradas X Oi , 1 ‚â§ i ‚â§ n, de XO = x(0) ;
el par√°metro v; tolerancia TOL; el n√∫mero m√°ximo de iteraciones N.
SALIDA la soluci√≥n aproximada x1 , . . . , xn o un mensaje que indique que se super√≥ el
n√∫mero de iteraciones.
Paso 1 Determine k = 1.
Paso 2 Mientras (k ‚â§ N ) haga los pasos 3‚Äì6.

7.5 Cotas de error y reÔ¨Ånamiento iterativo

347

Paso 3 Para i = 1, . . . , n
determine xi = (1 ‚àí œâ)X Oi +
1
n
œâ ‚àí i‚àí1
.
j=1 ai j x j ‚àí
j=i+1 ai j X O j + bi
aii
Paso 4 Si||x ‚àí XO|| < TOL entonces SALIDA (x1 , . . . , xn );
(El procedimiento fue exitoso.)
PARE.
Paso 5 Determine k = k + 1.
Paso 6 Para i = 1, . . . , n determine X Oi = xi .
Paso 7 SALIDA (‚ÄòN√∫mero m√°ximo de interacciones alcanzado‚Äô);
(El procedimiento no fue exitoso.)
PARE.

La secci√≥n Conjunto de ejercicios 7.4 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

7.5 Cotas de error y reÔ¨Ånamiento iterativo
Parece intuitivamente razonable que si xÃÉ es una aproximaci√≥n a la soluci√≥n x de Ax 5 b y
el vector residual r = b ‚àí AxÃÉ tiene la propiedad de que r es peque√±a, entonces x ‚àí xÃÉ
tambi√©n ser√≠a peque√±a. A menudo, √©ste es el caso, pero ciertos sistemas, a menudo presentes
en la pr√°ctica carecen de esta propiedad.
Ejemplo 1

El sistema lineal Ax 5 b dado por

1
2
1.0001 2

x1
x2

=

3
3.0001

tiene la soluci√≥n √∫nica x = (1, 1)t . 'HWHUPLQHHOYHFWRUUHVLGXDOSDUDODDSUR[LPDFLyQGH√Äciente xÃÉ = (3, ‚àí0.0001)t .
Soluci√≥n

Tenemos

r = b ‚àí AxÃÉ =

3
3.0001

1
2
1.0001 2

‚àí

3
‚àí0.0001

=

0.0002
0

,

por lo que r ‚àû = 0.0002. A pesar de que la norma del vector residual es peque√±a, la aproximaci√≥n xÃÉ = (3, ‚àí0.0001)tHVREYLDPHQWHEDVWDQWHGH√ÄFLHQWHGHKHFKR x ‚àí xÃÉ ‚àû = 2.
/DGL√ÄFXOWDGHQHOHMHPSORVHH[SOLFDFRQIDFLOLGDGDOREVHUYDUTXHODVROXFLyQGHO
sistema representa la intersecci√≥n de las rectas

l1 :

x1 + 2x2 = 3

y

l2 :

1.0001x1 + 2x2 = 3.0001.

El punto (3, ‚àí0.0001) se encuentra en l2, y las rectas son casi paralelas. Esto implica
que (3, ‚àí0.0001) tambi√©n se encuentra cerca de l1DXQTXHGL√ÄHUHVLJQL√ÄFDWLYDPHQWHGHOD
VROXFLyQGHOVLVWHPDGHWHUPLQDGDSRUHOSXQWRGHLQWHUVHFFLyQ   &RQVXOWHOD√ÄJXUD

348

CAP√çTULO 7

T√©cnicas iterativas en √°lgebra de matrices

Figura 7.7

x2
2
(1, 1)

1

(3, 0)
1

l1 4
l2

(3, 20.0001)

x1

(OHMHPSORVHFRQVWUX\yFODUDPHQWHSDUDPRVWUDUODVGL√ÄFXOWDGHVTXHSXHGHQVXUJLU\
GHKHFKRVXUJHQ6LODVUHFWDVQRFRLQFLGHQSRUFRPSOHWRHVSHUDUtDPRVTXHXQYHFWRUUHVLdual peque√±o implique una aproximaci√≥n precisa.
En la situaci√≥n general no podemos depender de la geometr√≠a del sistema para proporcionar una indicaci√≥n de cu√°ndo pueden surgir los problemas. Sin embargo, podemos obtener esta informaci√≥n al considerar las normas de la matriz A y su inversa.
Teorema 7.27

Suponga que xÃÉ es una aproximaci√≥n a la soluci√≥n de Ax 5 b, A es una matriz no singular y
r es el vector residual para xÃÉ. Entonces, para cualquier norma natural,

r ¬∑ A‚àí1 ,

x ‚àí xÃÉ
y si x = 0 y b = 0,

x ‚àí xÃÉ
x

A ¬∑ A‚àí1

r
.
b

(7.20)

Puesto que r = b ‚àí AxÃÉ = Ax ‚àí AxÃÉ y A es no singular, tenemos
x ‚àí xÃÉ = A‚àí1 r. El corolario 7.10 en la p√°gina 326 implica que
Demostraci√≥n

A‚àí1 r

x ‚àí xÃÉ

Adem√°s, puesto que b = Ax, tenemos b

A‚àí1 ¬∑ r .
A ¬∑ x . Por lo que 1/ x

x ‚àí xÃÉ
A ¬∑ A‚àí1
‚â§
x
b

A / b y

r .

N√∫meros de condici√≥n
Las desigualdades en el teorema 7.27 implican que A‚àí1 y A ¬∑ A‚àí1 proveen una indicaci√≥n de la conexi√≥n entre el vector residual y la precisi√≥n de la aproximaci√≥n. En general
el error relativo x‚àí xÃÉ / x es de mayor inter√©s, y, mediante la desigualdad (7.20), este
error est√° acotado por el producto de A ¬∑ A‚àí1 con el residuo relativo de esta aproximaci√≥n, r / b . &XDOTXLHUQRUPDFRQYHQLHQWHVHSXHGHXVDUSDUDHVWDDSUR[LPDFLyQHO~QLFR
UHTXLVLWRHVTXHVHXVHFRQVWDQWHPHQWHGHSULQFLSLRD√ÄQ
DeÔ¨Ånici√≥n 7.28

El n√∫mero de condici√≥n de la matriz no singular A relativo a la norma ¬∑ es

K (A)

A ¬∑ A‚àí1 .

Con esta notaci√≥n, las desigualdades en el teorema 7.27 se convierten en

x ‚àí xÃÉ

K (A)

r
A

7.5 Cotas de error y reÔ¨Ånamiento iterativo

349

y
x ‚àí xÃÉ
r
‚â§ K (A)
.
x
b
Para cualquier matriz A no singular y norma natural ¬∑ ,

1

A ¬∑ A‚àí1

I

A ¬∑ A‚àí1

K (A).

Una matriz A est√° bien condicionada si K (A) est√° cerca de 1 y est√° mal condicionada cuando K (A HVVLJQL√ÄFDWLYDPHQWHPD\RUTXH(OFRQGLFLRQDPLHQWRHQHVWHFRQWH[WRVHUH√ÄHUH
a la seguridad relativa de que un vector residual peque√±o implica una soluci√≥n aproximada
correspondientemente precisa.
Ejemplo 2

Determine el n√∫mero de condici√≥n para la matriz

A=

1
2
1.0001 2

.

Soluci√≥n (Q HO HMHPSOR  REVHUYDPRV TXH OD PLVPD DSUR[LPDFLyQ GH√ÄFLHQWH 
‚àí0.0001)t para la soluci√≥n exacta (1, 1)t ten√≠a un vector residual con norma peque√±a,
por lo que deber√≠amos esperar que el n√∫mero de condici√≥n de A sea grande. Tenemos
A ‚àû = m√°x{|1| + |2|, |1.001| + |2|} = 3.0001, que no se podr√≠a considerar grande. Sin
embargo,

A‚àí1 =

‚àí10000
5000.5

10000
‚àí5000

,

as√≠

A‚àí1 ‚àû = 20000,

\SDUDODQRUPDGHLQ√ÄQLGDGK (A) = (20000)(3.0001) = 60002. El tama√±o del n√∫mero de
condici√≥n para este ejemplo deber√≠a evitar que tom√°ramos decisiones apresuradas con base
en el residuo de una aproximaci√≥n.
A pesar de que un n√∫mero de condici√≥n de una matriz depende solamente de las normas de
la matriz y su inversa, el c√°lculo de la inversa est√° sujeto a error de redondeo y depende
de la precisi√≥n con la que se realizan los c√°lculos. Si las operaciones implican aritm√©tica con t
d√≠gitos de precisi√≥n, el n√∫mero de condici√≥n aproximada para la matriz A es la norma de la
matriz multiplicada por la norma de la aproximaci√≥n para la inversa de A, que se obtiene a
trav√©s de aritm√©tica de tGtJLWRV'HKHFKRHVWHQ~PHURGHFRQGLFLyQWDPELpQGHSHQGHGHO
m√©todo utilizado para calcular la inversa de A. Adem√°s, debido al n√∫mero de c√°lculos necesarios para calcular la inversa, necesitamos ser capaces de calcular el n√∫mero de condici√≥n
sin determinar directamente la inversa.
Si suponemos que la soluci√≥n aproximada para el sistema lineal Ax 5 b se determina por
medio de la aritm√©tica de t d√≠gitos y la eliminaci√≥n gaussiana, es posible mostrar (consulte
[FM], p. 45-47) que el vector r para la aproximaci√≥n xÃÉ tiene

r

10‚àít A ¬∑ xÃÉ .

(7.21)

A partir de esta aproximaci√≥n se puede obtener un c√°lculo para el n√∫mero de condici√≥n
efectivo en aritm√©tica de t d√≠gitos sin necesidad de invertir la matriz A. En la actualidad esta
aproximaci√≥n supone que todas las operaciones aritm√©ticas en la t√©cnica de eliminaci√≥n
gaussiana se realizan usando la aritm√©tica de t d√≠gitos, pero que las operaciones necesarias
para determinar el residuo se realizan en aritm√©tica de doble precisi√≥n (es decir, 2t d√≠gitos).
(VWDWpFQLFDQRVHVXPDVLJQL√ÄFDWLYDPHQWHDOHVIXHU]RFRPSXWDFLRQDO\HOLPLQDODPD\RU
parte de la p√©rdida de precisi√≥n implicada con la resta de los n√∫meros casi iguales que se
presentan en el c√°lculo del residuo.
La aproximaci√≥n para el n√∫mero de condici√≥n K (A) con t d√≠gitos proviene de la consideraci√≥n del sistema lineal

Ay = r.

350

CAP√çTULO 7

T√©cnicas iterativas en √°lgebra de matrices

La soluci√≥n para este sistema se puede aproximar f√°cilmente porque los multiplicadores para
HOPpWRGRGHHOLPLQDFLyQJDXVVLDQD\DVHKDQFDOFXODGR3RUHOORA se puede factorizar de
la forma Pt L UFRPRVHGHVFULEHHQODVHFFLyQGHOFDStWXOR'HKHFKR yÃÉ, la soluci√≥n
aproximada de Ay = r, satisface

yÃÉ ‚âà A‚àí1 r = A‚àí1 (b ‚àí AxÃÉ) = A‚àí1 b ‚àí A‚àí1 AxÃÉ = x ‚àí xÃÉ,

(7.22)

y

x ‚âà xÃÉ + yÃÉ.
Por lo que yÃÉ es un c√°lculo del error producido cuando xÃÉ se aproxima a la soluci√≥n x del sistema original. Las ecuaciones (7.21) y (7.22) implican que

yÃÉ

x ‚àí xÃÉ

A‚àí1 r

A‚àí1 ¬∑ r

A‚àí1 (10‚àít A ¬∑ xÃÉ ) = 10‚àít xÃÉ K (A).

Esto nos da una aproximaci√≥n para el n√∫mero de condici√≥n que participa en la soluci√≥n
del sistema Ax 5 b usando eliminaci√≥n gaussiana y el tipo de t d√≠gitos de la aritm√©tica que
acabamos de describir:

K (A) ‚âà
Ilustraci√≥n

El sistema lineal dado por
‚é°
3.3330 15920
‚é£ 2.2220 16.710
1.5611 5.1791

yÃÉ
10t .
xÃÉ

(7.23)

‚é§‚é°
‚é§ ‚é°
‚é§
‚àí10.333
x1
15913
9.6120 ‚é¶ ‚é£ x2 ‚é¶ = ‚é£ 28.544 ‚é¶
x3
1.6852
8.4254

tiene la soluci√≥n exacta x = (1, 1, 1)t .
Mediante eliminaci√≥n gaussiana y aritm√©tica de redondeo de cinco d√≠gitos conduce
sucesivamente a las matrices aumentadas
‚é°
‚é§
3.3330
15920
‚àí10.333
15913
‚é£ 0
‚àí10596
16.501
10580 ‚é¶
0
‚àí7451.4
6.5250 ‚àí7444.9

y
‚é°

3.3330
‚é£ 0
0

15920
‚àí10596
0

‚àí10.333
16.501
‚àí5.0790

‚é§
15913
‚àí10580 ‚é¶ .
‚àí4.7000

La soluci√≥n aproximada para este sistema es

xÃÉ = (1.2001, 0.99991, 0.92538)t .
El vector residual correspondiente a xÃÉ se calcula con precisi√≥n doble como

r = b ‚àí AxÃÉ
‚é°
‚é§ ‚é°
‚é§‚é°
‚é§
15913
3.3330 15920 ‚àí10.333
1.2001
9.6120 ‚é¶ ‚é£ 0.99991 ‚é¶
= ‚é£ 28.544 ‚é¶ ‚àí ‚é£ 2.2220 16.710
8.4254
1.5611 5.1791
1.6852
0.92538
‚é§ ‚é°
‚é§ ‚é°
‚é§
‚é°
15913
15913.00518
‚àí0.00518
= ‚é£ 28.544 ‚é¶ ‚àí ‚é£ 28.26987086 ‚é¶ = ‚é£ 0.27412914 ‚é¶ ,
8.4254
8.611560367
‚àí0.186160367

7.5 Cotas de error y reÔ¨Ånamiento iterativo

351

Por lo que

r ‚àû = 0.27413.
El c√°lculo para el n√∫mero de condici√≥n provisto en el an√°lisis anterior se obtiene al
resolver primero el sistema Ay = r para yÃÉ:
‚é°
‚é§‚é°
‚é§ ‚é°
‚é§
3.3330 15920
‚àí10.333
y1
‚àí0.00518
‚é£ 2.2220 16.710
9.6120 ‚é¶ ‚é£ y2 ‚é¶ = ‚é£ 0.27413 ‚é¶ .
1.5611 5.1791
1.6852
y3
‚àí0.18616
Esto implica que yÃÉ = (‚àí0.20008, 8.9987 √ó 10‚àí5 , 0.074607)t . Mediante el c√°lculo en la
ecuaci√≥n (7.23) da

K (A) ‚âà

yÃÉ ‚àû 5
0.20008 5
10 = 16672.
10 =
1.2001
xÃÉ ‚àû

(7.24)

Para determinar el n√∫mero de condici√≥n exacto de A, primero debemos encontrar A21. Usando aritm√©tica de cinco d√≠gitos para los c√°lculos obtenemos la aproximaci√≥n

‚é°

‚àí1.1701 √ó 10‚àí4
‚àí1
A ‚âà ‚é£ 6.2782 √ó 10‚àí5
‚àí8.6631 √ó 10‚àí5

‚àí1.4983 √ó 10‚àí1
1.2124 √ó 10‚àí4
1.3846 √ó 10‚àí1

‚é§
8.5416 √ó 10‚àí1
‚àí3.0662 √ó 10‚àí4 ‚é¶ .
‚àí1.9689 √ó 10‚àí1

El teorema 7.11 en la p√°gina 328 implica que A‚àí1 ‚àû = 1.0041 y A ‚àû = 15934.
Como consecuencia, la matriz mal condicionada A tiene

K (A) = (1.0041)(15934) = 15999.
El c√°lculo en la ecuaci√≥n (7.24) es bastante cercano a K (A) y requiere considerablemente menos esfuerzo computacional.
Puesto que se conoce la soluci√≥n real x = (1, 1, 1)t para este sistema, podemos calcular
tanto

x ‚àí xÃÉ ‚àû = 0.2001 y

x ‚àí xÃÉ ‚àû
0.2001
= 0.2001.
=
x ‚àû
1

Las cotas de error dadas en el teorema 7.27 para estos valores son

x ‚àí xÃÉ ‚àû ‚â§ K (A)

r ‚àû
(15999)(0.27413)
= 0.27525
=
A ‚àû
15934

y
x ‚àí xÃÉ ‚àû
r ‚àû
(15999)(0.27413)
= 0.27561.
‚â§ K (A)
=
x ‚àû
b ‚àû
15913

352

CAP√çTULO 7

T√©cnicas iterativas en √°lgebra de matrices

ReÔ¨Ånamiento iterativo
En la ecuaci√≥n (7.22) utilizamos el c√°lculo yÃÉ ‚âà x ‚àí xÃÉ, donde yÃÉ es la soluci√≥n aproximada
para el sistema Ay = r. En general, xÃÉ+yÃÉ es una aproximaci√≥n m√°s precisa del sistema lineal
Ax 5 b que la original xÃÉ. El m√©todo que usa esta suposici√≥n recibe el nombre de UH√ÄQDmiento iterativo, o mejora iterativa, y consiste en realizar iteraciones sobre el sistema cuyo
ODGRGHUHFKRHVHOYHFWRUUHVLGXDOSDUDDSUR[LPDFLRQHVVXFHVLYDVKDVWDREWHQHUUHVXOWDGRV
precisos satisfactorios.
Si se aplica el proceso mediante aritm√©tica de t d√≠gitos y si K ‚àû (A) ‚âà 10q , entonces
despu√©s de k LWHUDFLRQHV GH UH√ÄQDPLHQWR LWHUDWLYR OD VROXFLyQ WLHQH DSUR[LPDGDPHQWH HO
d√≠gito m√°s peque√±o de t y k(t 2 q) d√≠gitos correctos. Si el sistema est√° bien condicionado,
una o dos iteraciones indicar√°n que la soluci√≥n es precisa. Existe la posibilidad de mejora
VLJQL√ÄFDWLYDHQVLVWHPDVPDOFRQGLFLRQDGRVDPHQRVTXHODPDWUL]A tambi√©n est√© tan mal
condicionada que K ‚àû (A) > 10t . En esa situaci√≥n, se usar√≠a la precisi√≥n incrementada para
ORVFiOFXORV(ODOJRULWPRLPSOHPHQWDHOPpWRGRGHUH√ÄQDPLHQWRLWHUDWLYR
ALGORITMO

7.4

ReÔ¨Ånamiento iterativo
Para aproximar la soluci√≥n del sistema lineal Ax 5 b:
ENTRADA el n√∫mero de ecuaciones y valores desconocidos n ODV HQWUDGDV
ai j , 1 ‚â§ i, j ‚â§ n de la matriz A ODV HQWUDGDV bi , 1 ‚â§ i ‚â§ n de b HO Q~PHUR Pi[LPR GH
iteraciones NWROHUDQFLDTOLQ~PHURGHGtJLWRVGHSUHFLVLyQt.
SALIDA la aproximaci√≥n xx = (x xi , . . . , x xn )t o un mensaje de que el n√∫mero de iteraciones fue excedido y una aproximaci√≥n COND para K ‚àû (A).

Paso 0 Resuelva el sistema Ax = b para x1 , . . . , xn usando eliminaci√≥n gaussiana al
guardar los multiplicadores m ji , j = i + 1, i + 2, . . . , n, i = 1, 2, . . . , n ‚àí 1
y observar los intercambios de fila.
Paso 1 Determine k = 1.
Paso 2 Mientras (k ‚â§ N ) haga los pasos 3‚Äì9.
Paso 3 Para i = 1, 2, . . . , n

(Calcule r.)
n

determine ri = bi ‚àí

ai j x j .
j=1

(Realice los c√°lculos en aritm√©tica de doble precisi√≥n .)
Paso 4 Resuelva el sistema lineal Ay = r mediante eliminaci√≥n gaussiana en
el mismo orden que en el paso 0.
Paso 5 Para i = 1, . . . , n determine x xi = xi + yi .
Paso 6 Sik = 1 entonces determine COND =

y ‚àû t
10 .
xx ‚àû

Paso 7 Si x ‚àí xx ‚àû < TOL entonces SALIDA(xx);
SALIDA ( COND);
(El procedimiento fue exitoso.)
PARE.
Paso 8 Determine k = k + 1.
Paso 9 Para i = 1, . . . , n determine xi = x xi .
Paso 10 SALIDA (‚ÄòN√∫mero m√°ximo de interacciones extedidas‚Äô);
SALIDA (COND);
(El procedimiento no fue exitoso.)
PARE.

7.5 Cotas de error y reÔ¨Ånamiento iterativo

353

Si se utiliza aritm√©tica de t d√≠gitos, un procedimiento recomendado para interrumpir el
SURFHVRHQHOSDVRHVLWHUDUKDVWDTXH|yi(k) | ‚â§ 10‚àít , para cada i = 1, 2, . . . , n.
Ilustraci√≥n

En nuestra ilustraci√≥n previa encontramos la aproximaci√≥n para el sistema lineal

‚é°

‚é§‚é°
‚é§ ‚é°
‚é§
‚àí10.333
x1
15913
9.6120 ‚é¶ ‚é£ x2 ‚é¶ = ‚é£ 28.544 ‚é¶
1.6852
8.4254
x3

3.3330 15920
‚é£ 2.2220 16.710
1.5611 5.1791

usando la aritm√©tica de cinco d√≠gitos y eliminaci√≥n gaussiana es

xÃÉ(1) = (1.2001, 0.99991, 0.92538)t
y la soluci√≥n para Ay = r(1) es

yÃÉ(1) = (‚àí0.20008, 8.9987 √ó 10‚àí5 , 0.074607)t .
Por el paso 5 en este algoritmo,

xÃÉ(2) = xÃÉ(1) + yÃÉ(1) = (1.0000, 1.0000, 0.99999)t ,
y el error real en esta aproximaci√≥n es

x ‚àí xÃÉ(2) ‚àû = 1 √ó 10‚àí5 .
Usando la t√©cnica para interrumpir el algoritmo, calculamos r(2) = b ‚àí AxÃÉ(2) y resolvemos
el sistema Ay(2) = r(2) , que nos da

yÃÉ(2) = (1.5002 √ó 10‚àí9 , 2.0951 √ó 10‚àí10 , 1.0000 √ó 10‚àí5 )t .
Puesto que yÃÉ(2) ‚àû ‚â§ 10‚àí5 , concluimos que

xÃÉ(3) = xÃÉ(2) + yÃÉ(2) = (1.0000, 1.0000, 1.0000)t
HVVX√ÄFLHQWHPHQWHSUHFLVRORFXDOHVVLQGXGDDOJXQDFRUUHFWR
$ORODUJRGHHVWDVHFFLyQVHKDVXSXHVWRTXHHQHOVLVWHPDOLQHDOAx 5 b, A y b se
pueden representar de forma exacta. Siendo realistas, las entradas ai j y b j, se alterar√≠an o
perturbar√≠an por una cantidad de Œ¥ai j y Œ¥b j , que causar√≠a que el sistema lineal

(A + Œ¥ A)x = b + Œ¥b
se resolviera en lugar de Ax 5 b. Normalmente, si Œ¥ A y Œ¥b son peque√±as (en el orden de
10-t), la aritm√©tica de t d√≠gitos produce una soluci√≥n xÃÉ para la que x ‚àí xÃÉ es correspondienWHPHQWHSHTXHxD6LQHPEDUJRHQFDVRGHVLVWHPDVPDOFRQGLFLRQDGRVKHPRVREVHUYDGR
que incluso si A y b se representan de manera exacta, los errores de redondeo pueden causar
que x ‚àí xÃÉ sea grande. El siguiente teorema relaciona las perturbaciones del sistema lineal
para el n√∫mero de condici√≥n de una matriz. La prueba de este resultado se puede encontrar
en [Or2], p. 33.
Teorema 7.29

Suponga que A es no singular y

Œ¥A <

1
.
A‚àí1

La soluci√≥n xÃÉ para (A + Œ¥ A)xÃÉ = b + Œ¥b aproxima la soluci√≥n x de Ax 5 b con el c√°lculo
de error

x ‚àí xÃÉ
K (A) A
‚â§
x
A
K (A) Œ¥ A

Œ¥b
Œ¥A
+
b
A

.

(7.25)

354

CAP√çTULO 7

T√©cnicas iterativas en √°lgebra de matrices

James Hardy Wilkinson (1919‚Äì
1986) es mejor conocido por su
amplio trabajo sobre m√©todos
num√©ricos para resolver
ecuaciones lineales y
problemas de eigenvalor.
Tambi√©n desarroll√≥ la t√©cnica de
an√°lisis de error regresivo.

El c√°lculo en la desigualdad (7.25) establece que si la matriz A est√° bien condicionada
(es decir, K (A) no es demasiado grande), entonces los cambios peque√±os en A y b producen
cambios proporcionalmente peque√±os en la soluci√≥n de x. Si, por otro lado, A est√° mal condicionada, entonces los cambios peque√±os en A y b pueden producir cambios grandes en x.
El teorema es independiente del procedimiento num√©rico particular que se us√≥ para resolver Ax 5 b. Se puede mostrar, mediante un an√°lisis de error regresivo (consulte [Wil1] o
[Wil2]), que si la eliminaci√≥n gaussiana con pivoteo se usa para resolver Ax 5 b con aritm√©tica de t d√≠gitos, la soluci√≥n num√©rica xÃÉ es la soluci√≥n real de un sistema lineal,

(A + Œ¥ A)xÃÉ = b,

donde Œ¥ A ‚àû ‚â§ f (n)101‚àít m√°x |ai(k)
j |,
i, j,k

para alguna funci√≥n f (n). En la pr√°ctica, Wilkinson descubri√≥ que f (n) ‚âà n y, en el peor de
los casos, que f (n) ‚â§ 1.01(n 3 + 3n 2 ).
La secci√≥n Conjunto de ejercicios 7.5 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

7.6 El m√©todo de gradiente conjugado

Magnus Hestenes (1906‚Äì1991)
y Eduard Steifel (1907‚Äì1998)
publicaron el art√≠culo original
sobre el m√©todo de gradiente
conjugado en 1952 mientras
trabajaban en el Instituto para
An√°lisis Num√©rico en el campus
de la UCLA.

El m√©todo de gradiente conjugado de Hestenes y Stiefel [HS] se desarroll√≥ originalmente
como un m√©todo directo dise√±ado para resolver un sistema lineal n 3 nGH√ÄQLGRSRVLWLYR
Como m√©todo directo, en general es inferior a la eliminaci√≥n gaussiana con pivoteo. Ambos
m√©todos requieren n pasos para determinar una soluci√≥n y los pasos para el m√©todo de gradiente conjugado son m√°s costosos computacionalmente que los de la eliminaci√≥n gaussiana.
Sin embargo, el m√©todo de gradiente conjugado es √∫til cuando se usa como m√©todo de
aproximaci√≥n iterativa para resolver los sistemas dispersos grandes con entradas diferentes a
cero que se presentan en patrones predecibles. Con frecuencia estos problemas surgen en la
VROXFLyQGHSUREOHPDVGHYDORUHVHQODIURQWHUD&XDQGRODPDWUL]KDVLGRSUHFRQGLFLRQDGD
SDUDUHDOL]DUFiOFXORVPiVH√ÄFLHQWHVVyORVHREWLHQHQEXHQRVUHVXOWDGRVHQDSUR[LPDGDPHQ‚àö
te n iteraciones. Empleado de esta manera, este m√©todo es preferible sobre la eliminaci√≥n
gaussiana y los m√©todos iterativos analizados previamente.
A lo largo de esta secci√≥n, suponemos que la matriz AHVGH√ÄQLGDSRVLWLYD8VDUHPRVOD
notaci√≥n del producto interno

x, y

xt y,

(7.26)

donde x y y son vectores n dimensionales. Tambi√©n necesitaremos algunos resultados est√°ndar adicionales a partir del √°lgebra lineal. Una revisi√≥n de este material se encuentra en
la secci√≥n 9.1.
El siguiente resultado sigue f√°cilmente las propiedades de transposiciones (consulte el
ejercicio 14).
Teorema 7.30

Para cualquier vector x, y y z y cualquier n√∫mero real a, tenemos

a)
c)

x, y
y, x ;
x + z, y
x, y

e)

x, x

z, y ;

b)
d)

Œ±x, y
x, x

x, Œ±y

Œ± x, y ;

0;

0 si y s√≥lo si x = 0.

Cuando A HV GH√ÄQLGD SRVLWLYD x, Ax
xt Ax > 0 a menos que x 5 0. Adem√°s,
t
t t
puesto que A es sim√©trica, tenemos x Ay = x A y = (Ax)t y, por lo que, adem√°s de los resultados en el teorema 7.30, tenemos, para cada x y y

x, Ay

(Ax)t y = xt At y = xt Ay

Ax, y .

(7.27)

7.6 El m√©todo de gradiente conjugado

355

(OVLJXLHQWHUHVXOWDGRHVXQDKHUUDPLHQWDEiVLFDHQHOGHVDUUROORGHOPpWRGRGHJUDGLHQWH
conjugado.
Teorema 7.31

El vector x‚àóHVXQDVROXFLyQSDUDHOVLVWHPDOLQHDOGH√ÄQLGRSRVLWLYRAx 5 b si y s√≥lo si x‚àó
produce el valor m√≠nimo de

g(x)
Demostraci√≥n

x, Ax

2 x, b .

Sean x y v = 0YHFWRUHV√ÄMRV\t una variable de n√∫mero real. Tenemos

g(x + tv)

x + tv, Ax + t Av

2 x + tv, b

x, Ax

t v, Ax

t x, Av

t 2 v, Av

2 x, b

x, Ax

2 x, b

2t v, Ax

2t v, b

t 2 v, Av ,

2t v, b

por lo que

g(x + tv) = g(x) ‚àí 2t v, b ‚àí Ax

t 2 v, Av .

(7.28)

Con x y v√ÄMRVSRGHPRVGH√ÄQLUODIXQFLyQFXDGUiWLFDh en t mediante

h(t) = g(x + tv).
Entonces h tiene un valor m√≠nimo cuando h (t) = 0 SRUTXHVXFRH√ÄFLHQWHt 2, v, Av , es
positivo. Puesto que

h (t) = ‚àí2 v, b ‚àí Ax

2t v, Av ,

el m√≠nimo se presenta cuando

tÃÇ =

v, b ‚àí Ax
,
v, Av

y, a partir de la ecuaci√≥n (7.28)

h( tÃÇ ) = g(x + tÃÇv)
= g(x) ‚àí 2tÃÇ v, b ‚àí Ax
= g(x) ‚àí 2
= g(x) ‚àí

tÃÇ 2 v, Av

v, b ‚àí Ax
v, b ‚àí Ax
v, Av

v, b ‚àí Ax
v, Av

2

v, Av

v, b ‚àí Ax 2
.
v, Av

As√≠, para cualquier vector v = 0, tenemos g(x + tÃÇv) < g(x) a menos que v, b ‚àí Ax
0,
en cuyo caso g(x) = g(x + tÃÇv). √âste es el resultado b√°sico que necesitamos para probar el
teorema 7.31.
0 para cualquier vector v y
Suponga que x‚àó satisface Ax‚àó = b. Entonces v, b ‚àí Ax‚àó
g(x QRVHSXHGHKDFHUPiVSHTXHxRTXHg(x‚àó ). Por lo tanto, x‚àó minimiza g.
Por otro lado, suponga que x‚àó es un vector que minimiza g. Entonces, para cualquier
0 . Esto implica que
vector v, tenemos g(x‚àó + tÃÇv) ‚â• g(x‚àó ). Por lo tanto, v, b ‚àí Ax‚àó
b ‚àí Ax‚àó = 0 y, por consiguiente, que Ax‚àó = b.
Para comenzar el m√©todo gradiente conjugado seleccionamos x, una soluci√≥n aproximada para Ax‚àó = b y v = 0, lo cual nos da una direcci√≥n de b√∫squeda para alejarnos de x con
HO√ÄQGHPHMRUDUODDSUR[LPDFLyQ6HDr = b ‚àí Ax el vector residual relacionado con x y

t=

v, b ‚àí Ax
v, r
=
.
v, Av
v, Av

356

CAP√çTULO 7

T√©cnicas iterativas en √°lgebra de matrices

Si r = 0 y v y r no son ortogonales, entonces x + tv da un valor m√°s peque√±o para g que g(x)
y es presumiblemente m√°s cercano a x‚àó que x. Esto sugiere el siguiente m√©todo.
Si x(0) es una aproximaci√≥n inicial para x‚àó y si v(1) = 0 es una direcci√≥n de b√∫squeda
inicial. Para k = 1, 2, 3, . . . , calculamos

tk =

v(k) , b ‚àí Ax(k‚àí1)
v(k) , Av(k)

x(k) = x(k‚àí1) + tk v(k)
y seleccionamos una direcci√≥n de b√∫squeda nueva v(k+1) . El objetivo es realizar esta selecci√≥n de tal forma que la sucesi√≥n de aproximaciones {x(k) } converja r√°pidamente a x‚àó.
Para seleccionar las direcciones de b√∫squeda, observamos g como una funci√≥n de los
componentes de x = (x1 , x2 , . . . , xn )t . Por lo tanto,
n

g(x1 , x2 , . . . , xn )

x, Ax

n

n

ai j xi x j ‚àí 2

2 x, b
i=1 j=1

xi bi .
i=1

Al tomar derivadas parciales respecto a las variables componentes xk obtenemos
n

‚àÇg
(x) = 2
aki xi ‚àí 2bk ,
‚àÇ xk
i=1
que es el k-√©simo componente del vector 2(Ax 2 b). Por lo tanto, el gradiente de g es

‚àÇg
‚àÇg
‚àÇg
(x),
(x), . . . ,
(x)
‚àÇ x1
‚àÇ x2
‚àÇ xn

‚àág(x) =

t

= 2(Ax ‚àí b) = ‚àí2r,

donde el vector r es el vector residual para x.
Del c√°lculo multivariable, sabemos que la direcci√≥n de mayor decrecimiento en el valor
de g(x) es la direcci√≥n dada por ‚àí‚àág(x), es decir, en la direcci√≥n del residuo r. El m√©todo
que selecciona

v(k+1) = r(k) = b ‚àí Ax(k)
recibe el nombre de m√©todo de descenso r√°pido. A pesar de que observaremos en la secci√≥n
10.4 que este m√©todo tiene m√©rito para problemas de sistemas no lineales y de optimizaci√≥n,
no se usa para sistemas lineales debido a la lenta convergencia.
Un enfoque alternativo utiliza un conjunto de vectores de direcci√≥n diferentes a cero
{v(1) , . . . , v(n) } que satisface

v(i) , Av( j)

0,

si i = j.

Esto se llama condici√≥n de ortogonalidad de A y se dice que el conjunto de los vectores
{v(1) , . . . , v(n) } es ortogonal a A. No es dif√≠cil mostrar que un conjunto de vectores ortogonales a A,DVRFLDGRVFRQODPDWUL]GH√ÄQLGDSRVLWLYDA, es linealmente independiente. (Consulte el ejercicio 15.) Este conjunto de direcciones de b√∫squeda da

tk =

v(k) , b ‚àí Ax(k‚àí1)
v(k) , r(k‚àí1)
=
v(k) , Av(k)
v(k) , Av(k)

y x(k) = x(k‚àí1) + tk v(k) .
El siguiente teorema muestra que esta selecci√≥n de direcciones de b√∫squeda provee
convergencia en la mayor parte de los n pasos, por lo que como m√©todo directo produce la
soluci√≥n exacta, al suponer que la aritm√©tica es exacta.

7.6 El m√©todo de gradiente conjugado

Teorema 7.32

357

Sea {v(1) , . . . , v(n) } un conjunto de vectores diferentes a cero ortogonal a A relacionados con
ODPDWUL]GH√ÄQLGDSRVLWLYDA y sea x(0)DUELWUDULR'H√ÄQD

tk =

v(k) , b ‚àí Ax(k‚àí1)
v(k) , Av(k)

y x(k) = x(k‚àí1) + tk v(k) ,

para k = 1, 2, . . . , n. Entonces, asumiendo la aritm√©tica exacta, Ax(n) = b.
Demostraci√≥n

Puesto que, para cada k = 1, 2, . . . , n, x(k) = x(k‚àí1) + tk v(k) , tenemos

Ax(n) = Ax(n‚àí1) + tn Av(n)
= (Ax(n‚àí2) + tn‚àí1 Av(n‚àí1) ) + tn Av(n)
..
.
= Ax(0) + t1 Av(1) + t2 Av(2) + ¬∑ ¬∑ ¬∑ + tn Av(n) .
Al restar b de este resultado obtenemos

Ax(n) ‚àí b = Ax(0) ‚àí b + t1 Av(1) + t2 Av(2) + ¬∑ ¬∑ ¬∑ + tn Av(n) .
$KRUDWRPDPRVHOSURGXFWRLQWHUQRDDPERVODGRVFRQHOYHFWRUv(k) y utilizamos las propieGDGHVGHORVSURGXFWRVLQWHUQRV\HOKHFKRGHTXHA es sim√©trica para obtener

Ax(n) ‚àí b, v(k)

Ax(0) ‚àí b, v(k)

t1 Av(1) , v(k)

tn Av(n) , v(k)

Ax(0) ‚àí b, v(k)

t1 v(1) , Av(k)

tn v(n) , Av(k) .

La propiedad de ortogonalidad de A provee, para cada k,
Ax(n) ‚àí b, v(k)

Sin embargo, tk v(k) , Av(k)

tk v(k) , Av(k)

Ax(0) ‚àí b, v(k)

tk v(k) , Av(k) .

(7.29)

v(k) , b ‚àí Ax(k‚àí1) , luego

v(k) , b ‚àí Ax(0) + Ax(0) ‚àí Ax(1) + ¬∑ ¬∑ ¬∑ ‚àí Ax(k‚àí2) + Ax(k‚àí2) ‚àí Ax(k‚àí1)
v(k) , b ‚àí Ax(0)

v(k) , Ax(0) ‚àí Ax(1)

v(k) , Ax(k‚àí2) ‚àí Ax(k‚àí1) .

Pero para cualquier i,

x(i) = x(i‚àí1) + ti v(i)

y

Ax(i) = Ax(i‚àí1) + ti Av(i) ,

por lo que
Ax(i‚àí1) ‚àí Ax(i) = ‚àíti Av(i) .
Por lo tanto,
tk v(k) , Av(k)

v(k) , b ‚àí Ax(0)

t1 v(k) , Av(1)

Puesto que la ortogonalidad de A v(k) , Av(i)

v(k) , Av(k) tk

tk‚àí1 v(k) , Av(k‚àí1) .

0, para cada i = k, entonces
v(k) , b ‚àí Ax(0) .

358

CAP√çTULO 7

T√©cnicas iterativas en √°lgebra de matrices

A partir de la ecuaci√≥n (7.29),

Ax(n) ‚àí b, v(k)

Ax(0) ‚àí b, v(k)

v(k) , b ‚àí Ax(0)

Ax(0) ‚àí b, v(k)

b ‚àí Ax(0) , v(k)

Ax(0) ‚àí b, v(k)

Ax(0) ‚àí b, v(k)

0.

(n)

Por lo tanto, el vector Ax ‚àíb es ortogonal al conjunto de vectores ortogonal de A
{v(1) , . . . , v(n) }. A partir de esto, sigue (consulte el ejercicio 15) que Ax(n) ‚àí b = 0, por lo
que Ax(n) = b.
Ejemplo 1

El sistema lineal

4x1 + 3x2
= 24,
3x1 + 4x2 ‚àí x3 = 30,
‚àí x2 + 4x3 = ‚àí24,
tiene la soluci√≥n exacta x‚àó = (3, 4, ‚àí5)t . Muestre que el procedimiento descrito en el teorema 7.32 con x(0) = (0, 0, 0)t produce esta soluci√≥n exacta despu√©s de tres iteraciones.
Soluci√≥n

(QHOHMHPSORGHODVHFFLyQHVWDEOHFLPRVODPDWUL]GHFRH√ÄFLHQWHV
‚é°
‚é§
4
3
0
4 ‚àí1 ‚é¶
A=‚é£ 3
0 ‚àí1
4

GHO VLVWHPD HV GH√ÄQLGD SRVLWLYD 6L v(1) = (1, 0, 0)t , v(2) = (‚àí3/4, 1, 0)t , y v(3) =
(‚àí3/7, 4/7, 1)t . Entonces

‚é°

v(1) , Av(2)

v(1) , Av(3)

‚é§ ‚é° 3 ‚é§
4
3
0
‚àí4
4 ‚àí1 ‚é¶ ‚é£ 1 ‚é¶ = 0,
v(1)t Av(2) = (1, 0, 0) ‚é£ 3
0 ‚àí1
4
0
‚é§
‚é°
‚é°
‚é§
‚àí 37
4
3
0
‚é•
‚é¢
4 ‚àí1 ‚é¶ ‚é£ 47 ‚é¶ = 0,
(1, 0, 0) ‚é£ 3
0 ‚àí1
4
1

y
‚é°

v(2) , Av(3)

4
3
‚àí , 1, 0 ‚é£ 3
4
0

‚é§ ‚é° ‚àí3 ‚é§
3
0
7
‚é•
‚é¢
4 ‚àí1 ‚é¶ ‚é£ 47 ‚é¶ = 0.
‚àí1
4
1

Por lo tanto, {v(1) , v(2) , v(3) } es un conjunto ortogonal de A.
Al aplicar las iteraciones descritas en el teorema 7.22 para A con x(0) = (0, 0, 0)t y
b = (24, 30, ‚àí24)t obtenemos

r(0) = b ‚àí Ax(0) = b = (24, 30, ‚àí24)t ,
por lo que
v(1) , r(0)

v(1)t r(0) = 24,

v(1) , Av(1)

4,

y

t0 =

24
= 6.
4

Por lo tanto,
x(1) = x(0) + t0 v(1) = (0, 0, 0)t + 6(1, 0, 0)t = (6, 0, 0)t .

7.6 El m√©todo de gradiente conjugado

359

Al continuar, tenemos
r(1) = b ‚àí Ax(1) = (0, 12, ‚àí24)t ,

t1 =

12
v(2) , r(1)
48
=
=
,
v(2) , Av(2)
7/4
7

x(2) = x(1) + t1 v(2) = (6, 0, 0)t +

48
7

3
‚àí , 1, 0
4

120
7

,

t2 =

r(2) = b ‚àí Ax(2) =

0, 0, ‚àí

t

=

6 48
, ,0
7 7

t

,

v(3) , r(2)
‚àí120/7
= ‚àí5,
=
v(3) , Av(3)
24/7

y

x(3) = x(2) + t2 v(3) =

6 48
, ,0
7 7

t

3 4
+ (‚àí5) ‚àí , , 1
7 7

t

= (3, 4, ‚àí5)t .

Puesto que aplicamos la t√©cnica n 5 3 veces, √©sta debe ser la soluci√≥n real.
Antes de analizar c√≥mo determinar el conjunto ortogonal a A, continuaremos con el desarrollo. El uso de un conjunto {v(1) , . . . , v(n) } de vectores de direcci√≥n ortogonal a A provee
lo que se conoce como m√©todo de direcci√≥n conjugada. El siguiente teorema muestra la
ortogonalidad de los vectores residuales r(k) y de los vectores de direcci√≥n v( j). Una prueba
de este resultado mediante inducci√≥n matem√°tica se considera en el ejercicio 16.
Teorema 7.33

Los vectores residuales r(k), donde k = 1, 2, . . . , n, para un m√©todo de direcci√≥n conjugada,
satisfacen las ecuaciones

r(k) , v( j)

0,

para cada

j = 1, 2, . . . , k.

El m√©todo de gradiente conjugado de Hestenes y Stiefel selecciona las direcciones de
b√∫squeda {v(k)} durante el proceso iterativo de tal forma que los vectores residuales {r(k)}
son mutuamente ortogonales. Para construir los vectores de direcci√≥n {v(1) , v(2) , . . . } y las
aproximaciones {x(1) , x(2) , . . . }, iniciamos con una aproximaci√≥n inicial x(0) y utilizamos
la direcci√≥n descendente m√°s pronunciada r(0) = b‚àí Ax(0) como la primera direcci√≥n de
b√∫squeda v(1).
Suponga que las direcciones conjugadas v(1) , . . . , v(k‚àí1) y las aproximaciones x(1) , . . . ,
(k‚àí1)
x
VHKDQFDOFXODGRFRQ

x(k‚àí1) = x(k‚àí2) + tk‚àí1 v(k‚àí1) ,
donde
v(i) , Av( j)

0

y

r(i) , r( j)

0,

para

i = j.

Si x(k‚àí1) es la soluci√≥n para Ax 5 b, terminamos. De lo contrario, r(k‚àí1) = b ‚àí Ax(k‚àí1) = 0,
0, para cada i = 1, 2, . . . , k ‚àí 1.
y el teorema 7.33 implica que r(k‚àí1) , v(i)
Utilizamos r(k‚àí1) para generar v(k)DOKDFHU

v(k) = r(k‚àí1) + sk‚àí1 v(k‚àí1) .
Queremos seleccionar sk‚àí1 de tal forma que

v(k‚àí1) , Av(k)

0.

Puesto que
Av(k) = Ar(k‚àí1) + sk‚àí1 Av(k‚àí1)
y
v(k‚àí1) , Av(k)

v(k‚àí1) , Ar(k‚àí1)

sk‚àí1 v(k‚àí1) , Av(k‚àí1) ,

360

CAP√çTULO 7

T√©cnicas iterativas en √°lgebra de matrices

tendremos v(k‚àí1) , Av(k)

0 cuando
sk‚àí1 = ‚àí

v(k‚àí1) , Ar(k‚àí1)
.
v(k‚àí1) , Av(k‚àí1)

0, para
Tambi√©n se puede mostrar que con esta selecci√≥n de sk‚àí1, tenemos v(k) , Av(i)
cada i = 1, 2, . . . , k ‚àí 2 (consulte [Lu], p. 245). Por lo tanto, {v(1) , . . . v(k) } es un conjunto
ortogonal a A.
$OKDEHUVHOHFFLRQDGRv(k) , calculamos
tk =

v(k) , r(k‚àí1)
r(k‚àí1) + sk‚àí1 v(k‚àí1) , r(k‚àí1)
=
v(k) , Av(k)
v(k) , Av(k)
=

r(k‚àí1) , r(k‚àí1)
v(k‚àí1) , r(k‚àí1)
+
s
.
k‚àí1
v(k) , Av(k)
v(k) , Av(k)

Mediante el teorema 7.33, v(k‚àí1) , r(k‚àí1)

0, por lo que

tk =

r(k‚àí1) , r(k‚àí1)
.
v(k) , Av(k)

(7.30)

Por lo tanto,
x(k) = x(k‚àí1) + tk v(k) .
Para calcular r(k) , multiplicamos A y restamos b para obtener

Ax(k) ‚àí b = Ax(k‚àí1) ‚àí b + tk Av(k)
o
r(k) = r(k‚àí1) ‚àí tk Av(k) .
Esto da
r(k) , r(k)

r(k‚àí1) , r(k)

tk Av(k) , r(k)

tk r(k) , Av(k) .

Adem√°s, a partir de la ecuaci√≥n (7.30)
r(k‚àí1) , r(k‚àí1)

tk v(k) , Av(k) ,

por lo que
sk = ‚àí

v(k) , Ar(k)
r(k) , Av(k)
r(k) , r(k)
(1/tk ) r(k) , r(k)
=
‚àí
=
=
.
v(k) , Av(k)
v(k) , Av(k)
(1/tk ) r(k‚àí1) , r(k‚àí1)
r(k‚àí1) , r(k‚àí1)

En resumen, tenemos
r(0) = b ‚àí Ax(0) ;

v(1) = r(0) ;

y para k = 1, 2, . . . , n,
tk =

r(k‚àí1) , r(k‚àí1)
r(k) , r(k)
, x(k) = x(k‚àí1) + tk v(k) , r(k) = r(k‚àí1) ‚àí tk Av(k) , sk = (k‚àí1) (k‚àí1) ,
(k)
(k)
v , Av
r
,r

y
v(k+1) = r(k) + sk v(k) .

(7.31)

7.6 El m√©todo de gradiente conjugado

361

Precondicionamiento

El precondicioamiento reemplaza
un sistema determinado por uno
que tiene las mismas soluciones,
pero con mejores caracter√≠sticas
de convergencia.

En lugar de presentar un algoritmo para el m√©todo de gradiente conjugado mediante estas
f√≥rmulas ampliamos el m√©todo para incluir precondicionamiento. Si la matriz A est√° mal
condicionada, el m√©todo de gradiente conjugado es altamente susceptible a errores de redondeo. Por lo tanto, a pesar de que se obtendr√≠a la respuesta exacta en los n pasos, normalmente, √©ste no es el caso. Como m√©todo directo, el m√©todo de gradiente conjugado no es tan
bueno como la eliminaci√≥n gaussiana con pivoteo. El uso principal del m√©todo de gradiente
conjugado es un m√©todo iterativo aplicado a un sistema mejor condicionado.
‚àö En este caso,
con frecuencia se obtiene una soluci√≥n aproximada aceptable alrededor de n pasos.
Cuando se usa precondicionamiento, el m√©todo de gradiente conjugado no se aplica
directamente a la matriz AVLQRDRWUDPDWUL]GH√ÄQLGDSRVLWLYDTXHWLHQHXQQ~PHURGHFRQGLFLyQPiVSHTXHxR1HFHVLWDPRVKDFHUHVWRGHWDOIRUPDTXHXQDYH]TXHVHHQFXHQWUDOD
soluci√≥n de este sistema, ser√° f√°cil obtener la soluci√≥n para el sistema original. La expectatiYDHVTXHHVWRUHGXFLUiHOHUURUGHUHGRQGHRDODSOLFDUHOPpWRGR3DUDPDQWHQHUODGH√ÄQLFLyQ
positiva de la matriz resultante, necesitamos multiplicar en cada lado por una matriz no
singular. Denotaremos esta matriz mediante C21 y consideraremos

AÃÉ = C ‚àí1 A(C ‚àí1 )t ,
con la esperanza de que AÃÉ tenga un n√∫mero de condici√≥n menor que A 3DUD VLPSOL√ÄFDU
t
la notaci√≥n, utilizamos notaci√≥n de matriz C ‚àít ‚â° C ‚àí1 . M√°s adelante en esta secci√≥n,
observaremos una forma razonable de seleccionar C, pero primero consideraremos el m√©todo de gradiente conjugado aplicado a AÃÉ.
Considere el sistema lineal

AÃÉxÃÉ = bÃÉ,
donde xÃÉ = C t x y bÃÉ = C ‚àí1 b. Entonces,

AÃÉxÃÉ = (C ‚àí1 AC ‚àít )(C t x) = C ‚àí1 Ax.
Por lo tanto, podemos resolver AÃÉxÃÉ = bÃÉ para xÃÉ y, despu√©s, obtener x al multiplicar por C ‚àít .
Sin embargo, en lugar de reescribir la ecuaci√≥n (7.31) mediante rÃÉ(k) , vÃÉ(k) , tÃÉk , xÃÉ(k) y sÃÉk , incluimos impl√≠citamente la precondici√≥n.
Puesto que

xÃÉ(k) = C t x(k) ,
tenemos
rÃÉ(k) = bÃÉ ‚àí AÃÉxÃÉ(k) = C ‚àí1 b ‚àí (C ‚àí1 AC ‚àít )C t x(k) = C ‚àí1 (b ‚àí Ax(k) ) = C ‚àí1 r(k) .
Si vÃÉ(k) = C t v(k) y w(k) = C ‚àí1 r(k) . Entonces
sÃÉk =

C ‚àí1 r(k) , C ‚àí1 r(k)
rÃÉ(k) , rÃÉ(k)
=
,
C ‚àí1 r(k‚àí1) , C ‚àí1 r(k‚àí1)
rÃÉ(k‚àí1) , rÃÉ(k‚àí1)

por lo que
sÃÉk =

w(k) , w(k)
.
w(k‚àí1) , w(k‚àí1)

Por lo tanto,
tÃÉk =

C ‚àí1 r(k‚àí1) , C ‚àí1 r(k‚àí1)
w(k‚àí1) , w(k‚àí1)
rÃÉ(k‚àí1) , rÃÉ(k‚àí1)
=
=
C t v(k) , C ‚àí1 AC ‚àít C t v(k)
C t v(k) , C ‚àí1 Av(k)
vÃÉ(k) , AÃÉvÃÉ(k)

(7.32)

362

CAP√çTULO 7

T√©cnicas iterativas en √°lgebra de matrices

y puesto que
C t v(k) , C ‚àí1 Av(k)

[C t v(k) ]t C ‚àí1 Av(k)
= [v(k) ]t CC ‚àí1 Av(k) = [v(k) ]t Av(k)

v(k) , Av(k) ,

tenemos
tÃÉk =

w(k‚àí1) , w(k‚àí1)
.
v(k) , Av(k)

(7.33)

Adem√°s
xÃÉ(k) = xÃÉ(k‚àí1) + tÃÉk vÃÉ(k) ,

entonces C t x(k) = C t x(k‚àí1) + tÃÉk C t v(k)

y
x(k) = x(k‚àí1) + tÃÉk v(k) .

(7.34)

Al continuar,
rÃÉ(k) = rÃÉ(k‚àí1) ‚àí tÃÉk AÃÉvÃÉ(k) ,
por lo tanto
C ‚àí1 r(k) = C ‚àí1 r(k‚àí1) ‚àí tÃÉk C ‚àí1 AC ‚àít vÃÉ (k) ,

r(k) = r(k‚àí1) ‚àí tÃÉk AC ‚àít C t v(k) ,

y
r(k) = r(k‚àí1) ‚àí tÃÉk Av(k) .

(7.35)

Finalmente,
vÃÉ(k+1) = rÃÉ(k) + sÃÉk vÃÉ(k)

y

C t v(k+1) = C ‚àí1 r(k) + sÃÉk C t v(k) ,

por lo que
v(k+1) = C ‚àít C ‚àí1 r(k) + sÃÉk v(k) = C ‚àít w(k) + sÃÉk v(k) .

(7.36)

El m√©todo gradiente conjugado precondicionado est√° basado en el uso de las ecuaciones
(7.32) a (7.36) en orden (7.33), (7.34), (7.35), (7.32) y (7.36). El algoritmo 7.5 implementa
este procedimiento.

ALGORITMO

7.5

M√©todo de gradiente conjugado precondicionado
Para resolver Ax 5 b dada la matriz precondicionada C ‚àí1 y la aproximaci√≥n inicial x(0) :
ENTRADA el n√∫mero de ecuaciones y valores desconocidos n ODV HQWUDGDV ai j ,
1 ‚â§ i, j ‚â§ n de la matriz A ODV HQWUDGDV b j , 1 ‚â§ j ‚â§ n del vector b ODV HQWUDGDV
Œ≥i j , 1 ‚â§ i, j ‚â§ n de la matriz precondicionada C ‚àí1, las entradas xi , 1 ‚â§ i ‚â§ n de la aproximaci√≥n inicial x = x(0) , el n√∫mero m√°ximo de iteraciones N; la tolerancia TOL.
SALIDA la soluci√≥n aproximada x1 , . . . xn y el residuo r1 , . . . rn o un mensaje de que se
excedi√≥ el n√∫mero de iteraciones.

Paso 1 Determine r = b ‚àí Ax; (Calcule r(0) )
w = C ‚àí1 r; (Nota: w = w(0) )
v = C ‚àít w; (Nota: v = v(1) )
Œ± = nj=1 w 2j .
Paso 2 Determine k = 1.
Paso 3 Mientras (k ‚â§ N ) haga los pasos 4‚Äì7.

7.6 El m√©todo de gradiente conjugado

363

Paso 3 Mientras (k ‚â§ N ) haga los pasos 4‚Äì7.
Paso 4 Si v < TOL, entonces
SALIDA (‚ÄòVector soluci√≥n‚Äô; x1 , . . . , xn );
SALIDA (‚ÄòCon residual‚Äô; r1 , . . . , rn );
(El procedimiento fue exitoso.)
PARE.
Paso 5 Determine u = Av; (Nota: u = Av(k) )
Œ±
t= n
; (Nota: t = tk )
j=1 v j u j
x = x + tv; (Nota: x = x(k) )
r = r ‚àí tu; (Nota: r = r(k) )
w = C ‚àí1 r; (Nota: w = w(k) )
w(k) , w(k) )
Œ≤ = nj=1 w 2j . (Nota: Œ≤
Paso 6 Si|Œ≤| < TOL entonces
si r < TOL entonces
SALIDA (‚ÄòVector soluci√≥n‚Äô; x1 , . . . , xn );
SALIDA (‚Äòcon residuo‚Äô; r1 , . . . , rn );
(El procedimiento fue exitoso. )
PARE.
Paso 7 Determine s = Œ≤/Œ±; (s = sk )
v = C ‚àít w + sv; (Nota: v = v(k+1) )
Œ± = Œ≤; (Actualice Œ±.)
k = k + 1.
Paso 8 Si (k > n) entonces
SALIDA (‚ÄòSe excedi√≥ el m√°ximo n√∫mero de iteraciones‚Äô);
(El procedimiento no fue exitoso.)
PARE.

El siguiente ejemplo ilustra los c√°lculos para un problema elemental.
Ejemplo 2

El sistema lineal Ax 5 b dado por

4x1 + 3x2
= 24,
3x1 + 4x2 ‚àí x3 = 30,
‚àí x2 + 4x3 = ‚àí24
tiene soluci√≥n (3, 4, ‚àí5)t . Utilice el m√©todo de gradiente conjugado con x(0) = (0, 0, 0)t y
sin precondicionamiento, es decir, con C = C ‚àí1 = I , para aproximar la soluci√≥n.
Soluci√≥n La soluci√≥n se consider√≥ en el ejemplo 2 de la secci√≥n 7.4 donde el m√©todo SOR
se utiliz√≥ con un valor casi √≥ptimo de v 5 1.25.
Para el m√©todo de gradiente conjugado iniciamos con

r(0) = b ‚àí Ax(0) = b = (24, 30, ‚àí24)t ;
w = C ‚àí1 r(0) = (24, 30, ‚àí24)t ;
v(1) = C ‚àít w = (24, 30, ‚àí24)t ;
Œ±

w, w

2052.

364

CAP√çTULO 7

T√©cnicas iterativas en √°lgebra de matrices

Iniciamos con la primera iteraci√≥n con k 5 1. Entonces,

u = Av(1) = (186.0, 216.0, ‚àí126.0)t ;
Œ±
= 0.1469072165;
t1 = (1)
v ,u
x(1) = x(0) + t1 v(1) = (3.525773196, 4.407216495, ‚àí3.525773196)t ;
r(1) = r(0) ‚àí t1 u = (‚àí3.32474227, ‚àí1.73195876, ‚àí5.48969072)t ;
w = C ‚àí1 r(1) = r(1) ;
Œ≤

w, w

s1 =

Œ≤
= 0.02153523222;
Œ±

44.19029651;

v(2) = C ‚àít w + s1 v(1) = (‚àí2.807896697, ‚àí1.085901793, ‚àí6.006536293)t .
Establezca
Œ± = Œ≤ = 44.19029651.
Para la segunda iteraci√≥n, tenemos

u = Av(2) = (‚àí14.48929217, ‚àí6.760760967, ‚àí22.94024338)t ;
t2 = 0.2378157558;
x

(2)

= (2.858011121, 4.148971939, ‚àí4.954222164)t ;

r(2) = (0.121039698, ‚àí0.124143281, ‚àí0.034139402)t ;
w = C ‚àí1 r(2) = r(2) ;
Œ≤ = 0.03122766148;
s2 = 0.0007066633163;
v(3) = (0.1190554504, ‚àí0.1249106480, ‚àí0.03838400086)t .
Determina Œ± = Œ≤ = 0.03122766148.
La tercera iteraci√≥n da

u = Av(3) = (0.1014898976, ‚àí0.1040922099, ‚àí0.0286253554)t ;
t3 = 1.192628008;
x(3) = (2.999999998, 4.000000002, ‚àí4.999999998)t ;
r(3) = (0.36 √ó 10‚àí8 , 0.39 √ó 10‚àí8 , ‚àí0.141 √ó 10‚àí8 )t .
Puesto que x(3) es aproximadamente la soluci√≥n exacta, el error de redondeo no afecta sigQL√ÄFDWLYDPHQWHHOUHVXOWDGR(QHOHMHPSORGHODVHFFLyQHOPpWRGR625FRQv 5 1.25
requer√≠a 14 iteraciones para una precisi√≥n de 1027. Sin embargo, se deber√≠a observar, en este
ejemplo que en verdad estamos comparando un m√©todo directo para m√©todos iterativos.
El siguiente ejemplo ilustra el efecto del precondicionamiento en una matriz pobremente
condicionada. En este ejemplo, usamos D ‚àí1/2 para representar la matriz diagonal cuyas entradas son los rec√≠procos de las ra√≠ces cuadradas de las entradas de la diagonal de la matriz A
GHFRH√ÄFLHQWHVeVWRVHXWLOL]DFRPRSUHFRQGLFLRQDGRU3XHVWRTXHODPDWUL]AHVGH√ÄQLGD
positiva, esperamos que los eigevalores de D ‚àí1/2 AD ‚àít/2 est√©n cerca de 1 con el resultado de
que el n√∫mero de condici√≥n de esta matriz ser√≠a relativamente peque√±o para el n√∫mero
de condici√≥n de A.

7.6 El m√©todo de gradiente conjugado

Ejemplo 3

365

Encuentre los eigenvalores y n√∫mero de condici√≥n de la matriz
‚é°
‚é§
0.2
0.1
1
1
0
‚é¢ 0.1
‚é•
4
‚àí1
1 ‚àí1
‚é¢
‚é•
‚é¢
‚é•
‚àí1
60
0 ‚àí2
A=‚é¢ 1
‚é•
‚é£ 1
‚é¶
1
0
8
4
0
‚àí1
‚àí2
4
700
y comp√°relos con los eigenvalores y n√∫meros de condici√≥n de la matriz precondicionada
D ‚àí1/2 AD ‚àít/2 .
Soluci√≥n Para determinar la matriz precondicionada, primero necesitamos la matriz diagoQDOODFXDODOVHUVLPpWULFDWDPELpQHVVXWUDQVSXHVWD6XVHQWUDGDVGLDJRQDOHVVHHVSHFL√ÄFDQ
mediante

1
1
1
1
1
; a4 = ‚àö ; a5 = ‚àö
,
a1 = ‚àö ; a2 = ‚àö ; a3 = ‚àö
0.2
4.0
60.0
8.0
700.0
y la matriz de precondicionamiento es
‚é°
2.23607
0
0
‚é¢ 0
.500000
0
‚é¢
0
.129099
C ‚àí1 = ‚é¢
‚é¢ 0
‚é£ 0
0
0
0
0
0

‚é§
0
0
‚é•
0
0
‚é•
‚é•.
0
0
‚é•
‚é¶
.353553
0
0
0.0377965

La matriz precondicionada es
AÃÉ = C ‚àí1 AC ‚àít
‚é°
1.000002
‚é¢0.1118035
‚é¢
=‚é¢
‚é¢0.2886744
‚é£0.7905693
0

0.1118035
1
‚àí0.0645495
0.1767765
‚àí0.0188983

0.2886744
0.7905693
‚àí0.0645495 0.1767765
0.9999931
0
0
0.9999964
‚àí0.00975898 0.05345219

‚é§
0
‚àí0.0188983 ‚é•
‚é•
‚àí0.00975898‚é•
‚é•.
0.05345219 ‚é¶
1.000005

Se encuentra que los eigenvalores de A y AÃÉ son
Eigenvalores de A 700.031, 60.0284, 0.0570747, 8.33845, 3.74533 y
Eigenvalores de AÃÉ 1.88052, 0.156370, 0.852686, 1.10159, 1.00884.
Los n√∫meros de condici√≥n de A y AÃÉ con la norma l‚àû que se encuentran son 13961.7 para A y
16.1155 para AÃÉ. Sin duda alguna, es verdad que en este caso AÃÉ est√° mejor condicionada que
la matriz original A.
Ilustraci√≥n

El sistema Ax 5 b con
‚é°

0.2
‚é¢ 0.1
‚é¢
A=‚é¢
‚é¢ 1
‚é£ 1
0

0.1
4
‚àí1
1
‚àí1

1
‚àí1
60
0
‚àí2

1
1
0
8
4

‚é§
0
‚àí1 ‚é•
‚é•
‚àí2 ‚é•
‚é•
‚é¶
4
700

‚é°

‚é§
1
‚é¢ 2 ‚é•
‚é¢
‚é•
‚é•
y b=‚é¢
‚é¢ 3 ‚é•
‚é£ 4 ‚é¶
5

tiene la soluci√≥n

x‚àó = (7.859713071, 0.4229264082, ‚àí0.07359223906, ‚àí0.5406430164, 0.01062616286)t .

366

CAP√çTULO 7

T√©cnicas iterativas en √°lgebra de matrices

La tabla 7.5 muestra los resultados obtenidos al utilizar los m√©todos iterativos de Jacobi,
Gauss-Siedel y SOR (con v 5 1.25) para el sistema con A con una tolerancia de 0.01, as√≠
como aquellos cuando el m√©todo de gradiente conjugado se aplica tanto en su forma no
precondicionada como mediante la matriz de precondicionamiento descrita en el ejemplo 3.
El m√©todo de gradiente conjugado no s√≥lo provee las aproximaciones m√°s precisas, sino que
tambi√©n utiliza un n√∫mero m√°s peque√±o de iteraciones.

Tabla 7.5
M√©todo

N√∫mero de
iteraciones

Jacobi

49

Gauss-Seidel

15

SOR (œâ = 1.25)

7

Gradiente conjugado

5

Gradiente conjugado
(precondicionado)

4

x(k)

x‚àó ‚àí x(k) ‚àû

(7.86277141, 0.42320802, ‚àí0.07348669,
‚àí0.53975964, 0.01062847)t
(7.83525748, 0.42257868, ‚àí0.07319124,
‚àí0.53753055, 0.01060903)t
(7.85152706, 0.42277371, ‚àí0.07348303,
‚àí0.53978369, 0.01062286)t
(7.85341523, 0.42298677, ‚àí0.07347963,
‚àí0.53987920, 0.008628916)t
(7.85968827, 0.42288329, ‚àí0.07359878,
‚àí0.54063200, 0.01064344)t

0.00305834
0.02445559
0.00818607
0.00629785
0.00009312

A menudo, el m√©todo de gradiente conjugado precondicionado se utiliza en la soluci√≥n
GH ORV JUDQGHV VLVWHPDV OLQHDOHV HQ ORV TXH OD PDWUL] HVWi GLVSHUVD \ HV GH√ÄQLGD SRVLWLYD
Estos sistemas se deben resolver para soluciones aproximadas para problemas de valores
en la frontera de ecuaciones diferenciales ordinarias (secciones 11.3, 11.4 y 11.5). Mientras
m√°s grande sea el sistema, m√°s prometedor ser√° el m√©todo de gradiente conjugado porque
UHGXFHVLJQL√ÄFDWLYDPHQWHHOQ~PHURGHLWHUDFLRQHVUHTXHULGDV(QHVWRVVLVWHPDVODPDWUL]
precondicionada C es aproximadamente igual a LHQODIDFWRUL]DFLyQGH&KROHVN\ L L t de
A(QJHQHUDOODVHQWUDGDVSHTXHxDVHQ$VRQLJQRUDGDV\HOPpWRGRGH&KROHVN\VHDSOLFD
para obtener lo que recibe el nombre de una factorizaci√≥n L L t incompleta de A. Por lo tanto,
C ‚àít C ‚àí1 ‚âà A‚àí1 , y se obtiene una buena aproximaci√≥n. M√°s informaci√≥n sobre el m√©todo
de gradiente conjugado se puede encontrar en [Kelley].
La secci√≥n Conjunto de ejercicios 7.6 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

7.7 Software num√©rico

$OHNVHL1LNRODHYLFK.U\ORY
(1863‚Äì1945) trabaj√≥ en
matem√°ticas aplicadas,
principalmente en las √°reas
de problemas de valores en la
frontera, la aceleraci√≥n de la
convergencia de series de Fourier
y varios problemas cl√°sicos
relacionados con sistemas
mec√°nicos. Durante principios
de la d√©cada de 1930 fue el
director del Instituto de F√≠sicaMatem√°ticas de la Academia
Sovi√©tica de Ciencias.

Casi todos los paquetes comerciales y de dominio p√∫blico que contienen m√©todos iterativos
para la soluci√≥n de un sistema de ecuaciones lineales requieren el uso de un precondicionador
con el m√©todo. A menudo, la r√°pida convergencia de los solucionadores se logra al utilizar
un precondicionador. Un precondicionador produce un sistema equivalente de ecuaciones
que, con suerte, presenta mejores caracter√≠sticas de convergencia que el sistema original. La
Biblioteca IMSL tiene un m√©todo de gradiente conjugado precondicionado y la Biblioteca
1$*WLHQHYDULDVVXEUXWLQDVTXHVRQSUH√ÄMRVSDUDODVROXFLyQLWHUDWLYDGHVLVWHPDVOLQHDOHV
Todas las subrutinas est√°n basadas en subespacios de Krylov. Saad [Sa2] tiene una
descripci√≥n detallada de los m√©todos de subespacios Krylov. Los paquetes LINPACK y
/$3$&. VyOR FRQWLHQHQ PpWRGRV GLUHFWRV SDUD OD VROXFLyQ GH VLVWHPDV OLQHDOHV VLQ HPEDUJRORVSDTXHWHVFRQWLHQHQPXFKDVVXEUXWLQDVTXHVHXWLOL]DQPHGLDQWHVROXFLRQDGRUHV
iterativos. Los paquetes de dominio p√∫blico IML++, ITPACK, SLAP y Templates contienen
m√©todos iterativos. MATLAB contiene varios m√©todos iterativos que tambi√©n est√°n basados
en subespacios Krylov.

7.7

Software num√©rico

367

Los conceptos de n√∫mero de condici√≥n y matrices pobremente condicionadas se introGXMHURQHQODVHFFLyQ0XFKDVGHODVVXEUXWLQDVSDUDUHVROYHUXQVLVWHPDOLQHDORSDUD
factorizar una matriz en una factorizaci√≥n LULQFOX\HQGRYHUL√ÄFDFLRQHVSDUDPDWULFHVPDO
condicionadas, y tambi√©n proporcionan un c√°lculo del n√∫mero de condici√≥n. LAPACK tiene
QXPHURVDVUXWLQDVTXHLQFOX\HQHOFiOFXORGHXQQ~PHURGHFRQGLFLyQFRPRORKDFHQODV
bibliotecas ISML y NAG.
Las bibliotecas LAPACK, LINPACK, IMSL y NAG tienen subrutinas que mejoran una
soluci√≥n para un sistema lineal que est√° pobremente condicionado. La subrutina prueba el
Q~PHURGHFRQGLFLyQ\GHVSXpVXWLOL]DHOUH√ÄQDPLHQWRLWHUDWLYRSDUDREWHQHUODVROXFLyQ
m√°s precisa posible dada la precisi√≥n de la computadora.
Las secciones Preguntas de an√°lisis, Conceptos clave y Revisi√≥n del cap√≠tulo est√°n disponibles en l√≠nea. Encuentre la ruta de acceso en las p√°ginas preliminares.

CAP√çTULO

8

Teor√≠a de aproximaci√≥n

Introducci√≥n
La ley de Hooke establece que cuando se aplica una fuerza a un resorte construido con material uniforme, la longitud del resorte es una funci√≥n lineal de esa fuerza. Podemos escribir la
funci√≥n lineal como F(l) = k(l ‚àí E), donde F(l) representa la fuerza requerida para estirar
el resorte l unidades, la constante E representa la longitud del resorte sin fuerza aplicada y la
constante k es la constante del resorte.
l
14
12
10
8
E

6
k(l 2 E) 5 F(l)

4

l
2
2

4

6

F

Suponga que queremos determinar la constante para un resorte que tiene una longitud inicial de 5.3 pulgadas. Aplicamos fuerzas de 2, 4 y 6 libras al resorte y encontramos que su longitud aumenta a 7.0, 9.4 y 12.3 pulgadas, respectivamente. Una revisi√≥n r√°pida muestra que los
puntos (0, 5.3), (2, 7.0), (4, 9.4) y (6, 12.3) no se encuentran completamente en l√≠nea recta. Aunque podr√≠amos usar un par aleatorio de estos puntos de datos para aproximar la constante del
resorte, parecer√≠a m√°s razonable encontrar la recta que mejor aproxima a todos los puntos de
datos para determinar la constante. En este cap√≠tulo se considerar√° este tipo de aproximaci√≥n y
es posible encontrar esta aplicaci√≥n de resorte en el ejercicio 7 de la secci√≥n 8.1.
La teor√≠a de la aproximaci√≥n implica dos tipos generales de problemas. Uno surge cuanGRXQDIXQFLyQVHGH√ÄQHGHPDQHUDH[SOtFLWDSHURQRVJXVWDUtDHQFRQWUDUXQWLSRGHIXQFLyQ
‚Äúm√°s simple‚Äù, como un polinomio, para los valores aproximados de una funci√≥n determinada. El otro problema se preocupa por ajustar funciones a un dato establecido y encontrar la
‚Äúmejor‚Äù funci√≥n de cierta clase para representar los datos.
Ambos problemas se han analizado en el cap√≠tulo 3. El en√©simo polinomio de Taylor
alrededor del n√∫mero x0 es una excelente aproximaci√≥n para una funci√≥n f (n 1 1) veces diferenciable en una vecindad de x0. Los polinomios de interpolaci√≥n de Lagrange o, de modo
m√°s general, osculantes, se analizaron como polinomios de interpolaci√≥n y para ajustar ciertos datos. Los splines c√∫bicos tambi√©n se analizaron en el cap√≠tulo 3. En este cap√≠tulo, se
consideran las limitaciones para estas t√©cnicas y se analizan otras v√≠as de enfoque.
369

CAP√çTULO 8

370

Teor√≠a de aproximaci√≥n

8.1 Aproximaci√≥n por m√≠nimos cuadrados discretos
Tabla 8.1
xi

yi

xi

yi

1
2
3
4
5

1.3
3.5
4.2
5.0
7.0

6
7
8
9
10

8.8
10.1
12.5
13.0
15.6

Considere el problema de calcular los valores de una funci√≥n en puntos no tabulados, dados
los datos experimentales en la tabla 8.1.
/D√ÄJXUDPXHVWUDXQDJUi√ÄFDGHORVYDORUHVGHODWDEOD$SDUWLUGHHVWDJUi√ÄFD
parece que la relaci√≥n real entre x y y es lineal. La raz√≥n probable para que ninguna l√≠nea se
ajuste con precisi√≥n a los datos son los errores en estos √∫ltimos. Por lo que es poco razonable
solicitar que la funci√≥n de aproximaci√≥n concuerde exactamente con los datos. De hecho,
dicha funci√≥n introducir√≠a oscilaciones que no estaban presentes originalmente. Por ejemplo,
ODJUi√ÄFDGHOSROLQRPLRGHLQWHUSRODFLyQGHQRYHQRJUDGRTXHVHPXHVWUDGHPRGROLEUHSDUD
ORVGDWRVHQODWDEODVHPXHVWUDHQOD√ÄJXUD

Figura 8.1
y
16
14
12
10
8
6
4
2
2

4

6

8

x

10

/DJUi√ÄFDREWHQLGD FRQSXQWRVGHGDWRVDGLFLRQDOHV VHPXHVWUDHQOD√ÄJXUD
Figura 8.2
(10, 15.6)

14

(9, 13.0)
(8, 12.5)

12
10

(7, 10.1)
(6, 8.8)

8
(5, 7.0)

6
4
2

(2, 3.5)

(3, 4.2)

(4, 5.0)

(1, 1.3)

2

4

x

6

8

10

Este polinomio es claramente una predicci√≥n de la informaci√≥n entre una serie de puntos
de datos. Un mejor enfoque ser√≠a encontrar la recta que se aproxima ‚Äúmejor‚Äù (en cierto sentido), incluso si no concuerda precisamente con los datos en ning√∫n punto.

8.1 Aproximaci√≥n por m√≠nimos cuadrados discretos

371

Sea que a1 xi + a0 denota el i-√©simo valor en la recta de aproximaci√≥n y que yi es el
i-√©simo valor de y dado. Suponemos que las variables independientes, las xi, son exactas; son
las variables dependientes, las yi, de las que sospechamos. Esto es una suposici√≥n razonable
en muchas situaciones experimentales.
El problema de encontrar la ecuaci√≥n de la mejor aproximaci√≥n lineal en el sentido
absoluto requiere encontrar los valores a0 y a1 para minimizar

E ‚àû (a0 , a1 ) = m√°x {|yi ‚àí (a1 xi + a0 )|}.
1‚â§i‚â§10

Normalmente esto recibe el nombre de problema minim√°x y no es posible manejarlo con
t√©cnicas fundamentales.
Otro enfoque para determinar la mejor aproximaci√≥n lineal implica encontrar los valores
de a0 y a1 para minimizar
10

E 1 (a0 , a1 ) =

|yi ‚àí (a1 xi + a0 )|.
i=1

Esta cantidad recibe el nombre de desviaci√≥n absoluta. Para minimizar una funci√≥n de dos
variables, necesitamos igualar sus derivadas parciales a cero y resolver simult√°neamente las
ecuaciones resultantes. En el caso de la desviaci√≥n absoluta, necesitamos encontrar a0 y a1 con
10

10

‚àÇ
0=
|yi ‚àí (a1 xi + a0 )|
‚àÇa0 i=1

y

‚àÇ
0 =
|yi ‚àí (a1 xi + a0 )|.
‚àÇa1 i=1

El problema es que la funci√≥n valor absoluto no es diferenciable en cero y podr√≠amos no
encontrar soluciones para este par de ecuaciones.

M√≠nimos cuadrados lineales
El enfoque de m√≠nimos cuadrados para este problema implica determinar la mejor l√≠nea
de aproximaci√≥n cuando el error relacionado es la suma de los cuadrados de las diferencias
entre los valores y en la l√≠nea de aproximaci√≥n y los valores y proporcionados. Por lo tanto,
deben encontrarse las constantes a0 y a1 que minimizan el error de m√≠nimos cuadrados:
10

E 2 (a0 , a1 ) =

[yi ‚àí (a1 xi + a0 )]2 .
i=1

El m√©todo de m√≠nimos cuadrados es el procedimiento m√°s conveniente para determinar
mejores aproximaciones lineales, pero tambi√©n hay consideraciones te√≥ricas importantes
que lo favorecen. En general, mientras el enfoque minim√°x asigna demasiado peso a un bit
GHGDWRVFRQXQJUDQHUURUHOPpWRGRGHGHVYLDFLyQDEVROXWDQRGDVX√ÄFLHQWHSHVRDXQSXQWR
que est√° fuera de la l√≠nea con la aproximaci√≥n. El enfoque de m√≠nimos cuadrados asigna considerablemente m√°s peso en un punto que est√° fuera de la l√≠nea que al resto de los datos, pero
no permitir√° que el punto domine por completo la aproximaci√≥n. Una raz√≥n adicional para
considerar el enfoque de m√≠nimos cuadrados implica el estudio de la distribuci√≥n estad√≠stica
del error (consulte [Lar], p. 463‚Äì481).
El problema general de ajustar la mejor l√≠nea de m√≠nimos cuadrados para una recopilam
implica minimizar el error total,
ci√≥n de datos {(xi , yi )}i=1
m

E ‚â° E 2 (a0 , a1 ) =

[yi ‚àí (a1 xi + a0 )]2 ,
i=1

372

CAP√çTULO 8

Teor√≠a de aproximaci√≥n

respecto a los par√°metros a0 y a1. Para que se presente un m√≠nimo, necesitamos que

‚àÇE
=0
‚àÇa0

‚àÇE
= 0,
‚àÇa1

y

es decir,
0=

m

m

m

m

‚àÇ
(yi ‚àí a1 xi ‚àí a0 )(‚àí1)
[(yi ‚àí (a1 xi ‚àí a0 )]2 = 2
‚àÇa0 i=1
i=1

y
‚àÇ
(yi ‚àí a1 xi ‚àí a0 )(‚àíxi ).
0=
[yi ‚àí (a1 xi + a0 )]2 = 2
‚àÇa1 i=1
i=1
La palabra ‚Äúnormal‚Äù como
aqu√≠ se usa implica la idea
de ‚Äúperpendicular‚Äù. Las
ecuaciones normales se obtienen
encontrando direcciones
perpendiculares para una
VXSHU√ÄFLHPXOWLGLPHQVLRQDO

(VWDVHFXDFLRQHVVHVLPSOL√ÄFDQHQODVecuaciones normales:
m

m

a0 ¬∑ m + a 1

xi =

m

yi

i=1

x i + a1

i=1

m

m

a0

y

i=1

xi2 =
i=1

xi yi .
i=1

La soluci√≥n para este sistema de ecuaciones es
m

m

m

xi2
a0 =

yi ‚àí

i=1

i=1

m

xi yi
i=1

m

m

m

xi2

xi
i=1
2

‚àí

(8.1)

xi

i=1

i=1

y
m

m

m
i=1

a1 =

xi
i=1

m

m

yi
i=1
2

m

xi2
i=1

Ejemplo 1

m

xi yi ‚àí
‚àí

.

(8.2)

xi
i=1

Encuentre la l√≠nea de m√≠nimos cuadrados que se aproxima a los datos en la tabla 8.1.
Soluci√≥n Primero ampliamos la tabla para incluir xi2 y xi yi y sumamos las columnas. Esto

se muestra en la tabla 8.2.

Tabla 8.2

xi

yi

xi2

xi yi

P(xi ) = 1.538xi ‚àí 0.360

1
2
3
4
5
6
7
8
9
10

1.3
3.5
4.2
5.0
7.0
8.8
10.1
12.5
13.0
15.6

1
4
9
16
25
36
49
64
81
100

1.3
7.0
12.6
20.0
35.0
52.8
70.7
100.0
117.0
156.0

1.18
2.72
4.25
5.79
7.33
8.87
10.41
11.94
13.48
15.02

55

81.0

385

572.4

E=

10
2
i=1 (yi ‚àí P(x i )) ‚âà 2.34

8.1 Aproximaci√≥n por m√≠nimos cuadrados discretos

373

Las ecuaciones normales (8.1) y (8.2) implican que

a0 =

385(81) ‚àí 55(572.4)
= ‚àí0.360
10(385) ‚àí (55)2

a1 =

10(572.4) ‚àí 55(81)
= 1.538,
10(385) ‚àí (55)2

y

por lo que P(x) = 1.538x ‚àí 0.360. /DJUi√ÄFDGHHVWDUHFWD\ORVSXQWRVGHGDWRVVHPXHVWUDQHQOD√ÄJXUD/RVYDORUHVDSUR[LPDGRVREWHQLGRVSRUODWpFQLFDGHPtQLPRVFXDGUDdos en los puntos de datos est√°n en la tabla 8.2.
Figura 8.3
y
16
14
12
10
8

y 5 1.538x 2 0.360

6
4
2
2

4

6

8

10

x

M√≠nimos cuadrados polinomiales
El problema general de aproximar un conjunto de datos { (xi , yi ) | i = 1, 2, . . . , m }, con
un polinomio algebraico

Pn (x) = an x n + an‚àí1 x n‚àí1 + ¬∑ ¬∑ ¬∑ + a1 x + a0 ,
de grado n < m ‚àí 1, por medio del procedimiento de m√≠nimos cuadrados se maneja de forma similar. Seleccionamos las constantes a0 , a1 , . . . , an para minimizar el error de m√≠nimos
cuadrados E = E 2 (a0 , a1 , . . . , an ), donde
m

(yi ‚àí Pn (xi ))2

E=
i=1
m

=

m

yi2 ‚àí 2
i=1

m

Pn (xi )yi +
i=1

(Pn (xi ))2
i=1

CAP√çTULO 8

374

Teor√≠a de aproximaci√≥n
m

‚éõ

m

=

‚éù

yi2 ‚àí 2
i=1

i=1

m

‚éû

n

j

a j xi ‚é† yi +

j=0

n

=

j

aj

i=1

j=0

yi xi
i=1

n

‚éû2

n

‚éù

i=1
m

yi2 ‚àí 2

‚éõ

m

j

a j xi ‚é†

j=0
m

n

+

a j ak

j+k

xi

j=0 k=0

.

i=1

Como en el caso lineal, para minimizar E es necesario que ‚àÇ E/‚àÇa j = 0, para cada
j = 0, 1, . . . , n. Por lo tanto, para cada j, debemos tener
m

0=

n

m

‚àÇE
j
j+k
= ‚àí2
yi xi + 2
ak
xi .
‚àÇa j
i=1
k=0
i=1

Esto nos da n 1 1 ecuaciones normales en las n 1 1 inc√≥gnitas a j. √âstas son
n

m

j+k

ak
k=0

xi

m

=

i=1

j

yi xi ,

para cada j = 0, 1, . . . , n.

(8.3)

i=1

Es √∫til escribir las ecuaciones de acuerdo con lo siguiente:
m

m

xi0 + a1

a0
i=1

i=1

m
i=1

m
i=1

m

m

xi2 + a2
i=1

xin =

xi2 + ¬∑ ¬∑ ¬∑ + an
i=1

m

xi1 + a1

a0

m

xi1 + a2

i=1

yi xi0 ,
i=1

xin+1 =

xi3 + ¬∑ ¬∑ ¬∑ + an

m

i=1

m

yi xi1 ,
i=1

..
.
m

a0
i=1

xin + a1

m

xin+1 + a2

i=1

m

xin+2 + ¬∑ ¬∑ ¬∑ + an

i=1

m

m

xi2n =
i=1

yi xin .

i=1

Estas ecuaciones normales tienen una √∫nica soluci√≥n siempre y cuando las x i sean distintas (consulte el ejercicio 14).
Ejemplo 2

Tabla 8.3
i

xi

yi

1
2
3
4
5

0
0.25
0.50
0.75
1.00

1.0000
1.2840
1.6487
2.1170
2.7183

Ajuste los datos en la tabla 8.3 con el polinomio de m√≠nimos cuadrados discretos de grado
m√°ximo 2.
Soluci√≥n Para este problema, n 5 2, m 5 5, y las tres ecuaciones normales son

5a0 +
2.5a1 + 1.875a2 = 8.7680,
2.5a0 + 1.875a1 + 1.5625a2 = 5.4514, y
1.875a0 + 1.5625a1 + 1.3828a2 = 4.4015.
Al resolver las ecuaciones obtenemos

a0 = 1.005075519,

a1 = 0.8646758482, y

a2 = 0.8431641518.

Por lo tanto, el polinomio de m√≠nimos cuadrados de grado 2 que se ajusta a los datos de
la tabla 8.3 es

P2 (x) = 1.0051 + 0.86468x + 0.84316x 2 ,
FX\DJUi√ÄFDVHPXHVWUDHQOD√ÄJXUD(QORVYDORUHVGHWHUPLQDGRVGHxi, tenemos las aproximaciones mostradas en la tabla 8.4.

8.1 Aproximaci√≥n por m√≠nimos cuadrados discretos

375

Figura 8.4
y

2

y 5 1.0051 1 0.86468x 1 0.84316x2

1

0.25

0.50

0.75

1.00

x

El error total,
5

(yi ‚àí P(xi ))2 = 2.74 √ó 10‚àí4 ,

E=
i=1

es el m√≠nimo que se puede obtener a trav√©s de un polinomio de grado m√°ximo 2.

Tabla 8.4

i

1

2

3

4

5

xi
yi
P(xi )
yi ‚àí P(xi )

0
1.0000
1.0051
‚àí0.0051

0.25
1.2840
1.2740
0.0100

0.50
1.6487
1.6482
0.0004

0.75
2.1170
2.1279
‚àí0.0109

1.00
2.7183
2.7129
0.0054

Algunas veces es adecuado asumir que los datos est√°n exponencialmente relacionados.
Esto requiere que la funci√≥n de aproximaci√≥n sea de la forma

y = beax

(8.4)

y = bx a ,

(8.5)

o

para algunas constantes a y b/DGL√ÄFXOWDGGHDSOLFDUHOSURFHGLPLHQWRGHPtQLPRVFXDGUDdos en una situaci√≥n de este tipo proviene de intentar minimizar
m

E=
i=1

(yi ‚àí beaxi )2 ,

en el caso de la ecuaci√≥n (8.4)

376

CAP√çTULO 8

Teor√≠a de aproximaci√≥n

o
m

E=

(yi ‚àí bxia )2 ,

en el caso de la ecuaci√≥n (8.5).

i=1

Las ecuaciones normales relacionadas con estos procedimientos se obtienen ya sea a
partir de
m

0=

‚àÇE
=2
(yi ‚àí beaxi )(‚àíeaxi )
‚àÇb
i=1

0=

‚àÇE
(yi ‚àí beaxi )(‚àíbxi eaxi ),
=2
‚àÇa
i=1

0=

‚àÇE
(yi ‚àí bxia )(‚àíxia )
=2
‚àÇb
i=1

0=

‚àÇE
(yi ‚àí bxia )(‚àíb(ln xi )xia ),
=2
‚àÇa
i=1

y
m

en el caso de la ecuaci√≥n (8.4),

o
m

y
m

en el caso de la ecuaci√≥n (8.5).

En general, no se puede encontrar una soluci√≥n exacta para estos sistemas en a y b.
El m√©todo que se utiliza normalmente cuando se sospecha que los datos est√°n exponencialmente relacionados es considerar el logaritmo de la ecuaci√≥n de aproximaci√≥n:

ln y = ln b + ax,

en el caso de la ecuaci√≥n (8.4),

y
ln y = ln b + a ln x,

en el caso de la ecuaci√≥n (8.5).

En cualquier caso, ahora aparece un problema lineal y las soluciones para ln b y a se pueden
REWHQHUDOPRGL√ÄFDUDGHFXDGDPHQWHODVHFXDFLRQHVQRUPDOHV  \  
Sin embargo, la obtenida de esta forma no es la aproximaci√≥n por m√≠nimos cuadrados para
el problema original, y HVWDDSUR[LPDFLyQSXHGHHQDOJXQRVFDVRVGLIHULUVLJQL√ÄFDWLYDPHQWH
de la aproximaci√≥n de m√≠nimos cuadrados para el problema original. La aplicaci√≥n en el
ejercicio 13 describe este problema. Esta aplicaci√≥n se reconsiderar√° en el ejercicio 9 en la
secci√≥n 10.3, donde la soluci√≥n exacta para el problema exponencial de m√≠nimos cuadrados
se aproxima con m√©todos adecuados para resolver sistemas de ecuaciones no lineales.
Ilustraci√≥n

Tabla 8.5

Considere el conjunto de datos en las primeras tres columnas de la tabla 8.5.
i

xi

yi

ln yi

xi2

xi ln yi

1
2
3
4
5

1.00
1.25
1.50
1.75
2.00

5.10
5.79
6.53
7.45
8.46

1.629
1.756
1.876
2.008
2.135

1.0000
1.5625
2.2500
3.0625
4.0000

1.629
2.195
2.814
3.514
4.270

9.404

11.875

14.422

7.50

8.1 Aproximaci√≥n por m√≠nimos cuadrados discretos

377

Si xiVHJUD√ÄFDFRQOQyi, los datos parecen tener una relaci√≥n lineal, por lo que es razonable suponer una aproximaci√≥n de la forma

y = beax ,

lo cual implica que

ln y = ln b + ax.

Al expandir la tabla y suponer que las columnas apropiadas dan los datos restantes en la
tabla 8.5.
Usando las ecuaciones normales (8.1) y (8.2),

a=

(5)(14.422) ‚àí (7.5)(9.404)
= 0.5056
(5)(11.875) ‚àí (7.5)2

y
ln b =

(11.875)(9.404) ‚àí (14.422)(7.5)
= 1.122.
(5)(11.875) ‚àí (7.5)2

Con ln b 5 1.222, tenemos b = e1.122 = 3.071, y la aproximaci√≥n asume la forma
y 5 3.071e0.5056xi.
(QORVSXQWRVGHGDWRVHVWRVYDORUHVVHGDQHQODWDEOD FRQVXOWHOD√ÄJXUD 

Tabla 8.6

i

xi

yi

3.071e0.5056xi

|yi ‚àí 3.071e0.5056xi |

1
2
3
4
5

1.00
1.25
1.50
1.75
2.00

5.10
5.79
6.53
7.45
8.46

5.09
5.78
6.56
7.44
8.44

0.01
0.01
0.03
0.01
0.02

Figura 8.5
y
9
8
7
6
y 5 3.071e0.5056x
5

0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00

x

La secci√≥n Conjunto de ejercicios 8.1 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

378

CAP√çTULO 8

Teor√≠a de aproximaci√≥n

8.2 Polinomios ortogonales y aproximaci√≥n por m√≠nimos cuadrados
La secci√≥n previa consideraba el problema de la aproximaci√≥n por m√≠nimos cuadrados para
ajustarse a un conjunto de datos. El otro problema de aproximaci√≥n mencionado en la introducci√≥n aborda la aproximaci√≥n de funciones.
Suponga que f ‚àà C[a, b] y que se requiere un polinomio Pn (x) de grado a lo sumo n
para minimizar el error
b
a

[ f (x) ‚àí Pn (x)]2 d x.

Para determinar un polinomio de aproximaci√≥n por m√≠nimos cuadrados, es decir, un
polinomio para minimizar esta aproximaci√≥n, sea

Pn (x) = an x n + an‚àí1 x n‚àí1 + ¬∑ ¬∑ ¬∑ + a1 x + a0 =

n

ak x k

k=0

\GH√ÄQDFRPRVHPXHVWUDHQOD√ÄJXUD
b

E ‚â° E 2 (a0 , a1 , . . . , an ) =

a

n

2

ak x k

f (x) ‚àí

d x.

k=0

Figura 8.6
y

f (x)
n

Pn (x) 5

Sax
k

k50

k

n

( f (x) 2 S a x (
k50

a

k

b

k

2

x

(OSUREOHPDHVHQFRQWUDUORVFRH√ÄFLHQWHVUHDOHVa0 , a1 , . . . , an que minimizar√°n E. Una
condici√≥n necesaria para los n√∫meros a0 , a1 , . . . , an para minimizar E es que

‚àÇE
= 0,
‚àÇa j

para cada j = 0, 1, . . . , n.

Puesto que

E=

b
a

n

[ f (x)]2 d x ‚àí 2

ak
k=0

b
a

b

x k f (x) d x +

a

n

ak x k

k=0

tenemos

‚àÇE
= ‚àí2
‚àÇa j

b
a

j

n

x f (x) d x + 2

ak
k=0

b
a

x j+k d x.

2

d x,

8.2 Polinomios ortogonales y aproximaci√≥n por m√≠nimos cuadrados

379

Por lo tanto, encontramos Pn (x), las ecuaciones normales lineales (n 1 1)
n

b

ak

a

k=0

b

x j+k d x =

a

x j f (x) d x, para cada j = 0, 1, . . . , n,

(8.6)

se deben resolver para las (n 1 1) inc√≥gnitas aj. Las ecuaciones normales siempre tienen una
√∫nica soluci√≥n siempre y cuando f ‚àà C[a, b]. (Consulte el ejercicio 15.)
Ejemplo 1

Encuentre el polinomio de aproximaci√≥n de grado 2 por m√≠nimos cuadrados para la funci√≥n
f (x) 5 sen œÄ x en el intervalo [0, 1].
Soluci√≥n

Las ecuaciones normales para P2 (x) = a2 x 2 + a1 x + a0 son
1

a0

1

1 d x + a1

0
1

a0

0

0
1

a0

1

x d x + a1

0

0

x sen œÄ x d x, y

0
1

x 3 d x + a2

1

x3 dx =

0
1

sen œÄ x d x,

0
1

x 2 d x + a2

1

x2 dx =

0

0

x 2 d x + a1

1

x d x + a2

1

x4 dx =

0

x 2 sen œÄ x d x.

0

Realizando la integraci√≥n obtenemos

1
1
2
a0 + a1 + a2 = ,
2
3
œÄ

1
1
œÄ2 ‚àí 4
1
.
a0 + a1 + a2 =
3
4
5
œÄ3

1
1
1
1
a0 + a1 + a2 = , y
2
3
4
œÄ

Estas tres ecuaciones en tres inc√≥gnitas se pueden resolver para obtener

a0 =

12œÄ 2 ‚àí 120
‚âà ‚àí0.050465
œÄ3

y

a1 = ‚àía2 =

720 ‚àí 60œÄ 2
‚âà 4.12251.
œÄ3

Por consiguiente, la aproximaci√≥n del polinomio de grado 2 por m√≠nimos cuadrados para
f (x) 5 sen œÄ x en [0, 1] es P2 (x) = ‚àí4.12251x 2 + 4.12251x ‚àí 0.050465. (Consulte la
√ÄJXUD 

Figura 8.7
y
y 5 sen px

1.0
0.8

y = P2(x)

0.6
0.4
0.2

0.2

0.4

0.6

0.8

1.0

x

380

CAP√çTULO 8

Teor√≠a de aproximaci√≥n

David Hilbert (1862‚Äì1943)
fue un matem√°tico dominante
D√ÄQDOHVGHOVLJORXX. Se le
recuerda mejor por impartir
una charla en el Congreso
Internacional de Matem√°ticos,
en Par√≠s, en 1900, donde plante√≥
23 problemas que hab√≠a pensado
que ser√≠a importante que los
matem√°ticos del siglo siguiente
resolvieran.

(O HMHPSOR  LOXVWUD XQD GL√ÄFXOWDG DO REWHQHU XQD DSUR[LPDFLyQ SROLQRPLDO GH PtQLmos cuadrados. Se debe resolver un sistema lineal (n 1 1) 3 (n 1 1) para las inc√≥gnitas
a0 , . . . , an\ORVFRH√ÄFLHQWHVHQHOVLVWHPDOLQHDOVRQGHODIRUPD
b
a

x j+k d x =

b j+k+1 ‚àí a j+k+1
,
j +k+1

un sistema lineal que no tiene una soluci√≥n num√©rica f√°cil de calcular. La matriz en el sistema lineal es conocida como matriz de HilbertXQHMHPSORFOiVLFRSDUDGHPRVWUDUGL√ÄFXOWDdes de error de redondeo (consulte el ejercicio 9 de la secci√≥n 7.5).
Otra desventaja es similar a la situaci√≥n que se present√≥ cuando los polinomios de
Lagrange se presentaron por primera vez en la secci√≥n 3.1. Los c√°lculos realizados para
obtener el mejor polinomio de en√©simo grado Pn (x), no reducen la cantidad de trabajo requerido para obtener Pn+1 (x), el polinomio del siguiente grado superior.

Funciones linealmente independientes
Ahora consideraremos una t√©cnica diferente para obtener aproximaciones de m√≠nimos cuaGUDGRVeVWDUHVXOWDVHUH√ÄFLHQWHGHVGHHOSXQWRGHYLVWDLQIRUPiWLFR\XQDYH]TXHVHFRQRFH
Pn (x) es sencillo determinar Pn+1 (x) Para facilitar el an√°lisis, necesitamos algunos conceptos nuevos.
DeÔ¨Ånici√≥n 8.1

Se dice que el conjunto de funciones {œÜ0 , . . . , œÜn } es linealmente independiente en [a, b] si,

c0 œÜ0 (x) + c1 œÜ1 (x) + ¬∑ ¬∑ ¬∑ + cn œÜn (x) = 0,

para todas las x ‚àà [a, b],

entonces c0 = c1 = ¬∑ ¬∑ ¬∑ = cn = 0. De lo contrario, se dice que el conjunto de funciones es
linealmente dependiente.
Teorema 8.2

Suponga que, para cada j = 0, 1, . . . , n, œÜ j (x) es un polinomio de grado j. Entonces el
conjunto {œÜ0 , . . . , œÜn } es linealmente independiente en cualquier intervalo [a, b].
Demostraci√≥n

Sean c0 , . . . , cn n√∫meros reales para los que

P(x) = c0 œÜ0 (x) + c1 œÜ1 (x) + ¬∑ ¬∑ ¬∑ + cn œÜn (x) = 0,

para todos x ‚àà [a, b].

El polinomio P (x) se anula en [a, b@SRUORTXHGHEHVHUHOSROLQRPLRFHUR\ORVFRH√ÄFLHQWHV
de todas las potencias de xVRQFHUR(QSDUWLFXODUHOFRH√ÄFLHQWHGHx n es cero. Pero cn œÜn (x)
es el √∫nico t√©rmino en P (x) que contiene x n, por lo que debemos tener cn 5 0. Por lo tanto
n‚àí1

P(x) =

c j œÜ j (x).
j=0

En esta representaci√≥n de P (x), el √∫nico t√©rmino que contiene una potencia de x n‚àí1 es
cn‚àí1 œÜn‚àí1 (x), por lo que este t√©rmino tambi√©n debe ser cero y
n‚àí2

P(x) =

c j œÜ j (x).
j=0

De la misma forma, las constantes restantes cn‚àí2 , cn‚àí3 , . . . , c1 , c0 son cero, lo cual implica
que {œÜ0 , œÜ1 , . . . , œÜn } es linealmente independiente en [a, b].
Ejemplo 2

Si œÜ0 (x) = 2, œÜ1 (x) = x ‚àí3, y œÜ2 (x) = x 2 +2x +7, y Q(x) = a0 +a1 x +a2 x 2 . Muestre que
existen constantes c0 , c1 , y c2 tales que Q(x) = c0 œÜ0 (x) + c1 œÜ1 (x) + c2 œÜ2 (x).
Soluci√≥n Por el teorema 8.2 {œÜ0 , œÜ1 , œÜ2 } es linealmente independiente en cualquier interva-

lo [a, b]. Primero observe que

8.2 Polinomios ortogonales y aproximaci√≥n por m√≠nimos cuadrados

1=

1
œÜ0 (x),
2

381

3
x = œÜ1 (x) + 3 = œÜ1 (x) + œÜ0 (x)
2

y que
3
1
x 2 = œÜ2 (x) ‚àí 2x ‚àí 7 = œÜ2 (x) ‚àí 2 œÜ1 (x) + œÜ0 (x) ‚àí 7 œÜ0 (x)
2
2
= œÜ2 (x) ‚àí 2œÜ1 (x) ‚àí

13
œÜ0 (x).
2

Por lo tanto,

Q(x) = a0
=

3
13
1
œÜ0 (x) + a1 œÜ1 (x) + œÜ0 (x) + a2 œÜ2 (x) ‚àí 2œÜ1 (x) ‚àí œÜ0 (x)
2
2
2
1
3
13
a0 + a1 ‚àí a2 œÜ0 (x) + [a1 ‚àí 2a2 ] œÜ1 (x) + a2 œÜ2 (x).
2
2
2

/D VLWXDFLyQ TXH VH LOXVWUD HQ HO HMHPSOR  VH PDQWLHQH HQ XQD FRQ√ÄJXUDFLyQ PXFKR
m√°s general. Si n denota el conjunto de todos los polinomios de grado a lo sumo n. El
siguiente resultado se utiliza ampliamente en muchas aplicaciones de √°lgebra lineal. Su demostraci√≥n se considera en el ejercicio 13.
Teorema 8.3

Suponga que {œÜ0 (x), œÜ1 (x), . . . , œÜn (x)} es un conjunto de polinomios linealmente independientes en n. Entonces, un polinomio en n se puede escribir de manera √∫nica como una
combinaci√≥n lineal de œÜ0 (x), œÜ1 (x), . . . , œÜn (x).

Funciones ortogonales
Analizar la aproximaci√≥n general de una funci√≥n requiere la introducci√≥n de las nociones de
funci√≥n de peso y ortogonalidad.
DeÔ¨Ånici√≥n 8.4

Una funci√≥n integrable w recibe el nombre de funci√≥n de peso en el intervalo I si w(x) ‚â• 0,
para todas las x en I, pero w(x) ‚â° 0 en cualquier subintervalo de I.
El objetivo de una funci√≥n de peso es asignar varios grados de importancia a las aproximaciones en ciertas partes del intervalo. Por ejemplo, la funci√≥n de peso

1
w(x) = ‚àö
1 ‚àí x2
asigna menos √©nfasis cerca del centro del intervalo (21, 1) y m√°s √©nfasis cuando |x| est√°
FHUFDGH FRQVXOWHOD√ÄJXUD (VWDIXQFLyQGHSHVRVHXVDHQODVLJXLHQWHVHFFLyQ
Suponga que {œÜ0 , œÜ1 , . . . , œÜn } es un conjunto de funciones linealmente independiente
en [a, b] y w es una funci√≥n de peso para [a, b]. Dada f ‚àà C[a, b], buscamos una combinaci√≥n lineal
Figura 8.8

n

P(x) =

(x)

ak œÜk (x)
k=0

para minimizar el error
1

21

E = E(a0 , . . . , an ) =
1

x

b
a

n

w(x) f (x) ‚àí

2

ak œÜk (x)

d x.

k=0

Este problema reduce la situaci√≥n considerada al inicio de esta secci√≥n en el caso especial
cuando w(x) ‚â° 1 y œÜk (x) = x k , para cada k = 0, 1, . . . , n.

382

CAP√çTULO 8

Teor√≠a de aproximaci√≥n

Las ecuaciones normales relacionadas con este problema se derivan del hecho de que
para cada j = 0, 1, . . . , n,
b

‚àÇE
0=
=2
‚àÇa j

a

n

w(x) f (x) ‚àí

ak œÜk (x) œÜ j (x) d x.
k=0

El sistema de ecuaciones normales se puede escribir como
b
a

n

w(x) f (x)œÜ j (x) d x =

ak
k=0

b
a

w(x)œÜk (x)œÜ j (x) d x,

para j = 0, 1, . . . , n.

Si las funciones œÜ0 , œÜ1 , . . . , œÜn se pueden seleccionar de tal forma que
b
a

w(x)œÜk (x)œÜ j (x) d x =

0,
cuando j = k,
Œ± j > 0, cuando j = k,

(8.7)

entonces las ecuaciones normales se reducir√°n a
b
a

w(x) f (x)œÜ j (x) d x = a j

b
a

w(x)[œÜ j (x)]2 d x = a j Œ± j ,

para cada j = 0, 1, . . . , n. Esto se resuelve f√°cilmente dado que

aj =

1
Œ±j

b
a

w(x) f (x)œÜ j (x) d x.

/DSDODEUD¬¥RUWRJRQDO¬µVLJQL√ÄFD
‚Äúen √°ngulo recto‚Äù. Por lo que,
en un sentido, las funciones
ortogonales son perpendiculares
entre s√≠.

3RU OR WDQWR HO SUREOHPD GH DSUR[LPDFLyQ GH PtQLPRV FXDGUDGRV VH VLPSOL√ÄFD HQ JUDQ
medida cuando se seleccionan las funciones œÜ0 , œÜ1 , . . . , œÜn para satisfacer la condici√≥n de
ortogonalidad en la ecuaci√≥n (8.7). El resto de esta secci√≥n est√° dedicado a estudiar conjuntos de este tipo.

DeÔ¨Ånici√≥n 8.5

Se dice que {œÜ0 , œÜ1 , . . . , œÜn } es un conjunto ortogonal de funciones en el intervalo [a, b]
respecto a la funci√≥n de peso w si
b
a

w(x)œÜk (x)œÜ j (x) d x =

0,
cuando j = k,
Œ± j > 0, cuando j = k.

Si, adem√°s, Œ± j = 1 para cada j = 0, 1, . . . , n, se dice que el conjunto es ortonormal.
(VWDGH√ÄQLFLyQMXQWRFRQODVREVHUYDFLRQHVDQWHULRUHVSURGXFHHOVLJXLHQWHWHRUHPD
Teorema 8.6

Si {œÜ0 , . . . , œÜn } es un conjunto ortogonal de funciones en un intervalo [a, b] respecto a la
funci√≥n de peso w, entonces la aproximaci√≥n por m√≠nimos cuadrados para f en [a, b] respecto
a w es
n

P(x) =

a j œÜ j (x),
j=0

donde, para cada j = 0, 1, . . . , n,

aj =

b
1
a w(x)œÜ j (x) f (x) d x
=
b
2
Œ±j
a w(x)[œÜ j (x)] d x

b
a

w(x)œÜ j (x) f (x) d x.

$SHVDUGHTXHODGH√ÄQLFLyQ\HOWHRUHPDSHUPLWHQFODVHVDPSOLDVGHIXQFLRQHV
ortogonales, en esta secci√≥n s√≥lo consideraremos conjuntos ortogonales de polinomios. El
siguiente teorema, que est√° basado en el proceso Gram-Schmidt, describe c√≥mo construir
polinomios ortogonales en [a, b] respecto a la funci√≥n de peso w.

8.2 Polinomios ortogonales y aproximaci√≥n por m√≠nimos cuadrados

Teorema 8.7

383

El conjunto de funciones polinomiales {œÜ0 , œÜ1 , . . . , œÜn } GH√ÄQLGR GH OD VLJXLHQWH IRUPD HV
ortogonal en [a, b] respecto a la funci√≥n de peso w:

œÜ0 (x) ‚â° 1

œÜ1 (x) = x ‚àí B1 ,para cada x en [a, b],

donde

B1 =
Erhard Schmidt (1876‚Äì1959)
recibi√≥ su doctorado bajo la
supervisi√≥n de David Hilbert,
en 1905, para un problema
relacionado con ecuaciones
integrales. Schmidt public√≥
en 1907 un art√≠culo en el que
proporcionaba lo que ahora se
conoce como proceso GramSchmidt para construir una base
ortonormal para un conjunto de
funciones. √âste generalizaba los
resultados de Jorgen Pedersen
Gram (1850‚Äì1916), quien
consider√≥ este problema al
estudiar los m√≠nimos cuadrados.
Sin embargo, Laplace present√≥
un proceso similar mucho antes
que Gram y que Schmidt.

Corolario 8.8

b
2
a xw(x)[œÜ0 (x)] d x
,
b
2
a w(x)[œÜ0 (x)] d x

y cuando k ‚â• 2,
œÜk (x) = (x ‚àí Bk )œÜk‚àí1 (x) ‚àí Ck œÜk‚àí2 (x), por cada x en [a, b]
donde
Bk =

b
2
a xw(x)[œÜk‚àí1 (x)] d x
b
2
a w(x)[œÜk‚àí1 (x)] d x

Ck =

b
a xw(x)œÜk‚àí1 (x)œÜk‚àí2 (x) d x
.
b
2
a w(x)[œÜk‚àí2 (x)] d x

y

El teorema 8.7 proporciona un procedimiento recursivo para construir un conjunto de
polinomios ortonormales. La prueba de este teorema se sigue al aplicar inducci√≥n matem√°tica al grado del polinomio œÜn (x).
Para cualquier n > 0, el conjunto de funciones polinomiales {œÜ0 , . . . , œÜn } dado en el teorema 8.7 es linealmente independiente en [a, b] y
b
a

w(x)œÜn (x)Q k (x) d x = 0,

para cualquier polinomio Q k (x) de grado k < n.
Demostraci√≥n Para cada k = 0, 1, . . . , n, œÜk (x) es un polinomio de grado k. Por lo que, el

teorema 8.2 implica que {œÜ0 , . . . , œÜn } es un conjunto linealmente independiente.
Sea Qk (x) un polinomio de grado k < n. Mediante el teorema 8.3, existen n√∫meros
c0 , . . . , ck de tal forma que
k

Q k (x) =

c j œÜ j (x).
j=0

Puesto que œÜn es ortogonal para œÜ j para cada j = 0, 1, . . . , k, tenemos
b
a

Ilustraci√≥n

k

w(x)Q k (x)œÜn (x) d x =

cj
j=0

b
a

k

w(x)œÜ j (x)œÜn (x) d x =

c j ¬∑ 0 = 0.
j=0

El conjunto de polinomios de Legendre, {Pn (x)}, es ortogonal en [21, 1] respecto a la
funci√≥n de peso w(x) ‚â° 1. /D GH√ÄQLFLyQ FOiVLFD GH ORV SROLQRPLRV GH /HJHQGUH UHTXLHUH
que Pn (1) = 1 para cada n y se utiliza una relaci√≥n recursiva para generar los polinomios
cuando n ‚â• 2. Esta normalizaci√≥n no ser√° necesaria en nuestro an√°lisis y los polinomios de
aproximaci√≥n por m√≠nimos cuadrados generados en cualquier caso son fundamentalmente
los mismos.

384

CAP√çTULO 8

Teor√≠a de aproximaci√≥n

Usando el proceso de Gram-Schmidth con P0 (x) ‚â° 1 obtenemos

B1 =

1
‚àí1 x d x
=0
1
‚àí1 d x

P1 (x) = (x ‚àí B1 )P0 (x) = x.

y

Adem√°s,
B2 =

1
3
‚àí1 x d x
=0
1
2
‚àí1 x d x

y C2 =

1
2
1
‚àí1 x d x
= ,
1
3
‚àí1 1 d x

por lo que

P2 (x) = (x ‚àí B2 )P1 (x) ‚àí C2 P0 (x) = (x ‚àí 0)x ‚àí

1
1
¬∑ 1 = x2 ‚àí .
3
3

/RVSROLQRPLRVGH/HJHQGUHGHJUDGRVXSHULRUTXHVHPXHVWUDQHQOD√ÄJXUDVHGHULvan de la misma forma. A pesar de que la integraci√≥n puede ser tediosa, no es dif√≠cil con un
sistema de √°lgebra para computadora.
Figura 8.9

y
1

y = P1(x)
y = P2(x)

0.5

y = P3(x)
y = P4(x)
y = P5(x)
1

21

x

20.5

21

Tenemos

P3 (x) = x P2 (x) ‚àí

4
1
3
4
P1 (x) = x 3 ‚àí x ‚àí x = x 3 ‚àí x,
15
3
15
5

y los siguientes dos polinomios de Legendre son

6
3
P4 (x) = x 4 ‚àí x 2 +
7
35

y

P5 (x) = x 5 ‚àí

10 3
5
x + x.
9
21

Los polinomios de Legendre fueron introducidos en la secci√≥n 4.7, en donde sus ra√≠ces,
determinadas en la p√°gina 168, se utilizaron como los nodos en la cuadratura gaussiana.
La secci√≥n Conjunto de ejercicios 8.2 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

8.3 Polinomios de Chebyshev y ahorro de series de potencia

385

8.3 Polinomios de Chebyshev y ahorro de series de potencia

Pafnuty Lvovich Chebyshev
(1821‚Äì1894) realiz√≥ un trabajo
excepcional en muchas √°reas
como matem√°ticas aplicadas,
teor√≠a de n√∫meros, teor√≠a de
aproximaci√≥n y probabilidad. En
1852, viaj√≥ desde St. Petersburgo
para visitar matem√°ticos en
Francia, Inglaterra y Alemania.
Lagrange y Legendre hab√≠an
estudiado los conjuntos
individuales de polinomios
ortogonales, pero Chebyshev
fue el primero en observar las
consecuencias importantes de
estudiar la teor√≠a en general.
√âl desarroll√≥ los polinomios
de Chebyshev para estudiar
la aproximaci√≥n por m√≠nimos
cuadrados y la probabilidad y,
despu√©s, aplicar sus resultados
a la interpolaci√≥n, cuadratura
aproximada y otras √°reas.

Los polinomios de Chebyshev {Tn (x)} son ortogonales en (21, 1) respecto a la funci√≥n de
peso w(x) = (1 ‚àí x 2 )‚àí1/2 . A pesar de que se pueden derivar con el m√©todo en la secci√≥n
SUHYLDHVIiFLOSURSRUFLRQDUVXGH√ÄQLFLyQ\GHVSXpVPRVWUDUTXHVDWLVIDFHQODVSURSLHGDGHV
de ortogonalidad requeridas.
Para x ‚àà [‚àí1, 1], GH√ÄQD

Tn (x) = cos[n arccos x],

para cada n ‚â• 0.

(8.8)

4XL]iDSDUWLUGHHVWDGH√ÄQLFLyQQRVHDREYLRTXHSDUDFDGDn, Tn (x) es un polinomio en x,
pero ahora mostraremos esto. Primero, observe que

T0 (x) = cos 0 = 1

y T1 (x) = cos(arccos x) = x.

Para n ‚â• 1, introducimos la sustituci√≥n Œ∏ = arccos x para cambiar esta ecuaci√≥n por

Tn (Œ∏ (x)) ‚â° Tn (Œ∏ ) = cos(nŒ∏), donde Œ∏ ‚àà [0, œÄ ].
Una relaci√≥n de recurrencia se deriva al observar que

Tn+1 (Œ∏ ) = cos(n + 1)Œ∏ = cos Œ∏ cos(nŒ∏) ‚àí sen Œ∏ sen(nŒ∏)
y
Tn‚àí1 (Œ∏ ) = cos(n ‚àí 1)Œ∏ = cos Œ∏ cos(nŒ∏) + sen Œ∏ sen(nŒ∏).
Al sumar estas ecuaciones obtenemos

Tn+1 (Œ∏) = 2 cos Œ∏ cos(nŒ∏) ‚àí Tn‚àí1 (Œ∏ ).
Al regresar a la variable x = cos Œ∏ , tenemos, para n ‚â• 1,

Tn+1 (x) = 2x cos(n arccos x) ‚àí Tn‚àí1 (x),
es decir,

Tn+1 (x) = 2x Tn (x) ‚àí Tn‚àí1 (x).

(8.9)

Puesto que T0 (x) = 1 y T1 (x) = x, la relaci√≥n de recurrencia implica que los siguientes
tres polinomios de Chebyshev son

T2 (x) = 2x T1 (x) ‚àí T0 (x) = 2x 2 ‚àí 1,
T3 (x) = 2x T2 (x) ‚àí T1 (x) = 4x 3 ‚àí 3x,
y
T4 (x) = 2x T3 (x) ‚àí T2 (x) = 8x 4 ‚àí 8x 2 + 1.
La relaci√≥n de recurrencia tambi√©n implica que cuando n ‚â• 1, Tn (x) es un polinomio
de grado nFRQFRH√ÄFLHQWHSULQFLSDO2 n‚àí1 . /DVJUi√ÄFDVGHT1 , T2 , T3 y T4 se muestran en la
√ÄJXUD

386

CAP√çTULO 8

Teor√≠a de aproximaci√≥n

Figura 8.10

y
1

y = T3(x)

y = T1(x)
y = T4(x)

1

21

x

21 y = T2(x)

Para mostrar la ortogonalidad de los polinomios de Chebyshev respecto a la funci√≥n de
peso w(x) = (1 ‚àí x 2 )‚àí1/2 , considere

Tn (x)Tm (x)
‚àö
dx =
1 ‚àí x2
‚àí1
1

cos(n arccos x) cos(m arccos x)
‚àö
d x.
1 ‚àí x2
‚àí1
1

Al reintroducir la sustituci√≥n Œ∏ = arccos x obtenemos

1
dŒ∏ = ‚àí ‚àö
dx
1 ‚àí x2
y

Tn (x)Tm (x)
‚àö
dx = ‚àí
1 ‚àí x2
‚àí1
1

0
œÄ

œÄ

cos(nŒ∏) cos(mŒ∏) dŒ∏ =

cos(nŒ∏) cos(mŒ∏) dŒ∏.

0

Suponga que n = m. Puesto que

cos(nŒ∏) cos(mŒ∏) =

1
[cos(n + m)Œ∏ + cos(n ‚àí m)Œ∏ ],
2

tenemos

1
Tn (x)Tm (x)
‚àö
dx =
2
2
1‚àíx
‚àí1

œÄ

1

cos((n + m)Œ∏ ) dŒ∏ +

0

1
2

œÄ

cos((n ‚àí m)Œ∏ ) dŒ∏

0

œÄ
1
1
sen((n + m)Œ∏ ) +
sen((n ‚àí m)Œ∏ ) = 0.
2(n + m)
2(n ‚àí m)
0

=

Mediante una t√©cnica similar (consulte el ejercicio 11), tambi√©n tenemos

œÄ
[Tn (x)]2
‚àö
dx = ,
2
2
1
‚àí
x
‚àí1
1

para cada n ‚â• 1.

(8.10)

Los polinomios de Chebyshev se utilizan para minimizar el error de aproximaci√≥n. Veremos c√≥mo se usan para resolver dos problemas de este tipo:

8.3 Polinomios de Chebyshev y ahorro de series de potencia

387

‚Ä¢ Una colocaci√≥n √≥ptima de puntos de interpolaci√≥n para minimizar el error en la interpolaci√≥n de Lagrange
‚Ä¢ Un medio para reducir el grado de un polinomio de aproximaci√≥n con p√©rdida m√≠nima de
precisi√≥n
El siguiente resultado afecta los ceros y los puntos extremos en Tn (x).
Teorema 8.9

El polinomio de Chebyshev Tn (x) de grado n ‚â• 1 tienen ceros simples en [21, 1] en

xÃÑk = cos

2k ‚àí 1
œÄ ,
2n

para cada k = 1, 2, . . . , n.

Adem√°s, Tn (x) toma sus m√°ximos absolutos en

xÃÑk = cos
Demostraci√≥n

kœÄ
n

con

Tn (xÃÑk ) = (‚àí1)k ,

para cada k = 0, 1, . . . , n.

Si

xÃÑk = cos

2k ‚àí 1
œÄ ,
2n

para k = 1, 2, . . . , n.

Entonces

2k ‚àí 1
œÄ
2n

Tn (xÃÑk ) = cos(n arccos xÃÑk ) = cos n arccos cos

= cos

2k ‚àí 1
œÄ
2

= 0.

Pero xÃÑk son distintas (consulte el ejercicio 12) y Tn (x) es un polinomio de grado n, por lo que
todos los ceros de Tn (x) deben tener esta forma.
Para mostrar la segunda declaraci√≥n, primero observe que

Tn (x) =

n sen(n arccos x)
d
‚àö
[cos(n arccos x)] =
dx
1 ‚àí x2

y que, cuando k = 1, 2, . . . , n ‚àí 1,

kœÄ
n

n sen n arccos cos
Tn (xÃÑk ) =

kœÄ
n

1 ‚àí cos

2

=

n sen(kœÄ )
= 0.
kœÄ
sen
n

Puesto que Tn (x) es un polinomio de grado n, su derivada Tn (x) es un polinomio de grado (n 2 1), y todos los ceros de Tn (x) se presentan en estos puntos distintos n 2 1 (que son
diferentes se considera en el ejercicio 13). Las √∫nicas otras posibilidades para los m√°ximos de
Tn (x) se presentan en los extremos del intervalo [21, 1], es decir, en xÃÑ0 = 1 y en xÃÑn = ‚àí1.
Para cualquier k 5 0, 1, . . ., n, tenemos

Tn (xÃÑk ) = cos n arccos

cos

kœÄ
n

= cos(kœÄ ) = (‚àí1)k .

Por lo que se presenta un m√°ximo en cada valor par de k y un m√≠nimo en cada valor impar.
Los polinomiosPyQLFRVGH&KHE\VKHY SROLQRPLRVFRQFRH√ÄFLHQWHSULQFLSDO  TÃÉn (x)
se derivan a partir de los polinomios de Chebyshev Tn (x)DOGLYLGLUHOFRH√ÄFLHQWHSULQFLSDO
2n21. Por lo tanto,

TÃÉ0 (x) = 1

y TÃÉn (x) =

1
2n‚àí1

Tn (x), por cada n ‚â• 1.

(8.11)

388

CAP√çTULO 8

Teor√≠a de aproximaci√≥n

La relaci√≥n de recurrencia satisfecha por los polinomios de Chebyshev implica que

1
TÃÉ2 (x) = x TÃÉ1 (x) ‚àí TÃÉ0 (x) y
2
1
TÃÉn+1 (x) = x TÃÉn (x) ‚àí TÃÉn‚àí1 (x), por cada n ‚â• 2.
4

(8.12)

/DVJUi√ÄFDVGHTÀú1 , TÀú2 , TÀú3 , TÀú4 y TÀú5VHPXHVWUDQHQOD√ÄJXUD

Figura 8.11

y
,

y = T1(x)

1

,

y = T2(x)
,

,

y = T3(x)

y = T5(x)
21

,

y = T4(x)
1

x

21

Puesto que TÃÉn (x) es s√≥lo un m√∫ltiplo de Tn (x), el teorema 8.9 implica que los ceros de
TÃÉn (x) tambi√©n se presentan en

xÃÑk = cos

2k ‚àí 1
œÄ ,
2n

para cada k = 1, 2, . . . , n,

y los valores extremos de TÃÉn (x) para n ‚â• 1, ocurre en

xÃÑk = cos

kœÄ
,
n

con TÃÉn (xÃÑk ) =

(‚àí1)k
,
2n‚àí1

para cada k = 0, 1, 2, . . . , n. (8.13)

Sea que n denota el conjunto de todos los polinomios m√≥nicos de grado n. La relaci√≥n expresada en la ecuaci√≥n (8.13) conduce a una propiedad de minimizaci√≥n importante
que distingue TÃÉn (x) de los otros miembros de n.
Teorema 8.10

Los polinomios de la forma TÃÉn (x) cuando n ‚â• 1, tienen la propiedad de que

1
= m√°x |TÀún (x)| ‚â§ m√°x |Pn (x)|,
x‚àà[‚àí1,1]
x‚àà[‚àí1,1]
2n‚àí1
Adem√°s, la igualdad se presenta s√≥lo si Pn ‚â° TÀún .

para toda Pn (x) ‚àà

n

.

8.3 Polinomios de Chebyshev y ahorro de series de potencia
Demostraci√≥n

Suponga que Pn (x) ‚àà

389

n y que

m√°x |Pn (x)| ‚â§

x‚àà[‚àí1,1]

1
= m√°x |TÀún (x)|.
x‚àà[‚àí1,1]
2n‚àí1

Sea Q = TÀún ‚àí Pn . Entonces tanto TÀún (x) como Pn (x) son polinomios m√≥nicos de grado n,
por lo que Q(x) es un polinomio de grado m√°ximo (n 2 1). Adem√°s, en los n 1 1 puntos
extremos xÃÑk de TÀún (x), tenemos
k

(‚àí1)
Q(xÃÑk ) = TÀún (xÃÑk ) ‚àí Pn (xÃÑk ) = n‚àí1 ‚àí Pn (xÃÑk ).
2
Sin embargo,
|Pn (xÃÑk )| ‚â§

1
2n‚àí1

,

para cada k = 0, 1, . . . , n,

por lo que tenemos
Q(xÃÑk ) ‚â§ 0,

cuando k es impar

y Q(xÃÑk ) ‚â• 0,

cuando k es par.

Puesto que Q es continuo, el teorema de valor intermedio implica que para cada j 5
0, 1, . . . , n ‚àí 1, el polinomio Q(x) tiene por lo menos un cero entre xÃÑ j y xÃÑ j+1 . Por lo tanto,
Q tiene por lo menos n ceros en el intervalo [21, 1]. Pero el grado de Q(x) es menor que n,
por lo que Q ‚â° 0. Esto implica que Pn ‚â° TÀún .

Minimizaci√≥n del error en la interpolaci√≥n de Lagrange
El teorema 8.10 se puede utilizar para responder la pregunta de cu√°ndo colocar nodos interpolantes para minimizar el error en la interpolaci√≥n de Lagrange. El teorema 3.3 en la p√°gina
83 aplicado al intervalo [21, 1] establece que, si x0 , . . . , xn son n√∫meros distintos en el
intervalo [21, 1] y si f ‚àà C n+1 [‚àí1, 1], entonces, para cada x ‚àà [‚àí1, 1], existe un n√∫mero
Œæ(x) en (21, 1) con

f (x) ‚àí P(x) =

f (n+1) (Œæ(x))
(x ‚àí x0 )(x ‚àí x1 ) ¬∑ ¬∑ ¬∑ (x ‚àí xn ),
(n + 1)!

donde P(x) es el polinomio de interpolaci√≥n de Lagrange. En general, no existe control sobre
Œæ(x), por lo que minimizar el error mediante la colocaci√≥n acertada de los nodos x0 , . . . , xn,
seleccionamos x0 , . . . , xn para minimizar la cantidad

|(x ‚àí x0 )(x ‚àí x1 ) ¬∑ ¬∑ ¬∑ (x ‚àí xn )|
a lo largo del intervalo [21, 1].
Puesto que (x ‚àí x0 )(x ‚àí x1 ) ¬∑ ¬∑ ¬∑ (x ‚àí xn ) es un polinomio m√≥nico de grado (n 1 1),
acabamos de observar que el m√≠nimo se obtiene cuando

(x ‚àí x0 )(x ‚àí x1 ) ¬∑ ¬∑ ¬∑ (x ‚àí xn ) = TÃÉn+1 (x).
El valor m√°ximo de |(x ‚àí x0 )(x ‚àí x1 ) ¬∑ ¬∑ ¬∑ (x ‚àí xn )| es m√°s peque√±o cuando se selecciona xk para cada k 5 0, 1, . . ., n es el (k 1 1)-√©simo cero de TÃÉn+1. Por lo tanto seleccionamos
xk como

xÃÑk+1 = cos

2k + 1
œÄ .
2(n + 1)

Puesto que m√°x x‚àà[‚àí1,1] |TÃÉn+1 (x)| = 2‚àín , esto tambi√©n implica que

1
= m√°x |(x ‚àí xÃÑ1 ) ¬∑ ¬∑ ¬∑ (x ‚àí xÃÑn+1 )| ‚â§ m√°x |(x ‚àí x0 ) ¬∑ ¬∑ ¬∑ (x ‚àí xn )|,
x‚àà[‚àí1,1]
x‚àà[‚àí1,1]
2n
para cualquier selecci√≥n de x0, x1, . . . , xn en el intervalo [21, 1]. El siguiente corolario sigue
estas observaciones.

390

CAP√çTULO 8

Teor√≠a de aproximaci√≥n

Corolario 8.11

Suponga que P(x) es el polinomio de interpolaci√≥n de grado a lo sumo n con nodos en los
ceros de Tn+1 (x). Entonces

m√°x | f (x) ‚àí P(x)| ‚â§

x‚àà[‚àí1,1]

1

m√°x | f (n+1) (x)|,

2n (n + 1)! x‚àà[‚àí1,1]

para cada f ‚àà C n+1 [‚àí1, 1].

Minimizaci√≥n del error de aproximaci√≥n en intervalos arbitrarios
La t√©cnica para seleccionar puntos para minimizar el error de interpolaci√≥n se ampl√≠a hasta
un intervalo cerrado general [a, b] al utilizar el cambio de variables

xÃÉ =

1
[(b ‚àí a)x + a + b]
2

para transformar los n√∫meros xÃÑk en el intervalo [21, 1] en el n√∫mero correspondiente xÃÑk en
el intervalo [a, b], como se muestra en el siguiente ejemplo.
Ejemplo 1

Sea f (x) = xe x en [0, 1.5]. Compare los valores determinados por el polinomio de Lagrange con cuatro nodos igualmente espaciados con los dados por el polinomio de Lagrange con
nodos determinados por los ceros del cuarto polinomio de Chebyshev.
Soluci√≥n

Los nodos igualmente espaciados x0 = 0, x1 = 0.5, x2 = 1, y x3 = 1.5 dan

L 0 (x) = ‚àí1.3333x 3 + 4.0000x 2 ‚àí 3.6667x + 1,
L 1 (x) = 4.0000x 3 ‚àí 10.000x 2 + 6.0000x,
L 2 (x) = ‚àí4.0000x 3 + 8.0000x 2 ‚àí 3.0000x, y
L 3 (x) = 1.3333x 3 ‚àí 2.000x 2 + 0.66667x,
que produce el polinomio

P3 (x) = L 0 (x)(0) + L 1 (x)(0.5e0.5 ) + L 2 (x)e1 + L 3 (x)(1.5e1.5 )
= 1.3875x 3 + 0.057570x 2 + 1.2730x.
Para el segundo polinomio de interpolaci√≥n, cambiamos los ceros xÃÑk = cos((2k + 1)
/8)œÄ, para k = 0, 1, 2, 3, de TÀú4 , desde [21, 1] hasta [0, 1.5],por medio de la transformaci√≥n
lineal

xÃÉk =

1
[(1.5 ‚àí 0)xÃÑ k + (1.5 + 0)] = 0.75 + 0.75xÃÑk .
2

Puesto que

œÄ
3œÄ
5œÄ
= 0.92388, xÃÑ1 = cos
= 0.38268, xÃÑ2 = cos
= ‚àí0.38268, y
8
8
8
7œÄ
xÃÑ3 = cos
= ‚àí0.92388,
8
xÃÑ0 = cos

tenemos

xÃÉ0 = 1.44291,

xÃÉ1 = 1.03701,

xÃÉ2 = 0.46299,

y

xÃÉ 3 = 0.05709.

/RVFRH√ÄFLHQWHVSROLQRPLDOHVGH/DJUDQJHSDUDHVWHFRQMXQWRGHQRGRVVRQ

LÃÉ 0 (x) = 1.8142x 3 ‚àí 2.8249x 2 + 1.0264x ‚àí 0.049728,
LÃÉ 1 (x) = ‚àí4.3799x 3 + 8.5977x 2 ‚àí 3.4026x + 0.16705,
LÃÉ 2 (x) = 4.3799x 3 ‚àí 11.112x 2 + 7.1738x ‚àí 0.37415, y
LÃÉ 3 (x) = ‚àí1.8142x 3 + 5.3390x 2 ‚àí 4.7976x + 1.2568.

8.3 Polinomios de Chebyshev y ahorro de series de potencia

391

Los valores funcionales requeridos para estos polinomios se dan en las √∫ltimas dos columnas de la tabla 8.7. El polinomio de interpolaci√≥n de grado m√°ximo 3 es

PÃÉ3 (x) = 1.3811x 3 + 0.044652x 2 + 1.3031x ‚àí 0.014352.

Tabla 8.7

x

f (x) = xe x

xÃÉ

f (xÃÉ) = xe x

x0 = 0.0
x1 = 0.5
x2 = 1.0
x3 = 1.5

0.00000
0.824361
2.71828
6.72253

xÃÉ0 = 1.44291
xÃÉ1 = 1.03701
xÃÉ2 = 0.46299
xÃÉ3 = 0.05709

6.10783
2.92517
0.73560
0.060444

Para comparaci√≥n, en la tabla 8.8 se listan varios valores de x, junto con los valores de
f (x), P3(x) y PÃÉ3 (x). A partir de esta tabla se puede observar que, a pesar de que el error por
medio de P3(x) es menor al que resulta de utilizar PÃÉ3 (x) alrededor de la mitad de la tabla, el
error m√°ximo relacionado con el uso de PÃÉ3 (x), 0.0180, es considerablemente menor cuando
se utiliza P3(x ORFXDOGDHOHUURUGH YpDVHOD√ÄJXUD 

Tabla 8.8

x

f (x) = xe x

P3 (x)

|xe x ‚àí P3 (x)|

PÃÉ3 (x)

|xe x ‚àí PÃÉ3 (x)|

0.15
0.25
0.35
0.65
0.75
0.85
1.15
1.25
1.35

0.1743
0.3210
0.4967
1.245
1.588
1.989
3.632
4.363
5.208

0.1969
0.3435
0.5121
1.233
1.572
1.976
3.650
4.391
5.237

0.0226
0.0225
0.0154
0.012
0.016
0.013
0.018
0.028
0.029

0.1868
0.3358
0.5064
1.231
1.571
1.974
3.644
4.382
5.224

0.0125
0.0148
0.0097
0.014
0.017
0.015
0.012
0.019
0.016

Figura 8.12
y
6

,

y = P3(x)

5

y 5 xe x

4
3
2
1
0.5

1.0

1.5

x

392

CAP√çTULO 8

Teor√≠a de aproximaci√≥n

Reducci√≥n del grado de los polinomios de aproximaci√≥n
Los polinomios de Chebyshev tambi√©n se pueden utilizar para reducir el grado de un polinomio de aproximaci√≥n con una p√©rdida m√≠nima de precisi√≥n. Puesto que los polinomios de
Chebyshev tienen un valor absoluto m√°ximo-m√≠nimo que se distribuye de manera uniforme
en un intervalo, se pueden utilizar para reducir el grado de un polinomio de aproximaci√≥n
sin exceder la tolerancia del error.
Considere aproximar un polinomio arbitrario de en√©simo grado.

Pn (x) = an x n + an‚àí1 x n‚àí1 + ¬∑ ¬∑ ¬∑ + a1 x + a0
en [21, 1] con un polinomio de grado como m√°ximo n 2 1. El objetivo es seleccionar
Pn‚àí1 (x) en n‚àí1, de tal forma que

m√°x |Pn (x) ‚àí Pn‚àí1 (x)|

x‚àà[‚àí1,1]

sea tan peque√±o como resulte posible.
Primero observamos que (Pn (x) ‚àí Pn‚àí1 (x))/an es un polinomio m√≥nico de grado n, por
lo que al aplicar el teorema 8.10 obtenemos

m√°x

x‚àà[‚àí1,1]

1
1
(Pn (x) ‚àí Pn‚àí1 (x)) ‚â• n‚àí1 .
an
2

La igualdad se presenta precisamente cuando

1
(Pn (x) ‚àí Pn‚àí1 (x)) = TÃÉn (x).
an
(VWRVLJQL√ÄFDTXHGHEHUtDPRVVHOHFFLRQDU

Pn‚àí1 (x) = Pn (x) ‚àí an TÃÉn (x),
y con esta selecci√≥n tenemos el valor m√≠nimo

|an |
1
(Pn (x) ‚àí Pn‚àí1 (x)) = n‚àí1 .
x‚àà[‚àí1,1] an
2

m√°x |Pn (x) ‚àí Pn‚àí1 (x)| = |an | m√°x

x‚àà[‚àí1,1]

Ilustraci√≥n

La funci√≥n f (x) = e x se aproxima en el intervalo [21, 1] mediante el cuarto polinomio de
Maclaurin

P4 (x) = 1 + x +

x2
x3
x4
+
+ ,
2
6
24

que tiene error de truncamiento

|R4 (x)| =

| f (5) (Œæ(x))||x 5 |
e
‚â§
‚âà 0.023,
120
120

para ‚àí 1 ‚â§ x ‚â§ 1.

Suponga que un error de 0.05 es tolerable y que nos gustar√≠a reducir el grado del polinomio de aproximaci√≥n mientras nos mantenemos dentro de esta cota.
El polinomio de grado 3 o menor, que mejor se aproxima de manera uniforme a P4(x)
en [21, 1]

P3 (x) = P4 (x) ‚àí a4 TÃÉ4 (x) = 1 + x +
=

x3
x4
1
x2
+
+
‚àí
2
6
24 24

191
13
1
+ x + x 2 + x 3.
192
24
6

x4 ‚àí x2 +

1
8

8.4

Aproximaci√≥n de funci√≥n racional

393

Con esta selecci√≥n, tenemos

|P4 (x) ‚àí P3 (x)| = |a4 TÃÉ4 (x)| ‚â§

1 1
1
¬∑ 3 =
‚â§ 0.0053.
24 2
192

Al sumar esta cota de error a la cota del error de truncamiento de Maclaurin obtenemos
0.023 1 0.0053 5 0.0283,
que est√° dentro del error permisible de 0.05.
El polinomio de grado 2 o menor, que mejor se aproxima de manera uniforme a P3 (x)
en [21, 1]

1
P2 (x) = P3 (x) ‚àí TÃÉ3 (x)
6
13
1
1
191
+ x + x2 + x3 ‚àí
192
24
6
6

=

3
x3 ‚àí x
4

=

13
191 9
+ x + x 2.
192 8
24

Sin embargo,

|P3 (x) ‚àí P2 (x)| =

1
1
TÃÉ3 (x) =
6
6

1
2

2

=

1
‚âà 0.042,
24

que, cuando se suma a la cota del error ya acumulada de 0.0283, excede la tolerancia de 0.05.
Por consiguiente, el polinomio de menor grado que mejor se aproxima a ex en [21, 1] con
una cota de error menor a 0.05 es

P3 (x) =

13
191
1
+ x + x 2 + x 3.
192
24
6

La tabla 8.9 muestra la funci√≥n y los polinomios de aproximaci√≥n en diferentes puntos en
[21, 1]. Observe que las entradas tabuladas para P2 se encuentran dentro de la tolerancia de
0.05, aunque la cota de error para P2(x) excedi√≥ la tolerancia.

Tabla 8.9

x

ex

P4 (x)

P3 (x)

P2 (x)

|e x ‚àí P2 (x)|

‚àí0.75
‚àí0.25
0.00
0.25
0.75

0.47237
0.77880
1.00000
1.28403
2.11700

0.47412
0.77881
1.00000
1.28402
2.11475

0.47917
0.77604
0.99479
1.28125
2.11979

0.45573
0.74740
0.99479
1.30990
2.14323

0.01664
0.03140
0.00521
0.02587
0.02623

La secci√≥n Conjunto de ejercicios 8.3 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

8.4 Aproximaci√≥n de funci√≥n racional
La clase de polinomios algebraicos tiene algunas ventajas diferentes para uso en aproximaci√≥n:
¬á ([LVWHXQQ~PHURVX√ÄFLHQWHGHSROLQRPLRVSDUDDSUR[LPDUFXDOTXLHUIXQFLyQFRQWLQXDHQ
un intervalo cerrado dentro de una tolerancia arbitraria.

394

CAP√çTULO 8

Teor√≠a de aproximaci√≥n

‚Ä¢ Los polinomios se eval√∫an f√°cilmente en valores arbitrarios.
‚Ä¢ Las derivadas e integrales de los polinomios existen y se determinan de manera sencilla.
La desventaja de utilizar polinomios para aproximaci√≥n es su tendencia a la oscilaci√≥n.
Con frecuencia, esto causa cotas de error en la aproximaci√≥n polinomial que exceden sigQL√ÄFDWLYDPHQWHHOHUURUSURPHGLRGHDSUR[LPDFLyQSRUTXHODVFRWDVGHOHUURUVHGHWHUPLQDQ
mediante el error m√°ximo de aproximaci√≥n. Ahora consideramos los m√©todos que distribuyen el error de aproximaci√≥n de modo uniforme sobre el intervalo de aproximaci√≥n. Estas
t√©cnicas implican funciones racionales.
Una funci√≥n racional r de grado N tiene la forma

r (x) =

p(x)
,
q(x)

donde p(x) y q(x) son polinomios cuyos grados suman N.
Todos los polinomios son una funci√≥n racional (simplemente haga q(x) ‚â° 1), por lo
que la aproximaci√≥n mediante funciones racionales proporciona resultados que no son peores que la aproximaci√≥n por medio de polinomios. Sin embargo, las funciones racionales
cuyo numerador y denominador tienen el mismo o casi el mismo grado, a menudo producen
resultados de aproximaci√≥n superiores a los m√©todos polinomiales para la misma cantidad
de esfuerzo computacional. (Esta declaraci√≥n est√° basada en la suposici√≥n de que la cantidad de esfuerzo computacional requerido para divisi√≥n es aproximadamente igual al de la
multiplicaci√≥n.)
/DVIXQFLRQHVUDFLRQDOHVWLHQHQODYHQWDMDDxDGLGDGHSHUPLWLUDSUR[LPDFLyQH√ÄFLHQWHGH
IXQFLRQHVFRQGLVFRQWLQXLGDGHVLQ√ÄQLWDVFHUFDSHURIXHUDGHOLQWHUYDORGHDSUR[LPDFLyQ(Q
general, la aproximaci√≥n polinomial es inaceptable en esta situaci√≥n.

Aproximaci√≥n de Pad√©
Henri Pad√© (1863‚Äì1953) aport√≥
un estudio sistem√°tico de lo que
hoy llamamos aproximaciones de
Pad√© en su tesis doctoral
en 1892. Prob√≥ los resultados en
su estructura general y tambi√©n
estableci√≥ claramente la conexi√≥n
entre las aproximaciones de
Pad√© y fracciones continuas.
Sin embargo, Daniel Bernoulli
(1700‚Äì1782) y otros hab√≠an
estudiado estas ideas desde 1730.
James Stirling (1692‚Äì1770)
present√≥ un m√©todo similar
en Methodus differentialis
(M√©todo diferencial) publicado
en el mismo a√±o y Euler us√≥
la aproximaci√≥n de Pad√© para
encontrar la suma de una serie.

Suponga que r es una funci√≥n racional de grado N 5 n 1 m de la forma

r (x) =

p(x)
p0 + p 1 x + ¬∑ ¬∑ ¬∑ + p n x n
,
=
q(x)
q0 + q1 x + ¬∑ ¬∑ ¬∑ + qm x m

que se utiliza para aproximar una funci√≥n f en un intervalo cerrado I que contiene a cero. Para
que rHVWpGH√ÄQLGDHQFHURVHUHTXLHUHTXHq0 = 0. De hecho, podemos suponer que q0 = 1,
si √©ste no es el caso, simplemente reemplazamos p(x) por p(x)/q0 y q(x) por q(x)/q0 . Por
consiguiente, existen N 1 1 par√°metros q1 , q2 , . . . , qm , p0 , p1 , . . . , pn disponibles para la
aproximaci√≥n de f mediante r.
La t√©cnica de aproximaci√≥n de Pad√© es la extensi√≥n de la aproximaci√≥n polinomial
de Taylor para las funciones racionales. Se seleccionan los par√°metros N 1 1 de tal forma
que f (k) (0) = r (k) (0), para cada k = 0, 1, . . . , N . Cuando n 5 N y m 5 0, la aproximaci√≥n
de Pad√© es simplemente el en√©simo polinomio de Maclaurin.
Considere la diferencia

f (x) ‚àí r (x) = f (x) ‚àí

p(x)
f (x)q(x) ‚àí p(x)
f (x)
=
=
q(x)
q(x)

m
i
i=0 qi x ‚àí

y suponga que f tiene la expansi√≥n de la serie de Maclaurin f (x) =

f (x) ‚àí r (x) =

‚àû
i
i=0 ai x

m
i
i=0 qi x ‚àí

q(x)

n
i
i=0 pi x

q(x)

‚àû
i
i=0 ai x .

.

n
i
i=0 pi x

Entonces
(8.14)

El objetivo es seleccionar las constantes q1 , q2 , . . . , qm y p0 , p1 , . . . , pn de tal forma que

f (k) (0) ‚àí r (k) (0) = 0, para cada k = 0, 1, . . . , N .

8.4

Aproximaci√≥n de funci√≥n racional

395

En la secci√≥n 2.4 (consulte, en particular, el ejercicio 10 en el Conjunto de ejercicios 2.4 en
l√≠nea) encontramos que esto es equivalente a f 2 r que tiene un cero de multiplicidad N 1 1
en x 5 0. Como consecuencia, seleccionamos q1 , q2 , . . . , qm y p0 , p1 , . . . , pn por lo que el
numerador en el lado derecho de la ecuaci√≥n (8.14),

(a0 + a1 x + ¬∑ ¬∑ ¬∑ )(1 + q1 x + ¬∑ ¬∑ ¬∑ + qm x m ) ‚àí ( p0 + p1 x + ¬∑ ¬∑ ¬∑ + pn x n ),

(8.15)

no tiene t√©rminos de grado menor o igual a N.
3DUDVLPSOL√ÄFDUODQRWDFLyQGH√ÄQLPRV pn+1 = pn+2 = ¬∑ ¬∑ ¬∑ = p N = 0 y qm+1 = qm+2
= ¬∑ ¬∑ ¬∑ = q N = 0. $KRUD SRGHPRV H[SUHVDU HO FRH√ÄFLHQWH GH x k en la expresi√≥n (8.15) de
manera m√°s compacta como
k

ai qk‚àíi

‚àí pk .

i=0

La funci√≥n racional para la aproximaci√≥n de Pad√© resulta a partir de la soluci√≥n de N 1 1
ecuaciones lineales
k

ai qk‚àíi = pk ,

k = 0, 1, . . . , N

i=0

en las N 1 1 inc√≥gnitas q1 , q2 , . . . , qm , p0 , p1 , . . . , pn .
Ejemplo 1

La expansi√≥n de la serie de Maclaurin para e‚àíx es
‚àû

(‚àí1)i i
x.
i!
i=0
Encuentre la aproximaci√≥n de Pad√© para e‚àíx de grado 5 con n 5 3 y m 5 2.
Para encontrar la aproximaci√≥n de Pad√©, necesitamos seleccionar p0 , p1 , p2 , p3 ,
q1 , y q2SRUORTXHORVFRH√ÄFLHQWHVGHxk para k = 0, 1, . . . , 5 son 0 en la expresi√≥n

Soluci√≥n

1‚àíx +

x3
x2
‚àí
+ ¬∑ ¬∑ ¬∑ (1 + q1 x + q2 x 2 ) ‚àí ( p0 + p1 x + p2 x 2 + p3 x 3 ).
2
6

La expansi√≥n y agrupaci√≥n de los t√©rminos produce

x5 :
x4 :
x3 :

‚àí

1
1
1
+ q1 ‚àí q2 = 0;
120 24
6
1
1
1
‚àí q1 + q2 = 0;
24
6
2
1
1
‚àí + q 1 ‚àí q 2 = p3 ;
6
2

x2 :

1
‚àí q 1 + q 2 = p2 ;
2

x1 :

‚àí1 + q1

= p1 ;

x0 :

1

= p0 .

La soluci√≥n de este sistema es

3
3
1
2
1
p1 = ‚àí , p2 =
.
, p3 = ‚àí , q 1 = , q 2 =
5
20
60
5
20
Por lo que la aproximaci√≥n de Pad√© es

r (x) =

3 2
1 3
x ‚àí 60
x
1 ‚àí 35 x + 20
1 2
1 + 25 x + 20
x

.

La tabla 8.10 muestra los valores de r(x) y P 5(x), el quinto polinomio de Maclaurin. La aproximaci√≥n de Pad√© es claramente superior en este ejemplo.

396

CAP√çTULO 8

Teor√≠a de aproximaci√≥n

Tabla 8.10

x

e‚àíx

P5 (x)

|e‚àíx ‚àí P5 (x)|

r (x)

|e‚àíx ‚àí r (x)|

0.2
0.4
0.6
0.8
1.0

0.81873075
0.67032005
0.54881164
0.44932896
0.36787944

0.81873067
0.67031467
0.54875200
0.44900267
0.36666667

8.64 √ó 10‚àí8
5.38 √ó 10‚àí6
5.96 √ó 10‚àí5
3.26 √ó 10‚àí4
1.21 √ó 10‚àí3

0.81873075
0.67031963
0.54880763
0.44930966
0.36781609

7.55 √ó 10‚àí9
4.11 √ó 10‚àí7
4.00 √ó 10‚àí6
1.93 √ó 10‚àí5
6.33 √ó 10‚àí5

El algoritmo 8.1 implementa la t√©cnica de aproximaci√≥n de Pad√©.

ALGORITMO

8.1

Aproximaci√≥n Racional de Pad√©
Para obtener la aproximaci√≥n

r (x) =

p(x)
=
q(x)

n
i
i=0 pi x
m
j
j=0 q j x

para una funci√≥n determinada f (x):
ENTRADA enteros no negativos m y n.
SALIDA

coeficientes q0 , q1 , . . . , qm y p0 , p1 , . . . , pn .

Paso 1 Determine N = m + n.
f (i) (0)
.
i!
(Los coeficientes del polinomio de Maclaurin son a 0 , . . . , a N , que ser√≠a la
entrada en lugar de calcularlos. )

Paso 2 Para i = 0, 1, . . . , N determine ai =

Paso 3 Determine q0 = 1;
p0 = a 0 .
Paso 4 Para i = 1, 2, . . . , N haga los pasos 5‚Äì10. (Establezca un sistema lineal con
matriz B. )
Paso 5 Para j = 1, 2, . . . , i ‚àí 1
si j ‚â§ n entonces haga bi, j = 0.
Paso 6 Si i ‚â§ n entonces haga bi,i = 1.
Paso 7 Para j = i + 1, i + 2, . . . , N determine bi, j = 0.
Paso 8 Para j = 1, 2, . . . , i
si j ‚â§ m entonces haga bi,n+ j = ‚àíai‚àí j .
Paso 9 Para j = n + i + 1, n + i + 2, . . . , N determine bi, j = 0.
Paso 10 Determina bi,N +1 = ai .
(Los pasos 11‚Äì22 resuelven el sistema lineal mediante pivoteo parcial.)
Paso 11 Para i = n + 1, n + 2, . . . , N ‚àí 1 haga los pasos 12‚Äì18.
Paso 12 Sea k el entero m√°s peque√±o con y i ‚â§ k ‚â§ N y |bk,i |
= m√°xi‚â§ j‚â§N |b j,i |.
(Encuentre el elemento pivote.)

8.4

Aproximaci√≥n de funci√≥n racional

397

Paso 13 Si bk,i = 0 entonces SALIDA (‚ÄúEl sistema es singular‚Äù);
PARE.
Paso 14 Si k = i entonces (Intercambia fila i y fila k.)
para j = i, i + 1, . . . , N + 1 determine
bCOPY = bi, j ;
bi, j = bk, j ;
bk, j = bCOPY .
Paso 15 Para j = i + 1, i + 2, . . . , N haga los pasos 16‚Äì18. (Realice la eliminaci√≥n.)
Paso 16 Determine xm =

b j,i
.
bi,i

Paso 17 Para k = i + 1, i + 2, . . . , N + 1
determine b j,k = b j,k ‚àí xm ¬∑ bi,k .
Paso 18 Determine b j,i = 0.
Paso 19 Si b N ,N = 0 entonces SALIDA (‚ÄúEl sistema es singular‚Äù);
PARE.
Paso 20 Si m > 0 entonces determine qm =

b N ,N +1
.
b N ,N

(Inicia la sustituci√≥n regresiva.)

Paso 21 Para i = N ‚àí 1, N ‚àí 2, . . . , n + 1 determine qi‚àín =
Paso 22 Para i = n, n ‚àí 1, . . . , 1 determine pi = bi,N +1 ‚àí

bi,N +1 ‚àí

N
j=i+1 bi, j q j‚àín

bi,i
N
b
q
j=n+1 i, j j‚àín .

.

Paso 23 SALIDA (q0 , q1 , . . . , qm , p0 , p1 , . . . , pn );
PARE. ( El procedimiento fue exitoso. )

Aproximaci√≥n de fracci√≥n continuada
Es interesante comparar el n√∫mero de operaciones aritm√©ticas requeridas para los c√°lculos
de P5(x) y r(x) en el ejemplo 1. Mediante multiplicaci√≥n anidada, P5(x) se puede expresar
como

P5 (x) =

‚àí

1
1
x+
120
24

x‚àí

1
6

x+

1
2

x ‚àí 1 x + 1.

$OVXSRQHUTXHORVFRH√ÄFLHQWHVGH 1 , x, x 2 , x 3 , x 4 y x 5 se representan como decimales, un
solo c√°lculo de P5(x) en forma anidada requiere cinco multiplicaciones y cinco sumas/restas.
Usando multiplicaci√≥n anidada, r(x) se expresa como

r (x) =

1
3
‚àí 60
x + 20
x ‚àí 35 x + 1
1
x + 25
20

x +1

,

por lo que un solo c√°lculo de r(x) requiere cinco multiplicaciones, cinco sumas/restas y
una divisi√≥n. Por lo tanto, el esfuerzo computacional parece favorecer la aproximaci√≥n
polinomial.

398

CAP√çTULO 8

Teor√≠a de aproximaci√≥n

Sin embargo, al reexpresar r(x) mediante divisi√≥n continua, podemos escribir

r (x) =
=

3 2
1 3
1 ‚àí 35 x + 20
x ‚àí 60
x
1 2
1 + 25 x + 20
x

‚àí 13 x 3 + 3x 2 ‚àí 12x + 20
x 2 + 8x + 20

x ‚àí 280
)
17 (‚àí 152
1
3
+ 2 3
=‚àí x+
3
3
x + 8x + 20
17
1
+
=‚àí x+
3
3

‚àí 152
3
x 2 +8x+20
x+(35/19)

o
1
17
r (x) = ‚àí x +
+
3
3

Usando fracciones continuadas
para aproximaci√≥n racional es
un tema que se origina en los
trabajos de Christopher Clavius
(1537‚Äì1612). Por ejemplo, Euler,
Lagrange y Hermite lo usaron en
los siglos XVIII y XIX.

‚àí 152
3

.

(8.16)

3125/361
x + 117
+ (x+(35/19))
19

Escrita de esta forma, un solo c√°lculo de r(x) requiere una multiplicaci√≥n, cinco sumas/
restas y dos divisiones. Si la cantidad de c√°lculos requeridos para divisi√≥n es aproximadamente igual para la multiplicaci√≥n, el esfuerzo computacional requerido para una evaluaci√≥n
del polinomio P5(x H[FHGHVLJQL√ÄFDWLYDPHQWHHOUHTXHULGRSDUDXQDHYDOXDFLyQGHODIXQci√≥n racional r(x).
Al expresar una aproximaci√≥n de funci√≥n racional en una forma como la ecuaci√≥n (8.16)
recibe el nombre de fracci√≥n continuada. √âsta es una t√©cnica de aproximaci√≥n cl√°sica de
LQWHUpVDFWXDOGHELGRDODH√ÄFLHQFLDFRPSXWDFLRQDOGHVXUHSUHVHQWDFLyQ(VVLQHPEDUJR
una t√©cnica especializada que analizaremos m√°s adelante. Un tratamiento bastante amplio de
este tema y de la aproximaci√≥n racional en general se puede encontrar en [RR], p. 285-322.
A pesar de que la aproximaci√≥n de la funci√≥n racional en el ejemplo 1 da resultados
superiores a la aproximaci√≥n polinomial del mismo grado observe que la aproximaci√≥n tiene
una amplia variaci√≥n en precisi√≥n. La aproximaci√≥n en 0.2 es precisa dentro de 8 3 1029,
pero en 1.0 la aproximaci√≥n y la funci√≥n s√≥lo concuerdan dentro de 7 3 1025. Se espera esta
variaci√≥n de precisi√≥n porque la aproximaci√≥n de Pad√© est√° basada en una representaci√≥n
de e2x, y la representaci√≥n de Taylor tiene una amplia variaci√≥n de precisi√≥n en [0.2, 1.0].

Aproximaci√≥n de la funci√≥n racional de Chebyshev
Para obtener aproximaciones de funci√≥n racional precisa de forma m√°s uniforme, usamos
los polinomios de Chebyshev. El m√©todo general de aproximaci√≥n de funci√≥n racional de
Chebyshev procede de la misma forma que la aproximaci√≥n de Pad√©, excepto que cada
t√©rmino x k en la aproximaci√≥n de Pad√© se reemplaza por el polinomio de k-√©simo grado de
Chebyshev T k (x).
Suponga que queremos aproximar la funci√≥n f mediante una funci√≥n racional r de en√©simo grado escrita en la forma

r (x) =

n
k=0 pk Tk (x)
,
m
k=0 qk Tk (x)

donde N = n + m y q0 = 1.

Al escribir f (x) en una serie relacionada con los polinomios de Chebyshev como
‚àû

f (x) =

ak Tk (x)
k=0

8.4

399

Aproximaci√≥n de funci√≥n racional

da
‚àû

f (x) ‚àí r (x) =

ak Tk (x) ‚àí
k=0

n
k=0 pk Tk (x)
m
k=0 qk Tk (x)

o
f (x) ‚àí r (x) =

‚àû
k=0 ak Tk (x)

m
k=0 qk Tk (x) ‚àí
m
k=0 qk Tk (x)

n
k=0 pk Tk (x)

.

(8.17)

/RV FRH√ÄFLHQWHV q1 , q2 , . . . , qm y p0 , p1 , . . . , pn se seleccionan de tal forma que el nuPHUDGRU HQ HO ODGR GHUHFKR GH HVWD HFXDFLyQ WLHQH FHUR FRH√ÄFLHQWHV SDUD Tk (x) cuando
k = 0, 1, . . . , N . Esto implica que la serie

(a0 T0 (x) + a1 T1 (x) + ¬∑ ¬∑ ¬∑ )(T0 (x) + q1 T1 (x) + ¬∑ ¬∑ ¬∑ + qm Tm (x))
‚àí ( p0 T0 (x) + p1 T1 (x) + ¬∑ ¬∑ ¬∑ + pn Tn (x))
no tiene t√©rminos de grado menor o igual a N.
Con el procedimiento de Chebyshev surgen dos problemas que lo hacen m√°s dif√≠cil de
implementar que el m√©todo de Pad√©. Uno se presenta porque el producto del polinomio q (x)
y la serie para f (x) implican productos de los polinomios de Chebyshev. Este problema se
resuelve al utilizar la relaci√≥n

Ti (x)T j (x) =

1
Ti+ j (x) + T|i‚àí j| (x) .
2

(8.18)

(Consulte el ejercicio 10 de la secci√≥n 8.3.) El otro problema es m√°s dif√≠cil de resolver e
implica el c√°lculo de la serie de Chebyshev para f (x). En teor√≠a, esto no es dif√≠cil, si
‚àû

f (x) =

ak Tk (x),
k=0

entonces la ortogonalidad de los polinomios de Chebyshev implica que

a0 =

1
œÄ

f (x)
‚àö
dx
1 ‚àí x2
‚àí1
1

y

ak =

2
œÄ

f (x)Tk (x)
‚àö
d x,
1 ‚àí x2
‚àí1
1

donde k ‚â• 1.

Pr√°cticamente, sin embargo, estas integrales rara vez se pueden evaluar de forma cerrada
y se requiere una t√©cnica de integraci√≥n num√©rica para cada evaluaci√≥n.
Ejemplo 2

Los primeros cinco t√©rminos de la expansi√≥n de Chebyshev para e‚àíx son

PÃÉ5 (x) = 1.266066T0 (x) ‚àí 1.130318T1 (x) + 0.271495T2 (x) ‚àí 0.044337T3 (x)
+ 0.005474T4 (x) ‚àí 0.000543T5 (x).
Determine la aproximaci√≥n racional de Chebyshev de grado 5 con n 5 3 y m 5 2.
Soluci√≥n Encontrar esta aproximaci√≥n requiere seleccionar p0 , p1 , p2 , p3 , q1, y q2 y de tal
forma que k 5\ORVFRH√ÄFLHQWHVGHTk (x) son 0 en la expansi√≥n

PÃÉ5 (x)[T0 (x) + q1 T1 (x) + q2 T2 (x)] ‚àí [ p0 T0 (x) + p1 T1 (x) + p2 T2 (x) + p3 T3 (x)].

400

CAP√çTULO 8

Teor√≠a de aproximaci√≥n

Usando la relaci√≥n (8.18) y agrupando t√©rminos se obtienen las ecuaciones

T0 :
T1 :
T2 :
T3 :
T4 :
T5 :

1.266066 ‚àí 0.565159q1 + 0.1357485q2 = p0 ,
‚àí1.130318 + 1.401814q1 ‚àí 0.587328q2 = p1 ,
0.271495 ‚àí 0.587328q1 + 1.268803q2 = p2 ,
‚àí0.044337 + 0.138485q1 ‚àí 0.565431q2 = p3 ,
0.005474 ‚àí 0.022440q1 + 0.135748q2 = 0, y
‚àí0.000543 + 0.002737q1 ‚àí 0.022169q2 = 0.

La soluci√≥n de este sistema produce la funci√≥n racional

r T (x) =

1.055265T0 (x) ‚àí 0.613016T1 (x) + 0.077478T2 (x) ‚àí 0.004506T3 (x)
.
T0 (x) + 0.378331T1 (x) + 0.022216T2 (x)

Al inicio de la secci√≥n 8.3 encontramos que

T0 (x) = 1, T1 (x) = x, T2 (x) = 2x 2 ‚àí 1, y T3 (x) = 4x 3 ‚àí 3x.
Al utilizarlas para convertir una expresi√≥n relacionada con potencias de x obtenemos

r T (x) =

0.977787 ‚àí 0.599499x + 0.154956x 2 ‚àí 0.018022x 3
.
0.977784 + 0.378331x + 0.044432x 2

La tabla 8.11 contiene los valores de rT (x) y, para prop√≥sitos de comparaci√≥n, los valores de
r (x) obtenidos en el ejemplo 1. Observe que la aproximaci√≥n dada por r (x) es superior al
de rT (x) para x 5 0.02 y 0.4 pero el error m√°ximo para r (x) es 6.33 5 1025 en comparaci√≥n
con 9.13 3 1026 para rT (x).

Tabla 8.11

x

e‚àíx

r (x)

|e‚àíx ‚àí r (x)|

r T (x)

|e‚àíx ‚àí r T (x)|

0.2
0.4
0.6
0.8
1.0

0.81873075
0.67032005
0.54881164
0.44932896
0.36787944

0.81873075
0.67031963
0.54880763
0.44930966
0.36781609

7.55 √ó 10‚àí9
4.11 √ó 10‚àí7
4.00 √ó 10‚àí6
1.93 √ó 10‚àí5
6.33 √ó 10‚àí5

0.81872510
0.67031310
0.54881292
0.44933809
0.36787155

5.66 √ó 10‚àí6
6.95 √ó 10‚àí6
1.28 √ó 10‚àí6
9.13 √ó 10‚àí6
7.89 √ó 10‚àí6

La aproximaci√≥n de Chebyshev se puede generar mediante el algoritmo 8.2.

ALGORITMO

8.2

Aproximaci√≥n racional de Chebyshev
Para obtener la aproximaci√≥n racional

r T (x) =

n
k=0 pk Tk (x)
m
k=0 qk Tk (x)

para una funci√≥n determinada f (x):
ENTRADA enteros no negativos m y n.
SALIDA los coeficientes q0 , q1 , . . . , qm y p0 , p1 , . . . , pn .

8.4

Aproximaci√≥n de funci√≥n racional

Paso 1 Determine N = m + n.
Paso 2 Determine a0 =

2
œÄ

œÄ

f (cos Œ∏ ) dŒ∏ ;

0

Para k = 1, 2, . . . , N + m determine
ak =

2
œÄ

œÄ

(El coeficiente a 0 se duplica para
eficiencia computacional. )

f (cos Œ∏ ) cos kŒ∏ dŒ∏ .

0

(Las integrales se pueden evaluar usando el procedimiento de integraci√≥n o
los coeficientes se pueden colocar directamente.)
Paso 3 Determine q0 = 1.
Paso 4 Para i = 0, 1, . . . , N haga los pasos 5‚Äì9.

(Configura un sistema lineal con
matriz B.)

Paso 5 Para j = 0, 1, . . . , i
si j ‚â§ n entonces determine bi, j = 0.
Paso 6 Si i ‚â§ n entonces determine bi,i = 1.

Paso 7 Para j = i + 1, i + 2, . . . , n determine bi, j = 0.
Paso 8 Para j = n + 1, n + 2, . . . , N
si i = 0 entonces determine bi, j = ‚àí 12 (ai+ j‚àín + a|i‚àí j+n| )
tambi√©n determine bi, j = ‚àí 12 a j‚àín .
Paso 9 Si i = 0 entonces determine bi,N +1 = ai
tambi√©n determine bi,N +1 = 12 ai .
(Pasos 10‚Äì21 resuelven el sistema lineal mediante pivoteo parcial.)
Paso 10 Para i = n + 1, n + 2, . . . , N ‚àí 1 haga los pasos 11‚Äì17.
Paso 11

Sea k el entero m√°s peque√±o con i ‚â§ k ‚â§ N y
|bk,i | = m√°xi‚â§ j‚â§N |b j,i |. (Encuentre el elemento de pivote. )

Paso 12 Si bk,i = 0 entonces SALIDA (‚ÄúEl sistema es singular‚Äù);
PARE.
Paso 13 Si k = i entonces (Intercambie la fila y y k.)
para j = i, i + 1, . . . , N + 1 determine
bCOPY = bi, j ;
bi, j = bk, j ;
bk, j = bCOPY .
Paso 14 Para j = i + 1, i + 2, . . . , N haga los pasos 15‚Äì17. (Realice la
eliminaci√≥n. )
b j,i
Paso 15 Determine xm =
.
bi,i
Paso 16 Para k = i + 1, i + 2, . . . , N + 1
determine b j,k = b j,k ‚àí xm ¬∑ bi,k .
Paso 17 Determine b j,i = 0.
Paso 18 Si b N ,N = 0 entonces SALIDA (‚ÄúEn sistema es singular‚Äù);
PARE.

401

402

CAP√çTULO 8

Teor√≠a de aproximaci√≥n

Paso 19 Si m > 0 entonces determine qm =

b N ,N +1
. (Comience la sustituci√≥n regresiva.)
b N ,N

Paso 20 Para i = N ‚àí 1, N ‚àí 2, . . . , n + 1 determine qi‚àín =
Paso 21 Para i = n, n ‚àí 1, . . . , 0 determine pi = bi,N +1 ‚àí

bi,N +1 ‚àí

N
j=i+1 bi, j q j‚àín

bi,i

.

N
j=n+1 bi, j q j‚àín .

Paso 22 SALIDA (q0 , q1 , . . . , qm , p0 , p1 , . . . , pn );
PARE. (El procedimiento fue exitoso. )
En 1930, Evgeny Remez
(1896‚Äì1975) cre√≥ m√©todos
computacionales generales de
aproximaci√≥n de Chebyshev
para polinomios. M√°s adelante,
desarroll√≥ un algoritmo similar
para la aproximaci√≥n racional
GHIXQFLRQHVFRQWLQXDVGH√ÄQLGDV
en un intervalo con un grado
prescrito de precisi√≥n. Su trabajo
abarcaba varias √°reas de la teor√≠a
de aproximaci√≥n, as√≠ como
los m√©todos para aproximar
las soluciones de ecuaciones
diferenciales.

El m√©todo de Chebyshev no produce la mejor aproximaci√≥n funcional racional en el
sentido de la aproximaci√≥n cuyo error m√°ximo es m√≠nimo. Sin embargo, es posible usar el
m√©todo como punto de inicio para un m√©todo iterativo conocido como segundo algoritmo
Remez, que converge en la mejor aproximaci√≥n. Un an√°lisis de las t√©cnicas relacionadas con
este procedimiento y una mejora de este algoritmo se puede encontrar en [RR], p. 292‚Äì305
o en [Pow], p. 90‚Äì92.
La secci√≥n Conjunto de ejercicios 8.4 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

8.5 Aproximaci√≥n polinomial trigonom√©trica

$√ÄQDOHVGHOVLJORXVII y principios
del XVIII, la familia Bernoulli
produjo no menos de ocho
matem√°ticos y f√≠sicos destacados.
El trabajo m√°s importante de
Daniel Bernoulli implicaba la
presi√≥n, la densidad y la velocidad
GHO√ÅXMRGH√ÅXLGRTXHUHVXOWyHQ
lo que se conoce como el principio
de Bernoulli.

El uso de series de funciones de seno y coseno para representar funciones arbitrarias tiene
su inicios en la d√©cada de 1750 con el estudio del movimiento de una cuerda vibrante. Este
problema fue considerado por Jean d‚ÄôAlembert y, despu√©s, por el matem√°tico m√°s destacado
de todos los tiempos, Leonhard Euler. Pero fue Daniel Bernoulli quien abog√≥ primero por
HOXVRGHODVVXPDVLQ√ÄQLWDVGHVHQR\FRVHQRFRPRXQDVROXFLyQSDUDHOSUREOHPDVXPDV
que ahora conocemos como series de Fourier. En la primera parte del siglo XIX, Jean Baptiste
-RVHSK)RXULHUXWLOL]yHVWDVVHULHVSDUDHVWXGLDUHO√ÅXMRGHFDORU\GHVDUUROOyXQDWHRUtDEDVtante compleja sobre el tema.
La primera observaci√≥n en el desarrollo de la serie de Fourier es que, para cada entero
positivo n, el conjunto de funciones {œÜ0 , œÜ1 , . . . , œÜ2n‚àí1 }, donde

œÜ0 (x) =

1
,
2

œÜk (x) = cos kx,

para cada k = 1, 2, . . . , n,

œÜn+k (x) = sen kx,

para cada k = 1, 2, . . . , n ‚àí 1,

y

es un conjunto ortogonal en [‚àíœÄ, œÄ ] respecto a w(x) ‚â° 1. Esta ortogonalidad sigue el hecho de que para cada entero j, las integrales de sen j x y cos j x sobre [‚àíœÄ, œÄ ] son 0 y podemos reescribir los productos de las funciones seno y coseno como sumas al utilizar las tres
identidades trigonom√©tricas

1
[cos(t1 ‚àí t2 ) ‚àí cos(t1 + t2 )],
2
1
cos t1 cos t2 = [cos(t1 ‚àí t2 ) + cos(t1 + t2 )], y
2
1
sen t1 cos t2 = [sen(t1 ‚àí t2 ) + sen(t1 + t2 )].
2

sen t1 sen t2 =

(8.19)

8.5 Aproximaci√≥n polinomial trigonom√©trica

403

Polinomios trigonom√©tricos ortogonales
Sea Tn el conjunto de todas las combinaciones lineales de las funciones œÜ0 , œÜ1 , . . . , œÜ2n‚àí1 .
Este conjunto recibe el nombre de conjunto de polinomios trigonom√©tricos de grado menor o igual a n. (Algunas fuentes tambi√©n incluyen una funci√≥n adicional en el conjunto
œÜ2n (x) = sen nx.)
Para una funci√≥n f ‚àà C[‚àíœÄ, œÄ ], queremos encontrar la aproximaci√≥n de m√≠nimos cuadrados continuos mediante las funciones en Tn de la forma
n‚àí1

Sn (x) =

a0
+ an cos nx +
(ak cos kx + bk sen kx).
2
k=1

Puesto que el conjunto de funciones {œÜ0 , œÜ1 , . . . , œÜ2n‚àí1 } es ortogonal en [‚àíœÄ, œÄ ] respecto
a w(x) ‚â° 1, se sigue del teorema 8.6 en la p√°gina 382 y las ecuaciones en (8.19) sobre que
ODVHOHFFLyQDGHFXDGDGHFRH√ÄFLHQWHVHV

ak =

œÄ
1
‚àíœÄ f (x) cos kx d x
=
œÄ
2
œÄ
‚àíœÄ (cos kx) d x

œÄ
‚àíœÄ

f (x) cos kx d x,

para cada k = 0, 1, 2, . . . , n,
(8.20)

y
Joseph Fourier (1768‚Äì1830)
public√≥ su teor√≠a de series
trigonom√©tricas en Th√©orie
analytique de la chaleur
(Teor√≠a anal√≠tica del calor)
para resolver el problema de
distribuci√≥n de calor de estado
estable en un s√≥lido.

Ejemplo 1

bk =

œÄ
1
‚àíœÄ f (x) sen kx d x
=
œÄ
2
œÄ
‚àíœÄ (sen kx) d x

œÄ
‚àíœÄ

f (x) sen kx d x, para cada k = 1, 2, . . . , n ‚àí 1.
(8.21)

El l√≠mite de Sn (x) cuando n ‚Üí ‚àû recibe el nombre de serie de Fourier de f. Las series
de Fourier se usan para describir la soluci√≥n de las diferentes ecuaciones ordinarias y diferenciales parciales que se presentan en situaciones f√≠sicas.
Determine el polinomio trigonom√©trico a partir de Tn que aproxima

f (x) = |x|,

para ‚àí œÄ < x < œÄ.

Soluci√≥n 3ULPHURQHFHVLWDPRVORVFRH√ÄFLHQWHV

a0 =
ak =

1
œÄ
1
œÄ

œÄ
‚àíœÄ
œÄ
‚àíœÄ

|x| d x = ‚àí

1
œÄ

0
‚àíœÄ

|x| cos kx d x =

2
œÄ

x dx +
œÄ
0

1
œÄ

œÄ

x dx =

0

x cos kx d x =

2
œÄ

œÄ

x d x = œÄ,

0

2
(‚àí1)k ‚àí 1 ,
œÄ k2

para cada k = 1, 2, . . . , n, y

bk =

1
œÄ

œÄ
‚àíœÄ

|x| sen kx d x = 0,

para cada k = 1, 2, . . . , n ‚àí 1.

Al hecho de que todas las b k‚Äôs son 0 sigue que g(x) = |x| sen k x es una funci√≥n impar para
cada k y que la integral de una funci√≥n impar continua sobre un intervalo de la forma [2a,
a] es 0 (consulte los ejercicios 15 y 16). El polinomio trigonom√©trico de Tn que se aproxima
a f es, por lo tanto,
n

œÄ
2
(‚àí1)k ‚àí 1
Sn (x) = +
cos kx.
2
œÄ k=1
k2
Los primeros polinomios trigonom√©tricos para f (x) = |x|VHPXHVWUDQHQOD√ÄJXUD

404

CAP√çTULO 8

Teor√≠a de aproximaci√≥n

Figura 8.13
y
y5 x

p

p

4

4

y 5 S 3(x) 5 2 2 p cos x 2 9p cos 3x
p

p
2

2p

4

y 5 S 1(x) 5 S 2(x) 5 2 2 p cos x
y 5 S 0(x) 5 p2

p

p
2

22

x

p

La serie de Fourier para f es
‚àû

S(x) = l√≠m Sn (x) =
n‚Üí‚àû

(‚àí1)k ‚àí 1
2
œÄ
+
cos kx.
2
œÄ k=1
k2

Puesto que | cos kx| ‚â§ 1 para cada k y x, la serie converge, y S(x) existe para todos los n√∫meros reales x.

Aproximaci√≥n trigonom√©trica discreta
Existe un an√°logo discreto que es √∫til para la aproximaci√≥n de m√≠nimos cuadrados discretos
y la interpolaci√≥n de grandes cantidades de datos.
Suponga que se proporciona un conjunto de puntos de datos pares 2m {(x j , y j )}2m‚àí1
j=0 ,
con los primeros elementos en pares que dividen de manera uniforme un intervalo cerrado.
Por conveniencia, suponemos que el intervalo es [‚àíœÄ, œÄ ]; por lo que, como se muestra en
OD√ÄJXUD

j
œÄ,
m

x j = ‚àíœÄ +

para cada j = 0, 1, . . . , 2m ‚àí 1.

(8.22)

Si no es [‚àíœÄ, œÄ ], se podr√≠a usar una transformaci√≥n lineal simple para transformar los datos
en esta forma.

Figura 8.14
24

23

22

21

2p 5 x0

0

1

2

xm

3

4

p 5 x 2m

La meta en el caso discreto es determinar el polinomio trigonom√©trico S n (x) en Tn que
minimizar√°
2m‚àí1

E(Sn ) =

[y j ‚àí Sn (x j )]2 .
j=0

8.5 Aproximaci√≥n polinomial trigonom√©trica

405

Para hacerlo, necesitamos seleccionar las constantes a0 , a1 , . . . , an , b1 , b2 , . . . , bn‚àí1 para
minimizar
n‚àí1

2m‚àí1

E(Sn ) =

a0
(ak cos kx j + bk sen kx j )
+ an cos nx j +
2
k=1

yj ‚àí
j=0

2

. (8.23)

/DGHWHUPLQDFLyQGHODVFRQVWDQWHVVHVLPSOL√ÄFDPHGLDQWHHOKHFKRGHTXHHOFRQMXQWR
{œÜ0 , œÜ1 , . . . , œÜ2n‚àí1 } es ortogonal respecto a la suma sobre los puntos uniformemente espaciados {x j }2m‚àí1
j=0 en [‚àíœÄ, œÄ ]. Con esto queremos decir que para cada k = l,
2m‚àí1

œÜk (x j )œÜl (x j ) = 0.

(8.24)

j=0

Para mostrar esta ortogonalidad utilizamos el siguiente lema.
Lema 8.12

Suponga que el entero r no es un m√∫ltiplo de 2m. Entonces
2m‚àí1

2m‚àí1

cos r x j = 0

‚Ä¢

sen r x j = 0.

y

j=0

j=0

Adem√°s, si r no es un m√∫ltiplo de m, entonces
2m‚àí1

2m‚àí1

(cos r x j )2 = m

‚Ä¢
j=0

Euler us√≥ primero el s√≠mbolo
i en
‚àö
1794 para representar ‚àí1
en sus memorias De formulis
differentialibus Angularibus.

(sen r x j )2 = m.

y
j=0

La f√≥rmula de Euler establece que con i 2 5 21, tenemos, para todos los
n√∫meros reales z,
Demostraci√≥n

ei z = cos z + i sen z.

(8.25)

Al aplicar estos resultados obtenemos
2m‚àí1

2m‚àí1

cos r x j + i

2m‚àí1

sen r x j =

j=0

j=0

2m‚àí1

eir x j .

cos r x j + i sen r x j =
j=0

j=0

Pero
eir x j = eir (‚àíœÄ+ jœÄ/m) = e‚àíir œÄ ¬∑ eir jœÄ/m ,
por lo que
2m‚àí1

2m‚àí1

2m‚àí1

sen r x j = e‚àíir œÄ

cos r x j + i
j=0

j=0

eir jœÄ/m .
j=0

2m‚àí1

Puesto que
eir jœÄ/m es una serie geom√©trica con el primer t√©rmino 1 y radio eir œÄ/m = 1,
j=0
tenemos
2m‚àí1

eir jœÄ/m =
j=0

1 ‚àí (eir œÄ/m )2m
1 ‚àí e2ir œÄ
=
.
1 ‚àí eir œÄ/m
1 ‚àí eir œÄ/m

Pero e2ir œÄ = cos 2r œÄ + i sen 2r œÄ = 1, por lo que 1 ‚àí e2ir œÄ = 0 y
2m‚àí1
j=0

2m‚àí1

2m‚àí1

sen r x j = e‚àíir œÄ

cos r x j + i
j=0

eir jœÄ/m = 0.
j=0

406

CAP√çTULO 8

Teor√≠a de aproximaci√≥n

Esto implica que tanto la parte real como la imaginaria son cero, por lo que
2m‚àí1

2m‚àí1

cos r x j = 0

sen r x j = 0.

y

j=0

j=0

Adem√°s, si r no es un m√∫ltiplo de m, estas sumas implican que
2m‚àí1

‚é°

2m‚àí1

(cos r x j )2 =
j=0

2m‚àí1

‚é§

1
1
1
1 + cos 2r x j = ‚é£2m +
cos 2r x j ‚é¶ = (2m + 0) = m
2
2
2
j=0
j=0

y, de igual forma, que
2m‚àí1

2m‚àí1

(sen r x j )2 =
j=0

1
1 ‚àí cos 2r x j = m.
2
j=0

Ahora podemos mostrar la ortogonalidad establecida en la ecuaci√≥n (8.24). Considere,
por ejemplo, el caso
2m‚àí1

2m‚àí1

œÜk (x j )œÜn+l (x j ) =

(cos kx j )(sen lx j ).

j=0

j=0

Puesto que

cos kx j sen lx j =

1
[sen(l + k)x j + sen(l ‚àí k)x j ]
2

y tanto (l 1 k) como (l 2 k) son enteros que no son multiplicadores de 2m, el lema 8.12
implica que
‚é°
‚é§
2m‚àí1
2m‚àí1
2m‚àí1
1
1
(cos kx j )(sen lx j ) = ‚é£
sen(l + k)x j +
sen(l ‚àí k)x j ‚é¶ = (0 + 0) = 0.
2
2
j=0
j=0
j=0
Esta t√©cnica se usa para mostrar que la condici√≥n de ortogonalidad se satisface para
cualquier par de funciones y para producir el siguiente resultado.
Teorema 8.13

Las constantes en la suma
n‚àí1

a0
Sn (x) =
+ an cos nx +
(ak cos kx + bk sen kx)
2
k=1
que minimizan la suma de m√≠nimos cuadrados
2m‚àí1

E(a0 , . . . , an , b1 , . . . , bn‚àí1 ) =

(y j ‚àí Sn (x j ))2
j=0

son

1
‚Ä¢ ak = m

2m‚àí1

1
m

2m‚àí1

y j cos kx j ,

para cada k = 0, 1, . . . , n,

y j sen kx j ,

para cada k = 1, 2, . . . , n ‚àí 1.

j=0

y
‚Ä¢ bk =

j=0

8.5 Aproximaci√≥n polinomial trigonom√©trica

407

El teorema se prueba al establecer las derivadas parciales de E respecto a las ak y las bk
para cero, como se realiza en las secciones 8.1 y 8.2 y al aplicar la ortogonalidad para simSOL√ÄFDUODVHFXDFLRQHV3RUHMHPSOR
2m‚àí1

0=

‚àÇE
=2
[y j ‚àí Sn (x j )](‚àí sen kx j ),
‚àÇbk
j=0

por lo que
2m‚àí1

0=

2m‚àí1

y j sen kx j ‚àí
j=0
2m‚àí1

=

Sn (x j ) sen kx j
j=0

y j sen kx j ‚àí
j=0
n‚àí1

‚àí

a0
2

2m‚àí1

2m‚àí1

sen kx j ‚àí an
j=0
n‚àí1

2m‚àí1

al
l=1

sen kx j cos nx j
j=0

sen kx j cos lx j ‚àí
j=0

2m‚àí1

bl
l=1,
l=k

2m‚àí1

sen kx j sen lx j ‚àí bk
j=0

(sen kx j )2 .
j=0

La ortogonalidad implica que todas las sumas, excepto la primera y la √∫ltima en el lado
GHUHFKRVRQFHUR\HOOHPDHVWDEOHFHTXHODVXPD√ÄQDOHVm. Por lo tanto,
2m‚àí1

0=

y j sen kx j ‚àí mbk ,
j=0

lo cual implica que

bk =

1
m

2m‚àí1

y j sen kx j .
j=0

El resultado para las ak es similar, pero necesita un paso adicional para determinar a0
(consulte el ejercicio 19).
Ejemplo 2

Encuentre S2(x), el polinomio trigonom√©trico de m√≠nimos cuadrados discretos de grado 2
para f (x) = 2x 2 ‚àí 9 cuando x est√° en [‚àíœÄ, œÄ ].
Soluci√≥n

Tenemos m 5 2(2) 2 1 5 3, por lo que los nodos son

xj = œÄ +

j
œÄ
m

y

y j = f (x j ) = 2x 2j ‚àí 9,

para j = 0, 1, 2, 3, 4, 5.

El polinomio trigonom√©trico es

S2 (x) =

1
a0 + a2 cos 2x + (a1 cos x + b1 sen x),
2

donde
5

ak =

5

1
1
y j cos kx j , para k = 0, 1, 2, y b1 =
y j sen x j .
3 j=0
3 j=0

408

CAP√çTULO 8

Teor√≠a de aproximaci√≥n

/RVFRH√ÄFLHQWHVVRQ

a0 =

1
3

f (‚àíœÄ) + f

‚àí

2œÄ
3

+ f ‚àí

œÄ
3

œÄ
3

+ f (0) + f

2œÄ
3

+ f

= ‚àí4.10944566,
a1 =

1
3

f (‚àíœÄ) cos(‚àíœÄ ) + f

+ f (0) cos 0 + f
a2 =

1
3

2œÄ
3

‚àí

œÄ
œÄ
cos
3
3

cos ‚àí
+ f

2œÄ
3

œÄ
œÄ
cos ‚àí
3
3

2œÄ
3

cos

2œÄ
3

= ‚àí8.77298169,

4œÄ
3

+ f ‚àí

œÄ
2œÄ
cos ‚àí
3
3

f (‚àíœÄ) cos(‚àí2œÄ ) + f

‚àí

2œÄ
3

cos ‚àí

œÄ
cos
3

2œÄ
3

+ f

2œÄ
3

+ f

+ f (0) cos 0 + f

+ f ‚àí

2œÄ
3

cos

4œÄ
3

sen ‚àí

œÄ
3

+ f ‚àí

2œÄ
3

2œÄ
3

= 2.92432723,

y
b1 =

1
3

f (‚àíœÄ) sen(‚àíœÄ ) + f

+ f (0) sen 0 + f

œÄ
3

‚àí
œÄ
3

œÄ
3

‚àí

œÄ
3

= 0.

Por lo tanto,

S2 (x) =

1
(‚àí4.10944562) ‚àí 8.77298169 cos x + 2.92432723 cos 2x.
2

/D√ÄJXUDPXHVWUDf (x) y el polinomio trigonom√©trico de m√≠nimos cuadrados discretos
S2(x).

Figura 8.15

y
10
8
6

y = f (x)

4

y = S2 (x)

2
23

21

22

1

3

x

24
26
210

El siguiente ejemplo da una ilustraci√≥n de c√≥mo encontrar una aproximaci√≥n por m√≠niPRVFXDGUDGRVSDUDXQDIXQFLyQGH√ÄQLGDHQXQLQWHUYDORFHUUDGRGLIHUHQWHD[‚àíœÄ, œÄ ].

8.5 Aproximaci√≥n polinomial trigonom√©trica

Ejemplo 3

409

Encuentre la aproximaci√≥n por m√≠nimos cuadrados discretos S3(x) para

f (x) = x 4 ‚àí 3x 3 + 2x 2 ‚àí tan x(x ‚àí 2) en [0, 2]
por medio de los datos {(x j , y j )}9j=0 , donde x j = j/5 y y j = f (x j ).
Soluci√≥n

Primero necesitamos la transformaci√≥n lineal de [0, 2] a [‚àíœÄ, œÄ ] dada por

z j = œÄ(x j ‚àí 1).
Entonces, los datos transformados tienen la forma

z j, f 1 +

zj
œÄ

9
j=0

.

El polinomio trigonom√©trico de m√≠nimos cuadrados es, por consiguiente,
2

a0
+ a3 cos 3z +
S3 (z) =
(ak cos kz + bk sen kz) ,
2
k=1
donde
9

ak =

1
zj
f 1+
5 j=0
œÄ

bk =

1
zj
f 1+
5 j=0
œÄ

cos kz j ,

para k = 0, 1, 2, 3,

y
9

sen kz j , para k = 1, 2.

La evaluaci√≥n de estas sumas produce la aproximaci√≥n

S3 (z) = 0.76201 + 0.77177 cos z + 0.017423 cos 2z + 0.0065673 cos 3z
‚àí 0.38676 sen z + 0.047806 sen 2z,
y convertir de nuevo a las variables x nos da

S3 (x) = 0.76201 + 0.77177 cos œÄ(x ‚àí 1) + 0.017423 cos 2œÄ(x ‚àí 1)
+ 0.0065673 cos 3œÄ(x ‚àí 1) ‚àí 0.38676 sen œÄ(x ‚àí 1) + 0.047806 sen 2œÄ(x ‚àí 1).
La tabla 8.12 lista los valores de f (x) y S3(x).

Tabla 8.12

x

f (x)

S3 (x)

| f (x) ‚àí S3 (x)|

0.125
0.375
0.625
0.875
1.125
1.375
1.625
1.875

0.26440
0.84081
1.36150
1.61282
1.36672
0.71697
0.07909
‚àí0.14576

0.24060
0.85154
1.36248
1.60406
1.37566
0.71545
0.06929
‚àí0.12302

2.38 √ó 10‚àí2
1.07 √ó 10‚àí2
9.74 √ó 10‚àí4
8.75 √ó 10‚àí3
8.94 √ó 10‚àí3
1.52 √ó 10‚àí3
9.80 √ó 10‚àí3
2.27 √ó 10‚àí2

La secci√≥n Conjunto de ejercicios 8.5 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

410

CAP√çTULO 8

Teor√≠a de aproximaci√≥n

8.6 Transformadas r√°pidas de Fourier
En la √∫ltima parte de la secci√≥n 8.5 determinamos la forma del polinomio de m√≠nimos
cuadrados discretos de grado n en los puntos de datos 2m 2 1 {(x j , y j )}2m‚àí1
j=0 , donde
x j = ‚àíœÄ +( j/m)œÄ, para cada j 5 0, 1,   , 2m 2 1.
El polinomio trigonom√©trico de interpolaci√≥n en Tm en estos puntos de datos 2m es casi
el mismo que el polinomio de m√≠nimos cuadrados. Esto se debe a que el polinomio trigonom√©trico de m√≠nimos cuadrados minimiza el t√©rmino de error
2m‚àí1

E(Sm ) =

y j ‚àí Sm (x j )

2

,

j=0

y para el polinomio trigonom√©trico de interpolaci√≥n, este error es 0 y, por lo tanto, minimizado cuando Sm (x j ) = y j , para cada j 5 0, 1,   , 2m 2 1.
6HQHFHVLWDXQDPRGL√ÄFDFLyQGHODIRUPDGHOSROLQRPLRVLQHPEDUJRVLTXHUHPRVTXH
ORVFRH√ÄFLHQWHVVXSRQJDQODPLVPDIRUPDHQHOFDVRGHPtQLPRVFXDGUDGRV(QHOOHPD
encontramos que si r no es un m√∫ltiplo de m, entonces
2m‚àí1

(cos r x j )2 = m.
j=0

Por el contrario, la interpolaci√≥n requiere calcular
2m‚àí1

(cos mx j )2 ,
j=0

que (consulte el ejercicio 10) tiene el valor 2m. Esto requiere que el polinomio de interpolaci√≥n se escriba como
m‚àí1

Sm (x) =

a0 + am cos mx
+
(ak cos kx + bk sen kx)
2
k=1

(8.26)

si queremos que la forma de las constantes ak y bk concuerde con las del polinomio de m√≠nimos cuadrados discretos; es decir,

‚Ä¢

‚Ä¢

ak =

bk =

1
m

2m‚àí1

1
m

2m‚àí1

y j cos kx j ,

para cada k = 0, 1, . . . , m, y

y j sen kx j

para cada k = 1, 2, . . . , m ‚àí 1.

j=0

j=0

La interpolaci√≥n de grandes cantidades de datos espaciados uniformemente mediante
polinomios trigonom√©tricos puede producir resultados muy precisos. Es la t√©cnica de apro[LPDFLyQDGHFXDGDHQiUHDVUHODFLRQDGDVFRQ√ÄOWURVGLJLWDOHVSDWURQHVGHFDPSRGHDQWHQD
mec√°nica cu√°ntica, √≥ptica en numerosos problemas de simulaci√≥n. Hasta mediados de la
d√©cada de 1960, sin embargo, el m√©todo no se hab√≠a aplicado ampliamente debido al n√∫mero de
c√°lculos aritm√©ticos requeridos para determinar las constantes en la aproximaci√≥n.
La interpolaci√≥n de 2m puntos de datos mediante la t√©cnica de c√°lculo directa requiere
aproximadamente (2m)2 multiplicaciones y (2m)2 sumas. La aproximaci√≥n de muchos miles
de puntos de datos es muy com√∫n en √°reas que requieren interpolaci√≥n trigonom√©trica, por lo
que los m√©todos directos para evaluar las constantes requieren operaciones de multiplicaci√≥n
y suma que llegan a millones. En general, el error de redondeo relacionado con este n√∫mero
de c√°lculos domina la aproximaci√≥n.

8.6 Transformadas r√°pidas de Fourier

411

En 1965, un art√≠culo de J. W. Cooley y J. W. Turkey en el diario Mathematics of Computation [CT] (Matem√°ticas de la computaci√≥n) describ√≠a un m√©todo diferente para calcular
las constantes en el polinomio trigonom√©trico de interpolaci√≥n. Este m√©todo s√≥lo requiere
O(m log2 m) multiplicaciones y O(m log2 m) sumas, siempre y cuando m sea seleccionada de
manera adecuada. Para un problema con miles de puntos de datos, esto reduce el n√∫mero
de c√°lculos de millones a miles. En realidad, el m√©todo hab√≠a sido descubierto algunos a√±os antes de que apareciera el art√≠culo de Cooley-Tukey pero hab√≠a pasado desapercibido. ([Brigh],
pp. 8‚Äì9, contiene un resumen hist√≥rico pero interesante del m√©todo).
El m√©todo descrito por Cooley y Tukey es conocido ya sea como algoritmo CooleyTukey o el algoritmo de la transformada r√°pida de Fourier (FFT) y ha conducido a una
revoluci√≥n en el uso de polinomios trigonom√©tricos de interpolaci√≥n. El m√©todo consiste
en organizar el problema de tal forma que el n√∫mero de puntos de datos utilizado se pueda
factorizar f√°cilmente, de modo especial en potencias de dos.
En lugar de evaluar directamente las constantes ak y bk, el procedimiento de la transforPDGDUiSLGDGH)RXULHUFDOFXODORVFRH√ÄFLHQWHVck complejos en
2m‚àí1

1
ck eikx ,
m k=0

(8.27)

donde
2m‚àí1

y j eikœÄ j/m ,

ck =

para cada k = 0, 1, . . . , 2m ‚àí 1.

(8.28)

j=0

Leonhard Euler proporcion√≥
primero esta f√≥rmula en 1748
en Introductio in analysin
LQ√ÄQLWRUXP que hizo que las
ideas de Johann Bernoulli fueran
m√°s precisas. Este trabajo basa
los c√°lculos en la teor√≠a de
funciones elementales en lugar
de curvas.

Una vez que las constantes ck se han determinado, ak y bk se pueden recuperar por medio
de la f√≥rmula de Euler,

ei z = cos z + i sen z.
Para cada k = 0, 1, . . . , m, tenemos

1
1
1
ck (‚àí1)k = ck e‚àíiœÄ k =
m
m
m

2m‚àí1

1
m

2m‚àí1

1
m

2m‚àí1

=

=

y j eikœÄ j/m e‚àíiœÄ k =
j=0

yj

cos k ‚àíœÄ +

j=0

1
m

2m‚àí1

y j eik(‚àíœÄ+(œÄ j/m))
j=0

œÄj
m

+ i sen k ‚àíœÄ +

œÄj
m

y j (cos kx j + i sen kx j ).
j=0

Por lo que dada ck, tenemos

ak + ibk =

(‚àí1)k
ck .
m

(8.29)

Por conveniencia notacional, b0 y bm se suman al conjunto, pero ambos son 0 y no contribuyen a la suma resultante.
La caracter√≠stica de reducci√≥n-operaci√≥n de la transformada r√°pida de Fourier resulta
GHFDOFXODUORVFRH√ÄFLHQWHVck en grupos y utiliza como relaci√≥n b√°sica el hecho de que para
cualquier entero n,

enœÄi = cos nœÄ + i sen nœÄ = (‚àí1)n .

412

CAP√çTULO 8

Teor√≠a de aproximaci√≥n

Suponga que m 5 2p para algunos enteros positivos p. Para cada k 5 0, 1,
tenemos
2m‚àí1

2m‚àí1

y j eikœÄ j/m +

ck + cm+k =
j=0

 , m 2 1,

2m‚àí1

y j ei(m+k)œÄ j/m =
j=0

y j eikœÄ j/m (1 + eœÄi j ).
j=0

Pero

1 + eiœÄ j =

si j es par,
si j es impar,

2,
0,

por lo que s√≥lo hay que sumar m t√©rminos diferentes de cero.
Si j se reemplaza por 2j en el √≠ndice de la suma, podemos escribir la suma como
m‚àí1

ck + cm+k = 2

y2 j eikœÄ(2 j)/m ;

j=0

es decir,
m‚àí1

ck + cm+k = 2

y2 j eikœÄ j/(m/2) .

(8.30)

j=0

De forma similar,

ck ‚àí cm+k = 2eikœÄ/m

m‚àí1

y2 j+1 eikœÄ j/(m/2) .

(8.31)

j=0

Puesto que ck y cm+k se pueden recuperar a partir de las ecuaciones (8.30) y (8.31), estas
UHODFLRQHVGHWHUPLQDQWRGRVORVFRH√ÄFLHQWHVck. Tambi√©n observe que las sumas en las ecuaciones (8.30) y (8.31) son de la misma forma que la suma en la ecuaci√≥n (8.28), excepto que
el √≠ndice m ha sido reemplazado por m/2.
Existen 2m FRH√ÄFLHQWHV c0 , c1 , . . . , c2m‚àí1 que deben calcularse. Usar la f√≥rmula b√°sica (8.28) requiere 2m PXOWLSOLFDFLRQHV FRPSOHMDV SRU FRH√ÄFLHQWH SDUD XQ WRWDO GH
(2m)2 operaciones. La ecuaci√≥n (8.30) requiere m multiplicaciones complejas para cada
k = 0, 1, . . . , m ‚àí 1, y la ecuaci√≥n (8.31) requiere m11 multiplicaciones complejas para
cada k 5 0, 1,   , m 2 1. Utilizar estas ecuaciones para calcular c0 , c1 , . . . , c2m‚àí1 reducen
el n√∫mero de multiplicaciones complejas desde (2m)2 5 4m2 hasta

m ¬∑ m + m(m + 1) = 2m 2 + m.
Las sumas en las ecuaciones (8.30) y (8.31) tienen la misma forma que la original y m es
una potencia de 2, por lo que la t√©cnica de reducci√≥n se puede volver a aplicar a las sumas en
las ecuaciones (8.30) y (8.31). Cada una de estas es reemplazada por dos sumas desde j 5 0
hasta j 5 (m / 2) 2 1. Esto reduce la parte 2m2 de la suma a

2

m
m m
m
¬∑ + ¬∑
+1
2 2
2
2

= m 2 + m.

Por lo que ahora se necesita un total de

(m 2 + m) + m = m 2 + 2m
multiplicaciones complejas en lugar de (2m)2.

8.6 Transformadas r√°pidas de Fourier

413

Al aplicar la t√©cnica una vez m√°s obtenemos cuatro sumas, cada una con m / 4 t√©rminos
y reduce la parte m2 de este total a

m
4

4

2

+

m
4

m
+1
4

=

m2
+ m,
2

para un total nuevo de (m 2 /2) + 3m multiplicaciones complejas. Repetir el proceso r veces
reduce el n√∫mero total de multiplicaciones complejas requeridas a

m2
+ mr.
2r ‚àí2
El proceso est√° completo cuando r = p + 1 porque entonces tenemos m = 2 p y 2m =
2 . Por consiguiente, despu√©s de r = p + 1 reducciones de este tipo, el n√∫mero de multiplicaciones complejas se reduce de (2m)2 a
p+1

(2 p )2
+ m( p + 1) = 2m + pm + m = 3m + m log2 m = O(m log2 m).
2 p‚àí1
Debido a la forma en que se ordenan los c√°lculos, el n√∫mero de adiciones complejas requeridas es comparable.
3DUDLOXVWUDUODVLJQL√ÄFDQFLDGHHVWDUHGXFFLyQVXSRQJDTXHWHQHPRVm 5 210 5 1 024.
El c√°lculo directo de ck , para k = 0, 1, . . . , 2m ‚àí 1, requerir√≠a

(2m)2 = (2048)2 ‚âà 4 200 000
de c√°lculos. El procedimiento de la transformada r√°pida de Fourier reduce el n√∫mero de
c√°lculos a

3(1024) + 1024 log2 1024 ‚âà 13 300.
Ilustraci√≥n

Considere la t√©cnica de la transformada r√°pida de Fourier aplicada a 8 5 23 puntos de datos
{(x j , y j )}7j=0 , donde x j = ‚àíœÄ + jœÄ/4, para cada j 5 0, 1, . . . , 7. En este caso, 2m 5 8,
as√≠ m 5 4 5 22 y p 5 2.
A partir de la ecuaci√≥n (8.26), tenemos
3

S4 (x) =

a0 + a4 cos 4x
+
(ak cos kx + bk sen kx),
2
k=1

donde
7

ak =

1
y j cos kx j
4 j=0

7

y bk =

1
y j sen kx j ,
4 j=0

k = 0, 1, 2, 3, 4.

'H√ÄQDODWUDQVIRUPDGDGH)RXULHUFRPR
7

1
ck eikx ,
4 j=0
donde
7

y j eikœÄ j/4 ,

ck =
j=0

para k = 0, 1, . . . , 7.

414

CAP√çTULO 8

Teor√≠a de aproximaci√≥n

Entonces mediante la ecuaci√≥n (8.31) para k 5 0, 1, 2, 3, 4, tenemos

1 ‚àíikœÄ
ck e
= ak + ibk .
4
Mediante c√°lculo directo, las constantes complejas ck est√°n dadas por

c0 = y0 + y1 + y2 + y3 + y4 + y5 + y6 + y7 ;
c1 = y0 +

i +1
‚àö
2

y1 + i y2 +

i ‚àí1
‚àö
2

y3 ‚àí y4 ‚àí

i +1
‚àö
2

y5 ‚àí i y6 ‚àí

i ‚àí1
‚àö
2

y7 ;

i ‚àí1
‚àö
2

y5 + i y6 ‚àí

i +1
‚àö
2

y7 ;

i +1
‚àö
2

y5 ‚àí i y6 +

i ‚àí1
‚àö
2

y7 ;

i ‚àí1
‚àö
2

y5 + i y6 +

i +1
‚àö
2

y7 .

c2 = y0 + i y1 ‚àí y2 ‚àí i y3 + y4 + i y5 ‚àí y6 ‚àí i y7 ;
c3 = y0 +

i ‚àí1
‚àö
2

y1 ‚àí i y2 +

i +1
‚àö
2

y3 ‚àí y4 ‚àí

c4 = y0 ‚àí y1 + y2 ‚àí y3 + y4 ‚àí y5 + y6 ‚àí y7 ;
c5 = y0 ‚àí

i +1
‚àö
2

y1 + i y2 ‚àí

i ‚àí1
‚àö
2

y3 ‚àí y4 +

c6 = y0 ‚àí i y1 ‚àí y2 + i y3 + y4 ‚àí i y5 ‚àí y6 + i y7 ;
c7 = y0 ‚àí

i ‚àí1
‚àö
2

y1 ‚àí i y2 ‚àí

i +1
‚àö
2

y3 ‚àí y4 +

'HELGRDOSHTXHxRWDPDxRGHOFRQMXQWRGHSXQWRVGHGDWRVPXFKRVGHORVFRH√ÄFLHQWHVGH
yj en estas ecuaciones son 1 o 21. Esta frecuencia disminuir√° en una aplicaci√≥n m√°s grande, para contar las operaciones computacionales de manera precisa, la multiplicaci√≥n por
1 o 21 se incluir√°, a pesar de que no ser√≠a necesaria en este ejemplo. Con esta comprensi√≥n, se requieren 64 multiplicaciones/divisiones y 56 sumas/restas para el c√°lculo directo
de c0 , c1 , . . . , c7 .
Para aplicar el procedimiento de transformada r√°pida de Fourier con r 5 1, primero
determinamos

c0 + c4
= y0 + y2 + y4 + y6 ;
2
c0 ‚àí c4
= y1 + y3 + y5 + y7 ;
d1 =
2
c1 + c5
= y0 + i y2 ‚àí y4 ‚àí i y6 ;
d2 =
2
c1 ‚àí c5
d3 =
2
d0 =

=

i +1
‚àö
2

c2 + c6
= y0 ‚àí y2 + y4 ‚àí y6 ;
2
c2 ‚àí c6
= i(y1 ‚àí y3 + y5 ‚àí y7 );
d5 =
2
c3 + c7
= y0 ‚àí i y2 ‚àí y4 + i y6 ;
d6 =
2
c3 ‚àí c7
d7 =
2
d4 =

(y1 + i y3 ‚àí y5 ‚àí i y7 );

=

i ‚àí1
‚àö
2

(y1 ‚àí i y3 ‚àí y5 + i y7 ).

(QWRQFHVGH√ÄQLPRVSDUDr 5 2,

e0 =

d0 + d4
= y0 + y4 ;
2

e4 =

d2 + d6
= y0 ‚àí y4 ;
2

e1 =

d0 ‚àí d4
= y2 + y6 ;
2

e5 =

d2 ‚àí d6
= i(y2 ‚àí y6 );
2

e2 =

id1 + d5
= i(y1 + y5 );
2

e6 =

id3 + d7
=
2

i ‚àí1
‚àö
2

(y1 ‚àí y5 );

e3 =

id1 ‚àí d5
= i(y3 + y7 );
2

e7 =

id3 ‚àí d7
=i
2

i ‚àí1
‚àö
2

(y3 ‚àí y7 ).

8.6 Transformadas r√°pidas de Fourier

415

Finalmente, para r 5 p 1 1 5GH√ÄQLPRV

‚àö
((i + 1)/ 2)e2 + e6
=
f4 =
2
‚àö
((i + 1)/ 2)e2 ‚àí e6
f5 =
=
2
‚àö
((i ‚àí 1)/ 2)e3 + e7
=
f6 =
2
‚àö
((i ‚àí 1)/ 2)e3 ‚àí e7
=
f7 =
2

e0 + e4
f0 =
= y0 ;
2
f1 =

e0 ‚àí e4
= y4 ;
2

f2 =

ie1 + e5
= i y2 ;
2

f3 =

ie1 ‚àí e5
= i y6 ;
2

i ‚àí1
‚àö
2

y1 ;

i ‚àí1
‚àö
2

y5 ;

‚àíi ‚àí 1
‚àö
2

y3 ;

‚àíi ‚àí 1
‚àö
2

y7 .

c0 , . . . , c7 , d0 , . . . , d7 , e0 , . . . , e7 , y f 0 , . . . , f 7 son independientes de los puntos de datos
particulares; dependen solamente del hecho de que m 5 4. Para cada m, existe un conjunto
2m‚àí1
2m‚àí1
2m‚àí1
√∫nico de constantes {ck }2m‚àí1
k=0 , {dk }k=0 , {ek }k=0 , y { f k }k=0 . Esta parte del trabajo no es
necesaria para una aplicaci√≥n particular; s√≥lo se requieren los siguientes c√°lculos:
fk :
f 0 = y0 ;
f4 =

f 1 = y4 ;

i ‚àí1
‚àö
2

f 2 = i y2 ;

y1 ;

f5 =

i ‚àí1
‚àö
2

f 3 = i y6 ;
y5 ;

f6 = ‚àí

i +1
‚àö
2

i +1
‚àö
2

y3 ;

f7 = ‚àí

i ‚àí1
‚àö
2

( f 4 + f 5 );

y7 .

ek :
e0 = f 0 + f 1 ;
e3 = ‚àí

i +1
‚àö
2

e4 = f 0 ‚àí f 1 ;

e1 = ‚àíi( f 2 + f 3 );

e2 = ‚àí

( f 6 + f 7 );
e5 = f 2 ‚àí f 3 ;

e6 = f 4 ‚àí f 5 ;

e7 = f 6 ‚àí f 7 .

d0 = e0 + e1 ;

d1 = ‚àíi(e2 + e3 );

d2 = e4 + e5 ;

d3 = ‚àíi(e6 + e7 );

d4 = e0 ‚àí e1 ;

d5 = e2 ‚àí e3 ;

dk :

d6 = e4 ‚àí e5 ;

d7 = e6 ‚àí e7 .

ck :
c0 = d0 + d1 ;

c1 = d2 + d3 ;

c2 = d4 + d5 ;

c3 = d6 + d7 ;

c4 = d0 ‚àí d1 ;

c5 = d2 ‚àí d3 ;

c6 = d4 ‚àí d5 ;

c7 = d6 ‚àí d7 .

Calcular las constantes c0, c1,   , c7 de esta forma requiere el n√∫mero de operaciones
mostradas en la tabla 8.31. Observe de nuevo que la multiplicaci√≥n por 1 o 21 se ha incluido
en el conteo, a pesar de que no requiere esfuerzo computacional.

Tabla 8.13

Paso

Multiplicaciones/divisiones

Sumas/restas

(La f k :)
(La ek :)
(La dk :)
(La ck :)
Total

8
8
8
0
24

0
8
8
8
24

416

CAP√çTULO 8

Teor√≠a de aproximaci√≥n

La falta de multiplicaciones/divisiones al encontrar ckUH√ÅHMDHOKHFKRGHTXHSDUDFXDO2m‚àí1
quier mORVFRH√ÄFLHQWHV{ck }2m‚àí1
k=0 se calculan a partir de {dk }k=0 de la misma forma:

ck = d2k + d2k+1 y ck+m = d2k ‚àí d2k+1 ,

para k = 0, 1, . . . , m ‚àí 1,

por lo que no existen multiplicaciones complejas.
(QUHVXPHQORVFiOFXORVGLUHFWRVGHORVFRH√ÄFLHQWHVc0, c1,   , c7 requieren 64 multiplicaciones/divisiones y 56 sumas/restas. La t√©cnica de la transformada r√°pida de Fourier
reduce los c√°lculos a 24 multiplicaciones/divisiones y 24 sumas/restas.
El algoritmo 8.3 realiza la transformada r√°pida de Fourier cuando m 5 2p para algunos
enteros positivos p6HSXHGHQKDFHUPRGL√ÄFDFLRQHVDODWpFQLFDFXDQGRm toma otras formas.

ALGORITMO

8.3

Transformada r√°pida de Fourier
3DUDFDOFXODUORVFRH√ÄFLHQWHVHQODVXPD

‚àö
1
1
ck eikx =
ck (cos kx + i sen kx), donde i = ‚àí1,
m k=0
m k=0
2m‚àí1

2m‚àí1

p
para los datos {(x j , y j )}2m‚àí1
j=0 , donde m = 2 y x j = ‚àíœÄ + jœÄ/m para j = 0, 1, . . . , 2m ‚àí 1:

ENTRADA m, p; y0 , y1 , . . . , y2m‚àí1 .
SALIDA

n√∫meros complejos c0 , . . . , c2m‚àí1 ; n√∫meros reales a0 , . . . , am ; b1 , . . . , bm‚àí1 .

Paso 1 Determine M = m;
q = p;
Œ∂ = eœÄi/m .
Paso 2 Para j = 0, 1, . . . , 2m ‚àí 1 determine c j = y j .
Paso 3 Para j = 1, 2, . . . , M

determine Œæ j = Œ∂ j ;
Œæ j+M = ‚àíŒæ j .

Paso 4 Determine K = 0;
Œæ0 = 1.
Paso 5 Para L = 1, 2, . . . , p + 1 haga los pasos 6‚Äì12.
Paso 6 Mientras K < 2m ‚àí 1 haga los pasos 7‚Äì11.
Paso 7 Para j = 1, 2, . . . , M haga los pasos 8‚Äì10.
Paso 8 Sea K = k p ¬∑ 2 p + k p‚àí1 ¬∑ 2 p‚àí1 + ¬∑ ¬∑ ¬∑ + k1 ¬∑ 2 + k0 ;
(Descomponga k.)
determine K 1 = K /2q = k p ¬∑ 2 p‚àíq + ¬∑ ¬∑ ¬∑ + kq+1 ¬∑ 2 + kq ;
K 2 = kq ¬∑ 2 p + kq+1 ¬∑ 2 p‚àí1 + ¬∑ ¬∑ ¬∑ + k p ¬∑ 2q .
Paso 9 Determine Œ∑ = c K +M Œæ K 2 ;
c K +M = c K ‚àí Œ∑;
c K = c K + Œ∑.
Paso 10 Determine K = K + 1.
Paso 11 Determine K = K + M.

8.6 Transformadas r√°pidas de Fourier

417

Paso 12 Determine K = 0;
M = M/2;
q = q ‚àí 1.
Paso 13 Mientras K < 2m ‚àí 1 haga los pasos 14‚Äì16.
Paso 14 Sea K = k p ¬∑ 2 p + k p‚àí1 ¬∑ 2 p‚àí1 + ¬∑ ¬∑ ¬∑ + k1 ¬∑ 2 + k0 ; (Decomponga k.)
determine j = k0 ¬∑ 2 p + k1 ¬∑ 2 p‚àí1 + ¬∑ ¬∑ ¬∑ + k p‚àí1 ¬∑ 2 + k p .
Paso 15 Si j > K entonces intercambie c j y ck .
Paso 16 Determine K = K + 1.
Paso 17 Determine a0 = c0 /m;
am = Re(e‚àíiœÄ m cm /m).
Paso 18 Para j = 1, . . . , m ‚àí 1 determine a j = Re(e‚àíiœÄ j c j /m);
b j = Im(e‚àíiœÄ j c j /m).
Paso 19 SALIDA (c0 , . . . , c2m‚àí1 ; a0 , . . . , am ; b1 , . . . , bm‚àí1 );
PARE.

Ejemplo 1

Encuentre el polinomio trigonom√©trico de interpolaci√≥n de grado 2 en [2œÄ, œÄ] para los datos
3
(x j , f (x j )) j=0 , donde f (x) = 2x 2 ‚àí 9.
Soluci√≥n

Tenemos
3

ak =

1
f (x j ) cos(kx j )
2 j=0

1
2
1
a1 =
2
a0 =

f (‚àíœÄ) + f ‚àí

œÄ
2

3

para k = 0, 1, 2
+ f (0) + f

f (‚àíœÄ) cos(‚àíœÄ ) + f ‚àí

y b1 =

œÄ
2

1
f (x j ) sen(x j ) por lo que,
2 j=0

= ‚àí3.19559339,
+ f (0) cos 0 + f

œÄ
œÄ
cos
2
2

œÄ
cos(‚àíœÄ ) + f (0) cos 0 + f
2

œÄ
cos (œÄ )
2

œÄ
œÄ
sen ‚àí
2
2

œÄ
œÄ
sen
2
2

œÄ
œÄ
cos ‚àí
2
2

= ‚àí9.86960441,
a2 =

1
2

f (‚àíœÄ) cos(‚àí2œÄ ) + f ‚àí

= 4.93480220,
y
b1 =

1
2

f (‚àíœÄ) sen(‚àíœÄ ) + f ‚àí

+ f (0) sen 0 + f

Por lo que,

S2 (x) =

1
(‚àí3.19559339 + 4.93480220 cos 2x) ‚àí 9.86960441 cos x.
2

/D√ÄJXUDPXHVWUDf(x) y el polinomio trigonom√©trico interpolante S2(x).

= 0.

418

CAP√çTULO 8

Teor√≠a de aproximaci√≥n

Figura 8.16
y
10
8
6

y = f (x)
y = S2 (x)

4
2
23

21

1

22

3

x

24
26
28
210

El siguiente ejemplo ilustra c√≥mo encontrar un polinomio trigonom√©trico interpolante
SDUDXQDIXQFLyQGH√ÄQLGDHQXQLQWHUYDORFHUUDGRGLIHUHQWHD>2œÄ, œÄ].
Ejemplo 2

Determine el polinomio de interpolaci√≥n trigonom√©trica de grado 4 en [0, 2] para los datos
{( j/4, f ( j/4))}7j=0 , donde f (x) = x 4 ‚àí 3x 3 + 2x 2 ‚àí tan x(x ‚àí 2).
Soluci√≥n

Primero necesitamos transformar el intervalo [0, 2] a [2œÄ, œÄ]. Esto est√° dado por

z j = œÄ(x j ‚àí 1),
por lo que los datos de entrada para el algoritmo 8.3 son

z j, f 1 +

zj
œÄ

7
j=0

.

El polinomio de interpolaci√≥n en z es

S4 (z) = 0.761979 + 0.771841 cos z + 0.0173037 cos 2z + 0.00686304 cos 3z
‚àí 0.000578545 cos 4z ‚àí 0.386374 sen z + 0.0468750 sen 2z ‚àí 0.0113738 sen 3z.
El polinomio trigonom√©trico S4(x) en [0, 2] se obtiene al sustituir z 5 œÄ (x 2 1) en S4(z).
/DVJUi√ÄFDVGHy 5 f (x) y S4(x VHPXHVWUDQHQOD√ÄJXUD/RVYDORUHVGHf (x) y S4(x) est√°n
determinados en la tabla 8.14.

8.7

Software num√©rico

419

Figura 8.17
y
2

y 5 f (x)
y 5 S4(x)
1

1

Tabla 8.14

x

f (x)

S4 (x)

| f (x) ‚àí S4 (x)|

0.125
0.375
0.625
0.875
1.125
1.375
1.625
1.875

0.26440
0.84081
1.36150
1.61282
1.36672
0.71697
0.07909
‚àí0.14576

0.25001
0.84647
1.35824
1.61515
1.36471
0.71931
0.07496
‚àí0.13301

1.44 √ó 10‚àí2
5.66 √ó 10‚àí3
3.27 √ó 10‚àí3
2.33 √ó 10‚àí3
2.02 √ó 10‚àí3
2.33 √ó 10‚àí3
4.14 √ó 10‚àí3
1.27 √ó 10‚àí2

2

x

0iVGHWDOOHVVREUHODYHUL√ÄFDFLyQGHODYDOLGH]GHOSURFHGLPLHQWRGHODWUDQVIRUPDGD
r√°pida de Fourier se pueden encontrar en [Ham], que presenta el m√©todo desde un enfoque
matem√°tico, o en [Brac], donde la presentaci√≥n est√° basada en m√©todos que quiz√° sean familiares para los ingenieros.
[AHU], p. 252‚Äì269 es una buena referencia para un an√°lisis de los aspectos computaFLRQDOHVGHOPpWRGR/DPRGL√ÄFDFLyQGHOSURFHGLPLHQWRSDUDHOFDVRFXDQGRm no es una
potencia de 2 se puede encontrar en [Win]. Una presentaci√≥n de las t√©cnicas y el material
relacionado desde el punto de vista del √°lgebra abstracta aplicada se da en [Lau], p. 438‚Äì465.
La secci√≥n Conjunto de ejercicios 8.6 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

8.7 Software num√©rico
La biblioteca IMSL proporciona un n√∫mero de rutinas de aproximaci√≥n incluyendo:
1.
2.
3.

Los m√≠nimos cuadrados lineales se ajustan a los datos con estad√≠stica
Los m√≠nimos cuadrados discretos se ajustan a los datos con la selecci√≥n del usuario
de funciones de bases
Aproximaci√≥n de m√≠nimos cuadrados de spline c√∫bico

420

CAP√çTULO 8

Teor√≠a de aproximaci√≥n

4.
5.

Aproximaci√≥n racional ponderada de Chebyshev
La transformada r√°pida de Fourier se ajusta a los datos

La biblioteca NAG proporciona rutinas que incluyen el c√°lculo de lo siguiente:
1.
2.
3.
4.
5.

La aproximaci√≥n polinomial de m√≠nimos cuadrados mediante una t√©cnica para minimizar el error de redondeo
La aproximaci√≥n de m√≠nimos cuadrados de spline c√∫bico
Mejor ajuste en el sentido l1
Mejor ajuste en el sentido l‚àû
La transformada r√°pida de Fourier se ajusta a los datos

La biblioteca netlib contiene una rutina para calcular la aproximaci√≥n de m√≠nimos cuadrados polinomiales para un conjunto discreto de puntos y una rutina para evaluar este polinomio y cualquiera de sus derivadas en un punto determinado.
Las secciones Preguntas de an√°lisis, Conceptos clave y Revisi√≥n del cap√≠tulo est√°n disponibles en l√≠nea. Encuentre la ruta de acceso en las p√°ginas preliminares.

CAP√çTULO

9

Aproximaci√≥n de eigenvalores

Introducci√≥n
Las vibraciones longitudinales de un barra el√°stica de rigidez local p(x) y densidad œÅ (x) se
describen mediante la ecuaci√≥n diferencial parcial

œÅ(x)

‚àÇ 2v
‚àÇ
(x, t) =
‚àÇt 2
‚àÇx

p(x)

‚àÇv
(x, t) ,
‚àÇx

donde v (x, t) es el desplazamiento longitudinal promedio de una secci√≥n de la barra desde
su posici√≥n de equilibrio x en el tiempo t. Las vibraciones pueden escribirse como una suma
de vibraciones arm√≥nicas simples:
‚àû

ck u k (x) cos

v(x, t) =

Œªk (t ‚àí t0 ),

k=0

donde
d
dx

p(x)

du k
(x) + Œªk œÅ(x)u k (x) = 0.
dx

Si la barra tiene longitud l\HVWi√ÄMDHQVXVH[WUHPRVHQWRQFHVODHFXDFLyQGLIHUHQFLDOVH
mantiene para 0 < x < l y v(0) = v(l) = 0.
v(x) en un tiempo fijo t

v(x,t)
0

x

l

x

Un sistema de estas ecuaciones diferenciales recibe el nombre de sistema Sturm-Liouville
y los n√∫meros Œªk son eigenvalores con eigenfunciones correspondientes uk (x).
Suponga que la barra es de 1 m de largo con rigidez uniforme p(x) 5 p y densidad uniforme œÅ(x) 5 œÅ. Para aproximar u y Œª, sea h 5 0.2. Entonces x j = 0.2 j, para 0 ‚â§ j ‚â§ 5, y
podemos usar la f√≥rmula de punto medio (4.5) en la secci√≥n 4.1 para aproximar las primeras
derivadas. Esto da el sistema lineal

‚é§
‚é§
‚é°
‚é§‚é°
w1
2 ‚àí1
0
0
w1
‚é•
‚é•
‚é¢
‚é¢ ‚àí1
‚é¢
2 ‚àí1
0 ‚é•
‚é• ‚é¢ w 2 ‚é• = ‚àí0.04 œÅ Œª ‚é¢ w 2 ‚é• = ‚àí0.04 œÅ Œªw.
Aw = ‚é¢
‚é¶
‚é£
‚é£ 0 ‚àí1
‚é¶
‚é£
w3
w3 ‚é¶
2 ‚àí1
p
p
w4
w4
0
0 ‚àí1
2
‚é°

421

422

CAP√çTULO 9

Aproximaci√≥n de eigenvalores

En este sistema, w j ‚âà u(x j ), para 1‚â§ j ‚â§ 4, y w 0 = w 5 = 0. Los cuatro eigenvalores
de A aproximan los eigenvalores del sistema Sturm-Liouville. Es la aproximaci√≥n de los
HLJHQYDORUHVTXHFRQVLGHUDUHPRVHQHVWHFDStWXOR(QHOHMHUFLFLRGHODVHFFLyQVHDQDliza una aplicaci√≥n Sturm-Liouville.

9.1 √Ålgebra lineal y eigenvalores
/RVHLJHQYDORUHV\ORVHLJHQYHFWRUHVVHSUHVHQWDURQHQHOFDStWXORHQUHODFL√≥n con la convergencia de m√©todos iterativos para la aproximaci√≥n de la soluci√≥n de un sistema lineal. Para
determinar los eigenvalores de una matriz A n 3 n, construimos el polinomio caracter√≠stico

p(Œª) = det(A ‚àí ŒªI )
y, entonces, determinamos sus ceros. Encontrar el determinante de una matriz n 3 n es caro
desde el punto computacional y hallar buenas aproximaciones para las ra√≠ces de p(Œª) tambi√©n
es dif√≠cil. En este cap√≠tulo exploraremos otros medios para aproximar los eigenvalores de
XQDPDWUL](QODVHFFLyQGDPRVXQDLQWURGXFFLyQDXQDWpFQLFDSDUDODIDFWRUL]DFLyQ
de una matriz m 3 n en una forma que tiene aplicaciones valiosas en numerosas √°reas.
(QHOFDStWXORHQFRQWUDPRVTXHXQDWpFQLFDLWHUDWLYDSDUDUHVROYHUXQVLVWHPDOLQHDO
converger√° si todos los eigenvalores asociados con el problema tienen magnitud menor que
1. Los valores exactos de los eigenvalores en este caso no son muy importantes, s√≥lo la
UHJLyQGHXQSODQRFRPSOHMRHQHOTXHVHHQFXHQWUDQ8QUHVXOWDGRLPSRUWDQWHHQHVWHVHQtido fue descubierto primero por S. A. Ger≈°gorin. Es el tema de un libro muy interesante de
Richard Varga [Var2].
Teorema 9.1
Semyon Aranovich Ger≈°gorin
¬≤ WUDEDMyHQHO
Instituto Tecnol√≥gico de
3HWURJUDGRKDVWDFXDQGRVH
mud√≥ al Instituto de Ingenier√≠a
Mec√°nica de Leningrado. Su
DUWtFXORGH√úber die
Abgrenzung der Eigenwerte
einer Matrix ([Ger]) inclu√≠a lo
que se conoce como teorema del
c√≠rculo.

(C√≠rculo de Ger≈°gorin)
Sea A una matriz n 3 n y RiGHQRWDHOFtUFXORHQHOSODQRFRPSOHMRFRQFHQWURai i y radio
n
j=1, j=i |ai j |; es decir,
n

Ri =

z ‚àà C |z ‚àí aii | ‚â§

|ai j | ,
j=1, j=i

donde CGHQRWDHOSODQRFRPSOHMR/RVHLJHQYDORUHVGHA est√°n contenidos en la uni√≥n de
n
Ri . Adem√°s, la uni√≥n de cualquier k de los c√≠rculos que no cruzan
estos c√≠rculos, R = ‚à™i=1
el resto de (n 2 k), contiene precisamente k (multiplicidades contadas) de los eigenvalores.
Suponga que Œª es un eigenvalor de A con un eigenvector asociado x, donde
x ‚àû = 1. Puesto que Ax = Œªx, la representaci√≥n del componente equivalente es

Demostraci√≥n

n

ai j x j = Œªxi ,

para cada i = 1, 2, . . . , n.

j=1

Sea k un entero con |xk

x ‚àû = 1. Cuando i 5 kODHFXDFLyQ  LPSOLFDTXH
n

ak j x j = Œªxk .
j=1

Por lo tanto,
n

ak j x j = Œªxk ‚àí akk xk = (Œª ‚àí akk )xk ,
j=1,
j=k



9.1 √Ålgebra lineal y eigenvalores

423

y
n

|Œª ‚àí akk | ¬∑ |xk | =

n

ak j x j ‚â§
j=1,
j=k

Pero, |xk

|ak j ||x j |.
j=1,
j=k

x ‚àû = 1, por lo que |x j | ‚â§ |xk | = 1 para toda j = 1, 2, . . . , n. Por lo tanto,
n

|Œª ‚àí akk | ‚â§

|ak j |.
j=1,
j=k

(VWRGHPXHVWUDODSULPHUDD√ÄUPDFLyQHQHOWHRUHPDTXH Œª ‚àà Rk . Una demostraci√≥n se encuentra en [Var2], p. 8 o en [Or2], p. 48.
Ejemplo 1

'HWHUPLQHORVFtUFXORV*HU≈çJRULQSDUDODPDWUL]
‚é°
‚é§
4 1 1
A=‚é£ 0 2 1 ‚é¶
‚àí2 0 9
y √∫selos para encontrar los l√≠mites del radio espectral de A.
Soluci√≥n

Los c√≠rculos en el teorema de Ger≈çJRULQVRQ FRQVXOWHOD√ÄJXUD

R1 = {z ‚àà C | |z ‚àí 4| ‚â§ 2}, R2 = {z ‚àà C | |z ‚àí 2| ‚â§ 1}, y R3 = {z ‚àà C | |z ‚àí 9| ‚â§ 2}.
Puesto que R1 y R2 est√°n separados de R, existen precisamente dos eigenvalores dentro de
R1 ‚à™ R2 y uno dentro de R. Adem√°s, œÅ(A) = m√°x1‚â§i‚â§3 |Œªi |, por lo que 7 ‚â§ œÅ(A) ‚â§ 11.
Figura 9.1
Eje
imaginario
Un eigenvalor

Dos eigenvalores

2
1

Eje real
21

1

2

3

4

5

6

7 8

9 10

11

22

Incluso cuando necesitamos encontrar los eigenvalores, muchas t√©cnicas para su aproximaci√≥n son iterativas. La determinaci√≥n de las regiones en las que se encuentran es el primer
paso para hallar las aproximaciones porque nos da aproximaciones iniciales.
Antes de considerar otros resultados concernientes a eigenvalores y a eigenvectores neFHVLWDPRVDOJXQDVGH√ÄQLFLRQHV\UHVXOWDGRVGHOiOJHEUDOLQHDO7RGRVORVUHVXOWDGRVJHQHUDOHV
que se requerir√°n en lo que resta de este cap√≠tulo se listan aqu√≠ para facilitar la referencia su
consulta. Las demostraciones de muchos de estos resultados no proporcionados se consideUDQHQORVHMHUFLFLRV\HVSRVLEOHHQFRQWUDUWRGRVHQGLYHUVRVWH[WRVHVWiQGDUVREUHiOJHEUD
OLQHDO FRQVXOWHSRUHMHPSOR>1'@>3RR@R>'*@ 
/DSULPHUDGH√ÄQLFLyQFRPSDUDODGH√ÄQLFLyQGHODLQGHSHQGHQFLDOLQHDOGHODVIXQFLRQHV
descritas en la secci√≥n 8.2. De hecho, mucho de lo que veremos en esta secci√≥n se compara
con el material del cap√≠tulo 8.

424

CAP√çTULO 9

Aproximaci√≥n de eigenvalores

DeÔ¨Ånici√≥n 9.2

Sea {v(1) , v(2) , v(3) , . . . , v(k) }XQFRQMXQWRGHYHFWRUHV(OFRQMXQWRHVlinealmente independiente si, siempre que

0 = Œ±1 v(1) + Œ±2 v(2) + Œ±3 v(3) + ¬∑ ¬∑ ¬∑ + Œ±k v(k) ,
entonces, Œ±i = 0, para cada i = 0, 1, . . . , k. 'H OR FRQWUDULR HO FRQMXQWR GH YHFWRUHV HV
linealmente dependiente.
2EVHUYHTXHFXDOTXLHUFRQMXQWRGHYHFWRUHVTXHFRQWLHQHQHOYHFWRUFHURHVOLQHDOPHQWH
dependiente.
Teorema 9.3

Suponga que {v(1) , v(2) , v(3) , . . . , v(k) } HV XQ FRQMXQWR GH n vectores linealmente independientes en Rn. Entonces, para cualquier vector x ‚àà Rn , H[LVWHXQ~QLFRFRQMXQWRGHFRQVWDQtes Œ≤1 , Œ≤2 , . . . , Œ≤n con

x = Œ≤1 v(1) + Œ≤2 v(2) + Œ≤3 v(3) + ¬∑ ¬∑ ¬∑ + Œ≤n v(n) .
Demostraci√≥n Sea A la matriz cuyas columnas son los vectores v(1) , v(2) , . . . , v(n) . Enton-

FHVHOFRQMXQWR {v(1) , v(2) , . . . , v(n) } es linealmente independiente si y s√≥lo si la ecuaci√≥n
matricial

A(Œ±1 , Œ±2 , . . . , Œ±n )t = 0 tiene la √∫nica soluci√≥n (Œ±1 , Œ±2 , . . . , Œ±n )t = 0.
3HUR SRU HO WHRUHPD  HQ OD SiJLQD  HVWR HV HTXLYDOHQWH D OD HFXDFLyQ PDWULFLDO
A(Œ≤1 , Œ≤2 , . . . , Œ≤n )t = x, que tiene una √∫nica soluci√≥n para cualquier vector x ‚àà Rn . Esto,
a su vez, es equivalente a la declaraci√≥n de que para cualquier vector x ‚àà Rn , existe un √∫nico
FRQMXQWRGHFRQVWDQWHVŒ≤1 , Œ≤2 , . . . , Œ≤n con

x = Œ≤1 v(1) + Œ≤2 v(2) + Œ≤3 v(3) + ¬∑ ¬∑ ¬∑ + Œ≤n v(n) .
DeÔ¨Ånici√≥n 9.4

&XDOTXLHUFRQMXQWRGHn vectores linealmente independientes en Rn recibe el nombre de base
para Rn.

Ejemplo 2

a) Muestre que v(1) = (1, 0, 0)t , v(2) = (‚àí1, 1, 1)t , y v(3) = (0, 4, 2)t es una base para R, y
b) dado un vector arbitrario x ‚àà R3 , encuentre Œ≤1 , Œ≤2 y Œ≤3 con

x = Œ≤1 v(1) + Œ≤2 v(2) + Œ≤3 v(3) .
Soluci√≥n

a) Sean Œ±1 , Œ±2 y Œ±3 n√∫meros con 0 = Œ±1 v(1) + Œ±2 v(2) + Œ±3 v(3) . Entonces

(0, 0, 0)t = Œ±1 (1, 0, 0)t + Œ±2 (‚àí1, 1, 1)t + Œ±3 (0, 4, 2)t
= (Œ±1 ‚àí Œ±2 , Œ±2 + 4Œ±3 , Œ±2 + 2Œ±3 )t ,
por lo que

Œ±1 ‚àí Œ±2 = 0,

Œ±2 + 4Œ±3 = 0,

y Œ±2 + 2Œ±3 = 0.

La √∫nica soluci√≥n para este sistema es Œ±1 = Œ±2 = Œ±3 = 0,  SRU OR TXH HVWH FRQMXQWR
{v(1) , v(2) , v(3) } de tres vectores linealmente independientes en R es una base para R.
b) Sea x = (x1 , x2 , x3 )t un vector en R. Resolviendo

x = Œ≤1 v(1) + Œ≤2 v(2) + Œ≤3 v(3)
= Œ≤1 (1, 0, 0)t + Œ≤2 (‚àí1, 1, 1)t + Œ≤3 ((0, 4, 2)t
= (Œ≤1 ‚àí Œ≤2 , Œ≤2 + 4Œ≤3 , Œ≤2 + 2Œ≤3 )t
es equivalente a resolver para Œ≤1 , Œ≤2 y Œ≤3 en el sistema

Œ≤1 ‚àí Œ≤ 2 = x 1 ,

Œ≤2 + 4Œ≤3 = x2 ,

Œ≤2 + 2Œ≤3 = x3 .

Este sistema tiene la soluci√≥n √∫nica

Œ≤1 = x1 ‚àí x2 + 2x3 ,

Œ≤2 = 2x3 ‚àí x2 y Œ≤3 =

1
(x2 ‚àí x3 ).
2

9.1 √Ålgebra lineal y eigenvalores

425

(OVLJXLHQWHUHVXOWDGRVHXVDUiHQODVHFFLyQSDUDGHVDUUROODUHOPpWRGRGHSRWHQFLD
SDUDDSUR[LPDUORVHLJHQYDORUHV(QHOHMHUFLFLRVHFRQVLGHUDXQDSUXHEDGHHVWHUHVXOWDGR
Teorema 9.5

Si A es una matriz y Œª1 , . . . , Œªk son eigenvalores distintos de A con eigenvectores asociados
x(1) , x(2) , . . . , x(k) , entonces {x(1) , x(2) , . . . , x(k) }HVXQFRQMXQWROLQHDOPHQWHLQGHSHQGLHQWH

Ejemplo 3

Muestre que se puede formar una base para RXVDQGRORVHLJHQYHFWRUHVGHODPDWUL]3
‚é°
‚é§
2
0 0
1 2 ‚é¶.
A=‚é£ 1
1 ‚àí1 4
Soluci√≥n (QHOHMHPSORGHODVHFFLyQHQFRQWUDPRVTXHA tiene el polinomio caracte-

r√≠stico

p(Œª) = p(A ‚àí ŒªI ) = (Œª ‚àí 3)(Œª ‚àí 2)2 .
Por lo tanto, existen dos eigenvalores distintos de A: Œª1 = 3 y Œª2 = 2. (QHVHHMHPSORWDPbi√©n encontramos que Œª1 = 3 tiene el eigenvector x1 = (0, 1, 1)t y que hay dos eigenvectores linealmente independientes x2 = (0, 2, 1)t y x3 = (‚àí2, 0, 1)t correspondientes a Œª2 = 2.
1RHVGLItFLOPRVWUDU FRQVXOWHHOHMHUFLFLR TXHHVWHFRQMXQWRGHWUHVHLJHQYHFWRUHV

{x1 , x2 , x3 } = {(0, 1, 1)t , (0, 2, 1)t , (‚àí2, 0, 1)t }
es linealmente independiente y, por lo tanto, forma una base para R.
(QHOVLJXLHQWHHMHPSORREVHUYDUHPRVXQDPDWUL]FX\RVHLJHQYDORUHVVRQLJXDOHVDORV
GHOHMHPSORSHURFX\RVHLJHQYHFWRUHVWLHQHQXQFDU√°cter diferente.
Ejemplo 4

0XHVWUHTXHQLQJ~QFRQMXQWRGHHLJHQYHFWRUHVGHODPDWUL]3
‚é°
‚é§
2 1 0
B=‚é£ 0 2 0 ‚é¶
0 0 3
puede formar una base para R.
Soluci√≥n

Esta matriz tambi√©n tiene el mismo polinomio caracter√≠stico que la matriz A en
HOHMHPSOR
‚é°
‚é§
2‚àíŒª
1
0
0 2‚àíŒª
0 ‚é¶ = (Œª ‚àí 3)(Œª ‚àí 2)2 ,
p(Œª) = det ‚é£
0
0 3‚àíŒª

por lo que sus eigenvalores son iguales a los de AHQHOHMHPSORHVGHFLUŒª1 = 3 y Œª2 = 2.
Para determinar los eigenvectores para B correspondientes al eigenvalor Œª1 = 3, necesitamos resolver el sistema (B 2I )x 5 0, por lo que
‚é§ ‚é°
‚é§‚é°
‚é§ ‚é°
‚é§
‚é° ‚é§
‚é°
‚àí1
1 0
x1
‚àíx1 + x2
0
x1
‚é£ 0 ‚é¶ = (B ‚àí 3I ) ‚é£ x2 ‚é¶ = ‚é£ 0 ‚àí1 0 ‚é¶ ‚é£ x2 ‚é¶ = ‚é£
‚àíx2 ‚é¶ .
0
0 0
x3
x3
0,
0
Por lo tanto, x2 = 0, x1 = x2 = 0 y x3 es arbitrario. Haciendo x3 = 1 esto nos da el √∫nico
eigenvector linealmente independiente (0, 0, 1)t correspondiente a Œª1 = 3 .

426

CAP√çTULO 9

Aproximaci√≥n de eigenvalores

Considerando Œª2 = 2. Si
‚é° ‚é§
‚é°
‚é§ ‚é°
‚é§ ‚é°
‚é§ ‚é°
‚é§
0
x1
0 1 0
x1
x2
‚é£ 0 ‚é¶ = (B ‚àí 2Œª) ‚é£ x2 ‚é¶ = ‚é£ 0 0 0 ‚é¶ ¬∑ ‚é£ x2 ‚é¶ = ‚é£ 0 ‚é¶ ,
0
0 0 1
x3 ,
x3
x3
entonces x2 = 0, x3 = 0, y x1 es arbitrario. Existe s√≥lo un eigenvector linealmente independiente que corresponde a Œª2 = 2 , lo que se puede expresar como (1, 0, 0)t, aun cuando
Œª2 = 2 fue un cero de multiplicidad dos del polinomio caracter√≠stico de B.
(VFODURTXHHVWRVGRVHLJHQYHFWRUHVQRVRQVX√ÄFLHQWHVSDUDIRUPDUXQDEDVHSDUDR.
En particular, (0, 1, 0)t no es una combinaci√≥n lineal de {(0, 0, 1)t , (1, 0, 0)t }.
Ahora veremos que cuando el n√∫mero de eigenvectores linealmente independientes no
FRUUHVSRQGHDOWDPDxRGHODPDWUL]FRPRHQHOFDVRGHOHMHPSORH[LVWHQGL√ÄFXOWDGHVFRQ
los m√©todos de aproximaci√≥n para encontrar los eigenvalores.
(QODVHFFLyQFRQVLGHUDPRVORVFRQMXQWRVRUWRJRQDOHV\RUWRQRUPDOHVGHIXQFLRQHV
/RVYHFWRUHVFRQHVWDVSURSLHGDGHVVHGH√ÄQHQGHIRUPDVLPLODU
DeÔ¨Ånici√≥n 9.6

8QFRQMXQWRGHYHFWRUHV {v(1) , v(2) , . . . , v(n) } recibe el nombre de ortogonal si (v(i) )t v( j)5
0, para toda i = j. Si, adem√°s (v(i) )t v(i) = 1, para toda i = 1, 2, . . . , n . Entonces el
FRQMXQWRUHFLEHHOQRPEUHGHortonormal.

x 22 para cualquier x en Rn XQ FRQMXQWR GH YHFWRUHV RUWRJRQDOHV
Puesto que xt x
(2)
(n)
{v , v , . . . , v } es ortonormal si y s√≥lo si
(1)

v(i) 2 = 1,
Ejemplo 5

para cada i = 1, 2, . . . , n.

a) Muestre que los vectores v(1) = (0, 4, 2)t , v(2) = (‚àí5, ‚àí1, 2)t y v(3) = (1, ‚àí1, 2)t forPDQ XQ FRQMXQWR RUWRJRQDO \ b) ~VHORV SDUD GHWHUPLQDU XQ FRQMXQWR GH YHFWRUHV RUWRQRU
males.
(a) Tenemos (v(1) )t v(2) = 0(‚àí5) + 4(‚àí1) + 2(2) = 0,

Soluci√≥n

(v(1) )t v(3) = 0(1) + 4(‚àí1) + 2(2) = 0, y (v(2) )t v(3) = ‚àí5(1) ‚àí 1(‚àí1) + 2(2) = 0,
por lo que los vectores son ortogonales y forman una base para Rn. Las normas l2 de estos
vectores son
‚àö
‚àö
‚àö
v(1) 2 = 2 5,
v(2) 2 = 30, y
v(3) 2 = 6.
b) Los vectores
(1)

u

v(1)
= (1) =
v 2

4
2
0
‚àö , ‚àö , ‚àö
2 5 2 5 2 5

v(2)
=
v(2) 2

2
‚àí5 ‚àí1
‚àö ,‚àö ,‚àö
30
30
30

u(2) =
(3)

u

v(3)
= (3) =
v 2

1 ‚àí1 2
‚àö ,‚àö ,‚àö
6
6
6

t

t

t

=

‚àö ‚àö
2 5
5
,
0,
5
5

=

‚àí

,

‚àö
‚àö
‚àö
30
30
30
,‚àí
,
6
30
15

‚àö ‚àö
6
6
6
,‚àí
,
6
6
3

‚àö
=

t

t

, y

t

IRUPDQXQFRQMXQWRRUWRQRUPDO\DTXHKHUHGDQODRUWRJRQDOLGDGDSDUWLUGHv(1) , v(2) , y v(3) .
Adem√°s,

u(1) 2

u(2) 2

u(3) 2 = 1.

9.1 √Ålgebra lineal y eigenvalores

427

/DGHPRVWUDFLyQGHOVLJXLHQWHUHVXOWDGRVHFRQVLGHUDHQHOHMHUFLFLR
Teorema 9.7

8QFRQMXQWRRUWRJRQDOGHYHFWRUHVGLIHUHQWHVDFHURHVOLQHDOPHQWHLQGHSHQGLHQWH
El proceso Gram-SchmidtSDUDFRQVWUXLUXQFRQMXQWRGHSROLQRPLRVTXHVRQRUWRJRQDOHVUHVSHFWRDODIXQFLyQGHSHVRGHWHUPLQDGDVHGHVFULELyHQHOWHRUHPDGHODVHFFL√≥n
 FRQVXOWH OD SiJLQD   ([LVWH XQ SURFHVR SDUDOHOR WDPELpQ FRQRFLGR FRPR *UDP
Schmidt, que nos permite construir una base ortogonal para RnGDGRXQFRQMXQWRGHn vectores linealmente independientes en Rn.

Teorema 9.8

Sea {x1 , x2 , . . . , xk }XQFRQMXQWRGH k vectores linealmente independientes en Rn. Entonces
{v1 , v2 , . . . , vk }GH√ÄQLGRPHGLDQWH

v1 = x1 ,
v2 = x2 ‚àí

vt1 x2
vt1 v1

v1 ,

v3 = x3 ‚àí

vt1 x3
vt1 v1

v1 ‚àí

vt2 x3
vt2 v2

v2 ,

..
.
k‚àí1

vk = xk ‚àí
i=1

vit xk
vit vi

vi

HVXQFRQMXQWRGHk vectores ortogonales en Rn.
/DGHPRVWUDFLyQGHHVWHWHRUHPDTXHVHDQDOL]DHQHOHMHUFLFLRHVXQDYHUL√ÄFDFLyQ
directa del hecho de que para cada 1 ‚â§ i ‚â§ k y 1 ‚â§ j ‚â§ k y con i = j, tenemos vit v j = 0.
2EVHUYHTXHFXDQGRHOFRQMXQWRRULJLQDOGHYHFWRUHVIRUPDXQDEDVHSDUDRn, es decir,
cuando k 5 n, entonces los vectores construidos forman una base ortogonal para Rn. A partir
de esto podemos formar una base ortonormal {u1 , u2 , . . . , un }VLPSOHPHQWHDOGH√ÄQLUSDUD
cada i = 1, 2, . . . , n

ui =

vi
.
||vi ||2

(OVLJXLHQWHHMHPSORLOXVWUDFyPRVHSXHGHFRQVWUXLUXQDEDVHRUWRJRQDOSDUDR a partir
de tres vectores linealmente independientes en R.
Ejemplo 6

8VHHOSURFHVR*UDP6FKPLGWSDUDGHWHUPLQDUXQFRQMXQWRGHYHFWRUHVRUWRJRQDOHVDSDUWLUGH
los vectores linealmente independientes

x(1) = (1, 0, 0)t ,
Soluci√≥n

x(2) = (1, 1, 0)t ,

y

x(3) = (1, 1, 1)t .

Tenemos los vectores ortogonales v(1) , v(2) y v(3) , dados por

v(1) = x(1) = (1, 0, 0)t
v(2) = (1, 1, 0)t ‚àí

((1, 0, 0)t )t (1, 1, 0)t
((1, 0, 0)t )t (1, 0, 0)t

(1, 0, 0)t = (1, 1, 0)t ‚àí (1, 0, 0)t = (0, 1, 0)t

v(3) = (1, 1, 1)t ‚àí

((1, 0, 0)t )t (1, 1, 1)t
((1, 0, 0)t )t (1, 0, 0)t

(1, 0, 0)t ‚àí

((0, 1, 0)t )t (1, 1, 1)t
((0, 1, 0)t )t (0, 1, 0)t

(0, 1, 0)t

= (1, 1, 1)t ‚àí (1, 0, 0)t ‚àí (0, 1, 0)t = (0, 0, 1)t .
(OFRQMXQWR{v(1) , v(2) , v(3) } resulta ser tanto ortonormal, como ortogonal, pero com√∫nmente,
√©sta no es la situaci√≥n.

428

CAP√çTULO 9

Aproximaci√≥n de eigenvalores

La secci√≥n Conjunto de ejercicios 9.1 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

9.2 Matrices ortogonales y transformaciones de similitud
(QHVWDVHFFLyQFRQVLGHUDUHPRVODFRQH[LyQHQWUHORVFRQMXQWRVGHYHFWRUHV\ODVPDWULFHV
formadas usando estos vectores como sus columnas. Primero consideramos algunos resultaGRVDOUHGHGRUGHXQDFODVHGHPDWULFHVHVSHFLDOHV/DWHUPLQRORJtDHQODVLJXLHQWHGH√ÄQLFLyQ
VLJXHGHOKHFKRGHTXHODVFROXPQDVGHXQDPDWUL]RUWRJRQDOIRUPDUiQXQFRQMXQWRRUWRJRQDO
de vectores.
DeÔ¨Ånici√≥n 9.9
3UREDEOHPHQWHVHUtDPHMRU
llamar ortonormales a las
matrices ortogonales porque las
columnas no s√≥lo forman un
FRQMXQWRRUWRJRQDOGHYHFWRUHV
sino tambi√©n uno ortonormal.

Teorema 9.10

Se dice que una matriz Q es ortogonal si sus columnas {qt1 , qt2 , . . . , qtn }IRUPDQXQFRQMXQWR
ortonormal en Rn.
Las siguientes propiedades importantes de las matrices ortogonales se consideran en el
HMHUFLFLR
Suponga que Q es una matriz n 3 n ortogonal. Entonces

i)

Q es invertible con Q ‚àí1 = Q t .

ii)

Para cualquier x y y en Rn , (Qx)t Qy = xt y.

iii)

Para cualquier x en Rn , ||Qx||2 = ||x||2 .

iv)

Cualquier matriz invertible Q con Q ‚àí1 = Q t es ortogonal.

&RPR HMHPSOR ODV PDWULFHV GH SHUPXWDFLyQ TXH VH KDQ DQDOL]DGR HQ OD VHFFLyQ 
tienen esta propiedad, por lo que son ortogonales.
A menudo, la propiedad iii) GHOWHRUHPDVHH[SUHVDDOHVWDEOHFHUTXHODVPDWULFHV
ortogonales preservan la norma l 2. Como consecuencia inmediata de esta propiedad, todas
las matrices ortogonales Q tienen ||Q||2 = 1.
Ejemplo 1

Muestre que la matriz

‚é°

‚àö

0

‚é¢ ‚àö
‚é¢
Q = [u(1) , u(2) , u(3) ] = ‚é¢ 2 5 5
‚é£ ‚àö

‚àí 630
‚àö

‚àí 3030

5
5

‚àö
30
15

‚àö
6
6
‚àö

‚é§

‚é•
‚é•
‚àí 66 ‚é•
‚é¶
‚àö
6
3

IRUPDGDDSDUWLUGHOFRQMXQWRRUWRQRUPDOGHYHFWRUHVHQFRQWUDGRHQHOHMHPSORGHODVHFFLyQHVXQDPDWUL]RUWRJRQDO
Soluci√≥n Observe que

‚é°

0

‚é¢ ‚àö
‚é¢
Q Qt = ‚é¢ 2 5 5
‚é£ ‚àö
5
5

‚àö
‚àí 30
6
‚àö
‚àí 3030
‚àö
30
15

‚àö
6
6
‚àö
‚àí 66
‚àö
6
3

‚é§ ‚é°

0

‚é• ‚é¢ ‚àö
‚é• ‚é¢
‚é• ¬∑ ‚é¢ ‚àí 630
‚é¶ ‚é£ ‚àö
6
6

‚àö
2 5
5
‚àö
‚àí 3030
‚àö
‚àí 66

‚àö
5
5
‚àö
30
15
‚àö
6
3

‚é§

‚é°
‚é§
1 0 0
‚é•
‚é• ‚é£
‚é• = 0 1 0 ‚é¶ = I.
‚é¶
0 0 1

0HGLDQWHHOFRURODULRHQODVHFFLyQ FRQVXOWHODSiJLQD pVWRHVVX√ÄFLHQWHSDUD
garantizar que Q t = Q ‚àí1 . Por ello, Q es una matriz ortogonal.
/DVLJXLHQWHGH√ÄQLFLyQHVWDEOHFHODVEDVHVGHPXFKDVWpFQLFDVSDUDGHWHUPLQDUORVHLgenvalores de una matriz.

9.2 Matrices ortogonales y transformaciones de similitud

DeÔ¨Ånici√≥n 9.11

429

Se dice que dos matrices A y B son similares si existe una matriz no singular S con A 5
S ‚àí1 B S.
Una caracter√≠stica importante de las matrices similares es que tienen los mismos eigenvalores.

Teorema 9.12

Suponga que A y B son matrices similares con A 5 S ‚àí1 B S y Œª es un eigenvalor de A con un eigenvector x relacionado. Entonces Œª es un eigenvalor de B con un eigenvector relacionado Sx.
Demostraci√≥n

Sea x = 0 tal que

S ‚àí1 B Sx = Ax = Œªx.
Multiplicando a la izquierda por la matriz S da

B Sx = ŒªSx.
Puesto que x = 0 y S es no singular, Sx = 0. Por lo tanto, Sx es un eigenvector de B correspondiente a su eigenvalor Œª.
Un uso especialmente importante de similitud se presenta cuando la matriz A n 3 n
es similar a la matriz diagonal, es decir, cuando existe una matriz diagonal D y una matriz
invertible S con

A = S ‚àí1 DS o, de manera equivalente, D = SAS‚àí1 .
(OVLJXLHQWHUHVXOWDGRQRHVGLItFLOGHPRVWUDU6HFRQVLGHUDHQHOHMHUFLFLR
Teorema 9.13

Una matriz A n 3 n es similar a una matriz diagonal D si y s√≥lo si A tiene n eigenvectores
linealmente independientes. En este caso, D = S ‚àí1 AS, donde las columnas de S consisten
en eigenvectores y el i-√©simo elemento diagonal de D es el eigenvalor de A que corresponde
a la i-√©sima columna de S.
El par de matrices S y D QRHV~QLFR3RUHMHPSORFXDOTXLHUUHRUGHQDPLHQWRGHODVFRlumnas de S y el reordenamiento correspondiente de los elementos de la diagonal de D dar√°n
XQSDUGLVWLQWR&RQVXOWHHOHMHUFLFLRSDUDXQDLOXVWUDFLyQ
(QHOWHRUHPDREVHUYDPRVTXHORVHLJHQYHFWRUHVGHXQDPDWUL]TXHFRUUHVSRQGHQD
ORVGLVWLQWRVHLJHQYDORUHVIRUPDQXQFRQMXQWROLQHDOPHQWHLQGHSHQGLHQWH&RPRFRQVHFXHQFLDWHQHPRVHOVLJXLHQWHFRURODULRSDUDHOWHRUHPD

Corolario 9.14

Una matriz A n 3 n que tiene n eigenvalores diferentes es similar a una matriz diagonal.
De hecho, no necesitamos que la matriz de similitud sea diagonal para que este concepto
sea √∫til. Suponga que A es similar a la matriz triangular B. La determinaci√≥n de los eigenvalores es f√°cil para una matriz triangular B, porque en este caso Œª es una soluci√≥n para la
ecuaci√≥n
n

0 = det(B ‚àí ŒªI ) =

(bii ‚àí Œª)
i=1

si y s√≥lo si Œª = bii para algunas i. El siguiente resultado describe una relaci√≥n, llamada
transformaci√≥n de similitud, entre las matrices arbitrarias y las triangulares.

430

CAP√çTULO 9

Aproximaci√≥n de eigenvalores

Teorema 9.15

(Teorema de Schur)
Sea A una matriz arbitraria. Existe una matriz no singular U con la propiedad de que

,VVDL6FKXU ¬≤ HV
conocido principalmente por su
WUDEDMRHQODWHRUtDGHJUXSRV
SHURWDPELpQWUDEDMyHQODWHRUtD
de n√∫meros, el an√°lisis y otras
√°reas. Public√≥ lo que se conoce
FRPRWHRUHPDGH6FKXUHQ
La norma l2 de una matriz
unitaria es 1.

Teorema 9.16

T = U ‚àí1 AU,
donde T es una matriz triangular superior, cuyas entradas diagonales consisten en eigenvalores de A.
La matriz UFX\DH[LVWHQFLDHVWiJDUDQWL]DGDSRUHOWHRUHPDVDWLVIDFHODFRQGLFLyQ
Ux 2
x 2 para cualquier vector x. Las matrices con esta propiedad reciben el nombre
de unitarias. A pesar de que no usaremos esta propiedad de preservaci√≥n de la norma, s√≠
DXPHQWDVLJQL√ÄFDWLYDPHQWHODDSOLFDFLyQGHOWHRUHPDGH6FKXU
(OWHRUHPDHVXQWHRUHPDGHH[LVWHQFLDTXHJDUDQWL]DTXHH[LVWHODPDWUL]T, pero
no proporciona un medio constructivo para encontrar T ya que requiere un conocimiento de
los eigenvalores de A. En muchos casos, es demasiado dif√≠cil determinar la transformaci√≥n
de similitud U.
El siguiente resultado para las matrices sim√©tricas reduce la complicaci√≥n porque, en
este caso, la matriz de transformaci√≥n es ortogonal.
La matriz A n 3 n es sim√©trica si y s√≥lo si existe una matriz diagonal D y una matriz ortogonal Q con A = QDQt .
Demostraci√≥n

Primero suponga que A = QDQt , donde Q es ortogonal y D es diagonal.

Entonces

At = QDQt

t

= Qt

t

D Qt = QDQt = A,

y A es sim√©trica.
Para demostrar que todas las matrices sim√©tricas A se pueden escribir de la forma
A = QDQt , primero considere los diferentes eigenvalores de A. Si Av1 = Œª1 v1 y Av2
= Œª2 v2 , con Œª1 = Œª2 , entonces, puesto que At = A, tenemos

(Œª1 ‚àí Œª2 )vt1 v2 = (Œª1 v1 )t v2 ‚àí vt1 (Œª2 v2 ) = (Av1 )t v2 ‚àí vt1 (Av2 ) = vt1 At v2 ‚àí vt1 Av2 = 0,
por lo que vt1 v2 = 0. Por lo tanto, seleccionamos vectores ortonormales para diferentes
eigenvalores simplemente al normalizar todos estos eigenvectores ortogonales. Cuando
los eigenvalores no son distintos, habr√° subespacios de eigenvectores para cada uno de los
m√∫ltiples eigenvalores y con la ayuda del proceso de ortogonalizaci√≥n de Gram-Schmidt,
SRGHPRVHQFRQWUDUXQFRQMXQWRFRPSOHWRGHn eigenvectores ortonormales.
(OVLJXLHQWHFRURODULRSDUDHOWHRUHPDGHPXHVWUDDOJXQDVGHODVSURSLHGDGHVPiV
interesantes de las matrices sim√©tricas.
Corolario 9.17

Suponga que A es una matriz sim√©trica n 3 n. Entonces existen n eigenvectores de A que
IRUPDQXQFRQMXQWRRUWRQRUPDO\ORVHLJHQYDORUHVGHA son n√∫meros reales.
Demostraci√≥n

Si Q = (qi j ) y D = (di j )VRQODVPDWULFHVHVSHFL√ÄFDGDVHQHOWHRUHPD

entonces

D = Q t AQ = Q ‚àí1 AQ implica que

AQ = Q D.

Sea 1 ‚â§ i ‚â§ n y vi = (q1i , q2i , . . . , qni )t la i-√©sima columna de Q. Entonces

Avi = dii vi ,
y di i es un eigenvalor de A con eigenvector vi, la i-√©sima columna de Q. Las columnas de Q
son ortonormales, por lo que los eigenvectores de A son ortonormales.
Al multiplicar esta ecuaci√≥n a la izquierda por vit obtenemos

vit Avi = dii vit vi .

9.3 El m√©todo de potencia

431

Puesto que vit Avi y vit vi son n√∫meros reales y vit vi = 1, el eigenvalor dii = vit Avi es un
n√∫mero real, para cada i = 1, 2, . . . , n.
A veces, una matriz sim√©trica
cuyos eigenvalores son todos
n√∫meros reales no negativos
recibe el nombre de GH√ÄQLGDQR
negativa (o VHPLGH√ÄQLGD
positiva).

Teorema 9.18

5HFXHUGHGHODVHFFLyQTXHXQDPDWUL]VLPpWULFDAHVOODPDGDGH√ÄQLGDSRVLWLYDVL
para todos los vectores diferentes de cero, se tiene xt Ax > 0. El siguiente teorema caracteri]DDODVPDWULFHVGH√ÄQLGDVSRVLWLYDVHQWpUPLQRVGHHLJHQYDORUHV(VWDSURSLHGDGGHHLJHQYDORUKDFHTXHODVPDWULFHVGH√ÄQLGDVSRVLWLYDVVHDQLPSRUWDQWHVHQODVDSOLFDFLRQHV
Una matriz sim√©trica AHVGH√ÄQLGDSRVLWLYDVL\V√≥lo si todos los eigenvalores de A son positivos.
Demostraci√≥n Primero suponga que AHVGH√ÄQLGDSRVLWLYD\TXHŒª es un eigenvalor de A con
un eigenvector asociado x, con ||x||2 = 1. Entonces

0 < xt Ax = Œªxt x = Œª x 22 = Œª.
Para mostrar el rec√≠proco, suponga que A es sim√©trica con eigenvalores positivos. Por
HOFRURODULRA tiene n eigenvectores, v(1) , v(2) , . . . , v(n) , TXHIRUPDQXQFRQMXQWRRUWRQRUPDO\SRUHOWHRUHPDXQFRQMXQWROLQHDOPHQWHLQGHSHQGLHQWH3RUORWDQWRSDUD
cualquier x = 0, H[LVWHXQ~QLFRFRQMXQWRGHFRQVWDQWHVGLIHUHQWHVGHFHUR Œ≤1 , Œ≤2 , . . . , Œ≤n
para las que
n

Œ≤i v(i) .

x=
i=1

Al multiplicar por xt A obtenemos

xt Ax = xt

n

Œ≤i Av(i)

= xt

i=1

n

Œ≤i Œªi v(i)

n

n

=

i=1

Œ≤ j Œ≤i Œªi (v( j) )t v(i) .

j=1 i=1

Pero los vectores v(1) , v(2) , . . . , v(n)IRUPDQXQFRQMXQWRRUWRQRUPDOSRUORTXH

(v( j) )t v(i) =

0, si i = j,
1, si i = j.

(VWRMXQWRFRQHOKHFKRGHTXHŒªi son todas positivas, implica que
t

n

n

x Ax =

( j) t (i)

Œ≤ j Œ≤i Œªi (v ) v
j=1 i=1

n

=

Œªi Œ≤i2 > 0.
i=1

Por lo tanto, AHVGH√ÄQLGDSRVLWLYD
La secci√≥n Conjunto de ejercicios 9.2 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

9.3 El m√©todo de potencia
El m√©todo de potencia es una t√©cnica iterativa que se usa para determinar el eigenvalor
GRPLQDQWHGHXQDPDWUL] HVGHFLUHOHLJHQYDORUFRQODPD\RUPDJQLWXG $OPRGL√ÄFDUOLJHramente el m√©todo, tambi√©n se puede usar para determinar otros eigenvalores. Una caracter√≠stica √∫til del m√©todo de potencia es que no s√≥lo produce un eigenvalor, sino tambi√©n un
eigenvector asociado. De hecho, a menudo, el m√©todo de potencia se aplica para encontrar
un eigenvector para un eigenvalor que es determinado por algunos otros medios.

432

CAP√çTULO 9

Aproximaci√≥n de eigenvalores

El nombre de m√©todo de potencia
se deriva del hecho de que las
iteraciones exageran el tama√±o
relativo de las magnitudes de los
eigenvalores.

Para aplicar el m√©todo de potencia, suponemos que la matriz n 3 n A tiene n eigenvalores Œª1 , Œª2 , . . . , ŒªnFRQXQFRQMXQWRDVRFLDGRGHHLJHQYHFWRUHVOLQHDOPHQWHLQGHSHQGLHQWHV
{v(1) , v(2) , v(3) , . . . , v(n) }. Adem√°s, suponemos que A tiene exactamente un eigenvalor Œª1,
que es m√°s grande en magnitud, por lo que

|Œª1 | > |Œª2 | ‚â• |Œª3 | ‚â• ¬∑ ¬∑ ¬∑ ‚â• |Œªn | ‚â• 0.
(OHMHPSORGHODVHFFLyQLOXVWUDTXHXQDPDWUL]n 3 n no necesita tener n eigenvectores
linealmente independientes. Cuando esto no es as√≠, el m√©todo de potencia puede seguir siendo exitoso, pero no se garantiza que lo sea.
Si x es cualquier vector en Rn, el hecho de que {v(1) , v(2) , v(3) , . . . , v(n) } es linealmente
independiente implica que existen constantes Œ≤1 , Œ≤2 , . . . , Œ≤n con
n

Œ≤ j v( j) .

x=
j=1

Al multiplicar ambos lados de esta ecuaci√≥n por A, A2 , . . . , Ak , . . . obtenemos
n

Ax =

Œ≤ j Av( j) =

j=1

n

n

Œ≤ j Œª j v( j) ,

Œ≤ j Œª j Av( j) =

A2 x =

j=1

j=1

n

Œ≤ j Œª2j v( j) ,

j=1

y, en general, Ak x = nj=1 Œ≤ j Œªkj v( j) .
Si Œª1k se factoriza a partir de cada t√©rmino en el lado derecho de la √∫ltima ecuaci√≥n,
entonces
n

Ak x = Œªk1

Œªj
Œª1

Œ≤j
j=1

k

v( j) .

Puesto que |Œª1 | > |Œª j |, para todas j = 2, 3, . . . , n, tenemos l√≠m k‚Üí‚àû (Œª j /Œª1 )k = 0, y

l√≠m Ak x = l√≠m Œªk1 Œ≤1 v(1) .

k‚Üí‚àû



k‚Üí‚àû

/DVXFHVLyQHQODHFXDFLyQ  FRQYHUJHDVL|Œª1 | < 1 y diverge si |Œª1 | > 1, siempre
y cuando, por supuesto, Œ≤1 = 0. Por consiguiente, las entradas en A k x aumentar√°n con k si
|Œª1 | > 1 y tienden a 0 si |Œª1 | < 1 , tal vez, al resultar en desborde y subdesborde. Para cuidar
esta posibilidad, escalamos las potencias de A k x en una forma apropiada para garantizar
TXHODFRWDHQODHFXDFLyQ  HV√ÄQLWD\GLIHUHQWHGHFHUR(OHVFDODPLHQWRFRPLHQ]DDO
(0)
seleccionar x como vector unitario x(0) relativo a
‚àû y seleccionar un componente x p0 de
(0)
x con
x (0)
x(0) ‚àû .
p0 = 1
Sea y(1) = Ax(0)\GH√ÄQDŒº(1) = y (1)
p0 . Entonces
(1)

Œº

= y (1)
p0 =

y (1)
p0

x (0)
p0

=

Œ≤1 Œª1 v (1)
p0 +

Œ≤1 v (1)
p0 +

n
( j)
j=2 Œ≤ j Œª j v p0
= Œª1
( j)
n
j=2 Œ≤ j v p0

Sea p1 el entero m√≠nimo tal que

|y (1)
p1

y(1) ‚àû

\GH√ÄQDx(1) mediante

x(1) =

1
y (1)
p1

y(1) =

1
y (1)
p1

n
( j)
j=2 Œ≤ j (Œª j /Œª1 )v p0
( j)
n
Œ≤1 v (1)
p0 +
j=2 Œ≤ j v p0

Œ≤1 v (1)
p0 +

Ax(0) .

.

9.3 El m√©todo de potencia

433

Entonces

x (1)
p1 = 1

x(1) ‚àû .

$KRUDGH√ÄQD

y(2) = Ax(1) =

1
y (1)
p1

A2 x(0)

y

Œº

(2)

= y (2)
p1 =

= Œª1

y (2)
p1

x (1)
p1

=

Œ≤1 Œª21 v (1)
p1 +

n
2 ( j)
j=2 Œ≤ j Œª j v p1

y (1)
p1

Œ≤1 Œª1 v (1)
p1 +

( j)
n
j=2 Œ≤ j Œª j v p1

y (1)
p1

n
2 ( j)
j=2 Œ≤ j (Œª j /Œª1 ) v p1
( j)
n
j=2 Œ≤ j (Œª j /Œª1 )v p1

Œ≤1 v (1)
p1 +

Œ≤1 v (1)
p1 +

.

Sea p2 el entero m√°s peque√±o con

|y (2)
p2

y(2) ‚àû

\GH√ÄQD

x(2) =

1
y (2)
p2

y(2) =

1
y (2)
p2

Ax(1) =

1
(1)
y (2)
p2 y p1

A2 x(0) .

(m) ‚àû
}m=1 y una suce'HPDQHUDVLPLODUGH√ÄQDODVVXFHVLRQHVGHYHFWRUHV {x(m) }‚àû
m=0 y {y
(m) ‚àû
si√≥n de escalares {Œº }m=1 de manera inductiva mediante

y(m) = Ax(m‚àí1) ,
Œº(m) = y (m)
pm‚àí1 = Œª1

n
m
( j)
j=2 (Œª j /Œª1 ) Œ≤ j v pm‚àí1
n
m‚àí1 Œ≤ v ( j)
j pm‚àí1
j=2 (Œª j /Œª1 )

Œ≤1 v (1)
pm‚àí1 +

Œ≤1 v (1)
pm‚àí1 +



y
x(m) =

y(m)

=
(m)

y pm

Am x(0)
m

k=1

,

y (k)
pk

donde en cada paso, pm se usa para representar el entero m√°s peque√±o para el que

|y (m)
pm

y(m) ‚àû .

$O H[DPLQDU OD HFXDFLyQ   REVHUYDPRV TXH GDGR |Œª j /Œª1 | < 1, para cada
j = 2, 3, . . . , n, l√≠mm‚Üí‚àû Œº(m) = Œª1 , siempre y cuando x(0) se seleccione de tal forma que
Œ≤1 = 0. Adem√°s, la sucesi√≥n de vectores {x(m) }‚àû
m=0 converge para un eigenvector asociado
con Œª1 que tiene norma l‚àû igual a uno.
Ilustraci√≥n

La matriz

A=

‚àí2
6

‚àí3
7

434

CAP√çTULO 9

Aproximaci√≥n de eigenvalores

tiene eigenvalores Œª1 = 4 y Œª2 = 1 con los eigenvectores correspondientes v1 = (1, ‚àí2)t
y v2 = (1, ‚àí1)t . Si comenzamos con el vector arbitrario x0 = (1, 1)t y multiplicamos por la
matriz A, obtenemos

x1 = Ax0 =

‚àí5
13

x4 = Ax3 =

‚àí509
1021

,
,

x2 = Ax1 =

‚àí29
61

x5 = Ax4 =

‚àí2045
4093

,
, y

x3 = Ax2 =

‚àí125
253

x6 = Ax5 =

‚àí8189
16381

,
.

Como consecuencia, las aproximaciones para el eigenvalor dominante Œª1 5 4 son

61
= 4.6923,
13
4093
Œª(4)
= 4.00881, y
1 =
1021
Œª(1)
1 =

253
= 4.14754,
61
16381
Œª(5)
= 4.00200.
1 =
4093

Œª(2)
1 =

Un eigenvector aproximado correspondiente a Œª(5)
1 =

x6 =

‚àí8189
16381

Œª(3)
1 =

1021
= 4.03557,
253

16381
= 4.00200 es
4093

, que, dividido entre 2VHQRUPDOL]DD

1
‚àí2.00037

‚âà v1 .

(OPpWRGRGHSRWHQFLDWLHQHODGHVYHQWDMDGHTXHHVGHVFRQRFLGRDOSULQFLSLRVLODPDWUL]
tiene un solo eigenvalor dominante. Tampoco se conoce c√≥mo x(0) deber√≠a seleccionarse para
garantizar que su representaci√≥n en t√©rminos de eigenvectores de la matriz contendr√° una
contribuci√≥n diferente de cero de eigenvectores asociados con el eigenvalor dominante, si
existiera.
(ODOJRULWPRLPSOHPHQWDHOPpWRGRGHSRWHQFLD

ALGORITMO

9.1

M√©todo de potencia
Para aproximar el eigenvalor dominante y un eigenvector asociado de la matriz A n 3 n dado
un vector x diferente a cero:
ENTRADA
ciones N.

dimensi√≥n n; matriz A; vector x; tolerancia TOL; n√∫mero m√°ximo de itera-

SALIDA aproximar el eigenvalor Œº; aproximar el eigenvector x (con ||x||‚àû = 1) o un
PHQVDMHGHTXHHOQ~PHURPi[LPRGHLWHUDFLRQHVIXHH[FHGLGR

Paso 1 Determine k = 1.
Paso 2 Determine el entero m√°s peque√±o p con 1‚â§ p ‚â§ n y |x p | = ||x||‚àû .
Paso 3 Determine x = x/x p .
Paso 4 Mientras ( k ‚â§ N ) haga los pasos 5‚Äì11.
Paso 5 Determine y = Ax.
Paso 6 Determine Œº = y p .
Paso 7 Encuentre el entero m√°s peque√±o p con 1 ‚â§ p ‚â§ n y |y p

y ‚àû.

Paso 8 Si y p = 0 entonces SALIDA (‚ÄòEigenvector‚Äô, x);
SALIDA (‚ÄòA tiene el eigenvalor 0, seleccione un
nuevo vector x y reinicie‚Äô);
PARE.

9.3 El m√©todo de potencia

435

Paso 9 Determine ERR = ||x ‚àí (y/y p )||‚àû ;
x = y/y p .
Paso 10 Si ERR < TOL entonces SALIDA(Œº, x);
(El procedimiento fue exitoso.)
PARE.
Paso 11 Determine k = k + 1.
Paso 12 SALIDA (‚ÄòEl n√∫mero m√°ximo de iteraciones excedido‚Äô);
(El procedimiento no fue exitoso.)
PARE.

Convergencia acelerada
y(m) ‚àû garanti$OVHOHFFLRQDUHQHOSDVRHOHQWHURPiVSHTXHxRpm para el que |y (m)
pm
]DUiHQJHQHUDOTXHDO√ÄQDOHVWHtQGLFHVHYXHOYHLQYDULDQWH/DYHORFLGDGDODTXH{Œº(m) }‚àû
m=1
converge en Œª1 se determina mediante los radios |Œª j /Œª1 |m , para j = 2, 3, . . . , n, y en particular por medio |Œª2 /Œª1 |m . La velocidad de convergencia es O(|Œª2 /Œª1 |m ) (consulte [IK], p. 148),
por lo que existe una constante k, de tal forma que para m grande,
|Œº(m) ‚àí Œª1 | ‚âà k

Œª2 m
,
Œª1

lo cual implica que

|Œº(m+1) ‚àí Œª1 |
Œª2
< 1.
‚âà
(m)
m‚Üí‚àû |Œº
‚àí Œª1 |
Œª1
l√≠m

La sucesi√≥n {Œº(m) } converge linealmente a Œª1, por lo que el procedimiento 2 de Aitkens,
que se analiza en la secci√≥n 2.5, se puede utilizar para acelerar la convergencia. Al implementar el procedimiento 2HQHODOJRULWPRVHORJUDPRGL√ÄFDUHODOJRULWPRGHDFXHUGR
con lo siguiente:
Paso 1

Determine k = 1;
Œº0 = 0;
Œº1 = 0.

Paso 6 Determine Œº = y p ;
ŒºÃÇ = Œº0 ‚àí

(Œº1 ‚àí Œº0 )2
.
Œº ‚àí 2Œº1 + Œº0

Paso 10 Si ERR < TOL y k ‚â• 4 entonces SALIDA (ŒºÃÇ, x);
PARE.
Paso 11 Determine k = k + 1;
Œº0 = Œº1 ;
Œº1 = Œº.
En la actualidad, no es necesario que la matriz tenga diferentes eigenvalores para que el
PpWRGRGHSRWHQFLDFRQYHUMD6LODPDWUL]WLHQHXQHLJHQYDORUGRPLQDQWH~QLFRŒª1, con multiplicidad r superior a 1 y v(1) , v(2) , . . . , v(r ) son eigenvectores linealmente independientes
asociados con Œª1, el procedimiento seguir√° convergiendo en Œª1. En este caso, la sucesi√≥n de
vectores {x(m) }‚àû
m=0, converger√° en un eigenvector de Œª1 en la norma l‚àû igual a uno que depende de la selecci√≥n del vector inicial x(0) y es una combinaci√≥n lineal de v(1) , v(2) , . . . , v(r ).

436

CAP√çTULO 9

Aproximaci√≥n de eigenvalores

Ejemplo 1

Use el m√©todo de potencia para aproximar el eigenvalor dominante de la matriz
‚é°
‚é§
‚àí4 14 0
A = ‚é£ ‚àí5 13 0 ‚é¶
‚àí1
0 2
y, a continuaci√≥n, aplique el m√©todo 2 de Aitkens para las aproximaciones para el eigenvalor de la matriz para acelerar la convergencia.
Esta matriz tiene eigenvalores Œª1 = 6, Œª2 = 3, y Œª3 = 2, por lo que el m√©todo
GHSRWHQFLDGHVFULWRHQHODOJRULWPRFRQYHUJHUiQ6Lx(0) = (1, 1, 1)t , entonces

Soluci√≥n

y(1) = Ax(0) = (10, 8, 1)t ,
por lo que

||y(1) ||‚àû = 10,

Œº(1) = y1(1) = 10,

y

x(1) =

y(1)
= (1, 0.8, 0.1)t .
10

$OFRQWLQXDUGHHVWDIRUPDOOHJDPRVDORVYDORUHVHQODWDEODGRQGH ŒºÃÇ(m) representa la
sucesi√≥n generada por el procedimiento 2 de Aitkens. Una aproximaci√≥n para el eigenvalor

Tabla 9.1

m

(x(m) )t

Œº(m)

ŒºÃÇ(m)

0
1
2
3
4
5
6
7
8
9
10
11
12

(1, 1, 1)
(1, 0.8, 0.1)
(1, 0.75, ‚àí0.111)
(1, 0.730769, ‚àí0.188803)
(1, 0.722200, ‚àí0.220850)
(1, 0.718182, ‚àí0.235915)
(1, 0.716216, ‚àí0.243095)
(1, 0.715247, ‚àí0.246588)
(1, 0.714765, ‚àí0.248306)
(1, 0.714525, ‚àí0.249157)
(1, 0.714405, ‚àí0.249579)
(1, 0.714346, ‚àí0.249790)
(1, 0.714316, ‚àí0.249895)

10
7.2
6.5
6.230769
6.111000
6.054546
6.027027
6.013453
6.006711
6.003352
6.001675
6.000837

6.266667
6.062473
6.015054
6.004202
6.000855
6.000240
6.000058
6.000017
6.000003
6.000000

GRPLQDQWHHQHVWDHWDSDHV ŒºÃÇ(10) = 6.000000. El eigenvector unitario l‚àû - aproximado
SDUDHOHLJHQYDORUHV(x(12) )t = (1, 0.714316, ‚àí0.249895)t .
Aunque la aproximaci√≥n para el eigenvalor es correcta para los lugares enumerados, la
aproximaci√≥n del eigenvector es considerablemente menos precisa para el eigenvector verdadero (1, 5/7, ‚àí1/4)t ‚âà (1, 0.714286, ‚àí0.25)t .

Matrices sim√©tricas
Cuando A es sim√©trica, es posible hacer una variaci√≥n en la selecci√≥n de los vectores
x(m) y y(m) y los escalares Œº(m)SDUDPHMRUDUVLJQL√ÄFDWLYDPHQWHHOtQGLFHGHFRQYHUJHQFLDGH
la sucesi√≥n {Œº(m) }‚àû
m=1 para el eigenvalor dominante Œª1. De hecho, a pesar de que el √≠ndice
de convergencia del m√©todo de potencia general es O(|Œª2 /Œª1 |m ), el √≠ndice de convergencia

9.3 El m√©todo de potencia

437

GHOSURFHGLPLHQWRPRGL√ÄFDGRTXHVHGLRHQHODOJRULWPRSDUDODVPDWULFHVVLPpWULFDVHV
O(|Œª2 /Œª1 |2m ). &RQVXOWH>,.@SII 3XHVWRTXHODVXFHVLyQ{Œº(m) } sigue siendo convergente, tambi√©n puede aplicarse el procedimiento 2 de Aitkens.

ALGORITMO

9.2

M√©todo de potencia sim√©trica
Para aproximar el eigenvalor dominante y un eigenvector asociado de la matriz sim√©trica
n 3 n A, dado un vector diferente de cero x:
ENTRADA
nes N.

dimensi√≥n n; matriz A; vector x; tolerancia TOL; n√∫mero m√°ximo de iteracio-

SALIDA aproxime el eigenvalor Œº; aproxime el eigenvector x (con x 2 = 1) o un menVDMHGHTXHHOQ~PHURPi[LPRGHLWHUDFLRQHVIXHH[FHGLGR

Paso 1 Determine k = 1;
x = x/ x 2 .
Paso 2 Mientras ( k ‚â§ N ) haga los pasos 3‚Äì8.
Paso 3 Determine y = Ax.
Paso 4 Determine Œº = xt y.
Paso 5 Si y 2 = 0, entonces SALIDA (‚ÄòEigenvector‚Äô, x);
SALIDA (‚ÄòA tiene un eigenvalor de 0, seleccione
un nuevo vector x y reinicie‚Äô);
PARE.
Paso 6 Determine ERR = x ‚àí

y
y 2

;
2

x = y/ y 2 .
Paso 7 Si ERR < TOL entonces SALIDA (Œº, x);
(El procedimiento fue exitoso.)
PARE.
Paso 8 Determine k = k + 1.
Paso 9 SALIDA (‚ÄòN√∫mero m√°ximo de iteraciones excedido‚Äô);
(El procedimiento no fue exitoso.)
PARE.

Ejemplo 2

Aplique tanto el m√©todo de potencia como el de potencia sim√©trica a la matriz
‚é°
‚é§
4 ‚àí1
1
3 ‚àí2 ‚é¶ ,
A = ‚é£ ‚àí1
1 ‚àí2
3
usando el m√©todo

2

de Aitkens para acelerar la convergencia.

Soluci√≥n Esta matriz tiene eigenvalores Œª1 = 6, Œª2 = 3 y Œª3 = 1. Un eigenvector para el
HLJHQYDORUHV (1, ‚àí1, 1)t . La aplicaci√≥n del m√©todo de potencia a esta matriz con vector
inicial (1, 0, 0)tGDORVYDORUHVGHODWDEOD

438

CAP√çTULO 9

Aproximaci√≥n de eigenvalores

Tabla 9.2
m

(y(m) )t

Œº(m)

0
1
2
3
4
5
6
7
8
9
10

(4, ‚àí1, 1)
(4.5, ‚àí2.25, 2.25)
(5, ‚àí3.5, 3.5)
(5.4, ‚àí4.5, 4.5)
(5.666ÃÑ, ‚àí5.1666ÃÑ, 5.1666ÃÑ)
(5.823529, ‚àí5.558824, 5.558824)
(5.909091, ‚àí5.772727, 5.772727)
(5.953846, ‚àí5.884615, 5.884615)
(5.976744, ‚àí5.941861, 5.941861)
(5.988327, ‚àí5.970817, 5.970817)

4
4.5
5
5.4
5.666ÃÑ
5.823529
5.909091
5.953846
5.976744
5.988327

ŒºÃÇ(m)

(x(m) )t con x(m) ‚àû = 1

7
6.2
6.047617
6.011767
6.002931
6.000733
6.000184

(1, 0, 0)
(1, ‚àí0.25, 0.25)
(1, ‚àí0.5, 0.5)
(1, ‚àí0.7, 0.7)
(1, ‚àí0.8333ÃÑ, 0.8333ÃÑ)
(1, ‚àí0.911765, 0.911765)
(1, ‚àí0.954545, 0.954545)
(1, ‚àí0.976923, 0.976923)
(1, ‚àí0.988372, 0.988372)
(1, ‚àí0.994163, 0.994163)
(1, ‚àí0.997076, 0.997076)

Ahora aplicaremos el m√©todo de potencia sim√©trica a esta matriz con el mismo vector
inicial (1, 0, 0)t. Los primeros pasos son

x(0) = (1, 0, 0)t ,

Ax(0) = (4, ‚àí1, 1)t , M (1) = 4,

y

x(1) =

1
¬∑ Ax(0) = (0.942809, ‚àí0.235702, 0.235702)t .
||Ax(0) ||2

/DVHQWUDGDVUHVWDQWHVVHPXHVWUDQHQODWDEOD

Tabla 9.3
m

(y(m) )t

Œº(m)

ŒºÃÇ(m)

0
1
2
3
4
5
6
7
8
9
10

(1, 0, 0)
(4, ‚àí1, 1)
(4.242641, ‚àí2.121320, 2.121320
(4.082483, ‚àí2.857738, 2.857738)
(3.837613, ‚àí3.198011, 3.198011)
(3.666314, ‚àí3.342816, 3.342816)
(3.568871, ‚àí3.406650, 3.406650)
(3.517370, ‚àí3.436200, 3.436200)
(3.490952, ‚àí3.450359, 3.450359)
(3.477580, ‚àí3.457283, 3.457283)
(3.470854, ‚àí3.460706, 3.460706)

4
5
5.666667
5.909091
5.976744
5.994152
5.998536
5.999634
5.999908
5.999977

7
6.047619
6.002932
6.000183
6.000012
6.000000
6.000000

(x(m) )t con x(m) 2 = 1
(1, 0, 0)
(0.942809, ‚àí0.235702, 0.235702)
(0.816497, ‚àí0.408248, 0.408248)
(0.710669, ‚àí0.497468, 0.497468)
(0.646997, ‚àí0.539164, 0.539164)
(0.612836, ‚àí0.558763, 0.558763)
(0.595247, ‚àí0.568190, 0.568190)
(0.586336, ‚àí0.572805, 0.572805)
(0.581852, ‚àí0.575086, 0.575086)
(0.579603, ‚àí0.576220, 0.576220)
(0.578477, ‚àí0.576786, 0.576786)

El m√©todo de potencia sim√©trica da una convergencia considerablemente m√°s r√°pida
para esta matriz que el m√©todo de potencia. Las aproximaciones del eigenvector en el m√©todo de potencia converge en (1, ‚àí1, 1)t , un vector con norma ‚àö
unitaria en
‚àö l‚àû . En
‚àö el m√©todo
de potencia sim√©trica, la convergencia es el vector paralelo ( 3/3, ‚àí 3/3, 3/3)t , que
tiene la norma unitaria en l2.
Si Œª es un n√∫mero real que aproxima un eigenvalor de una matriz sim√©trica A y x es un
eigenvector asociado aproximado, entonces Ax ‚àí Œªx es aproximadamente el vector cero. El
siguiente teorema relaciona la norma de este vector para la precisi√≥n del eigenvalor Œª.
Teorema 9.19

Suponga que A es una matriz sim√©trica n 3 n con eigenvalores Œª1 , Œª2 , . . . , Œªn . Si Ax‚àíŒªx 2 < Œµ
para algunos n√∫meros reales Œª y vector x con x 2 = 1. Entonces

m√≠n |Œª j ‚àí Œª| < Œµ.

1‚â§ j‚â§n

9.3 El m√©todo de potencia

439

Suponga que v(1) , v(2) , . . . , v(n) IRUPDQ XQ FRQMXQWR RUWRQRUPDO GH HLJHQvectores asociados de A, respectivamente, con los eigenvalores Œª1 , Œª2 , . . . , Œªn . MedianWH ORV WHRUHPDV  \  x VH SXHGH H[SUHVDU SDUD DOJ~Q FRQMXQWR ~QLFR GH FRQVWDQWHV
Œ≤1 , Œ≤2 , . . . , Œ≤n , como
Demostraci√≥n

n

x=

Œ≤ j v( j) .

j=1

Por lo tanto,
2

n

Ax ‚àí Œªx

2
2 =

Œ≤ j (Œª j ‚àí Œª)v

( j)

j=1

n

n

=

|Œ≤ j |2 |Œª j ‚àí Œª|2 ‚â• m√≠n |Œª j ‚àí Œª|2
1‚â§ j‚â§n

j=1

2

|Œ≤ j |2 .
j=1

Pero
n

|Œ≤ j |2

x 22 = 1,

por lo que Œµ

Ax ‚àí Œªx 2 > m√≠n |Œª j ‚àí Œª|.

j=1

1‚â§ j‚â§n

M√©todo de potencia inversa
El m√©todo de potencia inversa HV XQD PRGL√ÄFDFLyQ GHO PpWRGR GH SRWHQFLD TXH GD XQD
convergencia m√°s r√°pida. Se usa para determinar el eigenvalor de A que est√° m√°s cerca de
XQQ~PHURHVSHFt√ÄFRq.
Suponga que la matriz A tiene eigenvalores Œª1 , . . . , Œªn con eigenvectores linealmente independientes v(1) , . . . , v(n) . Los eigenvalores de (A ‚àí q I )‚àí1 , donde q = Œªi , para
i = 1, 2, . . . , n, son

1
,
Œª1 ‚àí q

1
1
, . . .,
,
Œª2 ‚àí q
Œªn ‚àí q

con estos mismos eigenvectores v(1) , v(2) , . . . , v(n) &RQVXOWHHOHMHUFLFLRGHODVHFFLyQ

Al aplicar el m√©todo de potencia a (A ‚àí q I )‚àí1 da

y(m) = (A ‚àí q I )‚àí1 x(m‚àí1) ,

Œº(m) = y (m)
pm‚àí1 =

y (m)
pm‚àí1

x (m‚àí1)
pm‚àí1

=

1
v ( j)
(Œª j ‚àí q)m pm‚àí1
,
1
( j)
n
Œ≤
v
p
j=1 j
(Œª j ‚àí q)m‚àí1 m‚àí1
n
j=1 Œ≤ j



y

x(m) =

y(m)
y (m)
pm

,

(m)
||‚àû . La
donde, en cada paso, pm representa el entero m√°s peque√±o para el que |y (m)
pm | = ||y
(m)
sucesi√≥n {Œº `HQODHFXDFLyQ  FRQYHUJHHQ1/(Œªk ‚àí q), donde

1
1
= m√°x
|Œªk ‚àí q| 1‚â§i‚â§n |Œªi ‚àí q|
y Œªk ‚âà q + 1/Œº(m) es el eigenvalor de A m√°s cercano a q.

440

CAP√çTULO 9

Aproximaci√≥n de eigenvalores

Conociendo kODHFXDFLyQ  VHSXHGHHVFULELUFRPR
‚é§
‚é°
m
n
Œªk ‚àíq
( j)
Œ≤k v (k)
+
Œ≤
v
j=1 j Œª j ‚àíq
pm‚àí1
pm‚àí1 ‚é•
1 ‚é¢
j=k
‚é•.
‚é¢
Œº(m) =
m‚àí1
Œªk ‚àí q ‚é£
( j) ‚é¶
(k)
n
Œªk ‚àíq
Œ≤k v pm‚àí1 + j=1 Œ≤ j Œª j ‚àíq
v pm‚àí1



j=k

Por lo tanto, la selecci√≥n de q determina la convergencia, siempre y cuando 1/(Œªk ‚àí q)
sea un √∫nico eigenvalor dominante de (A ‚àí q I )‚àí1 (a pesar de que puede ser un eigenvalor
m√∫ltiple). Mientras q est√° m√°s cerca de un eigenvalor Œªk, m√°s r√°pida ser√° la convergencia ya
que la convergencia es de orden

O

(Œª ‚àí q)‚àí1
(Œªk ‚àí q)‚àí1

m

=O

(Œªk ‚àí q) m
,
(Œª ‚àí q)

donde Œª representa el eigenvalor de A que es el segundo m√°s cercano a q.
El vector y(m) se obtiene al resolver el sistema lineal

(A ‚àí q I )y(m) = x(m‚àí1) .
En general, se usa la eliminaci√≥n gaussiana con pivoteo pero, como en el caso de la factorizaci√≥n LU, los multiplicadores se pueden guardar para reducir el c√°lculo. La selecci√≥n de q
puede basarse en el teorema del c√≠rculo de Ger≈°gorin o en otros medios de localizaci√≥n de
un eigenvalor.
(ODOJRULWPRFDOFXODq a partir de una aproximaci√≥n inicial para el eigenvalor x(0)
mediante

q=

x(0)t Ax(0)
.
x(0)t x(0)

Esta selecci√≥n de q resulta de la observaci√≥n de que si x es un eigenvector de A respecto al
eigenvalor Œª, entonces Ax = Œªx. Por lo que, xt Ax = Œªxt x y

Œª=

xt Ax
xt Ax
.
=
xt x
x 22

Si q est√° cerca de un eigenvalor, la convergencia ser√≠a bastante r√°pida, pero se deber√≠a usar
XQDWpFQLFDGHSLYRWHRHQHOSDVRSDUDHYLWDUODFRQWDPLQDFLyQSRUHUURUGHUHGRQGHR
$PHQXGRVHXVDHODOJRULWPRSDUDDSUR[LPDUXQHLJHQYDORUFXDQGRVHFRQRFHXQ
eigenvalor q aproximado.

ALGORITMO

9.3

M√©todo de potencia inversa
Para aproximar un eigenvalor y un eigenvector asociado de la matriz A n 3 n, dado un vector
x diferente de cero:
ENTRADA dimensi√≥n n; matriz A; vector x; tolerancia TOL; n√∫mero m√°ximo de iteraciones N.
SALIDA aproxima el eigenvalor Œº; aproxima el eigenvector x (con x ‚àû = 1) o un
PHQVDMHGHTXHHOQ~PHURPi[LPRGHLWHUDFLRQHVIXHH[FHGLGR

xt Ax
.
xt x
Paso 2 Determine k = 1.
Paso 1 Determine q =

9.3 El m√©todo de potencia

Paso 3 Encuentre el entero m√°s peque√±o p con 1 ‚â§ p ‚â§ n y |x p

441

x ‚àû.

Paso 4 Determine x = x/x p .
Paso 5 Mientras (k ‚â§ N ) haga los pasos 6‚Äì12.
Paso 6 Resuelva el sistema lineal (A ‚àí q I )y = x.
Paso 7 Si el sistema no tiene una soluci√≥n √∫nica, entonces
SALIDA (‚Äòq es un eigenvalor‚Äô, q);
PARE.
Paso 8 Determine Œº = y p .
Paso 9 Encuentre el entero m√°s peque√±o p con 1‚â§ p ‚â§ n y |y p

y ‚àû.

Paso 10 Determine ERR = x ‚àí (y/y p ) ‚àû ;
x = y/y p .
Paso 11 Si ERR < TOL entonces determine Œº = (1/Œº) + q;
SALIDA (Œº, x);
(El procedimiento fue exitoso .)
PARE.
Paso 12 Determine k = k + 1.
Paso 13 SALIDA (‚ÄòN√∫mero m√°ximo de iteraciones excedido‚Äô);
(El procedimiento no fue exitoso.)
PARE.

La convergencia del m√©todo de potencia inversa es lineal, por lo que, de nuevo, el m√©todo 2GH$LWNHQVSXHGHXVDUVHSDUDDFHOHUDUODFRQYHUJHQFLD(OVLJXLHQWHHMHPSORLOXVWUDOD
r√°pida convergencia del m√©todo de potencia inversa si q est√° cerca de un eigenvalor.
Ejemplo 3

Aplique el m√©todo de potencia inversa con x(0) = (1, 1, 1)t a la matriz
‚é°
‚é§
‚àí4 14 0
x(0)t Ax(0)
19
A = ‚é£ ‚àí5 13 0 ‚é¶ con q = (0)t (0) =
x
x
3
‚àí1
0 2
y use el m√©todo

2

de Aitkens para acelerar la convergencia.

Soluci√≥n (O PpWRGR GH SRWHQFLD VH DSOLFy D HVWD PDWUL] HQ HO HMHPSOR  XVDQGR HO YHF-

tor inicial x(0) = (1, 1, 1)t . √âste nos dio el eigenvalor Œº(12) = 6.000837 y el eigenvector
(x(12) )t = (1, 0.714316, ‚àí0.249895)t .
Para el m√©todo de potencia inversa, consideramos
‚é§
‚é° 31
0
‚àí 3 14
A ‚àí q I = ‚é£ ‚àí5 20
0 ‚é¶.
3
‚àí1
0 ‚àí 13
3
Con x(0) = (1, 1, 1)t , el m√©todo encuentra primero y(1) al resolver (A ‚àí q I )y(1) = x(0) . Esto
da

y(1) =

‚àí

33
24 84
,‚àí ,
5
5 65

t

= (‚àí6.6, ‚àí4.8, 1.292307692)t .

442

CAP√çTULO 9

Aproximaci√≥n de eigenvalores

Por lo que

||y(1) ||‚àû = 6.6,

x(1) =

1 (1)
y = (1, 0.7272727, ‚àí0.1958042)t ,
‚àí6.6

y

Œº(1) = ‚àí

1
19
+
= 6.1818182.
6.6
3

/RVUHVXOWDGRVVXEVLJXLHQWHVVHLQFOX\HQHQODWDEOD\ODFROXPQDGHUHFKDOLVWDORVUHVXOtados del m√©todo 2 de Aitkens aplicado a Œº(m) . Estos son resultados claramente superiores
a los obtenidos con el m√©todo de potencia.

Tabla 9.4

m

x(m)t

Œº(m)

ŒºÃÇ(m)

0
1
2
3
4
5
6

(1, 1, 1)
(1, 0.7272727, ‚àí0.1958042)
(1, 0.7155172, ‚àí0.2450520)
(1, 0.7144082, ‚àí0.2495224)
(1, 0.7142980, ‚àí0.2499534)
(1, 0.7142869, ‚àí0.2499954)
(1, 0.7142858, ‚àí0.2499996)

6.1818182
6.0172414
6.0017153
6.0001714
6.0000171
6.0000017

6.000098
6.000001
6.000000
6.000000

Si A es sim√©trica, entonces, para cualquier n√∫mero real q, la matriz (A ‚àí q I )‚àí1 tambi√©n
HVVLPpWULFDSRUORTXHHOPpWRGRGHSRWHQFLDVLPpWULFDDOJRULWPRVHSXHGHDSOLFDUD
(A ‚àí q I )‚àí1 para acelerar la convergencia en

O

Œªk ‚àí q 2m
Œª‚àíq

.

M√©todos de deÔ¨Çaci√≥n
Existen numerosas t√©cnicas para obtener aproximaciones para los otros eigenvalores de una
matriz, una vez que se ha calculado una aproximaci√≥n al eigenvalor dominante. Restringiremos nuestra presentaci√≥n a las WpFQLFDVGHGH√ÅDFLyQ.
/DVWpFQLFDVGHGH√ÅDFLyQLPSOLFDQODIRUPDFLyQGHXQDQXHYDPDWUL]B, cuyos eigenvalores sean iguales a los de A, excepto que el eigenvalor dominante de A es reemplazado en
BSRUHOHLJHQYDORU(OVLJXLHQWHUHVXOWDGRMXVWL√ÄFDHOSURFHGLPLHQWR/DGHPRVWUDFLyQGH
HVWHWHRUHPDVHSXHGHHQFRQWUDUHQ>:LO@S
Teorema 9.20

Suponga que Œª1 , Œª2 , . . . , Œªn son eigenvalores de A con eigenvectores asociados v(1) , v(2) ,. . .
v(n) y Œª1 tiene multiplicidad 1. Sea x un vector con xt v(1) = 1. Entonces la matriz

B = A ‚àí Œª1 v(1) xt
tiene eigenvalores 0 , Œª2 , Œª3 , . . . , Œªn con eigenvectores asociados v(1) , w(2) , w(3) , . . . , w(n) ,
donde v(i) y w(i) est√°n relacionados por medio de la ecuaci√≥n

v(i) = (Œªi ‚àí Œª1 )w(i) + Œª1 (xt w(i) )v(1) ,
para cada i = 2, 3, . . . , n.



9.3 El m√©todo de potencia

443

Existen muchas selecciones para el vector x TXH SRGUtDQ XVDUVH HQ HO WHRUHPD 
La GH√ÅDFLyQGH:LHODQGWLQLFLDFRQODGH√ÄQLFLyQ

x=

1
Œª1 vi(1)

(ai1 , ai2 , . . . , ain )t ,



(1)

+HOPXW:LHODQGW ¬≤ 
WUDEDMyRULJLQDOPHQWHHQ
grupos de permutaci√≥n, pero
durante la Segunda Guerra
Mundial se comprometi√≥ con la
investigaci√≥n en meteorolog√≠a,
criptolog√≠a y aerodin√°mica.
Esto implicaba problemas
de vibraci√≥n que requer√≠an
el c√°lculo de eigenvalores
asociados con ecuaciones y
matrices diferenciales.

donde vi es una coordenada diferente de cero del eigenvector v(1) y los valores ai1 , ai2 , . . . , ain
son las entradas en la i-√©sima√ÄODGHA.
&RQHVWDGH√ÄQLFLyQ

xt v(1) =

1

[a , a , . . . , ain ](v1(1) , v2(1) , . . . , vn(1) )t =
(1) i1 i2

Œª1 vi

1

n

Œª1 vi(1) j=1

ai j v (1)
j ,

donde la suma es la i-√©sima coordenada del producto Av(1) . Puesto que Av(1) = Œª1 v(1) ,
tenemos
n

(1)
ai j v (1)
j = Œª1 vi ,

j=1

lo cual implica que

xt v(1) =

1
Œª1 vi(1)

(Œª1 vi(1) ) = 1.

Por lo tanto, x VDWLVIDFH OD KLSyWHVLV GHO WHRUHPD $GHPiV FRQVXOWH HO HMHUFLFLR  
la i-√©sima√ÄODGHB = A ‚àí Œª1 v(1) xt contiene completamente entradas cero.
Si Œª = 0 es un eigenvalor con un eigenvector asociado w, la relaci√≥n Bw = Œªw implica
que la i-√©sima coordenada de w tambi√©n debe ser cero. Por consiguiente, la i-√©sima columna
de la matriz B no contribuye al producto Bw = Œªw. Por lo tanto, la matriz B se puede reemplazar con una matriz B9 (n ‚àí 1) √ó (n ‚àí 1) obtenida al eliminar la i-√©sima√ÄOD\ODi-√©sima
columna de B. La matriz B9 tiene eigenvalores Œª2 , Œª3 , . . . , Œªn .
Si |Œª2 | > |Œª3 |, el m√©todo de potencia se aplica nuevamente a la matriz B9 para determinar este eigenvalor dominante y un eigenvector w(2) , asociado con Œª2, respecto a la matriz
B9. Para encontrar el eigenvector asociado w(2) para la matriz B, inserte una coordinada cero
(2)
entre las coordenadas w i‚àí1
y w i(2) del vector (n 2 1) dimensional w(2) y, despu√©s, calcule
(2)
v FRQODHFXDFLyQ  
Ejemplo 4

La matriz

‚é°

4
A = ‚é£ ‚àí1
1

‚àí1
3
‚àí2

‚é§
1
‚àí2 ‚é¶
3

tiene el eigenvalor dominante Œª1 5FRQHLJHQYHFWRUXQLWDULRDVRFLDGRv(1) = (1, ‚àí1, 1)t .
6XSRQJDTXHFRQRFHPRVHVWHHLJHQYDORUGRPLQDQWH\DSOLTXHODGH√ÅDFLyQSDUDDSUR[LPDU
los otros eigenvalores y eigenvectores.
Soluci√≥n El procedimiento para obtener un segundo eigenvalor Œª2 procede de acuerdo con
lo siguiente:
‚é°
‚é§
4
1‚é£
1 1 t
2
‚àí1 ‚é¶ =
x=
,
,‚àí ,
6
3
6 6
1

‚é§
1
v(1) xt = ‚é£ ‚àí1 ‚é¶
1

‚é°

‚é°

2
,
3

‚àí 16 ,

1
6

‚é¢
=‚é£

2
3
‚àí 23
2
3

‚àí 16
1
6
1
‚àí6

1
6
‚àí 16
1
6

‚é§

‚é•
‚é¶,

444

CAP√çTULO 9

Aproximaci√≥n de eigenvalores

y
‚é°

4
B = A ‚àí Œª1 v(1) xt = ‚é£ ‚àí1
1

‚àí1
3
‚àí2

‚é° 2
‚é§
1
3
‚é¢
‚àí2 ‚é¶ ‚àí 6 ‚é£‚àí 23
2
3
3

‚àí 16
1
6
‚àí 16

1
6
‚àí 16
1
6

‚é§

‚é°

0
‚é• ‚é£
3
‚é¶=
‚àí3

0
2
‚àí1

‚é§
0
‚àí1 ‚é¶.
2

$OHOLPLQDUODSULPHUD√ÄOD\ODSULPHUDFROXPQDREWHQHPRV

‚àí1
2

2
‚àí1

B =

,

que tiene eigenvalores Œª2 = 3 y Œª3 = 1. Para Œª2 5HOHLJHQYHFWRUw(2) se puede obtener
al resolver el sistema lineal

(B ‚àí 3I )w(2) = 0,

que resulta en w(2) = (1, ‚àí1)t .

Al agregar un cero al primer componente obtenemos w(2) = (0, 1, ‚àí1)t , y, a partir de la
HFXDFLyQ  WHQHPRVHOHLJHQYHFWRUv(2) de A correspondiente a x2 5

v(2) = (Œª2 ‚àí Œª1 )w(2) + Œª1 (xt w(2) )v(1)
= (3 ‚àí 6)(0, 1, ‚àí1)t + 6

1 1
2
,‚àí ,
3
6 6

(0, 1, ‚àí1)t (1, ‚àí1, 1)t = (‚àí2, ‚àí1, 1)t .

$XQTXHHVWHSURFHVRGHGH√ÅDFLyQVHSXHGHXVDUSDUDHQFRQWUDUODVDSUR[LPDFLRQHVSDUD
todos los eigenvalores y eigenvectores de una matriz, el proceso es susceptible al error de
UHGRQGHR'HVSXpVGHXVDUODGH√ÅDFLyQSDUDDSUR[LPDUHOHLJHQYDORUGHXQDPDWUL]ODDSURximaci√≥n deber√≠a utilizarse como valor inicial para el m√©todo de potencia inversa aplicado a
la matriz original. Esto garantizar√° la convergencia para un eigenvalor de la matriz original,
no la de las matrices reducidas, que probablemente contiene errores. Cuando se requieren
todos los eigenvalores de una matriz, deber√≠an usarse las t√©cnicas consideradas en la secci√≥n
FRQEDVHHQWUDQVIRUPDFLRQHVGHVLPLOLWXG
&HUUDPRVHVWDVHFFLyQFRQHODOJRULWPRTXHFDOFXODHOVHJXQGRHLJHQYDORUGRPLQDQte y el eigenvector asociado para una matriz, una vez que se ha determinado el eigenvalor
dominante y el eigenvector asociado.

ALGORITMO

9.4

DeÔ¨Çaci√≥n de Wielandt
Para aproximar el segundo eigenvalor m√°s dominante y un eigenvector asociado de la matriz
A n 3 n dada una aproximaci√≥n Œª para el eigenvalor dominante, una aproximaci√≥n v para un
eigenvector asociado y un vector x ‚àà Rn‚àí1 :
ENTRADA dimensi√≥n n; matriz A; eigenvalor Œª aproximado con eigenvector v ‚àà Rn ;
vector x ‚àà Rn‚àí1; tolerancia TOL; n√∫mero m√°ximo de iteraciones N.
SALIDA eigenvalor Œº aproximado; eigenvector aproximado u R XQ PHQVDMH GH TXH HO
m√©todo falla.

Paso 1 Sea i el entero m√°s peque√±o con 1 ‚â§ i ‚â§ n y |vi | = m√°x1‚â§ j‚â§n |v j |.
Paso 2 Si i = 1 entonces
para k = 1, . . . , i ‚àí 1
para j = 1, . . . , i ‚àí 1
determine bk j = ak j ‚àí

vk
ai j .
vi

9.4

M√©todo de Householder

445

Paso 3 Si i = 1 y i = n entonces
para k = i, . . . , n ‚àí 1
para j = 1, . . . , i ‚àí 1
vk+1
ai j ;
vi
vj
b jk = a j,k+1 ‚àí ai,k+1 .
vi

determine bk j = ak+1, j ‚àí

Paso 4 Si i = n entonces
para k = i, . . . , n ‚àí 1
para j = i, . . . , n ‚àí 1
determine bk j = ak+1, j+1 ‚àí

vk+1
ai, j+1 .
vi

Paso 5 Realice el m√©todo de potencia en la matriz (n ‚àí 1) √ó (n ‚àí 1) B = (bk j ) con x
como aproximaci√≥n inicial.
Paso 6 Si el m√©todo falla, entonces SALIDA (‚ÄòEl m√©todo falla‚Äô);
PARE.
si Œº es el eigenvalor aproximado
w = (w 1 , . . . , w n‚àí1 )t el eigenvector aproximado.
Paso 7 Si i = 1 entonces para k = 1, . . . , i ‚àí 1 determine w k = w k .
Paso 8 Determine w i = 0.
Paso 9 Si i = n entonces para k = i + 1, . . . , n determine w k = w k‚àí1 .
Paso 10 Parak = 1, . . . , n

‚éõ

determine u k = (Œº ‚àí Œª)w k + ‚éù

n
j=1

‚éû
ai j w j ‚é†

vk
.
vi

(Calcule el eigenvector con la ecuaci√≥n (9.6).)
Paso 11 SALIDA (Œº, u); (El procedimiento fue exitoso .)
PARE.
La secci√≥n Conjunto de ejercicios 9.3 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

9.4 M√©todo de Householder

$OVWRQ+RXVHKROGHU ¬≤ 
realiz√≥ investigaciones en
biolog√≠a matem√°tica antes de ser
GLUHFWRUGHO/DERUDWRULR1DFLRQDO
de Oak Ridge en Tennessee en
&RPHQ]yDWUDEDMDUHQOD
soluci√≥n de sistemas lineales en
ODGpFDGDGHFXDQGRVH
desarrollaron estos m√©todos.

(QODVHFFLyQXVDUHPRVHOPpWRGR45SDUDUHGXFLUXQDPDWUL]WULGLDJRQDOVLPpWULFDHQXQD
matriz similar que es casi diagonal. Las entradas diagonales de la matriz reducida son aproximaciones para los eigenvalores de la matriz dada. En esta secci√≥n presentamos un m√©todo
concebido por Alston Householder para reducir una matriz sim√©trica arbitraria en una matriz
tridiagonal similar. A pesar de que existe una conexi√≥n entre los problemas que estamos resolviendo en estas dos secciones, el m√©todo de Householder tiene una aplicaci√≥n tan amplia en
√°reas diferentes a la aproximaci√≥n de eigenvalores que merece un trato especial.
El m√©todo de Householder se usa para encontrar una matriz tridiagonal sim√©trica B
que es similar a una matriz sim√©trica A GHWHUPLQDGD (O WHRUHPD  LPSOLFD TXH A es
similar a la matriz diagonal D, ya que existe una matriz ortogonal Q con la propiedad de
que D = Q ‚àí1 AQ = Q t AQ. Puesto que, en general, la matriz Q (y por consiguiente, D)
es dif√≠cil de calcular, el m√©todo de Householder ofrece un compromiso. Despu√©s de haber
LPSOHPHQWDGRHOPpWRGRGH+RXVHKROGHUHVSRVLEOHXVDUPpWRGRVH√ÄFLHQWHVFRPRHODOJRULWPR45SDUDDSUR[LPDUFRQH[DFWLWXGORVHLJHQYDORUHVGHODPDWUL]WULGLDJRQDOVLPpWULFD
resultante.

446

CAP√çTULO 9

Aproximaci√≥n de eigenvalores

Transformaciones de Householder
DeÔ¨Ånici√≥n 9.21

Sea w ‚àà Rn con wt w = 1. Entonces la matriz n 3 n

P = I ‚àí 2wwt
recibe el nombre de transformaci√≥n de Householder.
Las transformaciones de Householder se usan para los bloques externos de entradas
cero en vectores o columnas de matrices de manera en extremo estable respecto al error
GHUHGRQGHR &RQVXOWH>:LO@SS¬≤SDUDPD\RUDQiOLVLV /DVSURSLHGDGHVGHODV
transformaciones se dan en el siguiente teorema.
Teorema 9.22

Si una transformaci√≥n de Householder, P 5 I 2 2ww t, es sim√©trica y ortogonal, entonces
P ‚àí1 = P.
Demostraci√≥n

(wwt )t = (wt )t wt = wwt
y de

P t = (I ‚àí 2wwt )t = I ‚àí 2wwt = P.
Adem√°s, wt w = 1, por lo que

P P t = (I ‚àí 2wwt )(I ‚àí 2wwt ) = I ‚àí 2wwt ‚àí 2wwt + 4wwt wwt
= I ‚àí 4wwt + 4wwt = I,
y P ‚àí1 = P t = P.
El m√©todo de Householder comienza determinando una transformaci√≥n P(1) tal que
A(2) = P (1) A P (1) tiene entradas ceros fuera de la primera columna de A, comenzando con la
WHUFHUD√ÄODHVGHFLU

a (2)
j1 = 0,

para cada j = 3, 4, . . . , n.



Por simetr√≠a, tambi√©n tenemos a1(2)j = 0.
Ahora seleccionamos un vector w = (w 1 , w 2 , . . . , w n )t de tal forma que wt w = 1, la
HFXDFLyQ  VHPDQWLHQH\HQODPDWUL]

A(2) = P (1) A P (1) = (I ‚àí 2wwt )A(I ‚àí 2wwt ),
(2)
= a11 y a (2)
tenemos a11
j1 = 0, para cada j = 3, 4, . . . , n. Esta selecci√≥n impone n condiciones en los n valores desconocidos w 1 , w 2 , . . . , w n .
(2)
= a114XHUHPRV
Al establecer w1 5 0 garantizamos que a11

P (1) = I ‚àí 2wwt
para satisfacer

P (1) (a11 , a21 , a31 , . . . , an1 )t = (a11 , Œ±, 0, . . . , 0)t ,
donde Œ±VHVHOHFFLRQDUiPiVDGHODQWH3DUDVLPSOL√ÄFDUODQRWDFLyQVL

wÃÇ = (w 2 , w 3 , . . . , w n )t ‚àà Rn‚àí1 ,

yÃÇ = (a21 , a31 , . . . , an1 )t ‚àà Rn‚àí1 ,

y PÃÇ es una transformaci√≥n de Householder (n 2 1) 3 (n 2 1)

PÃÇ = In‚àí1 ‚àí 2wÃÇwÃÇt .



9.4

M√©todo de Householder

447

(QWRQFHVODHFXDFLyQ  VHFRQYLHUWHHQ
‚é°
‚é§ ‚é°
‚é§
.
‚é§
‚é°
1 .. 0 . . . . . . 0
a11
a11
‚é°
‚é§ ‚é°
‚é§
.
‚é¢ a21 ‚é• ‚é¢. . . . . .. . . . . . . . . . . . . . .‚é•
a11
a11
‚é¢ ---‚é¢
‚é• ‚é¢
‚é•
Œ± ‚é•
‚é•
‚é¢ 0. ...
‚é• ‚é¢ ---- ‚é• ‚é¢ ---- ‚é• ‚é¢
(1) ‚é¢ a31 ‚é•
‚é•
P ‚é¢ . ‚é•=‚é¢ . .
‚é• ¬∑ ‚é£ yÃÇ ‚é¶ = ‚é£ PÃÇ yÃÇ ‚é¶ = ‚é¢
0
‚é¢
.. ‚é•
‚é¢ .. ‚é• ‚é¢ .. ..
‚é•
‚é£
‚é£ . ‚é¶ ‚é£ . .
‚é¶
.. ‚é¶
PÃÇ
.
. ..
0
an1
0 .
con

PÃÇ yÃÇ = (In‚àí1 ‚àí 2wÃÇwÃÇt )yÃÇ = yÃÇ ‚àí 2(wÃÇt yÃÇ)wÃÇ = (Œ±, 0, . . . , 0)t .



Sea r = wÃÇt yÃÇ. Entonces

(Œ±, 0, . . . , 0)t = (a21 ‚àí 2r w 2 , a31 ‚àí 2r w 3 , . . . , an1 ‚àí 2r w n )t ,
y podemos determinar todas las wi una vez que conocemos Œ± y r. Al equiparar los componentes da

Œ± = a21 ‚àí 2r w 2
y

0 = a j1 ‚àí 2r w j , para cada j = 3, . . . , n.
Por lo tanto,

2r w 2 = a21 ‚àí Œ±



2r w j = a j1 , para cada j = 3, . . . , n.



y

Al elevar al cuadrado ambos lados de cada una de las ecuaciones y sumar los t√©rminos
correspondientes da
n

n

w 2j = (a21 ‚àí Œ±)2 +

4r 2
j=2

a 2j1 .
j=3

n
2
j=2 w j = 1,

Puesto que wt w = 1 y w 1 = 0, tenemos
n

4r 2 =

a 2j1 ‚àí 2Œ±a21 + Œ± 2 .
j=2

/DHFXDFLyQ  \HOKHFKRGHTXHP es ortogonal implica que

Œ± 2 = (Œ±, 0, . . . , 0)(Œ±, 0, . . . , 0)t = ( PÃÇ yÃÇ)t PÃÇ yÃÇ = yÃÇt PÃÇ t PÃÇ yÃÇ = yÃÇt yÃÇ.
Por lo tanto,
n

Œ±2 =

a 2j1 ,
j=2

ORTXHDOVXVWLWXLUHQODHFXDFLyQ  GD
n

2r 2 =

a 2j1 ‚àí Œ±a21 .
j=2



448

CAP√çTULO 9

Aproximaci√≥n de eigenvalores

Para garantizar 2 r 2 = 0 si y s√≥lo si a21 = a31 = ¬∑ ¬∑ ¬∑ = an1 = 0, seleccionamos

‚éõ

n

Œ± = ‚àísgn(a21 ) ‚éù

‚éû1/2
a 2j1 ‚é†

,

j=2

lo cual implica que
n

2r 2 =

‚éõ
a 2j1 + |a21 | ‚éù

j=2

‚éû1/2

n

a 2j1 ‚é†

.

j=2

Con esta selecci√≥n de Œ± y 2r 2UHVROYHPRVODVHFXDFLRQHV  \  SDUDREWHQHU

w2 =

a j1
a21 ‚àí Œ±
,
y wj =
2r
2r

para cada j = 3, . . . , n.

Para resumir la selecci√≥n de P (1), tenemos

‚éõ

‚éû1/2

n

Œ± = ‚àísgn(a21 ) ‚éù

a 2j1 ‚é†

,

j=2
1/2

1 2 1
Œ± ‚àí a21 Œ±
2
2

r=

,

w 1 = 0,
w2 =

a21 ‚àí Œ±
,
2r

wj =

a j1
, por cada j = 3, . . . , n.
2r

y

Con esta selecci√≥n

‚é°

(2)
a11

‚é¢ (2)
‚é¢ a
‚é¢ 21
‚é¢
(2)
(1)
(1)
A = P AP = ‚é¢
‚é¢ 0
‚é¢ .
‚é¢ .
‚é£ .
0

‚é§

(2)
a12

0

(2)
a22

(2)
a23

(2)
a32
..
.

(2)
a33
..
.

‚é•
(2) ‚é•
¬∑ ¬∑ ¬∑ a2n
‚é•
‚é•
(2) ‚é• .
¬∑ ¬∑ ¬∑ a3n ‚é•
.. ‚é•
‚é•
. ‚é¶

(2)
an2

(2)
an3

(2)
¬∑ ¬∑ ¬∑ ann

¬∑¬∑¬∑

0

Al haber encontrado P (1) y calculado A(2), el proceso se repite para k = 2, 3, . . . , n ‚àí 2 de
acuerdo con lo siguiente:

‚éõ
(k)
Œ± = ‚àísgn(ak+1,k
)‚éù

‚éû1/2

n

2‚é†
(a (k)
jk )

j=k+1

r=

1 2 1 (k)
Œ± ‚àí Œ±Œ±k+1,k
2
2

1/2

w 1(k) = w 2(k) = ¬∑ ¬∑ ¬∑ = w k(k) = 0,

,

,

9.4

M√©todo de Householder

449

w 1(k) = w 2(k) = ¬∑ ¬∑ ¬∑ = w k(k) = 0,
(k)
=
w k+1

(k)
‚àíŒ±
ak+1,k
,
2r

a (k)
jk
(k)
,
wj =

para cada j = k + 2, k + 3, . . . , n,

2r

P (k) = I ‚àí 2w(k) ¬∑ (w(k) )t ,
y

A(k+1) = P (k) A(k) P (k) ,
donde

‚é§
(k+1)
. . . . 0 . .. .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . 0.
a12
.
.
.
.
..
....
‚é•
‚é¢ (k+1) . . . . . .
.... ........
..
....
‚é•
‚é¢ a21 . . .
.
.
.
.
.
.
.
.
‚é•
‚é¢
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
....
....
0
0
‚é•
‚é¢ 0.. . . . . . . . .
. . . . (k+1) . . . (k+1) . . . (k+1)
‚é•
‚é¢
.
.
.
(k+1)
.
.
(k+1)
.
.
.
.
.
‚é•
‚é¢
.
a
a
a
a
....
..
=‚é¢
A
k+1,k
k+1,.k+1
k+1,k+2
k+1,
n ‚é•.
.
.
.
.
.
.
.
.. . . .
.. ‚é•
..
...
‚é¢
....
‚é¢
..
.. ‚é•
..
....
0.
‚é¢
.
. ‚é•
.
....
.
..
..
‚é£
..
. . . . .. ‚é¶
.
(k+1) . . . . . . . . . . . . . . . . . (k+1)
ann
0 ................0
an,k+1
‚é°

(k+1)
a11
...

Si continuamos de esta manera, se forma la matriz tridiagonal y sim√©trica A(n‚àí1), donde

A(n‚àí1) = P (n‚àí2) P (n‚àí3) ¬∑ ¬∑ ¬∑ P (1) A P (1) ¬∑ ¬∑ ¬∑ P (n‚àí3) P (n‚àí2) .

Ejemplo 1

Aplique las transformaciones de Householder a la matriz sim√©trica 4 3 4

‚é°

4
‚é¢ 1
‚é¢
A=‚é£
‚àí2
2

1
2
0
1

‚àí2
0
3
‚àí2

‚é§
2
1 ‚é•
‚é•
‚àí2 ‚é¶
‚àí1

para producir una matriz tridiagonal sim√©trica que es similar a A.
Soluci√≥n

Para la primera aplicaci√≥n de una transformaci√≥n de Householder,

‚éõ
Œ± = ‚àí(1) ‚éù

4

‚éû1/2
a 2j1 ‚é†

1
1
(‚àí3)2 ‚àí (1)(‚àí3)
2
2

= ‚àí3, r =

j=2

‚àö ‚àö
‚àö
6
6
6
w = 0,
,‚àí
,
,
3
6
6
‚é°
‚é§
1 0 0 0
‚àö
‚é¢ 0 1 0 0 ‚é•
6
‚é•‚àí2
P (1) = ‚é¢
‚é£ 0 0 1 0 ‚é¶
6
0 0 0 1

‚é°
2

‚é§
0
‚é¢ 2 ‚é•
‚é¢
‚é•
‚é£ ‚àí1 ‚é¶ ¬∑ (0, 2, ‚àí1, 1)
1

1/2

=

‚àö
6,

450

CAP√çTULO 9

Aproximaci√≥n de eigenvalores

‚é°

1

0

0

‚àí 23

2
3
2
3
1
3

‚é¢ 0 ‚àí1
‚é¢
3
=‚é¢
2
‚é£ 0
3
0

0

‚é§

‚àí 23 ‚é•
‚é•
,
1 ‚é•
‚é¶
3
2
3

y

‚é°

4

‚é¢ ‚àí3
‚é¢
A(2) = ‚é¢
‚é£ 0
0

‚àí3

0

0

10
3

1

1

5
3
‚àí 43

4
3
‚àí 43

4
3

‚é§
‚é•
‚é•
‚é•.
‚é¶

‚àí1

Al continuar con la segunda iteraci√≥n,

5
Œ±=‚àí ,
3

‚àö
‚àö
‚àö
5
2 5
r=
, w = 0, 0, 2 5,
3
5
‚é§
‚é°
1 0
0
0
‚é¢
0
1
0
0 ‚é•
‚é•
P (2) = ‚é¢
‚é£ 0 0 ‚àí 35 ‚àí 45 ‚é¶ ,
‚àí 45

t

,

3
5

0

0

4
‚é¢ ‚àí3
‚é¢
A(3) = ‚é¢
‚é£ 0

‚àí3
10
3
‚àí 53

‚àí 33
25

0

0

68
75

y la matriz tridiagonal sim√©trica es

‚é°

‚é§
0
0 ‚é•
‚é•
68 ‚é• .
75 ‚é¶

0
‚àí 53

149
75

El algoritmo 9.5 realiza el m√©todo de Householder de acuerdo con lo que se describe
aqu√≠, a pesar de que se eluden las multiplicaciones reales de la matriz.

ALGORITMO

9.5

M√©todo de Householder
Para obtener una matriz tridiagonal sim√©trica A(n‚àí1) similar a la matriz sim√©trica A = A(1) ,
construya las siguientes matrices
A(2) , A(3) , . . . , A(n‚àí1) , donde A(k) = (ai(k)
j ) para cada k = 1, 2, . . . , n ‚àí 1:
ENTRADA
SALIDA

dimensi√≥n n; matriz A.

A(n‚àí1) . (En cada paso, A se puede sobreescribir.)

Paso 1 Para k = 1, 2, . . . , n ‚àí 2 haga los pasos 2‚Äì14.
Paso 2 Determine

n

q=

a (k)
jk

2

.

j=k+1
(k)
= 0 entonces determine Œ± = ‚àíq 1/2
Paso 3 Si ak+1,k

si no determine Œ± = ‚àí
(k)
.
Paso 4 Determine RSQ = Œ± 2 ‚àí Œ±ak+1,k

(k)
q 1/2 ak+1,k
(k)
|ak+1,k
|

.

(Nota: RSQ = 2r 2 )

9.4

M√©todo de Householder

451

Paso 5 Determine vk = 0; (Nota: v1 = ¬∑ ¬∑ ¬∑ = vk‚àí1 = 0, pero no necesaria.)
(k)
‚àí Œ±;
vk+1 = ak+1,k
Para j = k + 2, . . . , n determine v j = a (k)
jk .
1
1
Nota: w = ‚àö
v = v.
2r
2RSQ
Paso 6 Para j = k, k + 1, . . . , n determine u j =
Nota: u =

1
RSQ

A(k) v =

1
RSQ

n

a (k)
ji vi .

i=k+1

1 (k)
1
A v = A(k) w.
2r 2
r

n

Paso 7 Determine PROD =

vi u i .
i=k+1

Nota: PROD = vt u =

1 t (k)
v A v.
2r 2

Paso 8 Para j = k, k + 1, . . . , n determine z j = u j ‚àí

PROD
2RSQ

vj.

1
1
vt uv = u ‚àí 2 vt uv
2RSQ
4r
1
1
= u ‚àí wwt u = A(k) w ‚àí wwt A(k) w.
r
r

Nota: z = u ‚àí

Paso 9 Para l = k + 1, k + 2, . . . , n ‚àí 1 haga los pasos 10 y 11.
(Nota: Calcule A (k+1) = A(k) ‚àí vzt ‚àí zvt = (I ‚àí 2wwt )A(k) (I ‚àí 2wwt ).)
Paso 10 Para j = l + 1, . . . , n determine
= a (k)
a (k+1)
jl
jl ‚àí vl z j ‚àí v j z l ;
= a (k+1)
.
al(k+1)
j
jl
Paso 11 Determine all(k+1) = all(k) ‚àí 2vl zl .
(k+1)
(k)
Paso 12 Determine ann
= ann
‚àí 2vn z n .

= a (k+1)
= 0.
Paso 13 Para j = k + 2, . . . , n determine ak(k+1)
j
jk
(k+1)
(k)
= ak+1,k
‚àí vk+1 z k ;
Paso 14 Determine ak+1,k
(k+1)
(k+1)
= ak+1,k
.
ak,k+1

(Nota: Los otros elementos de A(k+1) son iguales a A(k) .)
Paso 15 SALIDA (A(n‚àí1) );
(El proceso est√° completo. A (n‚àí1) es sim√©trica, tridiagonal y similar a A .)
PARE.

En la siguiente secci√≥n, examinaremos c√≥mo se puede aplicar el algoritmo QR para determinar los eigenvalores de A (n‚àí1), que son iguales a los de la matriz original A.
El algoritmo de Householder se puede aplicar a una matriz arbitraria n 3 n, pero deben
KDFHUVH PRGL√ÄFDFLRQHV SDUD UHSUHVHQWDU OD SRVLEOH IDOWD GH VLPHWUtD /D PDWUL] UHVXOWDQWH
A (n‚àí1) no ser√° tridiagonal a menos que la matriz original A sea sim√©trica, pero todas las
entradas debajo de la subdiagonal inferior ser√°n 0. Una matriz de este tipo recibe el nombre
de Hessenberg superior. Es decir, H = (h i j ) es Hessenberg superior si h i j = 0, para toda
i ‚â• j + 2.

452

CAP√çTULO 9

Aproximaci√≥n de eigenvalores

/RVVLJXLHQWHVSDVRVVRQODV~QLFDVPRGL√ÄFDFLRQHVUHTXHULGDVSDUDPDWULFHVDUELWUDULDV
n

Paso 6 Para j = 1, 2, . . . , n determine u j =

1
a (k) vi ;
RSQ i=k+1 ji

yj =

1
a (k) vi .
RSQ i=k+1 i j

n

Paso 8 Para j = 1, 2, . . . , n determine z j = u j ‚àí

PROD
vj.
RSQ

Paso 9 Para l = k + 1, k + 2, . . . , n haga los pasos 10 y 11.
Paso 10 Para j = 1, 2, . . . , k determine a (k+1)
= a (k)
jl
jl ‚àí z j vl ;
= al(k)
al(k+1)
j
j ‚àí y j vl .
= a (k)
Paso 11 Para j = k + 1, . . . , n determine a (k+1)
jl
jl ‚àí z j vl ‚àí yl v j .
'HVSXpVGHPRGL√ÄFDUHVWRVSDVRVERUUHGHODO\ODVDOLGDA (n‚àí1). Observe que el paso 7
permanece sin cambios.
La secci√≥n Conjunto de ejercicios 9.4 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

9.5 El algoritmo QR
(QJHQHUDOORVPpWRGRVGHGH√ÅDFLyQTXHVHKDQDQDOL]DGRHQODVHFFLyQQRVRQDGHFXDGRV
para calcular todos los eigenvalores de una matriz debido al crecimiento del error de redondeo. En esta secci√≥n consideramos el algoritmo QR, una t√©cnica de reducci√≥n de matriz que
se usa para determinar en forma sistem√°tica todos los eigenvalores de una matriz sim√©trica.
Para aplicar el m√©todo QR comenzamos con una matriz sim√©trica en forma diagonal; es
GHFLUODV~QLFDVHQWUDGDVGLIHUHQWHVGHFHURVHHQFXHQWUDQ\DVHDVREUHODGLDJRQDORVREUHODV
subdiagonales directamente sobre la diagonal o por debajo de ella. Si √©sta no es la forma de
una matriz sim√©trica, el primer paso es aplicar el m√©todo de Householder para calcular una
matriz tridiagonal sim√©trica similar a la matriz dada.
En el resto de esta secci√≥n, se supondr√° que la matriz sim√©trica para la que se calculan
estos eigenvalores es tridiagonal. Si dejamos que A denote una matriz de este tipo, podemos
VLPSOL√ÄFDUODQRWDFLyQHQFLHUWDPHGLGDDOHWLTXHWDUODVHQWUDGDVGHA de acuerdo con lo
VLJXLHQWH
‚é°
‚é§
b2
0 .. .. . . . . . . . . 0.
a1
...
.. ‚é•
‚é¢
...
. ‚é•
‚é¢ b2
a
b
.
2
3
.
...
‚é¢
. .. ‚é•
‚é¢
... .. . ‚é•
‚é•
A=‚é¢

‚é¢ 0.. . . . . b3 . . . . a3 . . . . . . . . 0 ‚é• .
.
‚é¢ ..
‚é•
...
.
.
‚é¢ .
. . . . . . . . . . . bn ‚é•
‚é£ ..
‚é¶
...
... ...
0 . . . . . . . . . . .0
bn a n
Si b2 = 0 o bn = 0, HQWRQFHVODPDWUL]3[a 1 ] o [ an ] produce de inmediato un eigenvalor
a o an de A. El m√©todo QR toma la ventaja de esta observaci√≥n al disminuir sucesivamente
los valores de las entradas por debajo de la diagonal principal hasta b2 ‚âà 0 o bn ‚âà 0.
Cuando b j = 0 para algunas j, donde 2 < j < n, es posible reducir el problema para
considerar, en lugar de A, las matrices m√°s peque√±as

9.5 El algoritmo QR

‚é°

‚é§
0.. .. . . . . . . . . . 0.
.
.
..
‚é¢
.. ‚é•
‚é¢ b2 a2 b3 . . . . .
‚é•
...
‚é¢
. . . ... ‚é•
‚é¢
‚é•
...
.
‚é¢ 0. . b3 . a3 .
‚é•
... ... .... 0 ‚é•
‚é¢ . ...
.
‚é¢ .. . .
‚é•
.
.
‚é¢ .
. . . . . . . . . . . b j‚àí1 ‚é•
‚é£ ..
‚é¶
... ...
...
..
.
.
.
.
.
.
.
.
.
0
0 b j‚àí1 a j‚àí1
a1

‚é°

‚é§
0.. .. . . . . . . . . . 0.
.
.
..
‚é¢
.. ‚é•
‚é¢ b j+1 a j+1 b j+2 . . . . .
. ‚é•
‚é¢
.
.
. . . . . . .. ‚é•
‚é¢
‚é•
.
‚é¢ 0. . b j+2 a j+2
‚é•
... .... 0 ‚é• .
...
‚é¢ . ...
‚é¢ ..
...
... .. ‚é•
...
‚é¢ .
...
. . . bn ‚é•
.
‚é£ ..
... ....
... ‚é¶
.
.
.
.
.
.
.
.
.
.
.
.
.
0
0
bn an

b2

y

aj

453

b j+1



Si ninguna b j HV FHUR HO PpWRGR 45 FRQWLQ~D DO IRUPDU XQD VXFHVLyQ GH PDWULFHV
A = A(1) , A(2) , A(3) , . . . ,GHDFXHUGRFRQORVLJXLHQWH
i.

A  5 A se factoriza como producto de A(1) = Q (1) R (1) , donde Q  es ortogonal y
R  es triangular superior.

ii.

A  VHGH√ÄQHFRPRA(2) = R (1) Q (1) .

En general, A(i) se factoriza como un producto A(i) = Q (i) R (i) de una matriz ortogonal Q(i) y
una matriz triangular superior R(i). Entonces, A(i1 VHGH√ÄQHPHGLDQWHHOSURGXFWRGHR(i) y Q(i)
t
en direcci√≥n inversa A(i+1) = R (i) Q (i) . Puesto que Q(i) es ortogonal, R (i) = Q (i) A(i) , y
t

t

A(i+1) = R (i) Q (i) = (Q (i) A(i) )Q (i) = Q (i) A(i) Q (i) .



Esto garantiza que A(i1 es sim√©trica con los mismos eigenvalores que A(i). Por la manera en
TXHGH√ÄQLPRVQ(i) y R(i), tambi√©n garantizamos que A(i1 es tridiagonal.
Al continuar mediante inducci√≥n, A(i1 tiene los mismos eigenvalores que la matriz original A, y A(i1 tiende a la matriz diagonal con los eigenvalores de A a lo largo de
la diagonal.

Matrices de rotaci√≥n
Para describir la construcci√≥n de las matrices de factorizaci√≥n Q(i) y R(i), necesitamos la
noci√≥n de matriz de rotaci√≥n.
DeÔ¨Ånici√≥n 9.23

Una matriz de rotaci√≥n PGL√ÄHUHGHODPDWUL]LGHQWLGDGHQPi[LPRFXDWURHOHPHQWRV(VWRV
cuatro elementos son de la forma

pii = p j j = cos Œ∏ y

Si AHVODPDWUL]GHURWDFLyQ3
cos Œ∏ ‚àí sen Œ∏
A=
,
sen Œ∏
cos Œ∏

pi j = ‚àí p ji = sen Œ∏,

para algunos Œ∏ y algunas i = j.

entonces Ax rota a x en el sentido
contrario a las manecillas del
reloj un √°ngulo Œ∏.

(VIiFLOPRVWUDU FRQVXOWHHOHMHUFLFLR TXHSDUDFXDOTXLHUPDWUL]GHURWDFLyQP, la
matriz A PGL√ÄHUHGHA s√≥lo en la i-√©sima y la j-√©sima columnas y la matriz P AGL√ÄHUHGHA
s√≥lo en la i-√©sima y la j-pVLPD√ÄODV3DUDFXDOTXLHUi = j , el √°ngulo Œ∏ se puede seleccionar de
tal forma que el producto P A tiene una entrada cero para (P A)i j . Adem√°s, todas las matrices
de rotaci√≥n PVRQRUWRJRQDOHVSRUTXHODGH√ÄQLFLyQLPSOLFDTXHP P t = I .

Ejemplo 1

Encuentre una matriz de rotaci√≥n P con la propiedad de que P A tiene una entrada cero en la
VHJXQGD√ÄOD\ODSULPHUDFROXPQDGRQGH
‚é°
‚é§
3 1 0
A = ‚é£ 1 3 1 ‚é¶.
0 1 3

A menudo √©stas reciben el
nombre de rotaciones de Givens
porque James Wallace Givens
 ODVXVyHQOD
GpFDGDGHFXDQGRHVWDED
HQORV/DERUDWRULRV1DFLRQDOHV
Argonne.

‚é°

Soluci√≥n /DIRUPDGHP es

cos Œ∏ sen Œ∏
P = ‚é£ ‚àí sen Œ∏ cos Œ∏
0
0

‚é§
‚é°
‚é§
0
3 cos Œ∏ + sen Œ∏
cos Œ∏ + 3 sen Œ∏ sen Œ∏
0 ‚é¶, de modo que PA = ‚é£ ‚àí3 sen Œ∏ + cos Œ∏ ‚àí sen Œ∏ + 3 cos Œ∏ cos Œ∏ ‚é¶.
1
0
1
3

454

CAP√çTULO 9

Aproximaci√≥n de eigenvalores

El √°ngulo Œ∏ se selecciona de tal forma que ‚àí3 sen Œ∏ + cos Œ∏ = 0, es decir; de tal forma que
1
Œ∏ = . Por lo tanto,
3
‚àö
‚àö
3 10
10
cos Œ∏ =
, sen Œ∏ =
10
10
y

‚é°
‚é¢
PA = ‚é£

‚àö
3 10
10
‚àö
‚àí 1010

10
10
‚àö
3 10
10

0

0

‚àö

‚é§ ‚é° ‚àö
10
3 1 0
‚é•‚é¢
‚é• ‚é¢
0 ‚é¶‚é£ 1 3 1 ‚é¶ = ‚é£ 0
0 1 3
0
1
0

‚é§‚é°

‚àö

‚àö

3
10
5
‚àö
4
10
5

1
10
10
‚àö
3
10
10

1

3

‚é§
‚é•
‚é¶.

Observe que la matriz resultante no es ni sim√©trica ni diagonal.
/DIDFWRUL]DFLyQGH A(1) como A(1) = Q (1) R (1) usa un producto de n 2PDWULFHVGH
rotaci√≥n para construir

R (1) = Pn Pn‚àí1 ¬∑ ¬∑ ¬∑ P2 A(1) .
Primero seleccionamos la matriz de rotaci√≥n P con

p11 = p22 = cos Œ∏2 y

p12 = ‚àí p21 = sen Œ∏2 ,

donde

sen Œ∏2 =

b2
2
b2 + a12

a1
.
2
b2 + a12

y cos Œ∏2 =

Esta selecci√≥n da

(‚àí sen Œ∏2 )a1 + (cos Œ∏2 )b2 =

‚àíb2 a1
b22 + a12

+

a 1 b2
b22 + a12

=0

SDUDODHQWUDGDHQODSRVLFLyQ  HVGHFLUHQODVHJXQGD√ÄOD\ODSULPHUDFROXPQDGHO
producto P2 A(1) . Por lo que la matriz
(1)
A(1)
2 = P2 A

WLHQHXQFHURHQODSRVLFLyQ  
/DPXOWLSOLFDFLyQ P2 A(1)DIHFWDDPEDV√ÄODV\GHA  , por lo que la matriz A(1)
2 no
QHFHVDULDPHQWHUHWLHQHFHURHQWUDGDVHQODVSRVLFLRQHV       \ n). Sin embargo, A  es tridiagonal, por lo que (1, 4), . . . , (1, n) entradas de A  tambi√©n deben ser 0.
S√≥ORODHQWUDGD  ODGHODSULPHUD√ÄOD\ODWHUFHUDFROXPQDVHSXHGHYROYHUGLIHUHQWHD
cero en A  .
En general, la matriz Pk se selecciona de tal forma que la entrada (k, k ‚àí 1) en
(1)
A(1)
k = Pk Ak‚àí1 es cero. Este resultado en la entrada (k 2k 1 VHYXHOYHGLIHUHQWHGH
FHUR/DPDWUL]Ak  tiene la forma

9.5 El algoritmo QR

455

‚é§
q1 . . r 1 . .
0 .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 0.
...
...
.
...
.. ‚é•
‚é¢
...
... .....
.. ‚é•
‚é¢ 0 . ....
.
.
.
...
...
...
.
‚é¢
.
.. ‚é•
‚é¢
...
... .....
... ....
.. ‚é•
‚é¢ 0. .
.
.
.
.
.
.
...
...
...
.. ‚é•
...
‚é•
‚é¢ . ... ...
...
..
...
... ..
.
‚é¢ ..
.. ‚é•
.
‚é¢ .
... 0
z k‚àí1 qk‚àí1 rk‚àí1 . . .
.. ‚é•
‚é¢ ..
...
.
.. ‚é•
.
‚é•
‚é¢
...
...
‚é¢ ...
.. ‚é• ,
.
.
0
x
y
0
A(1)
=
..
k
k
...
.
k
‚é¢ .
.
..
.. . ‚é•
...
‚é•
‚é¢ ..
. . . bk+1 ak+1 bk+2 . . . . . 0 ‚é•
‚é¢ .
...
..
... ...
‚é•
‚é¢ ..
...
‚é•
‚é¢ .
... ...
.
... ..
‚é•
‚é¢ ..
... ... ....
.
0
.
.
‚é•
‚é¢ .
.
... ...
...
...
‚é•
‚é¢ ..
.
.
.
... ...
...
‚é¢ .
bn ‚é•
... ...
.
‚é¶
‚é£ ..
.
...
.. ..
0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. 0 . .bn an
‚é°

z1 . .

y Pk+1 tiene la forma

‚é°

Ik‚àí1

‚é¢
‚é¢
‚é¢
Pk+1 = ‚é¢ O
‚é¢
‚é£
O

O

O

ck+1

sk+1

‚àísk+1

ck+1

‚Üë
columna k

O

O

‚é§
‚é•
‚é•
‚é•
‚é•
‚é•
‚é¶

‚Üê fila k

,



In‚àík‚àí1

donde 0 denota la matriz dimensional adecuada con todas las entradas cero.
/DVFRQVWDQWHVck+1 = cos Œ∏k+1 y sk+1 = sen Œ∏k+1 en Pk+1 se seleccionan de tal forma
que la entrada (k + 1, k) en A(1)
k+1 sea cero; es decir, ‚àísk+1 x k + ck+1 bk+1 = 0.
2
2
+ sk+1
= 1, la soluci√≥n de esta ecuaci√≥n es
Puesto que ck+1

sk+1 =
y A(1)
k+1 tiene la forma
‚é°

bk+1
2
bk+1
+ xk2

y

ck+1 =

xk
2
bk+1
+ xk2

,

‚é§
q1 . . r1 . . 0 .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
.
.
.
.
...
...
...
..
‚é¢
.. ‚é•
...
.
.
‚é¢ 0 .. ....
.. ‚é•
...
...
‚é¢
... ..... .....
.. ‚é•
‚é•
‚é¢
...
...
...
...
...
‚é¢ 0. .
.. ‚é•
.
.
...
...
... ...
...
‚é¢ . ...
.. ‚é•
..
..
.. ..
...
‚é¢ .. . .
.. ‚é•
...
.
‚é•
‚é¢ .
...
zk
qk
rk
‚é¢ ..
... 0
.. ‚é•
.
.
‚é¢
...
.
.. ‚é•
‚é•.
‚é¢ ..
...
0 xk+1 yk+1
0. . . . . . .
A(1)
k+1 = ‚é¢ .
...
...
. . .. ‚é•
‚é•
‚é¢ ...
...
...
‚é¢ .
ak+2
bk+3
. . . bk+2
... 0 ‚é•
.
.
.
‚é•
‚é¢ ..
.
.
.
.
.
... ...
.
‚é•
‚é¢ .
... ... ..... .....
‚é•
‚é¢ ..
0
...
... ...
...
‚é•
‚é¢ .
‚é•
‚é¢ ..
...
... ...
..
‚é¢ .
.
.
... ..
b ‚é•
‚é£ ..
... ... ..... n ‚é¶
.
.
.
0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 0 . . bn . a n
z1 . .

456

CAP√çTULO 9

Aproximaci√≥n de eigenvalores

Si continuamos con esta construcci√≥n en la sucesi√≥n P2 , . . . , Pn, obtenemos la matriz
triangular superior
‚é§
‚é°
z 1 . . q1 . . r1 . . 0 .... . . . . . . . . . 0.
... ...
..
..
..
‚é•
‚é¢
... ...
.
‚é•
‚é¢ 0. . . . . . . . . .
.
.
...
...
‚é•
‚é¢ . ...
. . . . . . ...
‚é•
‚é¢ .. . .
...
...
.
...
.
...
‚é•
‚é¢ .
.
.
0
.
.
.
.
‚é•
‚é¢
.
.
...
...
...
...
R (1) ‚â° A(1)
‚é•.
n = ‚é¢ ..
.
.
... ..
...
‚é•
‚é¢ ..
... ...
‚é¢ .
. . . rn‚àí2 ‚é•
.
‚é•
‚é¢ ..
... .
.
‚é¢ .
. . . z n‚àí1 qn‚àí1 ‚é•
‚é¶
‚é£ ..
...
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
0
xn
/DRWUDPLWDGGHODIDFWRUL]DFLyQ45HVODPDWUL]

Q (1) = P2t P3t ¬∑ ¬∑ ¬∑ Pnt
debido a la ortogonalidad de las matrices de rotaci√≥n implica que

Q (1) R (1) = (P2t P3t ¬∑ ¬∑ ¬∑ Pnt ) ¬∑ (Pn ¬∑ ¬∑ ¬∑ P3 P2 )A(1) = A(1) .
/DPDWUL]Q  es ortogonal porque

(Q (1) )t Q (1) = (P2t P3t ¬∑ ¬∑ ¬∑ Pnt )t (P2t P3t ¬∑ ¬∑ ¬∑ Pnt ) = (Pn ¬∑ ¬∑ ¬∑ P3 P2 ) ¬∑ (P2t P3t ¬∑ ¬∑ ¬∑ Pnt ) = I.
Adem√°s, Q  es una matriz Hessenberg superior. Para observar porqu√© esto es verdad, puede
VHJXLUORVSDVRVHQORVHMHUFLFLRV\
Por consiguiente, A(2) = R (1) Q (1) tambi√©n es una matriz Hessenberg superior porque al
multiplicar Q  a la izquierda de la matriz triangular superior R  no se afectan las entradas
en el tri√°ngulo inferior. Ya sabemos que es sim√©trica, por lo que A  es tridiagonal.
En general, las entradas fuera de la diagonal de A  , ser√°n de magnitud m√°s peque√±a que
las entradas correspondientes de A  , por lo que A  est√° m√°s cerca de ser una matriz diagonal que A  . El proceso se repite para construir A(3) , A(4) , . . . hasta obtener convergencia
satisfactoria.
Ejemplo 2

$SOLTXHXQDLWHUDFLyQGHOPpWRGR45DODPDWUL]GDGDHQHOHMHPSOR
‚é°
‚é§
3 1 0
A = ‚é£ 1 3 1 ‚é¶.
0 1 3
Sea A  5 A una matriz dada y P representa la matriz de rotaci√≥n determinada
HQHOHMHPSOR(QFRQWUDPRVSRUPHGLRGHODQRWDFLyQSUHVHQWDGDHQHOPpWRGR45TXH
‚àö
‚àö
‚é§‚é°
‚é§
‚é° 3‚àö10 ‚àö10
‚é§ ‚é° ‚àö
10
0
10 35 10
3
1
0
10
10
10
‚àö
‚àö
‚àö
‚àö
‚é•
‚é¢
‚é•
‚é¢
(1)
4 10
3 10 ‚é¶
A(1)
= ‚é£ ‚àí 10 3 10 0 ‚é¶ ‚é£ 1 3 1 ‚é¶ = ‚é£ 0
2 = P2 A
10
10
5
10
0 1 3
0
0
1
0
1
3
‚é°
‚é§
z 1 q1
r1
y2 ‚é¶ .
‚â° ‚é£ 0 x2
(1)
0 b3
a3(1)

Soluci√≥n

Al continuar, tenemos

s3 =

b3(1)
x22 + (b3(1) )2

= 0.36761

y

c3 =

x2
x22 + (b3(1) )2

= 0.92998,

9.5 El algoritmo QR

457

por lo que
‚é§‚é° ‚àö
10
1
0
0
‚é¢
(1)
(1)
(1)
‚é£
‚é¶
0.92998 0.36761 ‚é£ 0
R ‚â° A3 = P3 A2 = 0
0 ‚àí0.36761 0.92998
0
‚àö
‚é§
‚é° ‚àö
‚àö
10
10 35 10
10
‚é•
‚é¢
=‚é£ 0
2.7203 1.9851 ‚é¶
‚é°

0

0

‚àö

3
10
5
‚àö
4 10
5

‚àö
10
10
‚àö
3 10
10

1

3

‚é§
‚é•
‚é¶

2.4412

y
‚é° 3‚àö10
‚é¢
Q (1) = P2t P3t = ‚é£
‚é°

10
‚àö
10
10

‚àö

‚àí 1010
‚àö

3 10
10

0

0

0.94868
= ‚é£ 0.31623
0

0

‚é§‚é°

1
‚é•‚é£
0 ‚é¶ 0
0
1

0
0.92998
0.36761

‚é§
0
‚àí0.36761 ‚é¶
0.92998

‚é§
‚àí0.29409
0.11625
0.88226 ‚àí0.34874 ‚é¶ .
0.36761
0.92998

Por consiguiente,
‚àö
‚é° ‚àö
‚é§‚é°
‚àö
10
0.94868
10 35 10
10
A(2) = R (1) Q (1) = ‚é£ 0
2.7203 1.9851 ‚é¶ ‚é£ 0.31623
0
0
0
2.4412
‚é°
‚é§
3.6
0.86024
0
= ‚é£ 0.86024 3.12973 0.89740 ‚é¶ .
0
0.89740 2.27027

‚àí0.29409
0.88226
‚àí0.36761

‚é§
0.11625
‚àí0.34874 ‚é¶
0.92998

/RVHOHPHQWRVGLDJRQDOHVGHA  VRQDSUR[LPDGDPHQWHPiVSHTXHxRVTXHORVGHA  ,
por lo que tenemos una reducci√≥n, pero no es considerable. Para disminuir por debajo de
QHFHVLWDPRVUHDOL]DULWHUDFLRQHVGHOPpWRGR45$OKDFHUORREWHQHPRV
‚é°
‚é§
4.4139 0.01941
0
A(13) = ‚é£ 0.01941 3.0003 0.00095 ‚é¶ .
0
0.00095 1.5858
√âVWD GDUtD XQ HLJHQYDORU DSUR[LPDGR GH  \ ORV HLJHQYDORUHV UHVWDQWHV VH SRGUtDQ
aproximar al considerar la matriz reducida

4.4139
0.01941

0.01941
3.0003

.

Aceleraci√≥n de la convergencia
Si los eigenvalores de A tienen diferentes m√≥dulos con |Œª1 | > |Œª2 | > ¬∑ ¬∑ ¬∑ > |Œªn |, entonces la
(i1
depende de la raz√≥n
velocidad de convergencia de la entrada b(i+1)
j+1 para 0 en la matriz A
|Œª j+1 /Œª j | FRQVXOWH>)U@ /DYHORFLGDGGHFRQYHUJHQFLDGHb(i+1)
para
0 determina la velocij+1
converge en el j-√©simo eigenvalor Œª j . Por lo tanto, la velocidad
dad a la que la entrada a (i+1)
j
de convergencia puede ser lenta si |Œª j+1 /Œª j |QRHVVLJQL√ÄFDWLYDPHQWHPHQRUTXH
Para acelerar esta convergencia se usa una t√©cnica de cambio similar a la que se usa
FRQHOPpWRGRGHSRWHQFLDLQYHUVDHQODVHFFLyQ6HVHOHFFLRQDXQDFRQVWDQWHœÉ cerca del
eigenvalor de A(VWRPRGL√ÄFDODIDFWRUL]DFLyQHQODHFXDFLyQ  SDUDVHOHFFLRQDUQ(i) y
R(i) de tal forma que

A(i) ‚àí œÉ I = Q (i) R (i) ,



458

CAP√çTULO 9

Aproximaci√≥n de eigenvalores

y, por consiguiente, la matriz A(i+1)HVWiGH√ÄQLGDFRPR

A(i+1) = R (i) Q (i) + œÉ I.



&RQHVWDPRGL√ÄFDFLyQODYHORFLGDGGHFRQYHUJHQFLDGH b(i+1)
j+1 para 0 depende de la raz√≥n
|(Œª j+1 ‚àí œÉ )/(Œª j ‚àí œÉ )|. (VWRSXHGHUHVXOWDUHQXQDPHMRUDVLJQL√ÄFDWLYDVREUHODYHORFLGDG
original de convergencia de a (i+1)
para Œª j si œÉ est√° cerca de Œª j+1, pero no cerca de Œª j .
j
Cambiamos œÉ en cada paso, de tal forma que cuando A tiene eigenvalores de diferentes
m√≥dulos, bn(i+1) converge a 0 m√°s r√°pido que b(i+1)
para cualquier entero j menor que n.
j
Cuando bn(i+1) HV VX√ÄFLHQWHPHQWH SHTXHxR VXSRQHPRV TXH Œªn ‚âà an(i+1) , elimina las en√©VLPDV√ÄOD\FROXPQDGHODPDWUL]\FRQWLQ~DGHODPLVPDIRUPDSDUDHQFRQWUDUXQDDSURximaci√≥n a Œªn‚àí1 . (OSURFHVRFRQWLQ~DKDVWDTXHVHGHWHUPLQDXQDDSUR[LPDFLyQSDUDFDGD
eigenvalor.
/DWpFQLFDGHFDPELRVHOHFFLRQDHQHOi-√©simo paso, la constante de cambio œÉi, donde œÉi
es el eigenvalor de la matriz
(i)
an‚àí1

E (i) =

bn(i)

bn(i)

an(i)

que est√° m√°s cerca de an(i) . Este cambio traduce los eigenvalores de A mediante un factor œÉi.
&RQHVWDWpFQLFDGHFDPELRQRUPDOPHQWHODFRQYHUJHQFLDHVF~ELFD &RQVXOWH>:5@S 
El m√©todo acumula estos cambios hasta que bn(i+1) ‚âà 0 y, despu√©s, a√±ade cambios a a (i+1)
j
para aproximar el eigenvalor Œªn.
Si A tiene eigenvalores del mismo m√≥dulo, b(i+1)
quiz√° tienda a 0 para algunas j = n a
j
una velocidad m√°s r√°pida que bn(i+1). En este caso, la t√©cnica de divisi√≥n de matriz descrita
HQ  VHSXHGHXVDUSDUDUHGXFLUHOSUREOHPDDXQRTXHLQFOX\DXQSDUGHPDWULFHVGH
orden reducido.
Ejemplo 3

Incorpore el cambio al m√©todo QR para la matriz
‚é°
‚é§ ‚é° a (1)
1
3 1 0
‚é¢ (1)
‚é£
‚é¶
A = 1 3 1 = ‚é£ b2
0 1 3
0

b2(1)

0

‚é§

a2(1)

‚é•
b3(1) ‚é¶ .

b3(1)

a3(1)

Soluci√≥n

Encontrar el par√°metro de aceleraci√≥n para el cambio requiere encontrar los
eigenvalores de

a2(1)

b3(1)

b3(1)

a3(1)

=

3
1

1
3

,

que son Œº1 = 4 y Œº2 = 2. /DVHOHFFLyQGHOHLJHQYDORUPiVFHUFDQRDa3(1) = 3 es arbitraria,
y seleccionamos Œº2 = 2 y cambiamos por esta cantidad. Entonces œÉ1 = 2 y
‚é°
‚é§ ‚é°
‚é§
0
d1 b2(1)
1 1 0
‚é¢ (1)
‚é•
d2 b3(1) ‚é¶ = ‚é£ 1 1 1 ‚é¶ .
‚é£ b2
0 1 1
0 b(1) d
3

3

Si continuamos con los cambios obtenemos

x1 = 1,

y1 = 1,

‚àö
q1 = 2,

‚àö
z 1 = 2,

x2 = 0,

‚àö
2
s2 =
,
2
‚àö
2
y2 =
,
2

‚àö
2
c2 =
,
2

‚àö
2
r1 =
, y
2

9.5 El algoritmo QR

por lo que

‚é° ‚àö
2
‚é¢
(1)
A2 = ‚é£ 0
0

‚àö
2

‚é§

‚àö

2
2

‚àö

‚é•
2 ‚é¶.
1

0
1

Adem√°s,
z 2 = 1,
por lo que

c3 = 0,

s3 = 1,

q2 = 1, y

‚é° ‚àö
2
‚é¢
(1)
(1)
R = A3 = ‚é£ 0

‚àö

0

0

Para calcular A  , tenemos
‚àö
2
z3 = ‚àí
, a1(2) = 2,
2

b2(2) =

‚àö
2
,
2

por lo que

‚é°

459

x3 = ‚àí

2

‚àö
2
2

1

1

‚é§
‚é•
‚é¶.

‚àö
‚àí 22

a2(2) = 1,

b3(2) = ‚àí

‚àö

2
2

2

‚é¢ ‚àö
2
A(2) = R (1) Q (1) = ‚é¢
‚é£ 2

1

0

‚àí 22

‚àö

‚àö
2
,
2

0

‚àö
2
, y a3(2) = 0,
2

‚é§

‚àö ‚é•
‚àí 22 ‚é•
‚é¶.

0

‚àö
‚àö
8QDLWHUDFLyQGHOPpWRGR45HVWiFRPSOHWD1Lb2(2) = 2/2 ni b3(2) = ‚àí 2/2 son peque√±as, por lo que se realiza
otra iteraci√≥n del m√©todo QR. Para esta iteraci√≥n, calculamos los
‚àö
eigenvalores 12 ¬± 12 3 de la matriz
a2(2)

b3(2)

b3(2)

a3(2)

‚àö

=

1

‚àí 22

‚àí 22

0

‚àö

‚àö
y seleccionamos œÉ2 = 12 ‚àí 12 3, el eigenvalor m√°s cercano a a3(2) = 0. Al completar los
c√°lculos, obtenemos
‚é°
‚é§
2.6720277
0.37597448
0
0.030396964 ‚é¶ .
A(3) = ‚é£ 0.37597448 1.4736080
0
0.030396964 ‚àí0.047559530
Si b3(3) = 0.030396964 HVVX√ÄFLHQWHPHQWHSHTXHxRHQWRQFHVODDSUR[LPDFLyQGHOHLJHQYD‚àö
lor ŒªHVODVXPDGH a3 (3) y los cambios œÉ1 + œÉ2 = 2 + (1 ‚àí 3)/2. Al borrar
ODWHUFHUD√ÄOD\FROXPQDREWHQHPRV

A(3) =

2.6720277 0.37597448
0.37597448 1.4736080

,

que tiene eigenvalores Œº1 = 2.7802140 y Œº2 = 1.3654218. Al sumar los cambios obtenemos las aproximaciones

Œª1 ‚âà 4.4141886 y

Œª2 ‚âà 2.9993964.

/RVHLJHQYDORUHVUHDOHVGHODPDWUL]AVRQ\SRUORTXHHOPpWRGR
45GLRFXDWURGtJLWRVVLJQL√ÄFDWLYRVGHH[DFWLWXGHQV√≥lo dos iteraciones.
(ODOJRULWPRLPSOHPHQWDHOPpWRGR45

460

CAP√çTULO 9

ALGORITMO

9.6

Aproximaci√≥n de eigenvalores

M√©todo QR
Para obtener los eigenvalores de la matriz sim√©trica, tridiagonal n 3 n

‚é°

a1(1)

b2(1) . .

0 . .... . . . . . . . . 0.
...
..
‚é¢ (1)
.
‚é¢ b . a (1) . . . . . . . . .
.
. .
‚é¢ 2 .. 2 ..
...
. . . . . . . . . . ...
‚é¢
..
.
...
A ‚â° A1 = ‚é¢
‚é¢ 0.. . . . . . . .
... .... 0
...
...
‚é¢ .
.
...
...
‚é¢ ..
(1)
.
. . . bn
‚é£ .
... ....
.
.
.
..
0 ............ 0
bn(1) an(1)

...

‚é§
‚é•
‚é•
‚é•
‚é•
‚é•
‚é•
‚é•
‚é•
‚é¶

ENTRADA n; a1(1) , . . . , an(1) , b2(1) , . . . , bn(1) ; tolerancia TOLQ~PHURPi[LPRGHLWHUDFLRnes M.
SALIDA eigenvalores de A, o la divisi√≥n recomendada de A o un mensaje que indica que
HOQ~PHURPi[LPRGHLWHUDFLRQHVIXHVXSHUDGR

Paso 1 Determine k = 1;
SHIFT = 0. (Cambio acumulado.)
Paso 2 Mientras k ‚â§ M haga los pasos 3‚Äì19.
Prueba de los pasos 3‚Äì7 para √©xito.)
Paso 3 Si |bn(k) | ‚â§ TOL entonces determine Œª = an(k) + SHIFT;
SALIDA (Œª);
determine n = n ‚àí 1.
Paso 4 Si |b2(k) | ‚â§ TOL entonces determine Œª = a1(k) + SHIFT;
SALIDA (Œª);
determine n = n ‚àí 1;
a1(k) = a2(k) ;
para j = 2, . . . , n
(k)
determine a (k)
j = a j+1 ;
(k)
(k)
b j = b j+1 .
Paso 5 Si n = 0 entonces
PARE.
Paso 6 Si n = 1 entonces
determine Œª = a1(k) + SHIFT;
SALIDA (Œª);
PARE.
Paso 7 Para j = 3, . . . , n ‚àí 1
si |b(k)
j | ‚â§ TOL entonces
(k)
(k)
SALIDA (‚Äòdividido en‚Äô, a1(k) , . . . , a (k)
j‚àí1 , b2 , . . . , b j‚àí1 ,
‚Äòy‚Äô,
(k)
(k)
(k)
a (k)
j , . . . , an , b j+1 , . . . , bn , SHIFT);
PARE.
Paso 8 (Calcule el cambio.)
(k)
+ an(k) );
Determine b = ‚àí(an‚àí1
(k)
‚àí bn(k) ;
c = an(k) an‚àí1
2

d = (b2 ‚àí 4c)1/2 .

9.5 El algoritmo QR

461

Paso 9 Si b > 0 entonces determine Œº1 = ‚àí2c/(b + d);
Œº2 = ‚àí(b + d)/2
si no determine Œº1 = (d ‚àí b)/2;
Œº2 = 2c/(d ‚àí b).
Paso 10 Si n = 2 entonces determine Œª1 = Œº1 + SHIFT;
Œª2 = Œº2 + SHIFT;
SALIDA (Œª1 , Œª2 );
PARE.
Paso 11 Escoja œÉ por lo que |œÉ ‚àí an(k) | = m√≠n{|Œº1 ‚àí an(k) |, |Œº2 ‚àí an(k) |}.
Paso 12 (Acumule el cambio.)
Determine SHIFT = SHIFT + œÉ .
Paso 13 (Realice el cambio.)
Para j = 1, . . . , n, determine d j = a (k)
j ‚àí œÉ.
Paso 14 (Los pasos 14 y 15 calculan R (k) .)
Determine x1 = d1 ;
y1 = b2 .
Paso 15 Para j = 2, . . . , n
determine z j‚àí1 =
cj =
œÉj =

x 2j‚àí1 + b(k)
j

2

1/2

;

x j‚àí1
;
z j‚àí1
b(k)
j
z j‚àí1

;

q j‚àí1 = c j y j‚àí1 + s j d j ;
x j = ‚àíœÉ j y j‚àí1 + c j d j ;
Si j = n entonces determine r j‚àí1 = œÉ j b(k)
j+1 ;
y j = c j b(k)
j+1 .

(k)
(k)
A(k)
= A(k)
j = P j A j‚àí1 se acaba de calcular y R
n .

Paso 16 (Pasos 16‚Äì18 calcule A(k+1 .)
Determine z n = xn ;
a1(k+1) = œÉ2 q1 + c2 z 1 ;
b2(k+1) = œÉ2 z 2 .
Paso 17 Para j = 2, 3, . . . , n ‚àí 1
= œÉ j+1 q j + c j c j+1 z j ;
determine a (k+1)
j
b(k+1)
j+1 = œÉ j+1 z j+1 .
Paso 18 Determine an(k+1) = cn z n .
Paso 19 Determine k = k + 1.
Paso 20 SALIDA (‚Äòel n√∫mero m√°ximo de iteraciones excedido‚Äô);
(El procedimiento no fue exitoso.)
PARE.

Se puede usar un procedimiento similar para encontrar las aproximaciones para los
eigenvalores de una matriz sim√©trica n 3 n. Primero, la matriz se reduce a una matriz
Hessenberg superior similar mediante el algoritmo de Hessenberg para matrices no sim√©triFDVGHVFULWRDO√ÄQDOGHODVHFFLyQ

462

CAP√çTULO 9

Aproximaci√≥n de eigenvalores

El proceso de factorizaci√≥n QR asume la siguiente forma. Primero,

H ‚â° H (1) = Q (1) R (1) .



Entonces H  VHGH√ÄQHSRUPHGLRGH

H (2) = R (1) Q (1)



H (2) = Q (2) R (2) .



y se factoriza en

(OPpWRGRSDUDIDFWRUL]DUFRQWLQ~DFRQHOPLVPRREMHWLYRTXHHODOJRULWPR45SDUDPDtrices sim√©tricas. Es decir, las matrices se seleccionan para introducir ceros en las entradas
adecuadas de la matriz y se usa un procedimiento de cambio similar al del m√©todo QR. Sin
embargo, el cambio es, en cierta medida, complicado para las matrices no sim√©tricas ya que
pueden presentarse eigenvalores complejos con el mismo m√≥dulo. El proceso de cambio moGL√ÄFDORVFiOFXORVHQODVHFXDFLRQHV    \  SDUDREWHQHUHOPpWRGR45GREOH

James Hardy Wilkinson
¬≤ HVPHMRUFRQRFLGR
por su amplio trabajo sobre
m√©todos num√©ricos para resolver
sistemas de ecuaciones lineales
y problemas de eigenvalor.
Tambi√©n desarroll√≥ la t√©cnica
de √°lgebra lineal num√©rica de
an√°lisis de error regresivo.

H (1) ‚àí œÉ1 I = Q (1) R (1) ,

H (2) = R (1) Q (1) + œÉ1 I,

H (2) ‚àí œÉ2 I = Q (2) R (2) ,

H (3) = R (2) Q (2) + œÉ2 I,

donde œÉ y œÉ son conjugados complejos y H (1) , H (2) , . . . son matrices Hessenberg superior.
Es posible encontrar una descripci√≥n completa del m√©todo QR en los trabajos de WilNLQVRQ>:LO@$OJRULWPRV\SURJUDPDVGHWDOODGRVSDUDpVWH\RWURVPpWRGRVTXHVHXVDQFRQ
mayor frecuencia se proporcionan en [WR]. Remitimos al lector hacia estos trabajos si el
m√©todo que hemos analizado no arroja resultados satisfactorios.
El m√©todo QR se puede realizar de forma que producir√° los eigenvectores de una matriz,
DVtFRPRHLJHQYDORUHVSHURHODOJRULWPRQRVHKDGLVHxDGRSDUDORJUDUHVWR6LVHQHFHsitan los eigenvectores de una matriz sim√©trica, as√≠ como los eigenvalores, sugerimos que se
XVH\DVHDHOPpWRGRGHSRWHQFLDLQYHUVDGHVSXpVGHKDEHUXWLOL]DGRORVDOJRULWPRV\
o una de las t√©cnicas m√°s poderosas mencionadas en [WR].
La secci√≥n Conjunto de ejercicios 9.5 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

9.6 Descomposici√≥n en valores singulares
En esta secci√≥n, consideramos la factorizaci√≥n de una matriz general A m 3 n en lo que se
conoce como descomposici√≥n en valores singulares. Esta factorizaci√≥n toma la forma

A = U S V t,
donde U es una matriz ortogonal m 3 m, V es una matriz ortogonal n 3 n y S es una matriz
m 3 n, cuyos elementos diferentes de cero se encuentran a lo largo de la diagonal principal.
En esta secci√≥n supondremos que m ‚â• n, y, en muchas aplicaciones importantes, m es mucho
m√°s grande que n.
/DGHVFRPSRVLFLyQHQYDORUHVVLQJXODUHVWLHQHXQDKLVWRULDEDVWDQWHODUJDORVPDWHPiWLFRVODFRQVLGHUDURQSRUSULPHUDYH]D√ÄQDOHVGHOVLJOR XIX. Sin embargo, sus aplicaciones
importantes tuvieron que esperar hasta que la potencia computacional estuviera disponible

9.6 Descomposici√≥n en valores singulares

463

en la segunda mitad del siglo XX, cuando los algoritmos pudieron desarrollarse para su implePHQWDFLyQH√ÄFLHQWHeVWHIXHHOWUDEDMRSULQFLSDOGH*HQH*ROXE ¬≤ HQXQDVHULHGH
DUWtFXORVHQODVGpFDGDVGH\ &RQVXOWHHQHVSHFLDO>*.@\>*5@ 8QDKLVWRULD
bastante completa de la t√©cnica puede encontrarse en el art√≠culo de G. W. Stewart, que est√°
GLVSRQLEOHSRUPHGLRGHLQWHUQHWHQODGLUHFFLyQSURYLVWDHQ>6WHZ@
Antes de continuar con la descomposici√≥n en valores singulares, necesitamos describir
algunas propiedades de las matrices arbitrarias.

DeÔ¨Ånici√≥n 9.24

Sea A una matriz m 3 n.
i)

El rango de A, que se denota rango(A HVHOQ~PHURGH√ÄODVOLQHDOPHQWHLQGHSHQdientes en A.

ii)

/Dnulidad de A, que se denota nulidad(A), es n 2 rango(A) y describe el conjunto
m√°s grande de vectores linealmente independientes v en Rn para el que Av 5 0.

El rango y la nulidad de una matriz son importantes para caracterizar la conducta de la
matriz. Cuando la matriz es cuadrada, por ejemplo, es invertible si y s√≥lo si su nulidad es 0 y
su rango es igual al tama√±o de la matriz.
El siguiente es uno de los teoremas b√°sicos en √°lgebra lineal.

Teorema 9.25

(OQ~PHURGH√ÄODVOLQHDOPHQWHLQGHSHQGLHQWHVGHXQDPDWUL]A m 3 nHVHOPLVPRQ~PHUR
de columnas linealmente independientes de A.
1HFHVLWDPRVFRQVLGHUDUODVGRVPDWULFHVFXDGUDGDVUHODFLRQDGDVFRQODPDWUL]A m 3 n,
concretamente, la matriz A t A m 3 n y la matriz A A t.

Teorema 9.26

Sea A una matriz m 3 n.
i)

/DVPDWULFHVA t A y A A t son sim√©tricas.

ii)

1XOLGDG A) 51XOLGDG A t A).

iii)

Rango(A) 5 Rango(A t A).

iv)

/RVHLJHQYDORUHVGHA t A son reales y no negativos.

v)

/RVHLJHQYDORUHVGLIHUHQWHVGHFHURGHA A t son iguales a los eigenvalores de A t A.

Demostraci√≥n i) Puesto que (A t A)t 5 At(At)t 5 A t A, esta matriz es sim√©trica, y, de igual

ii)

forma, lo es A A t.
Sea v = 0 un vector con Av 5 0. Entonces

(At A)v = At (Av) = At 0 = 0, por lo que nulidad(A) ‚â§1XOLGDG A t A).
Ahora suponga que v es un vector n 3FRQA t Av 5 0. Entonces,

0 = vt At Av = (Av)t Av = ||Av||22 , lo cual implica que

Av 5 0.

3RU OR WDQWR 1XOLGDG A t A) ‚â§ 1XOLGDG A  3RU FRQVLJXLHQWH 1XOLGDG A t A) 5
1XOLGDG A).
iii)

/DVPDWULFHVA y A t A tienen n columnas y sus nulidades concuerdan, por lo que
Rango(A) 5 n 21XOLGDG A) 5 n 21XOLGDG A t A) 5 Rango(A t A).

CAP√çTULO 9

Aproximaci√≥n de eigenvalores

iv)

/DVPDWULFHVAt A y A At son sim√©tricas mediante la parte i), por lo que el corolario
LPSOLFDTXHVXVHLJHQYDORUHVVRQQ~PHURVUHDOHV6XSRQJDTXHv es un eigenvector de At A con ||v||2 = 1 asociado al eigenvalor Œª. Entonces

0 ‚â§ || Av||22 = (Av)t (Av) = vt At Av = vt (At Av) = vt (Œªv) = Œªvt v = Œª||v||22 = Œª.
  /DGHPRVWUDFLyQGHTXHORVHLJHQYDORUHVGHA At son no negativos sigue la demostraci√≥n de la parte v).
v)

Sea v un eigenvector correspondiente al eigenvalor diferente de cero de A t A. Entonces

At Av = Œªv

implica que (A At )Av = ŒªAv.

Sea Av 5 0, entonces, At Av = At 0 = 0, lo cual contradice la suposici√≥n de que
Œª = 0. Por lo tanto, Av 0 y Av es un eigenvector de A At relacionado con Œª el rec√≠proco tambi√©n sigue este argumento porque si Œª es un eigenvalor diferente de cero
de A At = (At )t At , entonces Œª tambi√©n es un eigenvalor de At (At )t = At A.
(QODVHFFLyQGHOFDStWXORREVHUYDPRVTXpWDQHIHFWLYDSXHGHVHUODIDFWRUL]DFLyQDO
resolver los sistemas lineales de la forma Ax 5 b cuando la matriz A se usa en forma repetida
para variar b. En esta secci√≥n consideraremos una t√©cnica para factorizar una matriz general
m 3 n. Tiene aplicaciones en muchas √°reas, incluyendo el ajuste de datos por m√≠nimos cuadrados, la compresi√≥n de im√°genes, el procesamiento de se√±al y la estad√≠stica.

Construcci√≥n de una descomposici√≥n en valores singulares
para una matriz A m 3 n
Una matriz no cuadrada AHVGHFLUFRQXQQ~PHURGLIHUHQWHGH√ÄODV\FROXPQDVQRSXHGH
tener un eigenvalor porque Ax y x ser√°n vectores de diferentes tama√±os. Sin embargo, existen
Q~PHURVTXHGHVHPSHxDQIXQFLRQHVSDUDODVPDWULFHVQRFXDGUDGDVTXHVRQVLPLODUHVDODV
representadas por los eigenvalores para las matrices cuadradas. Una de las caracter√≠sticas
m√°s importantes de la descomposici√≥n en valores singulares de una matriz general es que
permite una generalizaci√≥n de eigenvalores y eigenvectores en esta situaci√≥n.
1XHVWURREMHWLYRHVGHWHUPLQDUXQDIDFWRUL]DFLyQGHODPDWUL]A m 3 n, donde m ‚â• n,
en la forma

A = U S V t,
donde U es una matriz ortogonal m 3 m, V es una matriz ortogonal n 3 n y S es una matriz
diagonal m 3 n; es decir, s√≥lo entradas diferentes de cero (S)ii ‚â° si ‚â• 0, para i = 1, . . . , n.
&RQVXOWHOD√ÄJXUD

Figura 9.2

columnas n

S

columnas m

columnas n

Vt
filas n

filas m

=

filas m

U

A
filas m

464

columnas n

9.6 Descomposici√≥n en valores singulares

465

Construcci√≥n de S en la factorizaci√≥n A 5 U S V t
Construimos la matriz S al encontrar los eigenvalores de la matriz sim√©trica At A n 3 n. Estos
HLJHQYDORUHVVRQWRGRVQ~PHURVUHDOHVQRQHJDWLYRVORVRUGHQDPRVGHPD\RUDPHQRU\ORV
denotamos mediante

s12 ‚â• s22 ‚â• ¬∑ ¬∑ ¬∑ ‚â• sk2 > sk+1 = ¬∑ ¬∑ ¬∑ = sn = 0.
Es decir, con Sk denotamos el eigenvalor m√°s peque√±o diferente de cero de At A/DVUDtFHV
cuadradas positivas de estos eigenvalores de At A dan las entradas diagonales en S. Reciben
el nombre de valores singulares de A. Por lo tanto.
‚é§
‚é°
s1 0 ¬∑ ¬∑ ¬∑ 0
‚é¢
. ‚é•
‚é¢ 0 s2 . . . .. ‚é•
‚é•
‚é¢
‚é•
‚é¢ .. . .
..
‚é•
‚é¢ .
.
.
0
‚é•
‚é¢
S = ‚é¢ 0 ¬∑ ¬∑ ¬∑ 0 sn ‚é•,
‚é•
‚é¢
‚é¢ 0 ¬∑¬∑¬∑ ¬∑¬∑¬∑ 0 ‚é•
‚é•
‚é¢
‚é¢ .
.. ‚é•
‚é£ ..
. ‚é¶
0 ¬∑¬∑¬∑ ¬∑¬∑¬∑ 0
donde si = 0 cuando k < i ‚â§ n.
DeÔ¨Ånici√≥n 9.27
Ejemplo 1

/RV valores singulares de una matriz A m 3 n son las ra√≠ces cuadradas positivas de los
eigenvalores diferentes de cero de la matriz sim√©trica At A n 3 n.
Determine los valores singulares de la matriz 5 3
‚é§
‚é°
1 0 1
‚é¢ 0 1 0 ‚é•
‚é•
‚é¢
‚é•
A=‚é¢
‚é¢ 0 1 1 ‚é•.
‚é£ 0 1 0 ‚é¶
1 1 0
Soluci√≥n

Tenemos
‚é°

‚é§
1 0 0 0 1
At = ‚é£ 0 1 1 1 1 ‚é¶,
1 0 1 0 0

‚é°

‚é§
2 1 1
por lo que At A = ‚é£ 1 4 1 ‚é¶.
1 1 2

El polinomio caracter√≠stico de At A es

œÅ(At A) = Œª3 ‚àí 8Œª2 + 17Œª ‚àí 10 = (Œª ‚àí 5)(Œª ‚àí 2)(Œª ‚àí 1),
2
2
por lo que los eigenvalores de At A son Œª1 = s12 = ‚àö5, Œª2 = s‚àö
2 = 2, y Œª3 = s3 = 1. Por
consiguiente, los valores singulares de A son s1 = 5, s2 = 2, y s3 = 1 y en la descomposici√≥n en valores singulares de A, tenemos
‚é° ‚àö
‚é§
5 ‚àö0 0
‚é¢ 0
2 0 ‚é•
‚é¢
‚é•
‚é¢
S=‚é¢ 0
0 1 ‚é•
‚é•.
‚é£ 0
0 0 ‚é¶
0
0 0

466

CAP√çTULO 9

Aproximaci√≥n de eigenvalores

Cuando A es una matriz sim√©trica n 3 n, todas las si2 son eigenvalores de A2 = At A,
y √©stos son cuadrados de los eigenvalores de A &RQVXOWHHOHMHUFLFLRGHODVHFFLyQ 
Por lo que en este caso, los valores singulares son los valores absolutos de los eigenvalores
de A. En el caso especial cuando AHVGH√ÄQLGDSRVLWLYD RLQFOXVRGH√ÄQLGDQRQHJDWLYD ORV
eigenvalores y valores singulares de A son los mismos.
Construcci√≥n de V en la factorizaci√≥n A 5 U S V t
/DPDWUL]At A n 3 nHVVLPpWULFDSRUORTXHHOWHRUHPDHQODVHFFLyQ FRQVXOWHOD
SiJLQD WHQHPRVXQDIDFWRUL]DFLyQ

At A = V D V t ,
donde D es una matriz diagonal cuya diagonal consiste en eigenvalores de At A y V es una
matriz ortogonal cuya i-√©sima columna es un eigenvector con normal asociado al eigenvalor de la i-√©sima diagonal de D/DPDWUL]GLDJRQDOHVSHFt√ÄFDGHSHQGHGHORUGHQGHORV
eigenvalores a lo largo de la diagonal. Seleccionamos D de tal forma que se escribe en orden
GHFUHFLHQWH/DVFROXPQDVGHQRWDGDV vt1 , vt2 , . . . , vtn , de la matriz ortogonal V n 3 n son
HLJHQYHFWRUHVRUWRQRUPDOHVFRUUHVSRQGLHQWHVDHVWRVHLJHQYDORUHV0~OWLSOHVHLJHQYDORUHVGH
At A permiten diversas selecciones de eigenvectores correspondientes, por lo que a pesar
de que DHVWiGH√ÄQLGDGHPDQHUD~QLFDODPDWUL]V podr√≠a no estarlo. Sin embargo, no hay
problema ya que podemos seleccionar cualquier V. Puesto que todos los eigenvalores de At A
son no negativos, tenemos dii = si2 para 1 ‚â§ i ‚â§ n.
Construcci√≥n de U en la factorizaci√≥n A 5 U S V t
Para construir la matriz U m 3 m, primero consideramos los valores diferentes de
cero s1 ‚â• s2 ‚â• ¬∑ ¬∑ ¬∑ ‚â• sk > 0 y las columnas correspondientes en V determinadas por
v1 , v2 , . . . , vk . 'H√ÄQLPRV

ui =

1
Avi ,
si

para i = 1, 2, . . . , k.

/DVXVDPRVFRPRODSULPHUDk de las m columnas de U. Puesto que A es m 3 n y cada vi es
n 3HOYHFWRUui es m 3VHJ~QORUHTXHULGR$GHPiVSDUDFDGD1‚â§ i ‚â§ k y 1 ‚â§ j ‚â§ k,
el hecho es que los vectores v1 , v2 , . . . , vn son eigenvectores de At A que forman un conjunto
ortonormal implica que

uit u j =

1
Avi
si

t

1
1 t t
1 t 2
sj
Av j =
vi A Av j =
vi s j v j = vit v j =
sj
si s j
si s j
si

0
1

si i = j,
si i = j.

Por lo que, las primeras k columnas de U forman un conjunto ortonormal de vectores en Rm.
Sin embargo, necesitamos columnas adicionales m 2 k de U. Para esto, primero necesitamos
encontrar los vectores m 2 k que, cuando se a√±adan a los vectores a partir de las primeras k
columnas, nos dar√°n un conjunto linealmente independiente. Entonces, podemos aplicar el
proceso Gram-Schmidt para obtener las columnas adicionales adecuadas.
/DPDWUL]U QRVHUi~QLFDDPHQRVTXHk 5 m y entonces s√≥lo si todos los eigenvalores
de At AVRQ~QLFRV/DQRVLQJXODULGDGQRHVSUHRFXSDQWHV√≥lo necesitamos una matriz U de
este tipo.
9HUL√ÄFDFLyQGHODIDFWRUL]DFLyQA 5 U S V t
3DUDYHUL√ÄFDUTXHHVWHSURFHVRHQUHDOLGDGGDODIDFWRUL]DFLyQA = U SV t , primero recordemos que la transpuesta de una matriz ortogonal tambi√©n es la inversa de la matriz (consulte
la parte (i GHODVHFFLyQGHOWHRUHPDHQODSiJLQD 3RUORWDQWRSDUDPRVWUDU
A = U SV t , podemos mostrar la declaraci√≥n equivalente AV = U S.

9.6 Descomposici√≥n en valores singulares

467

/RVYHFWRUHV v1 , v2 , . . . , vn forman una base para Rn, Avi = si ui , para i = 1, . . . , k,
y Avi = 0, para i = k + 1, . . . , n. S√≥lo las primeras k columnas de U producen entradas
diferentes de cero en el producto US, por lo que tenemos

AV = A [v1 v2 ¬∑ ¬∑ ¬∑ vk vk+1 ¬∑ ¬∑ ¬∑ vn ]
= [Av1 Av2 ¬∑ ¬∑ ¬∑ Avk Avk+1 ¬∑ ¬∑ ¬∑ Avn ]
= [s1 u1 s2 u2 ¬∑ ¬∑ ¬∑ sk uk 0 ¬∑ ¬∑ ¬∑ 0]
‚é°
s1
‚é¢
‚é¢ 0
‚é¢
‚é¢ ..
‚é¢ .
‚é¢
= [u1 u2 ¬∑ ¬∑ ¬∑ uk 0 ¬∑ ¬∑ ¬∑ 0] ‚é¢ 0
‚é¢
‚é¢ 0
‚é¢
‚é¢ .
‚é£ ..

¬∑¬∑¬∑
..
.
..
..
.
.
¬∑¬∑¬∑ 0
¬∑¬∑¬∑ ¬∑¬∑¬∑
0
..
.

¬∑¬∑¬∑ ¬∑¬∑¬∑

0

0
..
.
..
.
sk
0
..
.

0
..
.
..
.
0
0
..
.

0

0

¬∑¬∑¬∑

¬∑¬∑¬∑
¬∑¬∑¬∑
¬∑¬∑¬∑

‚é§
0
.. ‚é•
. ‚é•
‚é•
.. ‚é•
. ‚é•
‚é•
= U S.
0 ‚é•
‚é•
0 ‚é•
‚é•
.. ‚é•
. ‚é¶
0

Esto completa la construcci√≥n de la descomposici√≥n en valores singulares de A.
Ejemplo 2

Determine la descomposici√≥n en valores singulares de la matriz 5 3
‚é°
‚é§
1 0 1
‚é¢ 0 1 0 ‚é•
‚é¢
‚é•
‚é•
A=‚é¢
‚é¢ 0 1 1 ‚é•.
‚é£ 0 1 0 ‚é¶
1 1 0
Soluci√≥n (QHOHMHPSORHQFRQWUDPRVTXHA tiene valores singulares s1 =

‚àö

5, s2 =

‚àö

2,

y s 5SRUORTXH

‚é° ‚àö
5
‚é¢ 0
‚é¢
S=‚é¢
‚é¢ 0
‚é£ 0
0

‚é§
‚àö0 0
2 0 ‚é•
‚é•
0 1 ‚é•
‚é•.
0 0 ‚é¶
0 0
‚àö
‚àö
/RVHLJHQYHFWRUHVGHAt A correspondientes a s1 = 5, s2 = 2, y s 5VRQUHVSHFWLvamente, (1, 2, 1)t , (1, ‚àí1, 1)t , y (‚àí1, 0, 1)t (consulte el ejercicio 5). Al normalizar estos
vectores y usar los valores para las columnas V obtenemos
‚àö ‚é§
‚àö
‚àö
‚àö ‚é§
‚é° ‚àö
‚é° ‚àö
6
3
6
6
6
‚àí 22
6
3
6
3
6
‚àö
‚àö
‚àö
‚àö ‚é•
‚é¢ ‚àö
‚é•
‚é¢
6
3
3
3
3 ‚é•.
V =‚é¢
y Vt = ‚é¢
0 ‚é•
‚é£ ‚àö3 ‚àí ‚àö3
‚é¶
‚é£ ‚àö3 ‚àí 3
3 ‚é¶
‚àö
‚àö
6
3
2
2
‚àí 22
0
6
3
2
2
/DVSULPHUDVWUHVFROXPQDVGHU son, por lo tanto,

u1 = ‚àö15 ¬∑ A
u2 = ‚àö12 ¬∑ A

‚àö

‚àö
‚àö
6
6
6
,
,
6
3
6

t

‚àö

‚àö
‚àö
3
, ‚àí 33 , 33
3

‚àö

‚àö

u3 = 1 ¬∑ A ‚àí 22 , 0, 22

t

‚àö

‚àö
‚àö
‚àö
‚àö
30
30
30
30
30
,
,
,
,
15
15
10
15
10

=
t

=

‚àö

t

,

‚àö
‚àö
t
6
, ‚àí 66 , 0, ‚àí 66 , 0 , y
3
‚àö

‚àö

= 0, 0, 22 , 0, ‚àí 22

t

.

468

CAP√çTULO 9

Aproximaci√≥n de eigenvalores

Para determinar las dos columnas restantes de U primero necesitamos dos vectores x4 y
x5 por lo que {u1 , u2 , u3 , x4 , x5 } es un conjunto linealmente independiente. Entonces, aplicamos el proceso Gram-Schmidt para obtener u4 y u5 de tal forma que {u, u, u, u4, u5}
es un conjunto ortogonal. Dos vectores que satisfacen el requisito de independencia lineal y
ortogonalidad son

u4 = (1, 1, ‚àí1, 1, ‚àí1)t

y

u5 = (0, 1, 0, ‚àí1, 0)t .

Al normalizar los vectores ui, para i 5\SURGXFHODPDWUL]U y la descomposici√≥n
en valores singulares como
‚àö
‚àö
‚é§
‚é° ‚àö
30
6
5
‚é§
‚é° ‚àö
0
0
‚àö3
‚àö5
‚àö ‚é•
5 ‚àö0 0
‚é¢ ‚àö1530
6
5
2 ‚é•
‚é¢
0
2 ‚é•‚é¢
‚é¢ ‚àö15 ‚àí 6
2 0 ‚é•
0
‚àö
‚àö5
‚é•
‚é•‚é¢
‚é¢ 30
t
2
5
‚é¢
A = U S V = ‚é¢ 10
0 1 ‚é•
0
‚àí 5
0 ‚é•‚é¢ 0
‚é•
2
‚é¢ ‚àö
‚àö
‚àö
‚àö ‚é•‚é£
‚é¢ 30 ‚àí 6
0
0 0 ‚é¶
5
2 ‚é•
0
‚àí
‚é¶
‚é£ ‚àö15
6
2
‚àö
‚àö5
0
0 0
30
2
5
0
‚àí
‚àí
0
10
2
5
‚àö
‚àö ‚é§
‚é° ‚àö6
6
6

‚é¢
√ó‚é£

‚àö6
3
‚àö3
‚àí 22

‚àö3

‚àí 33
0

‚àö6
3
‚àö3
2
2

‚é•
‚é¶.

8QDGL√ÄFXOWDGFRQHOSURFHVRHQHOHMHPSORHVODQHFHVLGDGGHGHWHUPLQDUORVYHFWRUHV
adicionales x4 y x5 para proporcionar un conjunto linealmente independiente sobre el que
podemos aplicar el proceso de Gram-Schmidt. Ahora consideraremos una forma de simpli√ÄFDUOR
Un m√©todo alternativo para encontrar U
/DSDUWHY GHOWHRUHPDHVWDEOHFHTXHORVHLJHQYDORUHVGLIHUHQWHVGHFHURGHAt A y los
de A At son iguales. Adem√°s, los eigenvectores correspondientes de las matrices sim√©tricas
At A y A At forman subconjuntos ortonormales completos de Rn y Rm, respectivamente. As√≠ el
conjunto ortonormal de n eigenvectores para At A forman las columnas de V, como se ha descrito antes y el conjunto ortonormal de m eigenvectores para A At forman las columnas de U
de la misma forma.
En resumen, entonces, para determinar la descomposici√≥n en valores singulares de la
matriz A m 3 nSRGHPRV
‚Ä¢ Encontrar los eigenvalores s12 ‚â• s22 ‚â• ¬∑ ¬∑ ¬∑ ‚â• sk2 ‚â• sk+1 = ¬∑ ¬∑ ¬∑ = sn = 0 para la matriz
sim√©trica At A y colocar la ra√≠z cuadrada positiva de si2 en la entrada (S) ii de la matriz S m 3 n.
‚Ä¢ Encontrar un conjunto de eigenvectores ortonormales {v1 , v2 , . . . , vn } correspondiente a
los eigenvalores de At A y, entonces, construir la matriz V n 3 n con estos vectores como
columnas.
‚Ä¢ Encontrar un conjunto de eigenvectores {u1 , u2 , . . . , um } correspondientes a los eigenvalores de A At y construir la matriz U m 3 m con estos vectores como columnas.
Entonces A tiene la descomposici√≥n en valores singulares A = U S V t .
Ejemplo 3

Determine la descomposici√≥n en valores singulares de la matriz 5 3
‚é°
‚é§
1 0 1
‚é¢ 0 1 0 ‚é•
‚é¢
‚é•
‚é•
A=‚é¢
‚é¢ 0 1 1 ‚é•
‚é£ 0 1 0 ‚é¶
1 1 0
al determinar U a partir de los eigenvectores de A At.

9.6 Descomposici√≥n en valores singulares
Soluci√≥n

469

Tenemos

‚é°

1
‚é¢ 0
‚é¢
A At = ‚é¢
‚é¢ 0
‚é£ 0
1

0
1
1
1
1

‚é§
‚é°
1 ‚é°
2
‚é§
‚é¢ 0
0 ‚é•
1
0
0
0
1
‚é•
‚é¢
‚é£
‚é¶ ‚é¢
1 ‚é•
‚é• 0 1 1 1 1 =‚é¢ 1
‚é£ 0
0 ‚é¶ 1 0 1 0 0
0
1

0
1
1
1
1

1
1
2
1
1

0
1
1
1
1

‚é§
1
1 ‚é•
‚é•
1 ‚é•
‚é•,
1 ‚é¶
2

que tiene los mismos eigenvalores diferentes de cero como A t A, es decir, Œª1 = 5, Œª2 = 2,
y Œª3 = 1 y, adem√°s, Œª4 = 0 y Œª5 = 0. /RVHLJHQYHFWRUHVFRUUHVSRQGLHQWHVDHVWRVHLJHQvalores son, respectivamente,

x1 = (2, 2, 3, 2, 3)t ,
x3 = (0, 0, 1, 0, ‚àí1)t ,

x2 = (2, ‚àí1, 0, ‚àí1, 0)t ,
x4 = (1, 2, ‚àí1, 0, ‚àí1)t ,

y x5 = (0, 1, 0, ‚àí1, 0)t .
Ambos conjuntos {x, x, x, x4} y {x, x, x, x5} son ortogonales porque son eigenvectores relacionados con distintos eigenvalores de la matriz sim√©trica A At. Sin embargo, x4 no
es ortogonal a x5. Mantendremos x4 como uno de los eigenvectores que se usan para formar U
y determinar el quinto vector que proporcionar√° el conjunto ortogonal. Para esto, usamos
HOSURFHVR*UDP6FKPLGWFRPRVHGHVFULEHHQHOWHRUHPDHQODSiJLQD8VDQGROD
notaci√≥n en ese teorema, tenemos

v1 = x1 , v2 = x2 , v3 = x3 , v4 = x4 ,
y, puesto que x5 es ortogonal para todo menos x4,

v5 = x5 ‚àí

vt4 x5
v
vt4 v4

= (0, 1, 0, ‚àí1, 0)t ‚àí

(1, 2, ‚àí1, 0, ‚àí1) ¬∑ (0, 1, 0, ‚àí1, 0)t
(1, 2, ‚àí1, 0, ‚àí1)
||(1, 2, ‚àí1, 0, ‚àí1)t ||22

2
1
= (0, 1, 0, ‚àí1, 0)t ‚àí (1, 2, ‚àí1, 0, ‚àí1)t = ‚àí (2, ‚àí3, ‚àí2, 7, ‚àí2)t .
7
7
6HYHUL√ÄFDIiFLOPHQWHTXHv5 es ortogonal a v4 5 x4. Tambi√©n es ortogonal para los vectores
en {v, v, v} porque es una combinaci√≥n lineal de x4 y x5. Al normalizar estos vectores
obtenemos la matriz U en la factorizaci√≥n. Por lo tanto,
‚àö
‚àö
‚àö
‚é§
‚é° ‚àö
30
6
7
70
0
15
3
7
35
‚àö
‚àö
‚àö
‚é•
‚é¢ ‚àö30
6
2 7
‚é¢
0
‚àí 3 7070 ‚é•
7
‚é•
‚é¢ ‚àö15 ‚àí 6
‚àö
‚àö
‚àö
‚é¢
2
7
70 ‚é• .
U = [u1 , u2 , u3 , u4 , u5 ] = ‚é¢ 1030
0
‚àí 7
‚àí 35 ‚é•
2
‚é•
‚é¢ ‚àö
‚àö
‚àö
‚é¢ 30 ‚àí 6
70 ‚é•
0
0
‚é¶
‚é£ ‚àö15
6
10
‚àö
‚àö
‚àö
30
2
7
70
0 ‚àí 2
‚àí 7
‚àí 35
10
Esta es una UGLIHUHQWHDODTXHVHHQFRQWUyHQHOHMHPSORSHURGDXQDIDFWRUL]DFLyQYiOLGD
A = U S V t usando las mismas S y V que en ese ejemplo.

Aproximaci√≥n por m√≠nimos cuadrados
/DGHVFRPSRVLFLyQHQYDORUHVVLQJXODUHVVHDSOLFDHQPXFKDViUHDVXQDGHODVFXDOHVHVXQ
medio alterno para encontrar los polinomios de m√≠nimos cuadrados para ajustar datos. Sea A
una matriz m 3 n, con m . n, y b un vector en Rm. El objetivo de m√≠nimos cuadrados es
encontrar un vector x en Rn que minimizar√° ||Ax ‚àí b||2 .

470

CAP√çTULO 9

Aproximaci√≥n de eigenvalores

Suponga que se conoce la descomposici√≥n en valores singulares de A, es decir,

A = U S V t,
donde U es una matriz ortogonal m 3 m, V es una matriz ortogonal n 3 n y S es una matriz
m 3 n que contiene los valores singulares diferentes de cero en orden decreciente a lo largo de la diagonal principal en las primeras k ‚â§ n√ÄODV\HQWUDGDVFHURHQRWUDSDUWH3XHVWR
que tanto U como V son ortogonales, tenemos U ‚àí1 = U t , V ‚àí1 = V t , y mediante la parte
LLL GHOWHRUHPDHQODVHFFLyQHQODSiJLQDU y V preservan la norma l. Por
consiguiente,

||Ax ‚àí b||2 = ||U S V t x ‚àí U U t b||2 = ||S V t x ‚àí U t b||2 .
Sea z = V t x y c = U t b. Entonces

||Ax ‚àí b||2 = ||(s1 z 1 ‚àí c1 , s2 z 2 ‚àí c2 , . . . , sk z k ‚àí ck , ‚àíck+1 , . . . , ‚àícm )t ||2
k

=

(si z i ‚àí ci ) +
i=1

1/2

m

(ci )

2

2

.

i=k+1

/DQRUPDVHPLQLPL]DFXDQGRVHVHOHFFLRQDHOYHFWRUz con

‚éß
‚é® ci ,
cuando i ‚â§ k,
z i = si
‚é©arbitrario, cuando k < i ‚â§ n.
Puesto que tanto c = U t b como x = V z son f√°ciles de calcular, la soluci√≥n de m√≠nimos
cuadrados tambi√©n es f√°cil de encontrar.
Ejemplo 4

Use la t√©cnica de descomposici√≥n en valores singulares para determinar el polinomio de
PtQLPRVFXDGUDGRVGHJUDGRSDUDORVGDWRVSURYLVWRVHQODWDEOD

Tabla 9.5
i

xi

yi

1
2
3
4
5

0
0.25
0.50
0.75
1.00

1.0000
1.2840
1.6487
2.1170
2.7183

Soluci√≥n (VWHSUREOHPDVHUHVROYLyXVDQGRHFXDFLRQHVQRUPDOHVFRPRHQHOHMHPSORHQ
ODVHFFLyQ3ULPHURQHFHVLWDPRVGHWHUPLQDUODIRUPDDGHFXDGDSDUDA, x y b. En el ejemSORHQODVHFFLyQHOSUREOHPDVHGHVFULELyFRPRHQFRQWUDUa0 , a1 y a2 con

P2 (x) = a0 + a1 x + a2 x 2 .
$√ÄQGHH[SUHVDUHVWRHQIRUPDGHPDWUL]KDFHPRV
‚é°
‚é§ ‚é°
‚é§
y0
1.0000
‚é§
‚é°
‚é¢ y1 ‚é• ‚é¢ 1.2840 ‚é•
a0
‚é¢
‚é• ‚é¢
‚é•
‚é• ‚é¢
‚é•
‚é¶
‚é£
x = a1 , b = ‚é¢
‚é¢ y2 ‚é• = ‚é¢ 1.6487 ‚é• , y
‚é£
‚é¶
‚é£
2.1170 ‚é¶
a2
y3
2.7183
y4
‚é°
‚é§
‚é°
‚é§
1 x0 x02
1 0
0
‚é¢ 1 x1 x 2 ‚é• ‚é¢ 1 0.25 0.0625 ‚é•
1 ‚é•
‚é¢
‚é¢
‚é•
2 ‚é•
‚é¢
‚é•.
1
x
x
0.25
A=‚é¢
2
2 ‚é• = ‚é¢ 1 0.5
‚é¢
‚é•
‚é£ 1 x3 x32 ‚é¶ ‚é£ 1 0.75 0.5625 ‚é¶
1 1
1
1 x4 x42

9.6 Descomposici√≥n en valores singulares

471

/DGHVFRPSRVLFLyQHQYDORUHVVLQJXODUHVGHA tiene la forma A = U S V t, donde
‚é°
‚é§
‚àí0.2945 ‚àí0.6327
0.6314 ‚àí0.0143 ‚àí0.3378
‚é¢ ‚àí0.3466 ‚àí0.4550 ‚àí0.2104
0.2555
0.7505 ‚é•
‚é¢
‚é•
‚é¢
U = ‚é¢ ‚àí0.4159 ‚àí0.1942 ‚àí0.5244 ‚àí0.6809 ‚àí0.2250 ‚é•
‚é•,
‚é£ ‚àí0.5025
0.1497 ‚àí0.3107
0.6524 ‚àí0.4505 ‚é¶
‚àí0.6063
0.5767
0.4308 ‚àí0.2127
0.2628
‚é°
‚é§
2.7117 0
0
‚é°
‚é§
‚é¢ 0
‚é•
0.9371
0
‚àí0.7987 ‚àí0.4712 ‚àí0.3742
‚é¢
‚é•
t
‚é£
0
0.1627 ‚é•
0.5102
0.6231 ‚é¶.
S=‚é¢
‚é¢ 0
‚é•, y V = ‚àí0.5929
‚é£ 0
‚é¶
0
0
0.1027 ‚àí0.7195
0.6869
0
0
0
Por lo que,

‚é°

‚é§ ‚é°
y0
‚àí0.2945
‚é¢ y1 ‚é• ‚é¢ ‚àí0.3466
‚é¢
‚é• ‚é¢
‚é• ‚é¢
c = Ut ‚é¢
‚é¢ y2 ‚é• = ‚é¢ ‚àí0.4159
‚é£ y3 ‚é¶ ‚é£ ‚àí0.5025
‚àí0.6063
y4
‚é°
‚é§
‚àí4.1372
‚é¢ 0.3473 ‚é•
‚é¢
‚é•
‚é•
=‚é¢
‚é¢ 0.0099 ‚é• ,
‚é£ ‚àí0.0059 ‚é¶
0.0155

‚àí0.6327
‚àí0.4550
‚àí0.1942
0.1497
0.5767

0.6314
‚àí0.2104
‚àí0.5244
‚àí0.3107
0.4308

‚àí0.0143
0.2555
‚àí0.6809
0.6524
‚àí0.2127

‚é§t ‚é°
‚é§
‚àí0.3378
1
‚é¢
‚é•
0.7505 ‚é•
‚é• ‚é¢ 1.284 ‚é•
‚é•
‚é¢
‚àí0.2250 ‚é• ‚é¢ 1.6487‚é•
‚é•
‚àí0.4505 ‚é¶ ‚é£ 2.117 ‚é¶
0.2628
2.7183

y los componentes de z son

z1 =

c1
‚àí4.1372
= ‚àí1.526,
=
s1
2.7117

z3 =

c3
0.0099
= 0.0609.
=
s3
0.1627

z2 =

c2
0.3473
= 0.3706, y
=
s2
0.9371

(VWRGDORVFRH√ÄFLHQWHVGHPtQLPRVFXDGUDGRVSDUDP(x) como

‚é°

‚é§
‚é°
a0
‚àí0.7987
‚é£ a1 ‚é¶ = x = V z = ‚é£ ‚àí0.4712
‚àí0.3742
a2

‚àí0.5929
0.5102
0.6231

‚é§‚é°
‚é§ ‚é°
‚é§
0.1027
‚àí1.526
1.005
‚àí0.7195 ‚é¶ ‚é£ 0.3706 ‚é¶ = ‚é£ 0.8642 ‚é¶,
0.6869
0.0609
0.8437

ORFXDOFRQFXHUGDFRQORVUHVXOWDGRVHQHOHMHPSORGHODVHFFLyQ(OHUURUGHPtQLPRV
cuadrados usando estos valores utiliza por lo menos dos componentes de c y es

||Ax ‚àí b||2 =

c42 + c52 =

(‚àí0.0059)2 + (0.0155)2 = 0.0165.

Otras aplicaciones
/DUD]yQGHODLPSRUWDQFLDGHODGHVFRPSRVLFLyQHQYDORUHVVLQJXODUHVHQPXFKDVDSOLFDciones es que nos permite obtener las caracter√≠sticas m√°s importantes de una matriz m 3 n
XVDQGR XQD PDWUL] TXH D PHQXGR HV GH WDPDxR VLJQL√ÄFDWLYDPHQWH PiV SHTXHxR 3XHVWR
que los valores singulares est√°n en la diagonal de S en orden decreciente, retener solamente
ODV√ÄODV\FROXPQDVk de S produce la mejor aproximaci√≥n posible para la matriz A. Como
LOXVWUDFLyQFRQVXOWHOD√ÄJXUDTXHLQGLFDODGHVFRPSRVLFLyQHQYDORUHVVLQJXODUHVGHOD
matriz A n 3 n.

CAP√çTULO 9

Aproximaci√≥n de eigenvalores

Figura 9.3

S
filas n

filas m

filas m

=

columnas n

Vt

filas m

U

A

columnas n

columnas m

columnas n

Reemplace la matriz S n 3 n con la matriz Sk k 3 k que contiene los valores singulares
PiVVLJQL√ÄFDWLYRV&RQWRGDFHUWH]DpVWRVVHUiQVRODPHQWHORVGLIHUHQWHVGHFHURSHURWDPbi√©n podr√≠amos eliminar algunos valores singulares que son relativamente peque√±os.
Determine las matrices Uk y Vkt , correspondientes k 3 n y m 3 k respectivamente, de
acuerdo con el procedimiento de descomposici√≥n en valores singulares. √âsto se muestra
VRPEUHDGRHQOD√ÄJXUD
Figura 9.4

columnas n

Sk

filas m

Uk
filas k

=

Vk t
filas k

Ak ‚¨üa
filas m

472

columnas k

columnas k

columnas n

Entonces la nueva matriz Ak = Uk Sk Vkt sigue siendo de tama√±o m 3 n y requerir√≠a
m ¬∑ n registros de almacenamiento para su representaci√≥n. Sin embargo, en forma factorizada, el requisito de almacenamiento para los datos es m ¬∑ k para Uk, k para Sk, y n u k para
Vkt para un total de k(m 1 n 1 
Suponga, por ejemplo, que m = 2n y k = n/3. Entonces la matriz original A contiene
mn = 2n 2SXQWRVGHGDWRV/DIDFWRUL]DFLyQTXHSURGXFHAk, sin embargo, contiene solamente mk = 2n 2 /3 para Uk , k para Sk, y nk = n 2 /3 para Vkt puntos de datos, que ocupan
un total de (n/3)(3n 2 + 1) registros de almacenamiento. √âsta es una reducci√≥n de aproximaGDPHQWHDSDUWLUGHODFDQWLGDGUHTXHULGDSDUDDOPDFHQDUWRGDODPDWUL]A y los resultados en lo que se conoce como compresi√≥n de datos.
(QHOHMHPSORGHPRVWUDPRVTXH
‚àö
‚àö
‚é° ‚àö
30
6
5
0
3
5
‚é¢ ‚àö15
‚àö
‚àö
‚é¢ 30 ‚àí 6
5
0
‚é¢ 15
6
5
‚é¢ ‚àö
‚àö
‚àö
‚é¢ 30
2
0
‚àí 55
A = U S V t = ‚é¢ 10
2
‚é¢ ‚àö
‚àö
‚àö
‚é¢ 30 ‚àí 6
5
0
‚é¢ 15
6
5
‚é£ ‚àö
‚àö
‚àö
30
0 ‚àí 22 ‚àí 55
10

Ilustraci√≥n

0

‚é§

‚é•‚é° ‚àö
5
‚é•
‚é•‚é¢
‚é•‚é¢ 0
0 ‚é•
‚é•‚é¢
‚é¢ 0
‚àö ‚é•‚é£
0
2 ‚é•
‚àí 2 ‚é•
0
‚é¶
0
‚àö
2
2

‚àö0
2
0
0
0

‚é§
0 ‚é° ‚àö6
6
0 ‚é•
‚é•‚é¢ ‚àö
3
‚é¢
1 ‚é•
‚é•‚é£
3
‚àö
0 ‚é¶
‚àí 22
0

‚àö
6
3
‚àö
‚àí 33

0

‚àö

‚é§

6
6
‚àö ‚é•
3 ‚é•.
3 ‚é¶
‚àö
2
2

9.6 Descomposici√≥n en valores singulares

473

Considere las matrices reducidas relacionadas con esta factorizaci√≥n
‚àö
‚é° ‚àö
‚é§
30
6
0
15
3
‚é¢ ‚àö
‚é•
‚àö
‚é¢ 30 ‚àí 6
‚é•
‚é§
‚é° ‚àö
0
‚é¢ 15
‚é•
6
5 ‚àö0 0
‚é¢ ‚àö
‚é•
‚àö
‚é¢ 30
2 ‚é•
0
U3 = ‚é¢ 10
‚é• , S3 = ‚é£ 0
2 0 ‚é¶, y
2
‚é¢ ‚àö
‚é•
‚àö
‚é¢ 30 ‚àí 6
0
0 1
0 ‚é•
‚é¢ 15
‚é•
6
‚é£ ‚àö
‚àö ‚é¶
30
0
‚àí 22
10

‚é°
‚é¢
V3t = ‚é¢
‚é£

‚àö

6
6
‚àö
3
3
‚àö

‚àö
6
3
‚àö

‚àí 33

‚àí 22

0

‚àö
6
6
‚àö
3
3
‚àö
2
2

‚é§
‚é•
‚é•.
‚é¶

Entonces

‚é°
‚é¢
S3 V3t = ‚é¢
‚é£

‚àö

30
6
‚àö
6
3
‚àö
‚àí 22

‚àö
30
3
‚àö
‚àí 36

0

‚àö
30
6
‚àö
6
3
‚àö
2
2

‚é°

‚é§
‚é•
‚é•
‚é¶

y

1
‚é¢ 0
‚é¢
A3 = U3 S3 V3t = ‚é¢
‚é¢ 0
‚é£ 0
1

0
1
1
1
1

‚é§
1
0 ‚é•
‚é•
1 ‚é•
‚é•.
0 ‚é¶
0

Puesto que los c√°lculos en la ilustraci√≥n se realizaron usando aritm√©tica exacta, la matriz A
concuerda precisamente con la matriz original A. En general, se usar√≠a la aritm√©tica de d√≠giWRV√ÄQLWRVSDUDKDFHUORVFiOFXORV\QRVHHVSHUDUtDXQDFRQFRUGDQFLDDEVROXWD/DHVSHUDQ]D
es que la compresi√≥n de datos no resulte en una matriz AkTXHGL√ÄHUDVLJQL√ÄFDWLYDPHQWHGHOD
matriz original A y esto depende de las magnitudes relativas de los valores singulares de A.
Cuando el rango de la matriz A es k, no habr√° deterioro ya que s√≥lo existen k√ÄODVGHODPDWUL]
original A que son linealmente independientes y, en teor√≠a, la matriz podr√≠a reducirse en una
TXHWLHQHWRGRVORVFHURVHQVXV~OWLPDV√ÄODVm 2 k o columnas n 2 k. Cuando k es menor al
rango de A, Ak diferir√° de A, pero no siempre ser√° en su detrimento.
Considere la situaci√≥n que se presenta cuando A es una matriz que consiste en pixeles en
una fotograf√≠a en escala de grises, que tal vez se tom√≥ a gran distancia, como una fotograf√≠a de
sat√©lite de una parte de la Tierra. Es probable que la fotograf√≠a incluya ruido, es decir, datos que
no representan verdaderamente la imagen, sino el deterioro de √©sta mediante part√≠culas atmosf√©ULFDVODFDOLGDGGHODVOHQWHV\SURFHVRVGHUHSURGXFFLyQ\DVtVXFHVLYDPHQWH/RVGDWRVGHUXLdo se incorporan en los datos determinados en A, pero con suerte este ruido es mucho menos
VLJQL√ÄFDWLYRTXHODYHUGDGHUDLPDJHQ(VSHUDPRVTXHORVYDORUHVVLQJXODUHVPiVJUDQGHV
representen la verdadera imagen y que los m√°s peque√±os, los m√°s cercanos a cero, sean las
contribuciones del ruido. Al realizar la descomposici√≥n en valores singulares que solamente
retiene esos valores singulares por encima de cierto umbral, podr√≠amos ser capaces de eliminar la mayor parte del ruido y, en realidad, obtener una imagen que no s√≥lo sea de menor
tama√±o sino tambi√©n una representaci√≥n verdadera de la fotograf√≠a original. (Consulte [AP]
SDUDPiVGHWDOOHVHVSHFLDOPHQWHOD√ÄJXUD 
Otras aplicaciones importantes de la descomposici√≥n en valores singulares incluyen deWHUPLQDUQ~PHURVGHFRQGLFLyQHIHFWLYRVSDUDODVPDWULFHVFXDGUDGDV FRQVXOWHHOHMHUFLFLR 
determinar el rango efectivo de una matriz y eliminar el ruido de la se√±al. Para m√°s informaci√≥n sobre este importante tema y una interpretaci√≥n geom√©trica de la factorizaci√≥n, examine
HODUWtFXORGH.DOPDQ>.D@\ODVUHIHUHQFLDVHQHVHGRFXPHQWR3DUDXQHVWXGLRPiVFRPSOHWR
\H[WHQVRGHODWHRUtDFRQVXOWH*ROXE\YDQ/RDQ>*9@
La secci√≥n Conjunto de ejercicios 9.6 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

474

CAP√çTULO 9

Aproximaci√≥n de eigenvalores

9.7 Software num√©rico
/DVVXEUXWLQDVHQODVELEOLRWHFDV,06/\1$*DVtFRPRODVVXEUXWLQDVHQ1HWOLE\ORVFRPDQGRVHQ0$7/$%0DSOH\0DWKHPDWLFDHVWiQEDVDGDVHQODVLQFOXLGDVHQORVSDTXHWHV
(,63$&.\/$3$&.TXHVHDQDOL]DQHQODVHFFLyQ(QJHQHUDOODVVXEUXWLQDVWUDQVIRUPDQXQDPDWUL]HQODIRUPDDGHFXDGDSDUDHOPpWRGR45RXQDGHVXVPRGL√ÄFDFLRQHVFRPR
HO PpWRGR 4/ /DV VXEUXWLQDV DSUR[LPDQ WRGRV ORV HLJHQYDORUHV \ SXHGHQ DSUR[LPDU XQ
eigenvector relacionado para cada eigenvalor. En general, las matrices no sim√©tricas se equiOLEUDQGHWDOIRUPDTXHODVVXPDVGHODVPDJQLWXGHVGHODVHQWUDGDVHQFDGD√ÄOD\FROXPQD
son casi iguales. Entonces se aplica el m√©todo Householder para determinar una matriz HesVHQEHUJVXSHULRUVLPLODU/RVHLJHQYDORUHVVHSXHGHQFDOFXODUFRQHOPpWRGR45R4/7DPbi√©n es posible aplicar la forma de Schur S DS t , donde S es ortogonal y la diagonal de D
mantiene los eigenvalores de A/RVHLJHQYHFWRUHVFRUUHVSRQGLHQWHVSXHGHQHQWRQFHVHVWDU
determinados. Para una matriz sim√©trica se calcula una matriz tridiagonal similar. Entonces,
VHSXHGHQFDOFXODUORVHLJHQYHFWRUHVFRUUHVSRQGLHQWHVXVDQGRHOPpWRGR45R4/
Existen rutinas especiales para encontrar todos los eigenvalores con un intervalo o una
regi√≥n, o que s√≥lo encuentran el eigenvalor m√°s grande o m√°s peque√±o. Tambi√©n existen
subrutinas para determinar la precisi√≥n de la aproximaci√≥n del eigenvalor y la sensibilidad
del proceso al error de redondeo.
8Q SURFHGLPLHQWR 0$7/$% TXH FDOFXOD XQ Q~PHUR VHOHFFLRQDGR GH HLJHQYDORUHV \
eigenvectores est√° basado en el m√©todo Arnoldi impl√≠citamente reiniciado de Sorensen [So].
([LVWHXQSDTXHWHGHVRIWZDUHLQFOXLGRHQ1HWOLESDUDUHVROYHUSUREOHPDVGHHLJHQYDORUHVGH
gran dispersi√≥n que tambi√©n est√° basado en el m√©todo Arnoldi impl√≠citamente reiniciado. El
PpWRGR$UQROGLLPSOtFLWDPHQWHUHLQLFLDGRHVXQPpWRGRGHVXEHVSDFLR.U\ORYTXHHQFXHQWUDXQDVXFHVLyQGHVXEHVSDFLRV.U\ORYTXHFRQYHUJHQHQXQVXEHVSDFLRTXHFRQWLHQHORV
eigenvalores.
Las secciones Preguntas de an√°lisis, Conceptos clave y Revisi√≥n del cap√≠tulo est√°n disponibles en l√≠nea. Encuentre la ruta de acceso en las p√°ginas preliminares.

CAP√çTULO

10

Soluciones num√©ricas de sistemas
de ecuaciones no lineales
Introducci√≥n
La cantidad de presi√≥n requerida para hundir un objeto grande y pesado en un suelo homog√©neo y blando que se encuentra sobre tierra de base dura se puede predecir por medio de la
cantidad de presi√≥n requerida para hundir objetos m√°s peque√±os en la misma tierra. Especialmente, la cantidad de presi√≥n p para hundir una placa circular de radio r a una distancia d
en tierra blanda, donde la tierra de base dura se encuentra a una distancia D . d por debajo
GHODVXSHU√ÄFLHVHSXHGHDSUR[LPDUPHGLDQWHXQDHFXDFLyQGHODIRUPD

p = k1 ek2 r + k3r,
donde k1 , k2 y k3 son constantes, dependiendo de d y de la consistencia del suelo pero no
del radio de la placa.
([LVWHQWUHVFRQVWDQWHVGHVFRQRFLGDVHQHVWDHFXDFLyQSRUORTXHWUHVSODFDVSHTXHxDV
FRQGLIHUHQWHVUDGLRVVHKXQGHQDODPLVPDGLVWDQFLD(VWRGHWHUPLQDUiHOWDPDxRPtQLPRGH
la placa requerido para mantener una carga grande. Se registran las cargas necesarias para
HVWHKXQGLPLHQWRFRPRVHPXHVWUDHQOD√ÄJXUDDGMXQWD

m3
m2
m1
r1

r2

r3

Esto produce las tres ecuaciones no lineales

m 1 = k 1 e k2 r1 + k 3 r 1 ,
m 2 = k 1 e k2 r2 + k 3 r 2 y
m 3 = k 1 e k2 r3 + k 3 r 3 .
475

476

CAP√çTULO 10

Soluciones num√©ricas de sistemas de ecuaciones no lineales

en los tres valores desconocidos k1 , k2 y k3 1RUPDOPHQWH ORV PpWRGRV GH DSUR[LPDFLyQ
num√©rica son necesarios para resolver sistemas de ecuaciones cuando √©stas no son lineales.
(OHMHUFLFLRGHODVHFFLyQVHSUHRFXSDSRUXQDDSOLFDFLyQGHOWLSRGHVFULWRDTXt
La resoluci√≥n de un sistema de ecuaciones no lineales es un problema que se evita
VLHPSUHTXHVHDSRVLEOHSRUUHJODJHQHUDODODSUR[LPDUHOVLVWHPDQROLQHDOPHGLDQWHXQ
VLVWHPDGHHFXDFLRQHVOLQHDOHV&XDQGRHVWRQRHVVDWLVIDFWRULRHOSUREOHPDGHEHDERUGDUVH
GHPDQHUDGLUHFWD(OHQIRTXHPiVVHQFLOORHVDGDSWDUORVPpWRGRVGHOFDStWXORORVFXDOHV
DSUR[LPDQODVVROXFLRQHVGHXQDHFXDFLyQQROLQHDOLQGLYLGXDOHQXQDYDULDEOHTXHVHDSOLcar√°n cuando el problema de una sola variable sea reemplazado por un problema vectorial
que incluye todas las variables.
/D SULQFLSDO KHUUDPLHQWD HQ HO FDStWXOR  HUD HO PpWRGR GH 1HZWRQ XQD WpFQLFD TXH
HQ JHQHUDO HV FXDGUiWLFDPHQWH FRQYHUJHQWH eVWD HV OD SULPHUD WpFQLFD TXH PRGL√ÄFDPRV
para resolver los sistemas de ecuaciones no lineales. La aplicaci√≥n delPpWRGRGH1HZWRQ
GHDFXHUGRFRQORPRGL√ÄFDGRSDUDlos sistemas de ecuaciones, es bastante costosa y en la
VHFFLyQGHVFULELPRVFyPRVHSXHGHXVDUHOPpWRGRGHVHFDQWHSDUDREWHQHUDSUR[LPDFLRQHVFRQPD\RUIDFLOLGDGDXQTXHFRQXQDSpUGLGDGHODFRQYHUJHQFLDHQH[WUHPRUiSLGD
TXHSXHGHSURGXFLUHOPpWRGRGH1HZWRQ
La secci√≥n 10.4 describe el m√©todo de descenso m√°s r√°pido. S√≥lo es linealmente converJHQWHSHURQRUHTXLHUHODVDSUR[LPDFLRQHVLQLFLDOHVSUHFLVDVQHFHVDULDVSDUDODVWpFQLFDVPiV
UiSLGDVGHFRQYHUJHQFLD$PHQXGRVHXVDSDUDHQFRQWUDUXQDEXHQDDSUR[LPDFLyQLQLFLDO
SDUDHOPpWRGRGH1HZWRQRXQDGHVXVPRGL√ÄFDFLRQHV
En la secci√≥n 10.5 proporcionamos una introducci√≥n para los m√©todos de continuaci√≥n,
TXHXVDQXQSDUiPHWURSDUDLUGHXQSUREOHPDFRQXQDVROXFLyQIiFLOPHQWHGHWHUPLQDGDDOD
soluci√≥n del problema no lineal original.
(Q HVWH FDStWXOR VH RPLWHQ PXFKDV GH ODV SUXHEDV GH ORV UHVXOWDGRV WHyULFRV SRUTXH
LPSOLFDQPpWRGRVTXHQRUPDOPHQWHVHHVWXGLDQHQFiOFXORDYDQ]DGR8QDEXHQDUHIHUHQcia general para este material es el libro de Ortega titulado Numerical Analysis‚ÄìA Second
Course (An√°lisis num√©rico ‚Äì Un segundo curso  >2U@ 8QD UHIHUHQFLD PiV FRPSOHWD
es [OR].

10.1 Puntos Ô¨Åjos para funciones de varias variables
8QVLVWHPDGHHFXDFLRQHVQROLQHDOHVWLHQHODIRUPD

f 1 (x1 , x2 , . . . , xn ) = 0,
f 2 (x1 , x2 , . . . , xn ) = 0,
..
..
.
.

(10.1)

f n (x1 , x2 , . . . , xn ) = 0,
GRQGHFDGDIXQFLyQf i se puede pensar como un mapeo de un vector x = (x1 , x2 , . . . , xn )t
del espacio n dimensional Rn en la recta real R(QOD√ÄJXUDVHPXHVWUDXQDUHSUHVHQWDci√≥n geom√©trica de un sistema no lineal cuando n 5 2.
Este sistema de n ecuaciones no lineales en n variables tambi√©n se puede representar al
GH√ÄQLUXQDIXQFLyQF de mapeo Rn en Rn.

F(x1 , x2 , . . . , xn ) = ( f 1 (x1 , x2 , . . . , xn ), f 2 (x1 , x2 , . . . , xn ), . . . , f n (x1 , x2 , . . . , xn ))t .
Si se utiliza notaci√≥n vectorial para representar las variables x1 , x2 , . . . , xn , entonces el sisWHPD  DVXPHODIRUPD

F(x) = 0.
/DVIXQFLRQHV f 1 , f 2 , . . . , f n reciben el nombre de funciones coordenadas de F.

(10.2)

10.1 Puntos Ô¨Åjos para funciones de varias variables

477

Figura 10.1
z

z 5 f 2(x1, x 2 )

z 5 f 1(x1, x 2 )

x2
f 1(x1, x 2 ) 5 0 y

x1

Ejemplo 1

f 2(x1, x2 ) 5 0

Escriba el sistema no lineal 3 3 3

3x1 ‚àí cos(x2 x3 ) ‚àí

1
= 0,
2

x12 ‚àí 81(x2 + 0.1)2 + sen x3 + 1.06 = 0,
e‚àíx1 x2 + 20x3 +

10œÄ ‚àí 3
= 0,
3

HQODIRUPD 
Soluci√≥n 'H√ÄQDODVWUHVIXQFLRQHVFRRUGHQDGDVf1, f2 y f3 desde R3 hasta R como

1
f 1 (x1 , x2 , x3 ) = 3x1 ‚àí cos(x2 x3 ) ‚àí ,
2
f 2 (x1 , x2 , x3 ) = x12 ‚àí 81(x2 + 0.1)2 + sen x3 + 1.06 y
f 3 (x1 , x2 , x3 ) = e‚àíx1 x2 + 20x3 +

10œÄ ‚àí 3
.
3

(QWRQFHVGH√ÄQDF desde R3 ‚Üí R3 mediante

F(x) = F(x1 , x2 , x3 )
= ( f 1 (x1 , x2 , x3 ), f 2 (x1 , x2 , x3 ), f 3 (x1 , x2 , x3 ))t
=

1
3x1 ‚àí cos(x2 x3 ) ‚àí , x12 ‚àí 81(x2 + 0.1)2
2
+ sen x3 + 1.06, e‚àíx1 x2 + 20x3 +

10œÄ ‚àí 3
3

t

.

$QWHVGHDQDOL]DUODVROXFLyQGHXQVLVWHPDSURYLVWRHQODIRUPD  R  QHFHVLWDPRV DOJXQRV UHVXOWDGRV UHVSHFWR D OD FRQWLQXLGDG \ GLIHUHQFLDELOLGDG GH ODV IXQFLRQHV
desde Rn hasta Rn$SHVDUGHTXHHVWHHVWXGLRSRGUtDUHSUHVHQWDUVHGLUHFWDPHQWH FRQVXOWH
el ejercicio 14), usamos un m√©todo alternativo que nos permite representar te√≥ricamente los
FRQFHSWRVPiVGLItFLOHVGHOtPLWHV\FRQWLQXLGDGHQWpUPLQRVGHIXQFLRQHVGHVGHRn hasta R.

478

CAP√çTULO 10

DeÔ¨Ånici√≥n 10.1

Soluciones num√©ricas de sistemas de ecuaciones no lineales

Sea fXQDIXQFLyQGH√ÄQLGDHQXQFRQMXQWR D ‚äÇ Rn y rango en R6HGLFHTXHODIXQFLyQf
tiene l√≠mite L en x0, escrito

l√≠m f (x) = L ,

x‚Üíx0

si, dado cualquier n√∫mero Œµ > 0, H[LVWHXQQ~PHURŒ¥ > 0 con

| f (x) ‚àí L| < Œµ,
siempre que x ‚àà D, y

0 < ||x ‚àí x0 || < Œ¥.
/DH[LVWHQFLDGHXQOtPLWHWDPELpQHVLQGHSHQGLHQWHGHODQRUPDYHFWRULDOSDUWLFXODUTXH
se usa, como se analiz√≥ en la secci√≥n 7.1. Cualquier norma conveniente se puede usar para
VDWLVIDFHUODFRQGLFLyQHQHVWDGH√ÄQLFLyQ(OYDORUHVSHFt√ÄFRGHŒ¥ depender√° de la norma
VHOHFFLRQDGDSHURODH[LVWHQFLDGHŒ¥ es independiente de la norma.
/D QRFLyQ GH OtPLWH QRV SHUPLWH GH√ÄQLU OD FRQWLQXLGDG SDUD ODV IXQFLRQHV GHVGH Rn
hasta R. Aunque es posible usar varias normas, la continuidad es independiente de la selecci√≥n particular.
DeÔ¨Ånici√≥n 10.2

Sea fXQDIXQFLyQGHOFRQMXQWR D ‚äÇ Rn en R/DIXQFLyQf es continua en x0 ‚àà D siempre
TXHH[LVWDl√≠m x‚Üíx0 f (x) y

l√≠m f (x) = f (x0 ).

x‚Üíx0

/DVGH√ÄQLFLRQHVGHFRQWLQXLGDG
SDUDODVIXQFLRQHVGHn variables
siguen de aquellas para una sola
variable al reemplazar, siempre
que sea necesario, los valores
absolutos por normas.

DeÔ¨Ånici√≥n 10.3

Adem√°s, f es continua en un conjunto D si f es continua en cada punto de D. Este concepto
VHH[SUHVDDOHVFULELU f ‚àà C(D).
$KRUDSRGHPRVGH√ÄQLUORVFRQFHSWRVGHOtPLWH\FRQWLQXLGDGSDUDODVIXQFLRQHVGHVGH
Rn hasta RnDOFRQVLGHUDUODVIXQFLRQHVFRRUGHQDGDVGHVGHRn en R.
Sea FXQDIXQFLyQGHVGHD ‚äÇ Rn a RnGHODIRUPD

F(x) = ( f 1 (x), f 2 (x), . . . , f n (x))t ,
donde fi es un mapeo de Rn hasta R para cada i'H√ÄQLPRV

l√≠m F(x) = L = (L 1 , L 2 , . . . , L n )t ,

x‚Üíx0

si y s√≥lo si l√≠mx‚Üíx0 f i (x) = L i para cada i = 1, 2, . . . , n.
/D IXQFLyQ F es continua en x0 ‚àà D VLHPSUH TXH H[LVWD l√≠m x‚Üíx0 F(x) y l√≠m x‚Üíx0
F(x) = F(x0 ). Adem√°s, F es continua en el conjunto D si F es continua en cada x en D.
(VWHFRQFHSWRVHH[SUHVDDOHVFULELUF ‚àà C(D).
3DUD ODV IXQFLRQHV GHVGH R hasta R, la continuidad a menudo se puede evidenciar al
GHPRVWUDUTXHODIXQFLyQHVGLIHUHQFLDEOH FRQVXOWHHOWHRUHPD $XQTXHHVWHWHRUHPDVH
JHQHUDOL]DHQIXQFLRQHVGHGLYHUVDVYDULDEOHVODGHULYDGD RGHULYDGDWRWDO GHXQDIXQFLyQ
GHGLYHUVDVYDULDEOHVHVWiPX\LQYROXFUDGD\QRVHSUHVHQWDUiDTXt3RUHOFRQWUDULRHVWDEOHFHPRVHOVLJXLHQWHWHRUHPDTXHUHODFLRQDODFRQWLQXLGDGGHXQDIXQFLyQGHn variables en un
SXQWRFRQODVGHULYDGDVSDUFLDOHVGHODIXQFLyQHQHOSXQWR

10.1 Puntos Ô¨Åjos para funciones de varias variables

Teorema 10.4

479

Sea fXQDIXQFLyQGHD ‚äÇ Rn a R y x0 ‚àà D6XSRQJDTXHH[LVWHQWRGDVODVGHULYDGDVSDUFLDles de f y las constantes Œ¥ > 0 y K > 0 GHWDOIRUPDTXHVLHPSUHTXH x ‚àí x0 < Œ¥ y x ‚àà D,
tenemos

‚àÇ f (x)
‚â§ K,
‚àÇx j

para cada j = 1, 2, . . . , n.

Entonces f es continua en x0.

Puntos Ô¨Åjos en Rn
(QHOFDStWXORVHGHVDUUROOyXQSURFHVRLWHUDWLYRSDUDUHVROYHUXQDHFXDFLyQf (x) 5 0, al
WUDQVIRUPDUSULPHURODHFXDFLyQHQODIRUPDGHSXQWR√ÄMRx 5 g(x). Se investigar√° un proceso
VLPLODUSDUDODVIXQFLRQHVGHVGHRn hasta Rn.
DeÔ¨Ånici√≥n 10.5

8QDIXQFLyQG desde D ‚äÇ Rn hasta RnWLHQHXQSXQWR√ÄMRHQp ‚àà D si G(p) 5 p.
(OVLJXLHQWHWHRUHPDJHQHUDOL]DHOWHRUHPDGHSXQWR√ÄMRHQODSiJLQDSDUDHOFDVR
nGLPHQVLRQDO (VWH WHRUHPD HV XQ FDVR HVSHFLDO GHO WHRUHPD GH IXQFLyQ FRQWUDFWLYD \ VX
demostraci√≥n se puede encontrar en [Or2], p. 153.

Teorema 10.6

Sea D = { (x1 , x2 , . . . , xn )t | ai ‚â§ xi ‚â§ bi , para cada i = 1, 2, . . . , n } para alg√∫n conjunto
de constantes a1 , a2 , . . . , an y b1 , b2 , . . . , bn . Suponga que GHVXQDIXQFLyQFRQWLQXDHQ
D ‚äÇ Rn a Rn con la propiedad de que G(x) ‚àà D, siempre que x ‚àà D. Entonces G tiene un
SXQWR√ÄMRHQD.
$GHPiVVXSRQJDTXHWRGDVODVIXQFLRQHVFRPSRQHQWHVGHG tienen derivadas parciales
FRQWLQXDV\TXHH[LVWHXQDFRQVWDQWHK , 1 con

‚àÇgi (x)
K
‚â§ , siempre que x ‚àà D,
‚àÇx j
n
para cada j = 1, 2, . . . , n\FDGDIXQFLyQFRPSRQHQWHgi(QWRQFHVODVXFHVLyQGHSXQWR√ÄMR
(0)
{x(k) }‚àû
seleccionada arbitrariamente en D y generada por medio de
k=0GH√ÄQLGDSRUx

x(k) = G(x(k‚àí1) ),

para cada k ‚â• 1

FRQYHUJHDO~QLFRSXQWR√ÄMRp ‚àà D y

x(k) ‚àí p ‚àû ‚â§
Ejemplo 2

Kk
x(1) ‚àí x(0) ‚àû .
1‚àíK

(10.3)

Considere el sistema no lineal

3x1 ‚àí cos(x2 x3 ) ‚àí

1
= 0,
2

x12 ‚àí 81(x2 + 0.1)2 + sen x3 + 1.06 = 0, y
e‚àíx1 x2 + 20x3 +

10œÄ ‚àí 3
=0
3

HQIRUPDGHSXQWR√ÄMRx = G(x) al resolver la i-√©sima ecuaci√≥n para xi0XHVWUHTXHH[LVWH
una soluci√≥n √∫nica en

D = { (x1 , x2 , x3 )t | ‚àí1 ‚â§ xi ‚â§ 1, para cada i = 1, 2, 3},
e itere a partir de x(0) = (0.1, 0.1, ‚àí0.1)t hasta una precisi√≥n dentro de 1025 en la norma l‚àû
obtenida.

480

CAP√çTULO 10

Soluciones num√©ricas de sistemas de ecuaciones no lineales
Soluci√≥n

Al resolver la i-√©sima ecuaci√≥n para xiREWHQHPRVHOSUREOHPDGHSXQWR√ÄMR

1
1
cos(x2 x3 ) + ,
3
6
1
x12 + sen x3 + 1.06 ‚àí 0.1,
x2 =
9
1
10œÄ ‚àí 3
.
x3 = ‚àí e‚àíx1 x2 ‚àí
20
60
x1 =

(10.4)

Sea G : R3 ‚Üí R3GH√ÄQLGRSRUG(x) = (g1 (x), g2 (x), g3 (x))t , donde

1
1
cos(x2 x3 ) + ,
3
6
1
x12 + sen x3 + 1.06 ‚àí 0.1,
g2 (x1 , x2 , x3 ) =
9
1
10œÄ ‚àí 3
g3 (x1 , x2 , x3 ) = ‚àí e‚àíx1 x2 ‚àí
.
20
60

g1 (x1 , x2 , x3 ) =

/RVWHRUHPDV\VHXVDUiQSDUDPRVWUDUTXHGWLHQHXQSXQWR√ÄMR~QLFRHQ

D = { (x1 , x2 , x3 )t | ‚àí1 ‚â§ xi ‚â§ 1, para cada i = 1, 2, 3}.
Para x = (x1 , x2 , x3 )t en D,

1
1
| cos(x2 x3 )| + ‚â§ 0.50,
3
6
1‚àö
1
|g2 (x1 , x2 , x3 )| =
x12 + sen x3 + 1.06 ‚àí 0.1 ‚â§
1 + sen 1+ 1.06 ‚àí 0.1 < 0.09,
9
9
|g1 (x1 , x2 , x3 )| ‚â§

y
|g3 (x1 , x2 , x3 )| =

1
10œÄ ‚àí 3
1 ‚àíx1 x2 10œÄ ‚àí 3
e
‚â§
e+
< 0.61.
+
20
60
20
60

Por lo que tenemos, para cada i = 1, 2, 3,

‚àí1 ‚â§ gi (x1 , x2 , x3 ) ‚â§ 1.
Por lo tanto, G(x) ‚àà D siempre que x ‚àà D.
(QFRQWUDUOtPLWHVSDUDODVGHULYDGDVSDUFLDOHVHQD nos da

‚àÇg1
= 0,
‚àÇ x1

‚àÇg2
= 0, y
‚àÇ x2

‚àÇg3
=0
‚àÇ x3

DVtFRPR

‚àÇg1
‚àÇg1
1
1
1
1
‚â§ |x3 | ¬∑ | sen x2 x3 | ‚â§ sen 1 < 0.281,
‚â§ |x2 | ¬∑ | sen x2 x3 | ‚â§ sen 1 < 0.281,
‚àÇ x2
3
3
‚àÇ x3
3
3
‚àÇg2
=
‚àÇ x1
9

1
|x1 |
< 0.238,
< ‚àö
2
x1 + sen x3 + 1.06 9 0.218

‚àÇg2
=
‚àÇ x3
18

| cos x3 |
1
< 0.119,
< ‚àö
2
x1 + sen x3 + 1.06 18 0.218

|x2 | ‚àíx1 x2
1
‚àÇg3
=
‚â§
e
e < 0.14, y
‚àÇ x1
20
20

|x1 | ‚àíx1 x2
1
‚àÇg3
=
‚â§
e
e < 0.14.
‚àÇ x2
20
20

10.1 Puntos Ô¨Åjos para funciones de varias variables

481

Todas las derivadas parciales de g1 , g2 y g3 est√°n acotadas en D, por lo que el teorema
LPSOLFDTXHHVWDVIXQFLRQHVVRQFRQWLQXDVHQD. Por consiguiente, G es continua en D.
Adem√°s, para cada x ‚àà D,

‚àÇgi (x)
‚â§ 0.281,
‚àÇx j

para cada i = 1, 2, 3

y j = 1, 2, 3,

\ODFRQGLFLyQHQODVHJXQGDSDUWHGHOWHRUHPDVHPDQWLHQHFRQK = 3(0.281) = 0.843.
'HODPLVPDIRUPDWDPELpQHVSRVLEOHPRVWUDUTXH‚àÇgi /‚àÇ x j es continua en D para cada
i = 1, 2, 3 y j = 1, 2, 3. (Esto se considera en el ejercicio 13.) Por consiguiente, G tiene
XQ~QLFRSXQWR√ÄMRHQD y el sistema no lineal tiene una soluci√≥n en D.
Observe que GWLHQHXQSXQWR√ÄMR~QLFRHQD, y esto no implica que la soluci√≥n para el
sistema original sea la √∫nica en este dominio porque la soluci√≥n para x2 en la ecuaci√≥n (10.4)
LPSOLFDEDODVHOHFFLyQGHODUDt]FXDGUDGDSULQFLSDO(OHMHUFLFLRG H[DPLQDORTXHRFXUUH
VLSRUHOFRQWUDULRVHVHOHFFLRQDODUDt]FXDGUDGDQHJDWLYDHQHVWHSDVR
3DUDDSUR[LPDUHOSXQWR√ÄMRp, seleccionamos x(0) = (0.1, 0.1, ‚àí0.1)t . La sucesi√≥n de
vectores generados por

x1(k) =

1
1
cos x2(k‚àí1) x3(k‚àí1) + ,
3
6

x2(k) =

1
9

x3(k) = ‚àí

x1(k‚àí1)

2

+ sen x 3(k‚àí1) + 1.06 ‚àí 0.1, y

1 ‚àíx (k‚àí1) x (k‚àí1) 10œÄ ‚àí 3
2
e 1
‚àí
20
60

converge con la soluci√≥n √∫nica del sistema en la ecuaci√≥n (10.4) Los resultados en la tabla
10.1 se generaron hasta que

x(k) ‚àí x(k‚àí1) ‚àû < 10‚àí5 .

Tabla 10.1

k

x1(k)

x2(k)

x3(k)

x(k) ‚àí x(k‚àí1) ‚àû

0
1
2
3
4
5

0.10000000
0.49998333
0.49999593
0.50000000
0.50000000
0.50000000

0.10000000
0.00944115
0.00002557
0.00001234
0.00000003
0.00000002

‚àí0.10000000
‚àí0.52310127
‚àí0.52336331
‚àí0.52359814
‚àí0.52359847
‚àí0.52359877

0.423
9.4 √ó 10‚àí3
2.3 √ó 10‚àí4
1.2 √ó 10‚àí5
3.1 √ó 10‚àí7

3RGUtDPRVXVDUODFRWDGHHUURU  FRQK 5 0.843 en el ejemplo previo. Esto da

x(5) ‚àí p ‚àû ‚â§

(0.843)5
(0.423) < 1.15,
1 ‚àí 0.843

lo que no indica la verdadera precisi√≥n de x(5). La soluci√≥n real es

p = 0.5, 0, ‚àí

œÄ
6

t

‚âà (0.5, 0, ‚àí0.5235987757)t , por lo que

x(5) ‚àí p ‚àû ‚â§ 2 √ó 10‚àí8 .

482

CAP√çTULO 10

Soluciones num√©ricas de sistemas de ecuaciones no lineales

Aceleraci√≥n de la convergencia
8QDIRUPDGHDFHOHUDUODFRQYHUJHQFLDGHODLWHUDFLyQGHSXQWR√ÄMRHVXVDUORV~OWLPRVFiOFX
(k)
(k)
(k)
(k‚àí1)
(k‚àí1)
, . . . , xi‚àí1
para calcular xi , como en el m√©todo de
los x1 , . . . , xi‚àí1 en lugar de x1
Gauss-Siedel para los sistemas lineales. Entonces, las ecuaciones del componente para el
problema en el ejemplo se convierten en

x1(k) =

1
1
cos x2(k‚àí1) x3(k‚àí1) + ,
3
6

x2(k) =

1
9

x3(k) = ‚àí

x1(k)

2

+ sen x3(k‚àí1) + 1.06 ‚àí 0.1, y

1 ‚àíx (k) x (k) 10œÄ ‚àí 3
e 1 2 ‚àí
.
20
60

Con x(0) = (0.1, 0.1, ‚àí0.1)t , los resultados de estos c√°lculos se muestran en la tabla 10.2

Tabla 10.2

k

x1(k)

x2(k)

x3(k)

x(k) ‚àí x(k‚àí1) ‚àû

0
1
2
3
4

0.10000000
0.49998333
0.49997747
0.50000000
0.50000000

0.10000000
0.02222979
0.00002815
0.00000004
0.00000000

‚àí0.10000000
‚àí0.52304613
‚àí0.52359807
‚àí0.52359877
‚àí0.52359877

0.423
2.2 √ó 10‚àí2
2.8 √ó 10‚àí5
3.8 √ó 10‚àí8

La iteraci√≥n x(4) es precisa dentro de 1027 en la norma l‚àû; por lo que la convergencia
estaba, de hecho, acelerada para este problema al usar el m√©todo de Gauss-Siedel. Sin embargo, este m√©todo no siempre acelera la convergencia.
La secci√≥n Conjunto de ejercicios 10.1 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

10.2 M√©todo de Newton
(OSUREOHPDHQHOHMHPSORGHODVHFFLyQVHWUDQVIRUPDHQXQSUREOHPDGHSXQWR√ÄMR
convergente al resolver algebraicamente las tres ecuaciones para las tres variables x1 , x2
y x3 .6LQHPEDUJRHVSRFRFRP~QVHUFDSD]GHHQFRQWUDUXQDUHSUHVHQWDFLyQH[SOtFLWDSDUD
WRGDVODVYDULDEOHV(QHVWDVHFFLyQFRQVLGHUDPRVXQSURFHGLPLHQWRDOJRUtWPLFRSDUDUHDOL]DUODWUDQVIRUPDFLyQHQXQDVLWXDFLyQPiVJHQHUDO
3DUDFRQVWUXLUHODOJRULWPRTXHFRQGXFHDXQPpWRGRGHSXQWR√ÄMRDGHFXDGRHQHOFDVR
XQLGLPHQVLRQDOHQFRQWUDPRVXQDIXQFLyQœÜ con la propiedad de que

g(x) = x ‚àí œÜ(x) f (x)
GDFRQYHUJHQFLDFXDGUiWLFDSDUDHOSXQWR√ÄMRpGHODIXQFLyQg (consulte la secci√≥n 2.4). A
SDUWLUGHHVWDFRQGLFLyQHOPpWRGRGH1HZWRQHYROXFLRQyDOVHOHFFLRQDU œÜ(x) = 1/ f (x),
suponiendo que f (x) = 0.
8QHQIRTXHVLPLODUHQHOFDVRn-dimensional implica una matriz
‚é°
‚é§
a11 (x) a12 (x) ¬∑ ¬∑ ¬∑ a1n (x)
‚é¢a21 (x) a22 (x) ¬∑ ¬∑ ¬∑ a2n (x)‚é•
‚é¢
‚é•
A(x) = ‚é¢ .
(10.5)
..
.. ‚é• ,
‚é£ ..
.
. ‚é¶

an1 (x) an2 (x)

¬∑ ¬∑ ¬∑ ann (x)

10.2

M√©todo de Newton

483

donde cada una de las entradas ai j (x)HVXQDIXQFLyQGHRn a R. Esto requiere encontrar A(x)
GHWDOIRUPDTXH

G(x) = x ‚àí A(x)‚àí1 F(x)
da convergencia cuadr√°tica para la soluci√≥n de F(x) 5 0, suponiendo que A(x) es no singular
HQHOSXQWR√ÄMRp de G.
El siguiente teorema se compara con el teorema 2.8 en la p√°gina 59. Su demostraci√≥n
UHTXLHUHVHUFDSD]GHH[SUHVDUG en t√©rminos de su serie de Taylor en n variables alrededor
de p.
Teorema 10.7

Si p es la soluci√≥n de G(x) 5 x6XSRQJDTXHH[LVWHXQQ~PHURŒ¥ > 0 con las propiedades:
i)

‚àÇgi /‚àÇ x j es continua en NŒ¥ = { x
j = 1, 2, . . . , n;

ii)

‚àÇ 2 gi (x)/(‚àÇ x j ‚àÇ xk ) es continua y |‚àÇ 2 gi (x)/(‚àÇ x j ‚àÇ xk )| ‚â§ M para algunas constantes
M, siempre que x ‚àà NŒ¥ , para cada i = 1, 2, ..., n, j = 1, 2, . . . , n, y k =
1, 2, . . . , n;

iii)

‚àÇgi (p)/‚àÇ xk = 0, para cada i = 1, 2, . . . , n y k = 1, 2, . . . , n.

x ‚àí p < Œ¥ }, para cada i = 1, 2, . . . , n y

Entonces, un n√∫mero Œ¥ÃÇ ‚â§ Œ¥H[LVWHGHWDOIRUPDTXHODVXFHVLyQJHQHUDGDSRUx(k) = G(x(k‚àí1) )
FRQYHUJH GH IRUPD FXDGUiWLFD HQ p para cualquier selecci√≥n de x(0), siempre y cuando
x(0) ‚àí p < Œ¥ÃÇ. Adem√°s,

x(k) ‚àí p ‚àû ‚â§

n 2 M (k‚àí1)
x
‚àí p 2‚àû ,
2

para cada k ‚â• 1.

Para aplicar el teorema 10.7, suponga que A(x) es una matriz n 3 nGHIXQFLRQHVGHRn
a RHQODIRUPDGHODHFXDFLyQ  GRQGHODVHQWUDGDVHVSHFt√ÄFDVVHVHOHFFLRQDUiQPiV
adelante. Suponga, adem√°s, que A(x) es no singular cerca de una soluci√≥n p de F(x) 5 0 y
sea que bi j (x) denote la entrada de A(x)21 en la i-pVLPD√ÄOD\ODj-√©sima columna.
Para G(x) = x ‚àí A(x)‚àí1 F(x), tenemos gi (x) = xi ‚àí nj=1 bi j (x) f j (x). De modo que,

‚éß
n
‚àÇfj
‚àÇbi j
‚é™
‚é™
‚é™
1‚àí
bi j (x)
(x) +
(x) f j (x) ,
‚é™
‚é™
‚àÇ
x
‚àÇ xk
‚é®
k
j=1

‚àÇgi
(x) =
n
‚é™
‚àÇ xk
‚é™
‚é™
‚é™
‚àí
‚é™
‚é©

bi j (x)

j=1

‚àÇfj
‚àÇbi j
(x) +
(x) f j (x) ,
‚àÇ xk
‚àÇ xk

si i = k,
si i = k.

El teorema 10.7 implica que necesitamos ‚àÇgi (p)/‚àÇ xk = 0, para cada i = 1, 2, . . . , n
y k = 1, 2, . . . , n. (VWRVLJQL√ÄFDTXHSDUDi 5 k,
n

0=1‚àí

bi j (p)
j=1

‚àÇfj
(p),
‚àÇ xi

es decir,
n

bi j (p)
j=1

‚àÇfj
(p) = 1.
‚àÇ xi

Cuando k = i,
n

bi j (p)

0=‚àí
j=1

‚àÇfj
(p),
‚àÇ xk



484

CAP√çTULO 10

Soluciones num√©ricas de sistemas de ecuaciones no lineales

de tal forma que
n

bi j (p)
j=1

‚àÇfj
(p) = 0.
‚àÇ xk

(10.7)

La matriz jacobiana
'H√ÄQDODPDWUL]J (x) mediante

‚é°‚àÇf

1

(x)

‚é¢ ‚àÇ x1
‚é¢
‚é¢ ‚àÇ f2
‚é¢
(x)
‚é¢
J (x) = ‚é¢ ‚àÇ x1
‚é¢ .
‚é¢ ..
‚é¢
‚é£‚àÇf
n
(x)
‚àÇ x1

‚àÇ f1
(x)
‚àÇ x2
‚àÇ f2
(x)
‚àÇ x2
..
.
‚àÇ fn
(x)
‚àÇ x2

¬∑¬∑¬∑
¬∑¬∑¬∑

¬∑¬∑¬∑

‚àÇ f1 ‚é§
(x)
‚àÇ xn ‚é•
‚é•
‚àÇ f2 ‚é•
(x)‚é•
‚àÇ xn ‚é•
‚é•.
.. ‚é•
. ‚é•
‚é•
‚àÇ fn ‚é¶
(x)
‚àÇ xn

(10.8)

(QWRQFHVODVFRQGLFLRQHV  \  UHTXLHUHQTXH

A(p)‚àí1 J (p) = I, la matriz de identidad, por lo que

A(p) = J (p).

Una selecci√≥n adecuada para A(x) es, por consiguiente, A(x) = J (x) ya que √©VWDVDWLVIDFHOD
FRQGLFLyQLLL HQHOWHRUHPD/DIXQFLyQGVHGH√ÄQHPHGLDQWH

G(x) = x ‚àí J (x)‚àí1 F(x),
\HOSURFHGLPLHQWRGHLWHUDFLyQGHSXQWR√ÄMRHYROXFLRQDDOVHOHFFLRQDUx(0) y generar, para
k ‚â• 1,

x(k) = G(x(k‚àí1) ) = x(k‚àí1) ‚àí J (x(k‚àí1) )‚àí1 F(x(k‚àí1) ).

La matriz jacobiana apareci√≥
por primera vez en 1815, en un
DUWtFXORGH&DXFK\SHUR-DFREL
escribi√≥ De determinantibus
functionalibus, en 1841, y prob√≥
numerosos resultados sobre esta
matriz.

ALGORITMO

10.1

(10.9)

Esto recibe el nombre de m√©todo de Newton para sistemas no lineales y en general se
espera que proporcione convergencia cuadr√°tica, siempre y cuando se conozca un valor iniFLDOVX√ÄFLHQWHPHQWHSUHFLVR\TXHJ (p)‚àí1H[LVWD/DPDWUL]J (x) recibe el nombre de matriz
jacobiana\WLHQHQXPHURVDVDSOLFDFLRQHVHQDQiOLVLV(QHVSHFLDOSRGUtDUHVXOWDUIDPLOLDU
SDUDHOOHFWRUGHELGRDVXDSOLFDFLyQHQODLQWHJUDFLyQP~OWLSOHGHXQDIXQFLyQGHGLYHUVDV
YDULDEOHVVREUHXQDUHJLyQTXHUHTXLHUHTXHVHHIHFW~HXQFDPELRGHYDULDEOHV
8QDGHELOLGDGHQHOPpWRGRGH1HZWRQVXUJHGHODQHFHVLGDGGHFDOFXODUHLQYHUWLUOD
matriz J (x)HQFDGDSDVR(QODSUiFWLFDHOFiOFXORH[SOtFLWRGH J (x)‚àí1 se evita al realizar
OD RSHUDFLyQ HQ XQD IRUPD GH GRV SDVRV 3ULPHUR VH HQFXHQWUD XQ YHFWRU y TXH VDWLVIDFH
J (x(k‚àí1) )y = ‚àíF(x(k‚àí1) ) (QWRQFHV OD QXHYD DSUR[LPDFLyQ x(k), se obtiene sumando y a
x(k‚àí1) . El algoritmo 10.1 utiliza este procedimiento de dos pasos.

M√©todo de Newton para sistemas
3DUDDSUR[LPDUODVROXFLyQGHOVLVWHPDQROLQHDOF(x) 5 0GDGDXQDDSUR[LPDFLyQLQLFLDOx:
ENTRADA n√∫mero n GH HFXDFLRQHV \ YDORUHV GHVFRQRFLGRV DSUR[LPDFLyQ LQLFLDO
x = (x1 , . . . , xn )t , tolerancia TOLQ~PHURPi[LPRGHLWHUDFLRQHVN.
SALIDA VROXFLyQDSUR[LPDGD x = (x1 , . . . , xn )t RXQPHQVDMHTXHLQGLFDTXHVHH[FHGLy
el n√∫mero de iteraciones.

10.2

M√©todo de Newton

485

Paso 1 Determine k = 1.
Paso 2 Mientras (k ‚â§ N ) haga los pasos 3‚Äì7.
Paso 3 Calcule F(x) y J (x), donde J (x)i, j = (‚àÇ f i (x)/‚àÇ x j ) para 1‚â§ i, j ‚â§ n.
Paso 4 Resuelva el sistema lineal n √ó n J (x)y = ‚àíF(x).
Paso 5 Determine x = x + y.
Paso 6 Si ||y|| < TOL entonces SALIDA (x);
(El procedimiento fue exitoso.)
PARE.
Paso 7 Determine k = k + 1.
Paso 8 SALIDA (‚ÄòN√∫mero m√°ximo de iteraciones excedido‚Äô);
(El procedimiento no fue exitoso.)
PARE.

Ejemplo 1

El sistema no lineal
3x1 ‚àí cos(x2 x3 ) ‚àí

1
= 0,
2

x12 ‚àí 81(x2 + 0.1)2 + sen x3 + 1.06 = 0, y
e‚àíx1 x2 + 20x3 +

10œÄ ‚àí 3
=0
3

VH PRVWUy HQ HO HMHPSOR  GH OD VHFFLyQ  SDUD WHQHU OD VROXFLyQ DSUR[LPDGD  
20.52359877)t$SOLTXHHOPpWRGRGH1HZWRQSDUDHVWHSUREOHPDFRQx(0) 5 (0.1, 0.1, 20.1)t.
Soluci√≥n

'H√ÄQD

F(x1 , x2 , x3 ) = ( f 1 (x1 , x2 , x3 ), f 2 (x1 , x2 , x3 ), f 3 (x1 , x2 , x3 ))t ,
donde
1
f 1 (x1 , x2 , x3 ) = 3x1 ‚àí cos(x2 x3 ) ‚àí ,
2
f 2 (x1 , x2 , x3 ) = x12 ‚àí 81(x2 + 0.1)2 + sen x3 + 1.06,
y
f 3 (x1 , x2 , x3 ) = e‚àíx1 x2 + 20x3 +

10œÄ ‚àí 3
.
3

La matriz jacobiana J (x) para este sistema es
‚é°
‚é§
x2 sen x2 x3
3
x3 sen x2 x3
‚é¢
‚é•
‚àí162(x2 + 0.1)
cos x3 ‚é¶ .
J (x1 , x2 , x3 ) = ‚é£ 2x1

‚àíx2 e‚àíx1 x2

‚àíx1 e‚àíx1 x2

20

Sea x(0) = (0.1, 0.1, ‚àí0.1)t . Entonces F(x(0) 5 (20.199995, 2 t
y
‚é§
‚é°
3
9.999833334 √ó 10‚àí4 9.999833334 √ó 10‚àí4
‚é•
‚é¢
0.2
‚àí32.4
0.9950041653 ‚é¶ .
J (x(0) ) = ‚é£
‚àí0.09900498337
‚àí0.09900498337
20

486

CAP√çTULO 10

Soluciones num√©ricas de sistemas de ecuaciones no lineales

La resoluci√≥n del sistema lineal J (x(0) )y(0) = ‚àíF(x(0) ) da
‚é°
‚é§
‚é°
‚é§
0.3998696728
0.4998696782
y(0) = ‚é£ ‚àí0.08053315147 ‚é¶ y x(1) = x(0) + y(0) = ‚é£ 0.01946684853 ‚é¶ .
‚àí0.4215204718
‚àí0.5215204718
Al continuar para k = 2, 3, . . . , tenemos
‚é° (k) ‚é§ ‚é° (k‚àí1) ‚é§ ‚é° (k‚àí1) ‚é§
x1
x1
y1
‚é¢ (k) ‚é• ‚é¢ (k‚àí1) ‚é• ‚é¢ (k‚àí1) ‚é•
‚é£x2 ‚é¶ = ‚é£x2
‚é¶ + ‚é£ y2
‚é¶,

x3(k)

x3(k‚àí1)

y3(k‚àí1)

donde

‚é° (k‚àí1) ‚é§
y1
‚é¢ (k‚àí1) ‚é•
(k‚àí1)
, x2(k‚àí1) , x3(k‚àí1)
‚é£ y2
‚é¶ = ‚àí J x1

‚àí1

F x1(k‚àí1) , x2(k‚àí1) , x3(k‚àí1) .

y3(k‚àí1)
Por lo tanto, en el k-√©simo paso, el sistema lineal J x(k‚àí1) y(k‚àí1) = ‚àíF x(k‚àí1) se debe
resolver, donde

‚é°
‚é¢
J x(k‚àí1) = ‚é¢
‚é£

3

x3(k‚àí1) sen x2(k‚àí1) x3(k‚àí1)

x2(k‚àí1) sen x2(k‚àí1) x3(k‚àí1)

2x1(k‚àí1)

‚àí162 x2(k‚àí1) + 0.1

cos x3(k‚àí1)

(k‚àí1) (k‚àí1)
x2

‚àíx2(k‚àí1) e‚àíx1
‚é° (k‚àí1) ‚é§
y1
‚é¢ (k‚àí1) ‚é•
(k‚àí1)
= ‚é£ y2
y
‚é¶,

(k‚àí1) (k‚àí1)
x2

‚àíx1(k‚àí1) e‚àíx1

‚é§
‚é•
‚é•,
‚é¶

20

y3(k‚àí1)
y
‚é°

3x1(k‚àí1) ‚àí cos x2(k‚àí1) x3(k‚àí1) ‚àí 12

‚é§

‚é•
‚é¢
2
2
‚é•
‚é¢
F x(k‚àí1) = ‚é¢ x1(k‚àí1) ‚àí 81 x2(k‚àí1) + 0.1 + sen x3(k‚àí1) + 1.06‚é• .
‚é¶
‚é£
(k‚àí1) (k‚àí1)
(k‚àí1)
10œÄ ‚àí3
‚àíx1
x2
e
+ 20x3
+ 3
Los resultados por medio del procedimiento iterativo se muestran en la tabla 10.3.

Table 10.3

k

x1(k)

x2(k)

x3(k)

x(k) ‚àí x(k‚àí1) ‚àû

0
1
2
3
4
5

0.1000000000
0.4998696728
0.5000142403
0.5000000113
0.5000000000
0.5000000000

0.1000000000
0.0194668485
0.0015885914
0.0000124448
8.516 √ó 10‚àí10
‚àí1.375 √ó 10‚àí11

‚àí0.1000000000
‚àí0.5215204718
‚àí0.5235569638
‚àí0.5235984500
‚àí0.5235987755
‚àí0.5235987756

0.4215204718
1.788 √ó 10‚àí2
1.576 √ó 10‚àí3
1.244 √ó 10‚àí5
8.654 √ó 10‚àí10

(OHMHPSORSUHYLRLOXVWUDTXHHOPpWRGRGH1HZWRQSXHGHFRQYHUJHUPX\UiSLGDPHQWH
XQD YH] TXH VH REWLHQH XQD EXHQD DSUR[LPDFLyQ TXH HVWi FHUFD GH OD VROXFLyQ YHUGDGHUD
6LQHPEDUJRQRVLHPSUHHVIiFLOGHWHUPLQDUEXHQRVYDORUHVLQLFLDOHV\HOXVRGHOPpWRGR
es comparativamente caro. En la siguiente secci√≥n consideramos un m√©todo para superar

10.3

M√©todos cuasi-Newton

487

la √∫ltima debilidad. Normalmente se encuentran buenos valores iniciales con el m√©todo de
descenso m√°s r√°pido, que se analizar√° en la secci√≥n 10.4.
La secci√≥n Conjunto de ejercicios 10.2 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

10.3 M√©todos cuasi-Newton
8QD GHELOLGDG VLJQL√ÄFDWLYD GHO PpWRGR GH 1HZWRQ SDUD UHVROYHU VLVWHPDV GH HFXDFLRQHV
no lineales es la necesidad, en cada iteraci√≥n, de determinar una matriz y resolver un sistema
lineal n 3 n que implica esta matriz. Considere la cantidad de c√°lculos relacionados con
XQDLWHUDFLyQGHOPpWRGRGH1HZWRQ/DPDWUL]MDFRELDQDUHODFLRQDGDFRQXQVLVWHPDGHn
HFXDFLRQHVOLQHDOHVHVFULWRHQODIRUPDF(x) 5 0 requiere que las derivadas parciales n2 de
las nIXQFLRQHVGHFRPSRQHQWHVGHF sean determinadas y evaluadas. En muchas situaciones,
ODHYDOXDFLyQH[DFWDGHODVGHULYDGDVSDUFLDOHVHVLQFRQYHQLHQWHDXQTXHHOSUREOHPDVHKD
vuelto m√°s tratable con el uso generalizado de los sistemas computacionales simb√≥licos,
como Maple, Mathematica y Matlab.
&XDQGRODHYDOXDFLyQH[DFWDQRHVSUiFWLFDSRGHPRVXVDUDSUR[LPDFLRQHVGHGLIHUHQFLD
√ÄQLWDSDUDODVGHULYDGDVSDUFLDOHV3RUHMHPSOR

‚àÇ f j (i)
f j (x(i) + ek h) ‚àí f j (x(i) )
(x ) ‚âà
,
‚àÇ xk
h

(10.10)

donde h es peque√±o en valor absoluto y ekHVHOYHFWRUFX\D~QLFDHQWUDGDGLIHUHQWHDFHUR
es un 1 en la k-pVLPD FRRUGHQDGD 6LQ HPEDUJR HVWD DSUR[LPDFLyQ VLJXH UHTXLULHQGR OD
realizaci√≥n de por lo menos n2HYDOXDFLRQHVIXQFLRQDOHVHVFDODUHVSDUDDSUR[LPDUODPDWUL]
jacobiana y no disminuye la cantidad de c√°lculos, en general O(n3), requerida para resolver
HOVLVWHPDOLQHDOUHODFLRQDGRFRQHVWDPDWUL]MDFRELDQDDSUR[LPDGD
(O HVIXHU]R FRPSXWDFLRQDO WRWDO SDUD XQD VROD LWHUDFLyQ GHO PpWRGR GH 1HZWRQ HV SRU
consiguiente, por lo menos n2 1 nHYDOXDFLRQHVIXQFLRQDOHVHVFDODUHV n2 para la evaluaci√≥n de
la matriz jacobiana y n para la evaluaci√≥n de F) junto con O(n3) operaciones aritm√©ticas para
UHVROYHUHOVLVWHPDOLQHDO(VWDFDQWLGDGGHHVIXHU]RFRPSXWDFLRQDOHVDPSOLDH[FHSWRSDUDORV
valores relativamente peque√±os de n\IXQFLRQHVHVFDODUHVIiFLOPHQWHHYDOXDGDV
En esta secci√≥n consideramos una generalizaci√≥n del m√©todo de secante para los sistemas de ecuaciones no lineales, una t√©cnica conocida como m√©todo de Broyden (consulte
[Broy]). El m√©todo s√≥lo requiere nHYDOXDFLRQHVIXQFLRQDOHVHVFDODUHVSRULWHUDFLyQ\WDPbi√©n reduce el n√∫mero de c√°lculos aritm√©ticos para O(n2). Pertenece a una clase de m√©todos
conocidos como actualizaciones secantes del cambio m√≠nimo que produce algoritmos llamados cuasi-Newton(VWRVPpWRGRVUHHPSOD]DQODPDWUL]MDFRELDQDHQHOPpWRGRGH1HZWRQ
FRQXQDPDWUL]GHDSUR[LPDFLyQTXHVHDFWXDOL]DIiFLOPHQWHHQFDGDLWHUDFLyQ
/DGHVYHQWDMDGHORVPpWRGRVFXDVL1HZWRQHVTXHODFRQYHUJHQFLDFXDGUiWLFDGHOPpWRGRGH1HZWRQVHSLHUGHDOVHUUHHPSOD]DGDHQJHQHUDOPHGLDQWHXQDFRQYHUJHQFLDOODPDGD
superlineal. Esto implica que

l√≠m

i‚Üí‚àû

x(i+1) ‚àí p
= 0,
x(i) ‚àí p

donde p denota la soluci√≥n para F(x) = 0 y x(i) y x(i+1)VRQDSUR[LPDFLRQHVFRQVHFXWLYDV
para p.
En muchas aplicaciones, la reducci√≥n de la convergencia superlineal es un intercambio
m√°s que aceptable para el decremento de la cantidad de c√°lculos. Una desventaja adicional
GHORVPpWRGRVFXDVL1HZWRQHVTXHDGLIHUHQFLDGHOPpWRGRGH1HZWRQQRVHDXWRFRUULJHQ

488

CAP√çTULO 10

Soluciones num√©ricas de sistemas de ecuaciones no lineales

(QJHQHUDOHOPpWRGRGH1HZWRQFRUUHJLUiHOHUURUGHUHGRQGHRFRQLWHUDFLRQHVVXFHVLYDV
pero a menos que se incluyan resguardos especiales, el m√©todo de Broyden no lo har√°.
3DUDGHVFULELUHOPpWRGRGH%UR\GHQVXSRQJDTXHVHGHWHUPLQDXQDDSUR[LPDFLyQLQLcial x(0) para la soluci√≥n p de F(x) 5 0 &DOFXODPRV OD VLJXLHQWH DSUR[LPDFLyQ x(1) de la
PLVPDIRUPDTXHHOPpWRGRGH1HZWRQ6LHVLQFRQYHQLHQWHGHWHUPLQDUJ (x(0) )H[DFWDPHQWH
XVDPRVODVHFXDFLRQHVGHGLIHUHQFLDGDGDVSRUODHFXDFLyQ  SDUDDSUR[LPDUODVGHULvadas parciales. Para calcular x(2)VLQHPEDUJRSDUWLPRVGHOPpWRGRGH1HZWRQ\H[DPLQDmos el m√©todo de secante para una ecuaci√≥n no lineal singular. El m√©todo de secante usa la
DSUR[LPDFLyQ

f (x1 ) ‚âà

f (x1 ) ‚àí f (x0 )
x1 ‚àí x0

como reemplazo para f (x1 )HQHOPpWRGRGH1HZWRQGHXQDVRODYDULDEOH
Para los sistemas no lineales, x(1) ‚àíx(0) es un vector, por lo que el cociente corresponGLHQWHHVLQGH√ÄQLGR6LQHPEDUJRHOPpWRGRSURFHGHGHPRGRSDUHFLGRHQTXHUHHPSOD]Dmos la matriz J x(1) HQHOPpWRGRGH1HZWRQSDUDVLVWHPDVSRUPHGLRGHXQDPDWUL]A1 con
la propiedad de que

A1 x(1) ‚àí x(0) = F x(1) ‚àí F x(0) .

(10.11)

&XDOTXLHUYHFWRUGLIHUHQWHGHFHURHQRn se puede escribir como la suma de un m√∫ltiplo
de x(1) ‚àíx(0) y un m√∫ltiplo de un vector en el complemento ortogonal de x(1) ‚àíx(0) (consulWHHOHMHUFLFLR 3RUORTXHSDUDGH√ÄQLU~QLFDPHQWHODPDWUL]A1, tambi√©n necesitamos
HVSHFL√ÄFDUFyPRDFW~DHQHOFRPSOHPHQWRRUWRJRQDOGH x(1) ‚àíx(0)1RH[LVWHLQIRUPDFLyQ
disponible sobre el cambio en F en una direcci√≥n ortogonal para x(1) ‚àíx(0), por lo que espeFL√ÄFDPRVTXHQRVHSXHGHUHDOL]DUQLQJ~QFDPELRHQHVWDGLUHFFLyQHVGHFLU
t

A1 z = J (x(0) )z, siempre que x(1) ‚àí x(0) z = 0.

(10.12)

Por lo tanto, cualquier vector ortogonal para x(1) ‚àíx(0)QRUHVXOWDDIHFWDGRSRUODDFWXDOL]Dci√≥n de J (x(0) ), que se us√≥ para calcular x(1) para A1, lo cual se usa en la determinaci√≥n de x(2).
/DVFRQGLFLRQHV  \  GH√ÄQHQ~QLFDPHQWHA1 (consulte [DM]) como
(0)

A1 = J (x ) +

F x(1) ‚àí F x(0) ‚àí J x(0)

x(1) ‚àí x(0)
2

x(1) ‚àí x(0) 2

x(1) ‚àí x(0)

t

.

Es la matriz que se usa en lugar de J x(1) para determinar x(2) como
(1)
x(2) = x(1) ‚àí A‚àí1
.
1 F x

Una vez que se ha determinado x(2), el m√©todo se repite para hallar x(3), por medio de A1 en
lugar de A0 ‚â° J x(0) y con x(2) y x(1) en lugar de x(1) y x(0).
En general, una vez que se ha determinado x(i) se calcula x(i+1) por medio de

yi ‚àí Ai‚àí1 si t
si
||si ||22

(10.13)

x(i+1) = x(i) ‚àí Ai‚àí1 F x(i) ,

(10.14)

Ai = Ai‚àí1 +
y

donde se introducen las notaciones yi = F x(i) ‚àí F x(i‚àí1) y si = x(i) ‚àí x(i‚àí1) para
VLPSOL√ÄFDUODVHFXDFLRQHV
Si los m√©todos se realizan de acuerdo con lo descrito en las ecuaciones (10.13) y (10.14),
HOQ~PHURGHHYDOXDFLRQHVIXQFLRQDOHVHVFDODUHVVHUHGXFLUtDGHVGHn2 1 n hasta n (las requeridas para evaluar F x(i) ), SHURVHVHJXLUtDQQHFHVLWDQGRO(n3) c√°lculos para resolver el
sistema lineal n 3 n (consulte el paso 4 en el algoritmo 10.1)

Ai si+1 = ‚àíF x(i) .

(10.15)

10.3

M√©todos cuasi-Newton

489

(VWDIRUPDGHXVDUHOPpWRGRSRGUtDQRHVWDUMXVWL√ÄFDGDGHELGRDODUHGXFFLyQGHODFRQYHUJHQFLDVXSHUOLQHDODSDUWLUGHODFRQYHUJHQFLDFXDGUiWLFDGHOPpWRGRGH1HZWRQ

F√≥rmula Sherman-Morrison
6LQHPEDUJRHVSRVLEOHLQFRUSRUDUXQDPHMRUDFRQVLGHUDEOHDOXVDUXQDIyUPXODGHLQYHUVLyQ
de matriz de Sherman y Morrison (consulte, por ejemplo, [DM], p. 55). La prueba de esta
IyUPXODVHFRQVLGHUDHQORVHMHUFLFLRV\
Teorema 10.8

(F√≥rmula Sherman-Morrison)
Suponga que A es una matriz no singular y que x y y son los vectores con yt A‚àí1 x = ‚àí1.
Entonces A + xyt es no singular y

A + xyt

‚àí1

= A‚àí1 ‚àí

A‚àí1 xyt A‚àí1
.
1 + yt A‚àí1 x

‚àí1
, lo
/DIyUPXOD6KHUPDQ0RUULVRQSHUPLWHFDOFXODUGLUHFWDPHQWH Ai‚àí1 a partir de Ai‚àí1
cual elimina la necesidad de una inversi√≥n de matriz con cada iteraci√≥n.
Si hacemos A = Ai‚àí1 , x = (yi ‚àí Ai‚àí1 si )/||si ||22 , y y = si , en la ecuaci√≥n (10.13) obtenemos

Ai‚àí1 =

Ai‚àí1 +

‚àí1
= Ai‚àí1
‚àí

‚àí1
= Ai‚àí1
‚àí

yi ‚àí Ai‚àí1 si t
si
||si ||22
‚àí1
Ai‚àí1

‚àí1

yi ‚àíAi‚àí1 si t
si
||si ||22

‚àí1
1 + sit Ai‚àí1

‚àí1
Ai‚àí1

yi ‚àí Ai‚àí1 si
||si ||22

‚àí1
‚àí1
Ai‚àí1
yi ‚àí si sit Ai‚àí1

‚àí1
||si ||22 + sit Ai‚àí1
yi ‚àí ||si ||22

,

por lo que
‚àí1
+
Ai‚àí1 = Ai‚àí1

‚àí1
‚àí1
si ‚àí Ai‚àí1
yi sit Ai‚àí1
‚àí1
sit Ai‚àí1
yi

.



Este c√°lculo s√≥lo implica multiplicaciones matriz-vector en cada paso y, por lo tanto,
solamente requiere O (n2) c√°lculos aritm√©ticos. Se evita el c√°lculo de Ai, al igual que la necesidad de resolver el sistema lineal (10.15).
(ODOJRULWPRVLJXHGLUHFWDPHQWHHVWDFRQVWUXFFLyQDOLQFRUSRUDUODHFXDFLyQ  
en la t√©cnica iterativa (10.14).

ALGORITMO

10.2

M√©todo de Broyden
3DUDDSUR[LPDUODVROXFLyQGHOVLVWHPDQROLQHDOF(x) = 0, GDGDXQDDSUR[LPDFLyQLQLFLDOx:
ENTRADA n√∫mero nGHHFXDFLRQHV\YDULDEOHVDSUR[LPDFLyQLQLFLDO x = (x1 , . . . , xn )t ;
tolerancia TOLQ~PHURPi[LPRGHLWHUDFLRQHVN.
SALIDA VROXFLyQDSUR[LPDGD x = (x1 , . . . , xn )t o un mensaje que indica que el n√∫mero
GHLWHUDFLRQHVIXHH[FHGLGR

490

CAP√çTULO 10

Soluciones num√©ricas de sistemas de ecuaciones no lineales

Paso 1 Determine A0 = J (x) donde J (x)i, j = ‚àÇ‚àÇxfij (x) para 1 ‚â§ i, j ‚â§ n;
v = F(x). (Nota: v = F(x(0) ).)
Paso 2 Determine A = A‚àí1
0 . (Use la eliminaci√≥n gaussiana.)
Paso 3 Determine s = ‚àí Av; (Nota: s = s1 .)
x = x + s; (Nota: x = x(1) .)
k = 2.
Paso 4 Mientras ( k ‚â§ N ) haga los pasos 5‚Äì13.
Paso 5 Determine w = v; (Conserve v.)
v = F(x); (Nota: v = F(x(k) ).)
y = v ‚àí w. (Nota: y = yk .)
Paso 6 Determine z = ‚àíAy. (Nota: z = ‚àíA‚àí1
k‚àí1 yk .)
Paso 7 Determine p = ‚àíst z.

(Nota: p = stk A‚àí1
k‚àí1 yk .)

Paso 8 Determine ut = st A.
Paso 9 Determine A = A + 1p (s + z)ut . (Nota: A = A‚àí1
k .)
Paso 10 Determine s = ‚àí Av.

(k)
(Nota: s = ‚àíA‚àí1
k F(x ).)

Paso 11 Determine x = x + s.

(Nota: x = x(k+1) .)

Paso 12 Si ||s|| < TOL entonces SALIDA (x);
(El procedimiento fue exitoso.)
PARE.
Paso 13 Determine k = k + 1.
Paso 14 SALIDA (‚ÄòN√∫mero m√°ximo de iteraciones excedido‚Äô);
(El procedimiento no fue exitoso.)
PARE.

Ejemplo 1

Use el m√©todo de Broyden con x(0) = (0.1, 0.1, ‚àí0.1)tSDUDDSUR[LPDUODVROXFLyQGHOVLVtema no lineal

3x1 ‚àí cos(x2 x3 ) ‚àí

1
= 0,
2

x12 ‚àí 81(x2 + 0.1)2 + sen x3 + 1.06 = 0,
e‚àíx1 x2 + 20x3 +

10œÄ ‚àí 3
= 0.
3

Soluci√≥n (VWH VLVWHPD VH UHVROYLy PHGLDQWH HO PpWRGR GH 1HZWRQ HQ HO HMHPSOR  GH OD
secci√≥n 10.2. La matriz jacobiana para este sistema es
‚é°
‚é§
3
x3 sen x2 x3
x2 sen x2 x3
‚àí162(x2 + 0.1)
cos x3 ‚é¶ .
J (x1 , x2 , x3 ) = ‚é£ 2x1
‚àíx2 e‚àíx1 x2
‚àíx1 e‚àíx1 x2
20

Si x(0) = (0.1, 0.1, ‚àí0.1)t y

F(x 1 , x2 , x3 ) = ( f 1 (x1 , x2 , x3 ), f 2 (x1 , x2 , x3 ), f 3 (x1 , x2 , x3 ))t ,

10.3

M√©todos cuasi-Newton

donde

1
f 1 (x1 , x2 , x3 ) = 3x1 ‚àí cos(x2 x3 ) ‚àí ,
2
f 2 (x1 , x2 , x3 ) = x12 ‚àí 81(x2 + 0.1)2 + sen x3 + 1.06,
y

f 3 (x1 , x2 , x3 ) = e‚àíx1 x2 + 20x3 +

10œÄ ‚àí 3
.
3

Entonces

‚é°

F x(0)

‚é§
‚àí1.199950
= ‚é£‚àí2.269833‚é¶ .
8.462025

Puesto que

A0 = J x1(0) , x2(0) , x3(0)
‚é°
3
0.2
=‚é£
‚àí9.900498 √ó 10‚àí2

9.999833 √ó 10‚àí4
‚àí32.4
‚àí9.900498 √ó 10‚àí2

‚é§
‚àí9.999833 √ó 10‚àí4
‚é¶,
0.9950042
20

1.023852 √ó 10‚àí5
‚àí3.086883 √ó 10‚àí2
‚àí1.527577 √ó 10‚àí4

‚é§
1.615701 √ó 10‚àí5
1.535836 √ó 10‚àí3 ‚é¶.
5.000768 √ó 10‚àí2

tenemos
‚àí1

(0)
(0)
(0)
A‚àí1
0 = J x1 , x2 , x3
‚é°
0.3333332
= ‚é£2.108607 √ó 10‚àí3
1.660520 √ó 10‚àí3

Por lo tanto

‚é§
0.4998697
(0)
x(1) = x(0) ‚àí A‚àí1
= ‚é£1.946685 √ó 10‚àí2 ‚é¶,
0 F x
‚àí0.5215205
‚é§
‚é°
‚àí3.394465 √ó 10‚àí4
(1)
‚é£
‚àí0.3443879 ‚é¶,
F x
=
3.188238 √ó 10‚àí2
‚é°
‚é§
1.199611
y1 = F x(1) ‚àí F x(0) = ‚é£ 1.925445‚é¶,
‚àí8.430143
‚é§
‚é°
0.3998697
s1 = ‚é£‚àí8.053315 √ó 10‚àí2 ‚é¶,
‚àí0.4215204
‚é°

st1 A‚àí1
0 y1 = 0.3424604,
‚àí1
‚àí1
t ‚àí1
A‚àí1
1 = A0 + (1/0.3424604) s1 ‚àí A0 y1 s1 A0
‚é§
‚é°
8.967344 √ó 10‚àí6
0.3333781
1.11050 √ó 10‚àí5
= ‚é£‚àí2.021270 √ó 10‚àí3 ‚àí3.094849 √ó 10‚àí2 2.196906 √ó 10‚àí3 ‚é¶,
1.022214 √ó 10‚àí3 ‚àí1.650709 √ó 10‚àí4 5.010986 √ó 10‚àí2

491

492

CAP√çTULO 10

Soluciones num√©ricas de sistemas de ecuaciones no lineales

y

‚é§
0.4999863
= ‚é£8.737833 √ó 10‚àí3 ‚é¶.
‚àí0.5231746
‚é°

(1)
x(2) = x(1) ‚àí A‚àí1
1 F x

Las iteraciones adicionales se listan en la tabla 10.4. La quinta iteraci√≥n del m√©todo de
%UR\GHQHVOLJHUDPHQWHPHQRVSUHFLVDTXHODFXDUWDLWHUDFLyQGHOPpWRGRGH1HZWRQGDGD
HQHOHMHPSORDO√ÄQDOGHODVHFFLyQDQWHULRU

Tabla 10.4

k

x1(k)

x2(k)

x3(k)

x(k) ‚àí x(k‚àí1) 2

3
4
5
6

0.5000066
0.5000003
0.5000000
0.5000000

8.672157 √ó 10‚àí4
6.083352 √ó 10‚àí5
‚àí1.448889 √ó 10‚àí6
6.059030 √ó 10‚àí9

‚àí0.5236918
‚àí0.5235954
‚àí0.5235989
‚àí0.5235988

7.88 √ó 10‚àí3
8.12 √ó 10‚àí4
6.24 √ó 10‚àí5
1.50 √ó 10‚àí6

/RVSURFHGLPLHQWRVWDPELpQHVWiQGLVSRQLEOHVGHWDOIRUPDTXHVHPDQWLHQHODFRQYHUJHQFLDSHURUHGXFHQVLJQL√ÄFDWLYDPHQWHHOQ~PHURGHHYDOXDFLRQHVIXQFLRQDOHVUHTXHULGDV
/RVPpWRGRVGHHVWHWLSRIXHURQSURSXHVWRVRULJLQDOPHQWHSRU%URZQ>%URZ.@8QDUHse√±a y comparaci√≥n de algunos m√©todos de este tipo que se usan de manera com√∫n puede
HQFRQWUDUVHHQ>0&@1RREVWDQWHHQJHQHUDOHVWRVPpWRGRVVRQPXFKRPiVGLItFLOHVGH
LPSOHPHQWDUGHPDQHUDH√ÄFLHQWHTXHHOPpWRGRGH%UR\GHQ
La secci√≥n Conjunto de ejercicios 10.3 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

10.4 T√©cnicas de descenso m√°s r√°pido

El nombre del m√©todo de
descenso m√°s r√°pido sigue
de la aplicaci√≥n tridimensional de
se√±alamiento en la direcci√≥n
descendente m√°s r√°pida.

/DYHQWDMDGHORVPpWRGRVGH1HZWRQ\FXDVL1HZWRQSDUDUHVROYHUVLVWHPDVGHHFXDFLRQHV
QROLQHDOHVHVVXYHORFLGDGGHFRQYHUJHQFLDXQDYH]TXHVHFRQRFHXQDDSUR[LPDFLyQVX√ÄFLHQWHPHQWHH[DFWD8QDGHELOLGDGGHHVWRV m√©todoVHVTXHVHQHFHVLWDXQDDSUR[LPDFLyQ
inicial precisa a la soluci√≥n para garantizar la convergencia. El m√©todo de descenso m√°s
r√°pido que se considera en esta secci√≥n s√≥lo converge linealmente para la soluci√≥n, pero
QRUPDOPHQWHWDPELpQFRQYHUJHUiSDUDDSUR[LPDFLRQHVLQLFLDOHVSREUHV3RUFRQVLJXLHQWH
HVWHPpWRGRVHXVDSDUDHQFRQWUDUDSUR[LPDFLRQHVLQLFLDOHVVX√ÄFLHQWHPHQWHH[DFWDVSDUDODV
WpFQLFDVFRQEDVHHQ1HZWRQGHODPLVPDIRUPDTXHVHXVDHOPpWRGRGHELVHFFLyQSDUDXQD
sola ecuaci√≥n.
(OPpWRGRGHGHVFHQVRPiVUiSLGRGHWHUPLQDXQPtQLPRORFDOSDUDXQDIXQFLyQPXOWLYDULDEOHGHODIRUPDg : Rn ‚Üí R. El m√©todo es valioso m√°s all√° de la aplicaci√≥n como m√©todo
de inicio para resolver sistemas no lineales. (En los ejercicios se consideran algunas otras
aplicaciones.)
/DFRQH[LyQHQWUHODPLQLPL]DFLyQGHXQDIXQFLyQGHRn a R y la soluci√≥n de un sistema
GHHFXDFLRQHVQROLQHDOHVVHGHEHDOKHFKRGHTXHXQVLVWHPDGHODIRUPD

f 1 (x1 , x2 , . . . , xn ) = 0,
f 2 (x1 , x2 , . . . , xn ) = 0,
..
.

..
.

f n (x1 , x2 , . . . , xn ) = 0,

10.4

T√©cnicas de descenso m√°s r√°pido

493

tiene una soluci√≥n en x = (x1 , x2 , . . . , xn )tSUHFLVDPHQWHFXDQGRODIXQFLyQgGH√ÄQLGDSRU
n

g(x1 , x2 , . . . , xn ) =

[ f i (x1 , x2 , . . . , xn )]2
i=1

WLHQHHOYDORUPtQLPR
(OPpWRGRGHOGHVFHQVRPiVUiSLGRSDUDHQFRQWUDUXQPtQLPRORFDOSDUDXQDIXQFLyQ
arbitraria g de Rn a R se puede describir de modo intuitivo de acuerdo con lo siguiente:
t

1.

Eval√∫e gHQXQDDSUR[LPDFLyQLQLFLDOx(0) = x1(0) , x2(0) , . . . , xn(0) .

2.

Determine una direcci√≥n desde x(0) que resulte en un decremento del valor de g.

3.

Mueva una cantidad adecuada en esta direcci√≥n y llame al nuevo valor x(1).

4.

Repita los pasos 1 a 3 con x(0) reemplazado por x(1).

El gradiente de una funci√≥n
Antes de describir c√≥mo seleccionar la direcci√≥n correcta y la distancia adecuada para moverse hacia esta direcci√≥n, necesitamos revisar algunos resultados a partir del c√°lculo. El
WHRUHPDGHOYDORUH[WUHPRHVWDEOHFHTXHXQDIXQFLyQGHXQDVRODYDULDEOHGLIHUHQFLDEOH
SXHGHWHQHUXQPtQLPRUHODWLYRV√≥lo cuando su derivada es cero. Para ampliar este resultado
SDUDIXQFLRQHVPXOWLYDULDEOHVQHFHVLWDPRVODVLJXLHQWHGH√ÄQLFLyQ
DeÔ¨Ånici√≥n 10.9

Para g : Rn ‚Üí R el gradiente de g en x = (x1 , x2 , . . . , xn )t se denota ‚àág(x)\VHGH√ÄQH
como

‚àág(x) =
(OJUDGLHQWHSURYLHQHGHODUDt]
de la palabra latina gradi, que
VLJQL√ÄFD¬¥FDPLQDU¬µ(QHVWH
sentido, el gradiente de una
VXSHU√ÄFLHHVODYHORFLGDGFRQOD
TXH¬¥FDPLQDFXHVWDDUULED¬µ

‚àÇg
‚àÇg
‚àÇg
(x),
(x), . . . ,
(x)
‚àÇ x1
‚àÇ x2
‚àÇ xn

t

.

(OJUDGLHQWHGHXQDIXQFLyQPXOWLYDULDEOHHVDQiORJRDODGHULYDGDGHXQDIXQFLyQGH
XQDVRODYDULDEOHHQHOVHQWLGRTXHXQDIXQFLyQPXOWLYDULDEOHGLIHUHQFLDEOHSXHGHWHQHUXQ
PtQLPRUHODWLYRHQx s√≥lo cuando el gradiente en x es el vector cero. El gradiente tiene otra
SURSLHGDGLPSRUWDQWHFRQHFWDGRFRQODPLQLPL]DFLyQGHIXQFLRQHVPXOWLYDULDEOH6XSRQJD
que v = (v1 , v2 , . . . , vn )t es un vector unitario en Rn; es decir,
n

||v||22 =

vi2 = 1.
i=1

La derivada direccional de g en x en la direcci√≥n de v mide el cambio en el valor de la
IXQFLyQg relativo al cambio en la variable en la direcci√≥n de v6HGH√ÄQHPHGLDQWH

Dv g(x) = l√≠m

1

h‚Üí0 h

[g(x + hv) ‚àí g(x)] = vt ¬∑ ‚àág(x).

Cuando gHVGLIHUHQFLDEOHODGLUHFFLyQTXHSURGXFHHOYDORUPi[LPRSDUDODGHULYDGDGLreccional se presenta cuando se selecciona vGHIRUPDSDUDOHODD ‚àág(x), siempre y cuando
‚àág(x) = 0. Por consiguiente, la direcci√≥n del mayor decremento en el valor de g en x es la
direcci√≥n dada por ‚àí‚àág(x). /D√ÄJXUDHVXQDLOXVWUDFLyQFXDQGRgHVXQDIXQFLyQGH
dos variables.

494

CAP√çTULO 10

Soluciones num√©ricas de sistemas de ecuaciones no lineales

Figura 10.2
z

z 5 g(x1, x 2 )

(x1, x 2, g(x1, x 2 ))
Direcci√≥n descendente m√°s r√°pida
x1

x2

x 5 (x1, x 2 ) t
2=g(x)

El objetivo es reducir g (x KDVWDVXYDORUPtQLPRGHFHURSRUORTXHXQDVHOHFFLyQDGHcuada para x(1) es apartarse de x(0) en la direcci√≥n que proporciona el mayor descenso en el
valor de g (x). Por lo tanto, hacemos

x(1) = x(0) ‚àí Œ±‚àág x(0) , para alguna constante Œ± . 0

(10.17)

Ahora, el problema se reduce a seleccionar un valor adecuado de Œ± por lo que g (x(1) VHUtD
VLJQL√ÄFDWLYDPHQWHPHQRUDg (x(0)).
Para determinar una selecci√≥n adecuada para el valor Œ±FRQVLGHUDPRVODIXQFLyQGHXQD
sola variable

h(Œ±) = g x(0) ‚àí Œ±‚àág x(0)

.

(10.18)

El valor de Œ± que minimiza h es el valor necesario para la ecuaci√≥n (10.17).
(QFRQWUDU XQ YDORU PtQLPR SDUD h GLUHFWDPHQWH UHTXHULUtD GLIHUHQFLDU h y, despu√©s,
UHVROYHUXQSUREOHPDGHORFDOL]DFLyQGHUDt]SDUDGHWHUPLQDUORVSXQWRVFUtWLFRVGHh. En
general, este procedimiento es demasiado costoso. Por el contrario, elegimos tres n√∫meros
Œ±1 < Œ±2 < Œ±3TXHHVSHUDPRVHVWpQFHUFDGHGRQGHVHSUHVHQWDHOYDORUPtQLPRGHh(Œ±).
Entonces construimos el polinomio cuadr√°tico P (x) que interpola h en Œ±1, Œ±2 y Œ±3(OPtQLPRGHOSROLQRPLRFXDGUiWLFRVHHQFXHQWUDIiFLOPHQWHGHIRUPDVLPLODUDODTXHVHXVDHQHO
PpWRGR0¬ÅOOHUHQODVHFFLyQ
'H√ÄQLPRVŒ±ÃÇ en [Œ±1, Œ±3@GHWDOIRUPDTXHP(Œ±ÃÇ)HVXQPtQLPRHQ>Œ±1, Œ±3] y usamos P(Œ±ÃÇ)
SDUDDSUR[LPDUHOYDORUPtQLPRGHh(Œ±). Entonces, se usa Œ±ÃÇ en la determinaci√≥n de la iteraFLyQQXHYDSDUDDSUR[LPDUHOYDORUPtQLPRGHg:

x(1) = x(0) ‚àí Œ±ÃÇ‚àág x(0) .
Puesto que g (x(0)) est√° disponible, para minimizar el c√°lculo, primero seleccionamos Œ±1 5 0.
A continuaci√≥n se encuentra un n√∫mero Œ±3 con h(Œ±3 ) < h(Œ±1 ). (Puesto que Œ±1 no minimiza
h, este n√∫mero Œ±3H[LVWH )LQDOPHQWHVHVHOHFFLRQDŒ±2 que ser√° Œ±3 /2.
(OYDORUPtQLPRGHP en [Œ±1, Œ±3@VHSUHVHQWD\DVHDHQHO~QLFRSXQWRFUtWLFRGHP o en el
H[WUHPRGHUHFKRŒ±3 porque, por suposici√≥n, P(Œ±3 ) = h(Œ±3 ) < h(Œ±1 ) = P(Œ±1 ). Puesto que
P(x HVXQSROLQRPLRFXDGUiWLFRHOSXQWRFUtWLFRVHSXHGHHQFRQWUDUDOUHVROYHUODHFXDFLyQ
lineal.

10.4

Ejemplo 1

T√©cnicas de descenso m√°s r√°pido

495

Utilice el m√©todo de descenso m√°s r√°pido con x(0) = (0, 0, 0)tSDUDHQFRQWUDUXQDDSUR[LPDci√≥n inicial razonable para la soluci√≥n del sistema no lineal

f 1 (x1 , x2 , x3 ) = 3x1 ‚àí cos(x2 x3 ) ‚àí

1
= 0,
2

f 2 (x1 , x2 , x3 ) = x12 ‚àí 81(x2 + 0.1)2 + sen x3 + 1.06 = 0,
f 3 (x1 , x2 , x3 ) = e‚àíx1 x2 + 20x3 +
Soluci√≥n

10œÄ ‚àí 3
= 0.
3

Si g(x1 , x2 , x3 ) = [ f 1 (x1 , x2 , x3 )]2 + [ f 2 (x1 , x2 , x3 )]2 + [ f 3 (x1 , x2 , x3 )]2.

Entonces

‚àág(x1 , x2 , x3 ) ‚â° ‚àág(x) =

2 f 1 (x)

‚àÇ f1
‚àÇ f2
‚àÇ f3
(x) + 2 f 2 (x)
(x) + 2 f 3 (x)
(x),
‚àÇ x1
‚àÇ x1
‚àÇ x1

2 f 1 (x)

‚àÇ f1
‚àÇ f2
‚àÇ f3
(x) + 2 f 2 (x)
(x) + 2 f 3 (x)
(x),
‚àÇ x2
‚àÇ x2
‚àÇ x2

2 f 1 (x)

‚àÇ f1
‚àÇ f2
‚àÇ f3
(x) + 2 f 2 (x)
(x) + 2 f 3 (x)
(x)
‚àÇ x3
‚àÇ x3
‚àÇ x3

= 2J(x)t F(x).
Para x(0) = (0, 0, 0)t , tenemos

g x(0) = 111.975

y

z 0 = ||‚àág x(0) ||2 = 419.554.

Sea

z=

1
‚àág x(0) = (‚àí0.0214514, ‚àí0.0193062, 0.999583)t .
z0

Con Œ±1 5 0, tenemos g1 = g x(0) ‚àí Œ±1 z = g x(0) = 111.975. De manera arbitraria hacemos que Œ±3 5GHWDOIRUPDTXH

g3 = g x(0) ‚àí Œ±3 z = 93.5649.
Puesto que g3 , g1, aceptamos Œ±3 y establecemos Œ±2 = Œ±3 /2 = 0.5. Por lo tanto,

g2 = g x(0) ‚àí Œ±2 z = 2.53557.
Ahora, encontramos el polinomio cuadr√°tico que interpola los datos (0, 111.975),
 \  3DUDHVWHSURSyVLWRHVPiVFRQYHQLHQWHXVDUXQSROLQRPLRGH
LQWHUSRODFLyQGHGLIHUHQFLDVGLYLGLGDVKDFLDDGHODQWHTXHWLHQHODIRUPD

P(Œ±) = g1 + h 1 Œ± + h 3 Œ±(Œ± ‚àí Œ±2 ).
Esto interpola

g x(0) ‚àí Œ±‚àág x(0)

= g x(0) ‚àí Œ±z

Œ±1 5 0, Œ±2 5 0.5, y Œ±3 5 1, como sigue:

Œ±1 = 0,

g1 = 111.975,

Œ±2 = 0.5,

g2 = 2.53557,

Œ±3 = 1,

g3 = 93.5649,

g2 ‚àí g1
= ‚àí218.878,
Œ±2 ‚àí Œ± 1
g3 ‚àí g2
h2 =
= 182.059,
Œ±3 ‚àí Œ± 2
h1 =

h3 =

h2 ‚àí h1
= 400.937.
Œ±3 ‚àí Œ±1

496

CAP√çTULO 10

Soluciones num√©ricas de sistemas de ecuaciones no lineales

Por lo tanto,

P(Œ±) = 111.975 ‚àí 218.878Œ± + 400.937Œ±(Œ± ‚àí 0.5).
Tenemos P (Œ±) = 0 cuando Œ± = Œ±0 = 0.522959. Puesto que g0 = g x(0) ‚àí Œ±0 z =
HVPiVSHTXHxRTXHg1 y g3, establecemos

x(1) = x(0) ‚àí Œ±0 z = x(0) ‚àí 0.522959z = (0.0112182, 0.0100964, ‚àí0.522741)t
y
g x(1) = 2.32762.
La tabla 10.5 contiene el resto de los resultados. La verdadera soluci√≥n para el sistema no
lineal es (0.5, 0, 20.5235988)t, por lo que x(2) SUREDEOHPHQWH VHUtD DGHFXDGR FRPR XQD
DSUR[LPDFLyQLQLFLDOSDUDHOPpWRGRGH1HZWRQRGH%UR\GHQ8QDGHODVWpFQLFDVTXHFRQYHUJHQPiVUiSLGDPHQWHVHUtDQDGHFXDGDVHQHVWDHWDSD\DTXHVHUHTXLHUHQLWHUDFLRQHV
del m√©todo de descenso m√°s r√°pido para encontrar x(k) ‚àí x ‚àû < 0.01.

Tabla 10.5

k

x1(k)

x2(k)

x3(k)

g(x1(k) , x2(k) , x3(k) )

2
3
4
5
6
7

0.137860
0.266959
0.272734
0.308689
0.314308
0.324267

‚àí0.205453
0.00551102
‚àí0.00811751
‚àí0.0204026
‚àí0.0147046
‚àí0.00852549

‚àí0.522059
‚àí0.558494
‚àí0.522006
‚àí0.533112
‚àí0.520923
‚àí0.528431

1.27406
1.06813
0.468309
0.381087
0.318837
0.287024

(ODOJRULWPRLPSOLFDHOPpWRGRGHGHVFHQVRPiVUiSLGRSDUDDSUR[LPDUHOYDORU
PtQLPRGHg (x). Para comenzar una iteraci√≥n, el valor 0 se asigna a Œ±1, y el valor 1 se asigna
a Œ±3. Si h(Œ±3 ) ‚â• h(Œ±1 ), entonces se realizan las divisiones sucesivas de Œ±3 entre 2 y el valor
de Œ±3 se reasigna hasta que h(Œ±3 ) < h(Œ±1 ) y Œ±3 = 2‚àík para alg√∫n valor de k.
3DUDXVDUHOPpWRGRSDUDDSUR[LPDUODVROXFLyQSDUDHOVLVWHPD

f 1 (x1 , x2 , . . . , xn ) = 0,
f 2 (x1 , x2 , . . . , xn ) = 0,
..
.

..
.

f n (x1 , x2 , . . . , xn ) = 0,
VLPSOHPHQWHUHHPSOD]DPRVODIXQFLyQg con

ALGORITMO

10.3

n
2
i=1 f i .

Descenso m√°s r√°pido
3DUDDSUR[LPDUXQDVROXFLyQp del problema de minimizaci√≥n

g(p) = m√≠nn g(x)
x‚ààR

GDGDXQDDSUR[LPDFLyQLQLFLDOx:
ENTRADA n√∫mero n GH YDULDEOHV DSUR[LPDFLyQ LQLFLDO x = (x1 , . . . , xn )t ; tolerancia
TOLQ~PHURPi[LPRGHLWHUDFLRQHVN.
SALIDA VROXFLyQDSUR[LPDGDx = (x1 , . . . , xn )t RXQPHQVDMHGHIDOOD

10.4

T√©cnicas de descenso m√°s r√°pido

497

Paso 1 Determine k = 1.
Paso 2 Mientras (k ‚â§ N ) haga los pasos 3‚Äì15.
Paso 3 Determine g1 = g(x1 , . . . , xn );

Nota: g1 = g x(k) .

z = ‚àág(x1 , . . . , xn );
z 0 = ||z||2 .

Nota: z = ‚àág x(k) .

Paso 4 Si z 0 = 0 entonces SALIDA (‚Äògradiente cero‚Äô);
SALIDA ( x1 , . . . , xn , g1 );
(Procedimiento completo, puede tener un m√≠nimo.)
PARE.
Paso 5 Determine z = z/z 0 ; (Convierta z en vector unidad.)
Œ±1 = 0;
Œ±3 = 1;
g3 = g(x ‚àí Œ±3 z).
Paso 6 Mientras (g3 ‚â• g1 ) haga los pasos 7 y 8.
Paso 7 Determine Œ±3 = Œ±3 /2;
g3 = g(x ‚àí Œ±3 z).
Paso 8 Si Œ±3 < TOL/2 entonces
SALIDA (‚ÄòSin probable mejora);
SALIDA (x1 , . . . , xn , g1 );
(Procedimiento completado, puede tener un m√≠nimo.)
PARE.
Paso 9 Determine Œ±2 = Œ±3 /2;
g2 = g(x ‚àí Œ±2 z).
Paso 10 Determine h 1 = (g2 ‚àí g1 )/Œ±2 ;
h 2 = (g3 ‚àí g2 )/(Œ±3 ‚àí Œ±2 );
h 3 = (h 2 ‚àí h 1 )/Œ±3 .
(Nota: La f√≥rmula de diferencias divididas hacia adelante de Newton
se usa para encontrar P (Œ±) = g1 + h 1 Œ± + h 3 Œ±(Œ± ‚àí Œ±2 ) la
cuadr√°tica que interpola h(Œ±) en Œ± = 0, Œ± = Œ±2 , Œ± = Œ±3 .)
Paso 11 Determine Œ±0 = 0.5(Œ±2 ‚àí h 1 / h 3 );
g0 = g(x ‚àí Œ±0 z).

(El punto cr√≠tico de P se presenta en Œ±0 .)

Paso 12 Encuentre Œ± de {Œ±0 , Œ±3 } de modo que g = g(x ‚àí Œ±z) = m√≠n{g0 , g3 }.
Paso 13 Determine x = x ‚àí Œ±z.
Paso 14 Si |g ‚àí g1 | < TOL entonces
SALIDA (x1 , . . . , xn , g);
(El procedimiento fue exitoso.)
PARE.
Paso 15 Determine k = k + 1.
Paso 16 SALIDA (‚Äòiteraciones m√°ximas excedidas‚Äô);
(El procedimiento no fue exitoso.)
PARE.

498

CAP√çTULO 10

Soluciones num√©ricas de sistemas de ecuaciones no lineales

([LVWHQPXFKDVYDULDFLRQHVGHOPpWRGRGHGHVFHQVRPiVUiSLGRDOJXQRVGHORVFXDOHV
implican m√©todos m√°s complejos para determinar el valor de ∆†TXHSURGXFLUiXQPtQLPR
SDUDODIXQFLyQGHYDULDEOH~QLFDhGH√ÄQLGDHQODHFXDFLyQ  2WUDVWpFQLFDVXVDQXQ
SROLQRPLRGH7D\ORUPXOWLGLPHQVLRQDOSDUDUHHPSOD]DUODIXQFLyQPXOWLYDULDEOHg original y
minimizar el polinomio en lugar de g$SHVDUGHTXHH[LVWHQYHQWDMDVSDUDDOJXQRVGHHVWRV
PpWRGRVVREUHHOSURFHGLPLHQWRDQDOL]DGRDTXtWRGRVORVPpWRGRVGHGHVFHQVRPiVUiSLGR
VRQHQJHQHUDOOLQHDOPHQWHFRQYHUJHQWHV\FRQYHUJHQLQGHSHQGLHQWHPHQWHGHODDSUR[LPDci√≥n inicial. En algunos casos, sin embargo, los m√©todos pueden converger en algo m√°s que
HOPtQLPRDEVROXWRGHODIXQFLyQJ
Un an√°lisis m√°s completo de los m√©todos de descenso m√°s r√°pido se puede encontrar en
[OR] o [RR].
La secci√≥n Conjunto de ejercicios 10.4 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

10.5 Homotop√≠a y m√©todos de continuaci√≥n
Los m√©todos de homotop√≠a, o continuaci√≥n, para los sistemas no lineales insertaron el proEOHPDTXHGHEHUHVROYHUVHGHQWURGHXQFRQMXQWRGHSUREOHPDV(QHVSHFt√ÄFRSDUDUHVROYHU
XQSUREOHPDGHODIRUPD

F(x) = 0,

8QDKRPRWRStDHVXQD
GHIRUPDFLyQFRQWLQXDXQD
IXQFLyQTXHWRPDXQLQWHUYDOR
real continuamente en un
FRQMXQWRGHIXQFLRQHV

que posee la soluci√≥n desconocida x‚àó FRQVLGHUDPRV XQD IDPLOLD GH SUREOHPDV GHVFULWRV
usando un par√°metro Œª que supone valores en [0, 1]. Un problema con una soluci√≥n conocida x(0) corresponde a la situaci√≥n cuando Œª 5 0 y el problema con la soluci√≥n desconocida
x(1) ‚â° x‚àó corresponde a Œª 5 1.
Por ejemplo suponga que x  HVXQDDSUR[LPDFLyQLQLFLDOSDUDODVROXFLyQGHF(x‚àó ) = 0.
'H√ÄQD

G : [0, 1] √ó Rn ‚Üí Rn
mediante

G(Œª, x) = ŒªF(x) + (1 ‚àí Œª) [F(x) ‚àí F(x(0))] = F(x) + (Œª ‚àí 1)F(x(0)). (10.19)
Determinaremos, para varios valores de Œª, una soluci√≥n para

G(Œª, x) = 0.
Cuando Œª 5HVWDHFXDFLyQDVXPHODIRUPD

0 = G(0, x) = F(x) ‚àí F(x(0)),
y x(0) es una soluci√≥n. Cuando Œª 5ODHFXDFLyQDVXPHODIRUPD

0 = G(1, x) = F(x),
y x(1) = x‚àó es una soluci√≥n.
/DIXQFLyQG, con el par√°metro ŒªQRVSURSRUFLRQDXQDIDPLOLDGHIXQFLRQHVTXHSXHGH
conducir al valor conocido x(0) para la soluci√≥n x(1) 5 x‚àó/DIXQFLyQG recibe el nombre
de homotop√≠aHQWUHODIXQFLyQG(0, x) = F(x)‚àíF(x(0))\ODIXQFLyQG(1, x) = F(x).

10.5 Homotop√≠a y m√©todos de continuaci√≥n

499

M√©todo de continuaci√≥n
El problema de continuaci√≥n es:
¬á 'HWHUPLQDUXQDIRUPDGHSURFHGHUFRQODVROXFLyQFRQRFLGDx(0) de G(0, x) = 0 para la
soluci√≥n desconocida x(1) 5 x‚àó de G(1, x) 5 0, es decir, la soluci√≥n para F(x) 5 0.
Primero suponemos que x(Œª) es una soluci√≥n √∫nica para la ecuaci√≥n

G(Œª, x) = 0,

(10.20)

para cada Œª ‚àà [0, 1]. El conjunto { x(Œª) | 0 ‚â§ Œª ‚â§ 1 } se puede observar como una
curva en Rn desde x(0) hasta x(1) = x‚àó parametrizada por Œª. Un m√©todo de continuaci√≥n encuentra una sucesi√≥n de pasos a lo largo de esta curva correspondiente a
{x(Œªk )}m
k=0 , donde Œª0 = 0 < Œª1 < ¬∑ ¬∑ ¬∑ < Œªm = 1.
6LODVIXQFLRQHVŒª ‚Üí x(Œª) y GVRQGLIHUHQFLDEOHVHQWRQFHVGHULYDUODHFXDFLyQ  
respecto a Œª nos da

0=

‚àÇG(Œª, x(Œª)) ‚àÇG(Œª, x(Œª))
+
x (Œª),
‚àÇŒª
‚àÇx

y resolver para x (Œª) nos da

x (Œª) = ‚àí

‚àÇG(Œª, x(Œª)) ‚àí1 ‚àÇG(Œª, x(Œª))
.
‚àÇx
‚àÇŒª

eVWHHVXQVLVWHPDGHHFXDFLRQHVGLIHUHQFLDOHVFRQODFRQGLFLyQLQLFLDOx(0).
Puesto que

G(Œª, x(Œª)) = F(x(Œª)) + (Œª ‚àí 1)F(x(0)),
podemos determinar tanto

‚é° ‚àÇf

1

(x(Œª))

‚é¢ ‚àÇ x1
‚é¢
‚é¢ ‚àÇ f2
‚é¢
(x(Œª))
‚àÇG
‚é¢
(Œª, x(Œª)) = ‚é¢ ‚àÇ x1
‚é¢
‚àÇx
..
‚é¢
.
‚é¢
‚é£ ‚àÇf
n
(x(Œª))
‚àÇ x1

‚àÇ f1
(x(Œª))
‚àÇ x2
‚àÇ f2
(x(Œª))
‚àÇ x2
..
.
‚àÇ fn
(x(Œª))
‚àÇ x2

...
...

...

‚é§
‚àÇ f1
(x(Œª))
‚é•
‚àÇ xn
‚é•
‚é•
‚àÇ f2
(x(Œª)) ‚é•
‚é•
‚àÇ xn
‚é• = J (x(Œª)),
‚é•
..
‚é•
.
‚é•
‚é¶
‚àÇ fn
(x(Œª))
‚àÇ xn

la matriz jacobiana como

‚àÇG(Œª, x(Œª))
= F(x(0)).
‚àÇŒª
3RUORWDQWRHOVLVWHPDGHHFXDFLRQHVGLIHUHQFLDOHVVHFRQYLHUWHHQ

x (Œª) = ‚àí[J (x(Œª))]‚àí1 F(x(0)),

para

0 ‚â§ Œª ‚â§ 1,

(10.21)

con la condici√≥n inicial x(0). El siguiente teorema (consulte [OR], pp. 230‚Äì231) da las conGLFLRQHVHQODVTXHHOPpWRGRGHFRQWLQXDFLyQHVIDFWLEOH
Teorema 10.10

Sea F(x  FRQWLQXDPHQWH GLIHUHQFLDEOH HQ x ‚àà Rn . Suponga que la matriz jacobiana J (x)
es no singular para todas las x ‚àà Rn y que la constante MH[LVWHFRQ J (x)‚àí1
M, para
todas las x ‚àà Rn . Entonces, para cualquier x(0) en RnH[LVWHXQD~QLFDIXQFLyQx(Œª), tal que

G(Œª, x(Œª)) = 0,

500

CAP√çTULO 10

Soluciones num√©ricas de sistemas de ecuaciones no lineales

para toda Œª en [0, 1]. Adem√°s, x(Œª HVFRQWLQXDPHQWHGLIHUHQFLDEOH\

x (Œª) = ‚àíJ (x(Œª))‚àí1 F(x(0)),

para cada Œª ‚àà [0, 1].

/RVLJXLHQWHPXHVWUDODIRUPDGHOVLVWHPDGHHFXDFLRQHVGLIHUHQFLDOHVUHODFLRQDGDVFRQ
un sistema de ecuaciones no lineales.
Ilustraci√≥n

Considere el sistema no lineal

f 1 (x1 , x2 , x3 ) = 3x1 ‚àí cos(x2 x3 ) ‚àí 0.5 = 0,
f 2 (x1 , x2 , x3 ) = x12 ‚àí 81(x2 + 0.1)2 + sen x3 + 1.06 = 0,
f 3 (x1 , x2 , x3 ) = e‚àíx1 x2 + 20x3 +
La matriz jacobiana es
‚é°

3
2x1
J (x) = ‚é£
‚àíx2 e‚àíx1 x2

10œÄ ‚àí 3
= 0.
3

‚é§
x2 sen x2 x3
x3 sen x2 x3
‚àí162(x2 + 0.1)
cos x3 ‚é¶ .
‚àíx1 e‚àíx1 x2
20

Sea x(0) = (0, 0, 0)t , DVtTXH

‚é°

‚é§
‚àí1.5
F(x(0)) = ‚é£ 0.25 ‚é¶ .
10œÄ/3
(OVLVWHPDGHHFXDFLRQHVGLIHUHQFLDOHVHV
‚é°
‚é§
‚é°
‚é§‚àí1 ‚é°
‚é§
x1 (Œª)
3
x3 sen x2 x3
x2 sen x2 x3
‚àí1.5
‚é£ x2 (Œª) ‚é¶ = ‚àí ‚é£
2x1
‚àí162(x2 + 0.1)
cos x3 ‚é¶ ‚é£ 0.25 ‚é¶ .
‚àíx1 x2
‚àíx1 x2
‚àíx2 e
‚àíx1 e
20
x3 (Œª)
10œÄ/3
(QJHQHUDOHOVLVWHPDGHHFXDFLRQHVGLIHUHQFLDOHVTXHQHFHVLWDPRVUHVROYHUSDUDQXHVWUR
SUREOHPDGHFRQWLQXDFLyQWLHQHODIRUPD

d x1
= œÜ1 (Œª, x1 , x2 , . . . , xn ),
dŒª
d x2
= œÜ2 (Œª, x1 , x2 , . . . , xn ),
dŒª
..
.
d xn
= œÜn (Œª, x1 , x2 , . . . , xn ),
dŒª
donde

‚é°

‚é°
‚é§
‚é§
œÜ1 (Œª, x1 , . . . , xn )
f 1 (x(0))
‚é¢ œÜ2 (Œª, x1 , . . . , xn ) ‚é•
‚é¢ f 2 (x(0)) ‚é•
‚é¢
‚é•
‚é•
‚àí1 ‚é¢
=
‚àíJ
(x
,
.
.
.
,
x
)
‚é¢
‚é¢
‚é•
‚é•.
..
..
1
n
‚é£
‚é£
‚é¶
‚é¶
.
.
œÜn (Œª, x1 , . . . , xn )
f n (x(0))

(10.22)

3DUD XVDU PpWRGR 5XQJH.XWWD GH RUGHQ  HQ OD UHVROXFLyQ GH HVWH VLVWHPD SULPHUR
seleccionamos un entero N . 0 y establecemos que h = (1 ‚àí 0)/N . La partici√≥n del intervalo [0, 1] en N subintervalos con los puntos de malla

Œª j = j h, para cada j = 0, 1, . . . , N .

10.5 Homotop√≠a y m√©todos de continuaci√≥n

501

Usamos la notaci√≥n wi j para cada j 5 0, 1, . . . , N y i 5 1, . . . , nSDUDGHQRWDUXQDDSUR[Lmaci√≥n para xi (Œª j ). Para las condiciones iniciales, establecemos

w 1,0 = x1 (0),

w 2,0 = x2 (0),

... ,

w n,0 = xn (0).

Suponga que se ha calculado w 1, j , w 2, j , . . . , w n, j. Obtenemos w 1, j+1 , w 2, j+1 , . . . ,
w n, j+1 usando las ecuaciones

k1,i = hœÜi (Œª j , w 1, j , w 2, j , . . . , w n, j ),

para cada i = 1, 2, . . . , n;

k2,i = hœÜi

Œªj +

h
1
1
, w 1, j + k1,1 , . . . , w n, j + k1,n ,
2
2
2

para cada i = 1, 2, . . . , n;

k3,i = hœÜi

Œªj +

h
1
1
, w 1, j + k2,1 , . . . , w n, j + k2,n ,
2
2
2

para cada i = 1, 2, . . . , n;

k4,i = hœÜi (Œª j + h, w 1, j + k3,1 , w 2, j + k3,2 , . . . , w n, j + k3,n ), para cada i = 1, 2, . . . , n;
\√ÄQDOPHQWH

w i, j+1 = w i, j +

1
k1,i + 2k2,i + 2k3,i + k4,i ,
6

para cada i = 1, 2, . . . , n.

La notaci√≥n de vector
‚é°
‚é°
‚é°
‚é°
‚é§
‚é§
‚é§
‚é§
k1,1
k2,1
k3,1
k4,1
‚é¢ k1,2 ‚é•
‚é¢ k2,2 ‚é•
‚é¢ k3,2 ‚é•
‚é¢ k4,2 ‚é•
‚é¢
‚é¢
‚é¢
‚é¢
‚é•
‚é•
‚é•
‚é•
k1 = ‚é¢ . ‚é•, k2 = ‚é¢ . ‚é•, k3 = ‚é¢ . ‚é•, k4 = ‚é¢ . ‚é•,
‚é£ .. ‚é¶
‚é£ .. ‚é¶
‚é£ .. ‚é¶
‚é£ .. ‚é¶

k1,n

k2,n

k3,n

k4,n

‚é°

y

‚é§
w 1, j
‚é¢ w 2, j ‚é•
‚é¢
‚é•
wj = ‚é¢ . ‚é•
‚é£ .. ‚é¶
w n, j

VLPSOL√ÄFDODSUHVHQWDFLyQ(QWRQFHVODHFXDFLyQ  QRVGD x(0) = x(Œª0 ) = w0 , y por
cada j = 0, 1, . . . , N ,

‚é°

‚é§
œÜ1 (Œª j , w 1, j , . . . , w n, j )
‚é¢ œÜ2 (Œª j , w 1, j , . . . , w n, j ) ‚é•
‚àí1
‚é¢
‚é•
F(x(0))
k1 = h ‚é¢
‚é• = h ‚àíJ (w 1, j , . . . , w n, j )
..
‚é£
‚é¶
.
œÜn (Œª j , w 1, j , . . . , w n, j )

= h ‚àíJ (w j )

‚àí1

F(x(0)),

k2 = h ‚àíJ

1
w j + k1
2

k3 = h ‚àíJ

1
w j + k2
2

k4 = h ‚àíJ w j + k3

‚àí1

‚àí1

F(x(0)),
‚àí1

F(x(0)),
F(x(0)),

y
x(Œª j+1 ) = x(Œª j ) +

1
1
(k1 + 2k2 + 2k3 + k4 ) = w j + (k1 + 2k2 + 2k3 + k4 ) .
6
6

)LQDOPHQWHx(Œªn ) = x(1)HVQXHVWUDDSUR[LPDFLyQSDUDx‚àó .

502

CAP√çTULO 10

Ejemplo 1

Soluciones num√©ricas de sistemas de ecuaciones no lineales

Use el m√©todo de continuaci√≥n con x(0) = (0, 0, 0)tSDUDDSUR[LPDUODVROXFLyQGH

f 1 (x1 , x2 , x3 ) = 3x1 ‚àí cos(x2 x3 ) ‚àí 0.5 = 0,
f 2 (x1 , x2 , x3 ) = x12 ‚àí 81(x2 + 0.1)2 + sen x3 + 1.06 = 0,
f 3 (x1 , x2 , x3 ) = e‚àíx1 ,x2 + 20x3 +
Soluci√≥n

10œÄ ‚àí 3
= 0.
3

La matriz jacobiana es

‚é°

3
2x1
J (x) = ‚é£
‚àíx2 e‚àíx1 x2

‚é§
x2 sen x2 x3
x3 sen x2 x3
‚àí162(x2 + 0.1)
cos x3 ‚é¶
‚àíx1 e‚àíx1 x2
20

y
F(x(0)) = (‚àí1.5, 0.25, 10œÄ/3)t .
Con N 5 4 y h 5 0.25, tenemos

‚é°

3
k1 = h[‚àíJ (x(0) )]‚àí1 F(x(0)) = 0.25 ‚é£ 0
0

‚é§
‚é§‚àí1 ‚é°
‚àí1.5
0
0
‚àí16.2 1 ‚é¶ ‚é£ 0.25 ‚é¶
10œÄ/3
0
20

= (0.125, ‚àí0.004222203325, ‚àí0.1308996939)t ,
k2 = h[‚àíJ (0.0625, ‚àí0.002111101663, ‚àí0.06544984695)]‚àí1 (‚àí1.5, 0.25, 10œÄ/3)t
‚é°

‚é§‚àí1 ‚é°
‚é§
3
‚àí0.9043289149 √ó 10‚àí5 ‚àí0.2916936196 √ó 10‚àí6
‚àí1.5
‚é¶ ‚é£ 0.25 ‚é¶
0.125
‚àí15.85800153
0.9978589232
= 0.25‚é£
10œÄ/3
0.002111380229
‚àí0.06250824706
20

= (0.1249999773, ‚àí0.003311761993, ‚àí0.1309232406)t ,
k3 = h[‚àíJ (0.06249998865,‚àí0.001655880997,‚àí0.0654616203)]‚àí1 (‚àí1.5, 0.25, 10œÄ/3)t
= (0.1249999844, ‚àí0.003296244825, ‚àí0.130920346)t ,
k4 = h[‚àíJ (0.1249999844, ‚àí0.003296244825, ‚àí0.130920346)]‚àí1 (‚àí1.5, 0.25, 10œÄ/3)t
= (0.1249998945, ‚àí0.00230206762, ‚àí0.1309346977)t ,
y
1
x(Œª1 ) = w1 = w0 + (k1 + 2k2 + 2k3 + k4 )
6
= (0.1249999697, ‚àí0.00329004743, ‚àí0.1309202608)t .
Al continuar, tenemos

x(Œª2 ) = w2 = (0.2499997679, ‚àí0.004507400128, ‚àí0.2618557619)t ,
x(Œª3 ) = w3 = (0.3749996956, ‚àí0.003430352103, ‚àí0.3927634423)t ,
y
x(Œª4 ) = x(1) = w4 = (0.4999999954, 0.126782 √ó 10‚àí7 , ‚àí0.5235987758)t .
Estos resultados son muy precisos porque la soluci√≥n real es (0.5, 0, ‚àí0.52359877)t .

10.5 Homotop√≠a y m√©todos de continuaci√≥n

503

2EVHUYHTXHHQHOPpWRGRGH5XQJH.XWWDORVSDVRVVLPLODUHVD

ki = h[‚àíJ (x(Œªi ) + Œ±i‚àí1 ki‚àí1 )]‚àí1 F(x(0))
se pueden escribir como soluci√≥n para ki en el sistema lineal

J (x(Œªi ) + Œ±i‚àí1 ki‚àí1 ) ki = ‚àíhF(x(0)).
3RUORTXHHQHOPpWRGR5XQJH.XWWDGHRUGHQHOFiOFXORGHFDGDwj requiere resolver
cuatro sistemas lineales, cada uno al calcular k1 , k2 , k3 y k4 . Por lo tanto, para usar N pasos
se requiere resolver 4NVLVWHPDVOLQHDOHV3RUFRPSDUDFLyQHOPpWRGRGH1HZWRQUHTXLHUH
resolver un sistema lineal por iteraci√≥n. Por tanto, el trabajo relacionado con el m√©todo de
5XQJH.XWWDHVDSUR[LPDGDPHQWHHTXLYDOHQWHDNLWHUDFLRQHVGHOPpWRGRGH1HZWRQ
8QDDOWHUQDWLYDHVXVDUHOPpWRGRGH5XQJH.XWWDGHRUGHQGHWDOIRUPDTXHHOPpWRGRGH(XOHUPRGL√ÄFDGRRLQFOXVRHOPpWRGRGH(XOHUSDUDGLVPLQXLUHOQ~PHURGHVLVWHPDV
lineales que se tienen que resolver. Otra posibilidad es usar valores m√°s peque√±os de N. Lo
siguiente ilustra estas ideas.
Ilustraci√≥n

/DWDEODUHVXPHXQDFRPSDUDFLyQGHOPpWRGRGH(XOHUHOPpWRGRGHSXQWRPHGLR\
HOPpWRGRGH5XQJH.XWWDGHRUGHQDSOLFDGRDOSUREOHPDHQHOHMHPSORFRQXQDDSUR[Lmaci√≥n inicial x(0) 5 (0, 0, 0)t. La columna a la derecha en la tabla enumera el n√∫mero de
sistemas lineales que se requieren para la soluci√≥n.

Tabla 10.6
N

M√©todo
Euler
Euler
Punto medio
Punto medio
Runge-Kutta
Runge-Kutta

1
4
1
4
1
4

Sistemas

x(1)
t

(0.5, ‚àí0.0168888133, ‚àí0.5235987755)
(0.499999379, ‚àí0.004309160698, ‚àí0.523679652)t
(0.4999966628, ‚àí0.00040240435, ‚àí0.523815371)t
(0.500000066, ‚àí0.00001760089, ‚àí0.5236127761)t
(0.4999989843, ‚àí0.1676151 √ó 10‚àí5 , ‚àí0.5235989561)t
(0.4999999954, 0.126782 √ó 10‚àí7 , ‚àí0.5235987758)t

1
4
2
8
4
16

El m√©todo de continuaci√≥n se puede usar como un m√©todo independiente y no requiere
una selecci√≥n particularmente buena de x(0). Sin embargo, el m√©todo tambi√©n puede usarse
SDUD SURSRUFLRQDU XQD DSUR[LPDFLyQ LQLFLDO SDUD ORV PpWRGRV GH 1HZWRQ R %UR\GHQ 3RU
ejemplo, el resultado obtenido en el ejemplo 2 usando el m√©todo de Euler y N 5SRGUtDVHU
IiFLOPHQWHVX√ÄFLHQWHSDUDLQLFLDUORVPpWRGRVGH1HZWRQR%UR\GHQPiVH√ÄFLHQWHV\SRGUtDQ
ser mejores para este objetivo que los m√©todos de continuaci√≥n, lo cual requiere m√°s c√°lculos. El algoritmo 10.4 es una implementaci√≥n del m√©todo de continuaci√≥n.

ALGORITMO

10.4

Algoritmo de continuaci√≥n
3DUDDSUR[LPDUODVROXFLyQGHOVLVWHPDQROLQHDOF(x) = 0GDGDXQDDSUR[LPDFLyQLQLFLDOx:
ENTRADA n√∫mero n de ecuaciones y de inc√≥gnitas; entero N .DSUR[LPDFLyQLQLFLDO
x = (x1 , x2 , . . . , xn )t .
SALIDA VROXFLyQDSUR[LPDGDx = (x1 , x2 , . . . , xn )t .

Paso 1 Determine h = 1/N ;
b = ‚àíhF(x).
Paso 2 Para i = 1, 2, . . . , N haga los pasos 3‚Äì7.

504

CAP√çTULO 10

Soluciones num√©ricas de sistemas de ecuaciones no lineales

Paso 3 Determine A = J (x);
Resuelva el sistema lineal Ak1 = b.
Paso 4 Determine A = J (x + 12 k1 );
Resuelva el sistema lineal Ak2 = b.
Paso 5 Determine A = J (x + 12 k2 );
Resuelva el sistema lineal Ak3 = b.
Paso 6 Determine A = J (x + k3 );
Resuelva el sistema lineal Ak3 = b.
Paso 7 Determine x = x + (k1 + 2k2 + 2k3 + k4 )/6.
Paso 8 SALIDA (x1 , x2 , . . . , xn );
PARE.
La secci√≥n Conjunto de ejercicios 10.5 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

10.6 Software num√©rico
El paquete Hompack en Netlib resuelve un sistema de ecuaciones no lineales al utilizar vaULRVPpWRGRVGHKRPRWRStD
Los sistemas no lineales en las bibliotecas IMSL y NAG usan el m√©todo Levenberg0DUTXDUGWTXHHVXQSURPHGLRSRQGHUDGRGHOPpWRGRGH1HZWRQ\HOPpWRGRGHGHVFHQVR
m√°s r√°pido. El peso se sesga hacia el m√©todo de descenso m√°s r√°pido hasta que se detecta
ODFRQYHUJHQFLDWLHPSRHQHOTXHHOSHVRFDPELDKDFLDXQPpWRGRGH1HZWRQTXHFRQYHUJH
PiVUiSLGR(QFXDOTXLHUUXWLQDVHSXHGHXVDUXQDDSUR[LPDFLyQGHGLIHUHQFLD√ÄQLWDSDUDHO
jacobiano o una subrutina proporcionada por el usuario para calcular el jacobiano.
Las secciones Preguntas de an√°lisis, Conceptos clave y Revisi√≥n del cap√≠tulo est√°n disponibles en l√≠nea. Encuentre la ruta de acceso en las p√°ginas preliminares.

CAP√çTULO

11

Problemas de valor en la frontera para
ecuaciones diferenciales ordinarias
Introducci√≥n
8QSUREOHPDFRP~QHQLQJHQLHUtDFLYLOFRQFLHUQHDODGH√ÅH[LyQGHXQDYLJDGHVHFFLyQWUDQVYHUVDOUHFWDQJXODUVXMHWDDFDUJDXQLIRUPHPLHQWUDVORVH[WUHPRVGHODYLJDHVWiQVXMHWRVGH
WDOIRUPDTXHQRVXIUHQGH√ÅH[LyQ

S

S
0

w(x)

l

x

6XSRQJDTXHl, q, E, S e IUHSUHVHQWDQUHVSHFWLYDPHQWHODORQJLWXGGHODYLJDODLQWHQVLGDGGHODFDUJDXQLIRUPHHOPyGXORGHHODVWLFLGDGHOHVIXHU]RHQORVH[WUHPRV\HOPRPHQWR
FHQWUDOGHLQHUFLD/DHFXDFLyQGLIHUHQFLDOTXHDSUR[LPDODVLWXDFLyQItVLFDHVGHODIRUPD

S
qx
d 2w
(x) =
w(x) +
(x ‚àí l),
dx2
EI
2E I
donde w (x HVODGH√ÅH[LyQGHXQDGLVWDQFLDxGHVGHHOH[WUHPRL]TXLHUGRGHODYLJD3XHVWR
TXHQRVHSUHVHQWDGH√ÅH[LyQHQORVH[WUHPRVGHODYLJDH[LVWHQGRVFRQGLFLRQHVHQODIURQWHUD

w(0) = 0

y

w(l) = 0.

&XDQGRODYLJDWLHQHXQJURVRUXQLIRUPHHOSURGXFWREIHVFRQVWDQWH(QHVWHFDVROD
VROXFLyQ H[DFWD VH REWLHQH IiFLOPHQWH &XDQGR HO JURVRU QR HV XQLIRUPH HO PRPHQWR GH
inercia IHVXQDIXQFLyQGHx,\VHQHFHVLWDQWpFQLFDVGHDSUR[LPDFLyQ/RVSUREOHPDVGHHVWH
WLSRVHFRQVLGHUDQHQHOHMHUFLFLRGHODVHFFLyQHQHOHMHUFLFLRGHODVHFFLyQ\
HQHOHMHUFLFLRGHODVHFFLyQ
/DVHFXDFLRQHVGLIHUHQFLDOHVHQHOFDStWXORVRQGHSULPHURUGHQ\WLHQHQXQDFRQGLFLyQ
LQLFLDOTXHVDWLVIDFHU0iVDGHODQWHHQHVHFDStWXORREVHUYDPRVTXHODVWpFQLFDVVHSRGUtDQ
DPSOLDUDORVVLVWHPDVGHHFXDFLRQHV\GHVSXpVDHFXDFLRQHVGHRUGHQVXSHULRUSHURWRGDV
ODVFRQGLFLRQHVHVSHFt√ÄFDVHVWiQHQHOPLVPRH[WUHPR(VWRVVRQSUREOHPDVGHYDORULQLFLDO(QHVWHFDStWXORPRVWUDPRVFyPRDSUR[LPDUODVROXFLyQDORVSUREOHPDVGH valor en
la fronteraHFXDFLRQHVGLIHUHQFLDOHVFRQFRQGLFLRQHVLPSXHVWDVHQGLIHUHQWHVSXQWRV3DUD
HFXDFLRQHVGLIHUHQFLDOHVGHSULPHURUGHQVyORVHHVSHFL√ÄFDXQDFRQGLFLyQSRUORTXHQR
H[LVWHGLVWLQFLyQHQWUHORVSUREOHPDVGHYDORULQLFLDO\GHYDORUHQODIURQWHUD1RVRWURVFRQVLGHUDUHPRVHFXDFLRQHVGHVHJXQGRRUGHQFRQGRVYDORUHVHQODIURQWHUD
$PHQXGRORVSUREOHPDVItVLFRVTXHGHSHQGHQGHODSRVLFLyQHQOXJDUGHOWLHPSRVH
GHVFULEHQHQWpUPLQRVGHHFXDFLRQHVGLIHUHQFLDOHVFRQFRQGLFLRQHVLPSXHVWDVHQPiVGHXQ
SXQWR
505

506

CAP√çTULO 11

Problemas de valor en la frontera para ecuaciones diferenciales ordinarias

(QHVWHFDStWXORORVSUREOHPDVGHYDORUHQODIURQWHUDGHGRVSXQWRVLPSOLFDQXQDHFXDFLRQHV
GLIHUHQFLDOGHVHJXQGRRUGHQGHODIRUPD

y = f (x, y, y ),

para a ‚â§ x ‚â§ b,



y(b) = Œ≤.



MXQWRFRQODVFRQGLFLRQHVGHIURQWHUD

y(a) = Œ±

y

11.1 El m√©todo de disparo lineal
(OVLJXLHQWHWHRUHPDHVWDEOHFHODVFRQGLFLRQHVJHQHUDOHVTXHJDUDQWL]DQODH[LVWHQFLDGHOD
VROXFLyQSDUDXQSUREOHPDGHYDORUHQODIURQWHUDGHVHJXQGRRUGHQ/DSUXHEDGHHVWHWHRUHPDVHSXHGHHQFRQWUDUHQ>.HOOHU+@
Teorema 11.1

6XSRQJDTXHODIXQFLyQfHQHOSUREOHPDGHYDORUHQODIURQWHUD

y = f (x, y, y ), para a ‚â§ x ‚â§ b, con y(a) = Œ± y y(b) = Œ≤,
HVFRQWLQXDHQHOFRQMXQWR

D = { (x, y, y ) | para a ‚â§ x ‚â§ b, con ‚àí‚àû < y < ‚àû y ‚àí‚àû < y < ‚àû },
\TXHODVGHULYDGDVSDUFLDOHV f y y f y WDPELpQVRQFRQWLQXDVHQD6L
i)

f y (x, y, y ) > 0, SDUDWRGDV(x, y, y ) ‚àà D, \

ii)

H[LVWHXQDFRQVWDQWHMFRQ

f y (x, y, y ) ‚â§ M,SDUDWRGDV(x, y, y ) ‚àà D,
HQWRQFHVHOSUREOHPDGHYDORUHQODIURQWHUDWLHQHXQD~QLFDVROXFLyQ
Ejemplo 1

8VHHOWHRUHPDSDUDPRVWUDUTXHHOSUREOHPDGHYDORUHQODIURQWHUD

y + e‚àíx y + sen y = 0, para 1 ‚â§ x ‚â§ 2, con y(1) = y(2) = 0,
WLHQHXQD~QLFDVROXFLyQ
Soluci√≥n WHQHPRV

f (x, y, y ) = ‚àíe‚àíx y ‚àí sen y
\SDUDWRGDVODVxHQ>@

f y (x, y, y ) = xe‚àíx y > 0

y

f y (x, y, y ) = | ‚àí cos y | ‚â§ 1.

3RUORTXHHOSUREOHPDWLHQHXQD~QLFDVROXFLyQ

11.1 El m√©todo de disparo lineal

507

Problema lineal de valor en la frontera
/DHFXDFLyQGLIHUHQFLDO

y = f (x, y, y )
HVOLQHDOFXDQGRODVIXQFLRQHVp (x q (x \r (x H[LVWHQFRQ
8QDHFXDFLyQOLQHDOLPSOLFD
VRODPHQWHSRWHQFLDVOLQHDOHVGHy
\VXVGHULYDGDV

f (x, y, y ) = p(x)y + q(x)y + r (x).
/RVSUREOHPDVGHHVWHWLSRVHSUHVHQWDQFRQIUHFXHQFLD\HQHVWDVLWXDFLyQHOWHRUHPD
VHSXHGHVLPSOL√ÄFDU

Corolario 11.2

6XSRQJDTXHHOSUREOHPDOLQHDOGHYDORUHQODIURQWHUD

y = p(x)y + q(x)y + r (x),

con a ‚â§ x ‚â§ b, con y(a) = Œ± y y(b) = Œ≤,

VDWLVIDFH
i)

p (x q (x \r (x VRQFRQWLQXDVHQ>ab@

ii)

q (x) .HQ>ab@

(QWRQFHVHOSUREOHPDGHYDORUHQODIURQWHUDWLHQHXQD~QLFDVROXFLyQ
3DUD DSUR[LPDU OD VROXFLyQ ~QLFD GH HVWH SUREOHPD OLQHDO SULPHUR FRQVLGHUDPRV ORV
SUREOHPDVGHYDORULQLFLDO

y = p(x)y + q(x)y + r (x), con

a ‚â§ x ‚â§ b,

y(a) = Œ±, y

a ‚â§ x ‚â§ b,

y(a) = 0, y

y (a) = 0,



y
y = p(x)y + q(x)y, con

y (a) = 1.



(OWHRUHPDHQODVHFFLyQ FRQVXOWDUODSiJLQD JDUDQWL]DTXHGHDFXHUGRFRQOD
KLSyWHVLVHQHOFRURODULRDPERVSUREOHPDVWLHQHQVROXFLyQ~QLFD
6HDTXH y1 (x)GHQRWDODVROXFLyQSDUDODHFXDFLyQ  \VHDTXH y2 (x)GHQRWDODVROXFLyQSDUDODHFXDFLyQ  6XSRQJDTXH y2 (b) = 0.  4XH y2 (b) = 0 HVWiHQFRQ√ÅLFWR
FRQODKLSyWHVLVGHOFRURODULRVHFRQVLGHUDHQHOHMHUFLFLR 'H√ÄQD

y(x) = y1 (x) +

Œ≤ ‚àí y1 (b)
y2 (x).
y2 (b)



(QWRQFHVy (x HVODVROXFLyQGHOSUREOHPDOLQHDOHQODIURQWHUD  3DUDREVHUYDUORSULPHURQRWHTXH

y (x) = y1 (x) +

Œ≤ ‚àí y1 (b)
y2 (x)
y2 (b)

y (x) = y1 (x) +

Œ≤ ‚àí y1 (b)
y2 (x).
y2 (b)

y

$OVXVWLWXLUSDUDy1 (x)\y2 (x)HQHVWDHFXDFLyQREWHQHPRV

y = p(x)y1 + q(x)y1 + r (x) +
= p(x) y1 +

Œ≤ ‚àí y1 (b)
y2
y2 (b)

Œ≤ ‚àí y1 (b)
p(x)y2 + q(x)y2
y2 (b)
+ q(x) y1 +

= p(x)y (x) + q(x)y(x) + r (x).

Œ≤ ‚àí y1 (b)
y2
y2 (b)

+ r (x)

508

CAP√çTULO 11

Problemas de valor en la frontera para ecuaciones diferenciales ordinarias

$GHPiV

y(a) = y1 (a) +

Œ≤ ‚àí y1 (b)
Œ≤ ‚àí y1 (b)
y2 (a) = Œ± +
¬∑0=Œ±
y2 (b)
y2 (b)

y(b) = y1 (b) +

Œ≤ ‚àí y1 (b)
y2 (b) = y1 (b) + Œ≤ ‚àí y1 (b) = Œ≤.
y2 (b)

y

Disparo lineal
(VWH¬¥GLVSDUR¬µJROSHDHOREMHWLYR
GHVSXpVGHXQGLVSDURGH
SUXHED(QODVLJXLHQWHVHFFLyQ
REVHUYDPRVTXHSUREOHPDVQR
OLQHDOHVUHTXLHUHQP~OWLSOHV
GLVSDURV

(O PpWRGR GH GLVSDUR SDUD HFXDFLRQHV OLQHDOHV HVWi EDVDGR HQ HO UHHPSOD]R GHO SUREOHPD
OLQHDO GH YDORU HQ OD IURQWHUD PHGLDQWH ORV GRV SUREOHPDV GH YDORU LQLFLDO   \  
([LVWHQ QXPHURVRV PpWRGRV D SDUWLU GHO FDStWXOR  SDUD DSUR[LPDU ODV VROXFLRQHV y(x  \
y(x \XQDYH]TXHHVWDVDSUR[LPDFLRQHVHVWiQGLVSRQLEOHVODVROXFLyQSDUDHOSUREOHPDGH
YDORUHQODIURQWHUDVHDSUR[LPDXVDQGRODHFXDFLyQ  *Ui√ÄFDPHQWHHOPpWRGRWLHQHHO
DVSHFWRPRVWUDGRHQOD√ÄJXUD

Figura 11.1
y
y 2(x)

b

y(x) 5 y1(x) 1

b 2 y1(b)
y 2(x)
y 2(b)

y 1(x)

a
a

b

x

(ODOJRULWPRXVDODWpFQLFD5XQJH.XWWDGHFXDUWRRUGHQSDUDHQFRQWUDUODDSUR[LPDFLyQSDUDy(x \y(x SHURRWUDVWpFQLFDVSDUDDSUR[LPDUODVVROXFLRQHVSDUDSUREOHPDV
GHYDORULQLFLDOVHSXHGHQVXVWLWXLUHQHOSDVR
3ULPHURHVFULELPRVODHFXDFLyQ  FRPRXQVLVWHPDGHGRVHFXDFLRQHVGLIHUHQFLDOHV
DOSHUPLWLUz 1 (x) = y(x) y z 2 (x) = y (x) GHWDOIRUPDTXH

z 1 (x) = z 2 (x)
z 2 (x) = p(x)z 2 (x) + q(x)z 1 (x) + r (x)
para a ‚â§ x ‚â§ b con z 1 (a) = Œ±\ z 2 (a) = 0. $FRQWLQXDFLyQHVFULELPRVODHFXDFLyQ  
FRPR XQ VLVWHPD GH GRV HFXDFLRQHV GLIHUHQFLDOHV OLQHDOHV KDFLHQGR z 3 (x) = y(x) y z 4 (x)
= y (x) GHWDOIRUPDTXH

z 3 (x) = z 4 (x)
z 4 (x) = p(x)z 4 (x) + q(x)z 3 (x)
para a ‚â§ x ‚â§ b con z 3 (a) = 0 y z 4 (a) = 1. /DVDSUR[LPDFLRQHVFDOFXODGDVHQHODOJRULWPRVRQ

u 1,i ‚âà z 1 (xi ) = y1 (xi ), u 2,i ‚âà z 2 (xi ) = y1 (xi )

11.1 El m√©todo de disparo lineal

509

\

v1,i ‚âà z 3 (xi ) = y2 (xi ), v2,i ‚âà z 4 (xi ) = y2 (xi ).
/DVDSUR[LPDFLRQHV√ÄQDOHVVRQ

w 1,i = u 1,i +

Œ≤ ‚àí u 1,N
v1,i ‚âà y1 (xi )
v1,N

w 2,i = u 2,i +

Œ≤ ‚àí u 1,N
v2,i ‚âà y1 (xi )
v1,N

y

(ODOJRULWPRWLHQHODFDUDFWHUtVWLFDDGLFLRQDOGHREWHQHUDSUR[LPDFLRQHVSDUDODGHULYDGD
GHODVROXFLyQGHOSUREOHPDGHYDORUHQODIURQWHUDDVtFRPRSDUDODVROXFLyQGHOSUREOHPD
PLVPR(OXVRGHODOJRULWPRQRHVWiUHVWULQJLGRDORVSUREOHPDVSDUDORVTXHODVKLSyWHVLVGHO
FRURODULRVHSXHGHQYHUL√ÄFDUVHUtD~WLOSDUDPXFKRVSUREOHPDVTXHQRVDWLVIDFHQHVWDV
KLSyWHVLV8QHMHPSORGHHVWHWLSRVHSXHGHHQFRQWUDUHQHOHMHUFLFLR

ALGORITMO

11.1

Disparo lineal
3DUDDSUR[LPDUODVROXFLyQGHOSUREOHPDGHYDORUHQODIURQWHUD

‚àíy + p(x)y + q(x)y + r (x) = 0, para a ‚â§ x ‚â§ b, con y(a) = Œ± y y(b) = Œ≤,
(Nota Las ecuaciones   y   se escriben como sistemas de primer orden y se
resuelven
ENTRADA H[WUHPRVabFRQGLFLRQHVGHIURQWHUDŒ±Œ≤Q~PHURGHVXELQWHUYDORVN
SALIDA DSUR[LPDFLRQHVw 1,i para y(xi ); w 2,i para y (xi ) por cada i = 0, 1, . . . , N .

Paso 1 Determine h = (b ‚àí a)/N ;
u 1,0 = Œ±;
u 2,0 = 0;
v1,0 = 0;
v2,0 = 1.
Paso 2 Para i = 0, . . . , N ‚àí 1 haga los pasos 3 y 4.
(Se usa el m√©todo Runge-Kutta para sistemas en los pasos 3 y 4.)
Paso 3 Determine x = a + i h.
Paso 4 Determine k1,1 = hu 2,i ;
k1,2 = h p(x)u 2,i + q(x)u 1,i + r (x) ;
k2,1 = h u 2,i + 12 k1,2 ;
k2,2 = h p(x + h/2) u 2,i + 12 k1,2
+q(x + h/2) u 1,i + 12 k1,1 + r (x + h/2) ;
k3,1 = h u 2,i + 12 k2,2 ;
k3,2 = h p(x + h/2) u 2,i + 12 k2,2
+q(x + h/2)(u 1,i + 12 k2,1 ) + r (x + h/2) ;
k4,1 = h u 2,i + k3,2 ;
k4,2 = h p(x + h)(u 2,i + k3,2 ) + q(x + h)(u 1,i + k3,1 ) + r (x + h) ;

510

CAP√çTULO 11

Problemas de valor en la frontera para ecuaciones diferenciales ordinarias

u 1,i+1 = u 1,i + 16 k1,1 + 2k2,1 + 2k3,1 + k4,1 ;
u 2,i+1 = u 2,i + 16 k1,2 + 2k2,2 + 2k3,2 + k4,2 ;
k1,1 = hv2,i ;
k1,2 = h p(x)v2,i + q(x)v1,i ;
k2,1 = h v2,i + 12 k1,2 ;
k2,2 = h p(x + h/2) v2,i + 12 k1,2 + q(x + h/2) v1,i + 12 k1,1 ;
k3,1 = h v2,i + 12 k2,2 ;
k3,2 = h p(x + h/2) v2,i + 12 k2,2 + q(x + h/2) v1,i + 12 k2,1 ;
k4,1 = h v2,i + k3,2 ;
k4,2 = h p(x + h)(v2,i + k3,2 ) + q(x + h)(v1,i + k3,1 ) ;
v1,i+1 = v1,i + 16 k1,1 + 2k2,1 + 2k3,1 + k4,1 ;
v2,i+1 = v2,i + 16 k1,2 + 2k2,2 + 2k3,2 + k4,2 .
Paso 5 Determine w 1,0 = Œ±;
Œ≤ ‚àí u 1,N
w 2,0 =
;
v1,N
SALIDA (a, w 1,0 , w 2,0 ).
Paso 6 Para i = 1, . . . , N
determine W 1 = u 1,i + w 2,0 v1,i ;
W 2 = u 2,i + w 2,0 v2,i ;
x = a + i h;
SALIDA (x, W 1, W 2). (La salida es x i , w 1,i , w 2,i .)
Paso 7 PARE.

Ejemplo 2

(El proceso est√° completo.)

$SOLTXHODWpFQLFDGHGLVSDUROLQHDOFRQN 5DOSUREOHPDGHYDORUHQODIURQWHUD

2
2
sen(ln x)
y =‚àí y + 2y+
,
x
x
x2

para 1 ‚â§ x ‚â§ 2, con y(1) = 1 y y(2) = 2,

\FRPSDUHORVUHVXOWDGRVFRQORVGHODVROXFLyQH[DFWD

y = c1 x +

c2
3
1
‚àí
sen(ln x) ‚àí
cos(ln x),
2
x
10
10

donde
c2 =

1
[8 ‚àí 12 sen(ln 2) ‚àí 4 cos(ln 2)] ‚âà ‚àí0.03920701320
70

y
c1 =

11
‚àí c2 ‚âà 1.1392070132.
10

Soluci√≥n $SOLFDUHODOJRULWPRDHVWHSUREOHPDUHTXLHUHDSUR[LPDUODVVROXFLRQHVSDUD
ORVSUREOHPDVGHYDORULQLFLDO

2
2
sen(ln x)
y1 = ‚àí y1 + 2 y1 +
,
x
x
x2

para 1 ‚â§ x ‚â§ 2, con y1 (1) = 1 y y1 (1) = 0,

512

CAP√çTULO 11

Problemas de valor en la frontera para ecuaciones diferenciales ordinarias

6LHVWDWpFQLFDGHGLVSDURLQYHUVRVLJXHGDQGRODFDQFHODFLyQGHORVGtJLWRVVLJQL√ÄFDWLYRV\VL
HOLQFUHPHQWRGHSUHFLVLyQQRDUURMDPD\RUH[DFWLWXGHVSUHFLVRXVDURWUDVWpFQLFDV$OJXQDV
GHHOODVVHSUHVHQWDQPiVDGHODQWHHQHVWHFDStWXOR6LQHPEDUJRHQJHQHUDOVLui\v1,iVRQ
O(hn  DSUR[LPDFLRQHV SDUD y (xi  \ y (xi  UHVSHFWLYDPHQWH SDUD FDGD i = 0, 1, . . . , N ,
HQWRQFHVw 1,iVHUiXQDDSUR[LPDFLyQO(hn) para y (xi (QSDUWLFXODU

|w 1,i ‚àí y(xi )| ‚â§ K h n 1 +

v1,i
,
v1,N

SDUDDOJXQDFRQVWDQWHK FRQVXOWH>,.@S 
La secci√≥n Conjunto de ejercicios 11.1 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

11.2 El m√©todo de disparo para problemas no lineales
/DWpFQLFDGHGLVSDURSDUDHOSUREOHPDQROLQHDOGHYDORUHQODIURQWHUDGHVHJXQGRRUGHQ

y = f (x, y, y ),

para a ‚â§ x ‚â§ b, con y(a) = Œ± y y(b) = Œ≤,



HVVLPLODUDODWpFQLFDOLQHDOH[FHSWRTXHODVROXFLyQSDUDXQSUREOHPDQROLQHDOQRVHSXHGH
H[SUHVDUFRPRXQDFRPELQDFLyQOLQHDOGHVROXFLRQHVSDUDGRVSUREOHPDVGHYDORULQLFLDO
3RUHOFRQWUDULRDSUR[LPDPRVODVROXFLyQDOSUREOHPDGHYDORUHQODIURQWHUDSRUPHGLRGH
ODVVROXFLRQHVSDUDXQDsucesi√≥nGHSUREOHPDVGHYDORULQLFLDOTXHLPSOLFDQXQSDUiPHWURt
(VWRVSUREOHPDVWLHQHQODIRUPD

y = f (x, y, y ),

para a ‚â§ x ‚â§ b, con y(a) = Œ± y y (a) = t.



/RKDFHPRVDOVHOHFFLRQDUORVSDUiPHWURVt = tkGHIRUPDTXHVHJDUDQWL]D

l√≠m y(b, tk ) = y(b) = Œ≤,

k‚Üí‚àû

/RVPpWRGRVGHGLVSDURSDUDORV
SUREOHPDVQROLQHDOHVUHTXLHUHQ
LWHUDFLRQHVSDUDDFHUFDUVHDO
¬¥REMHWLYR¬µ

donde y(x, tk )GHQRWDODVROXFLyQGHOSUREOHPDGHYDORULQLFLDO  FRQt 5 tk\y (x GHQRWD
ODVROXFLyQGHOSUREOHPDGHYDORUHQODIURQWHUD  
(VWDWpFQLFDUHFLEHHOQRPEUHGHPpWRGRGH¬¥GLVSDUR¬µSRUODDQDORJtDFRQHOSURFHGLPLHQWRGHGLVSDUDUDREMHWRVHQXQREMHWLYRLQPyYLO &RQVXOWHOD√ÄJXUD &RPHQ]DPRV
FRQXQSDUiPHWURt0TXHGHWHUPLQDODHOHYDFLyQLQLFLDODODTXHVHGLVSDUDDOREMHWRGHVGHHO
SXQWR aŒ± \DORODUJRGHODFXUYDGHVFULWDSRUPHGLRGHODVROXFLyQGHOSUREOHPDGHYDORU
LQLFLDO

y = f (x, y, y ),

Figura 11.2

para a ‚â§ x ‚â§ b, con y(a) = Œ± y y (a) = t0 .

y
(b, b)
(b, y(b, t 0 ))

b
y(b, t 0)

y(x, t 0)
Pendiente t 0

a

(a, a)
a

b

x

11.2 El m√©todo de disparo para problemas no lineales

513

Si y(b, t0 )QRHVWiVX√ÄFLHQWHPHQWHFHUFDGHŒ≤FRUUHJLPRVQXHVWUDDSUR[LPDFLyQDOVHOHFFLRQDUHOHYDFLRQHVt t \DVtVXFHVLYDPHQWHKDVWDTXHy(bt k)HVWpVX√ÄFLHQWHPHQWHFHUFDGH
¬¥JROSHDU¬µŒ≤ &RQVXOWHOD√ÄJXUD 
3DUDGHWHUPLQDUORVSDUiPHWURVt kVXSRQJDTXHXQSUREOHPDGHYDORUHQODIURQWHUDGH
ODIRUPD  VDWLVIDFHODVKLSyWHVLVGHOWHRUHPD6Ly(xt GHQRWDODVROXFLyQGHOSUREOHPDGHYDORULQLFLDO  DFRQWLQXDFLyQGHWHUPLQDPRVt con

y(b, t) ‚àí Œ≤ = 0.



eVWDHVXQDHFXDFLyQQROLQHDOHQODYDULDEOHt/RVSUREOHPDVGHHVWHWLSRVHFRQVLGHUDURQHQHOFDStWXOR\H[LVWHQGLIHUHQWHVPpWRGRV
3DUDXWLOL]DUHOPpWRGRGHODVHFDQWHSDUDUHVROYHUHOSUREOHPDQHFHVLWDPRVVHOHFFLRQDU
DSUR[LPDFLRQHVLQLFLDOHVt 0\t \GHVSXpVJHQHUDUORVWpUPLQRVUHVWDQWHVGHODVXFHVLyQD
WUDYpVGH

tk = tk‚àí1 ‚àí

(y(b, tk‚àí1 ) ‚àí Œ≤)(tk‚àí1 ‚àí tk‚àí2 )
,
y(b, tk‚àí1 ) ‚àí y(b, tk‚àí2 )

k = 2, 3, . . . .

Figura 11.3
y
y(b, t 2 )
y(b, t 3)

(b, b )
b

y(x, t 3)

y(b, t 1 )
y(b, t 0)

y(x, t 1 )

y(x, t 2 )

y(x, t 0)

a

(a, a)
a

b

x

Iteraci√≥n de Newton
3DUDXVDUHOPpWRGRGH1HZWRQPiVSRGHURVRSDUDJHQHUDUODVXFHVLyQ{tk }, VyORVHQHFHVLWD
XQDDSUR[LPDFLyQLQLFLDOt 06LQHPEDUJRODLWHUDFLyQWLHQHODIRUPD

tk = tk‚àí1 ‚àí

y(b, tk‚àí1 ) ‚àí Œ≤
dy
(b, tk‚àí1 )
dt

,



\ UHTXLHUH HO FRQRFLPLHQWR GH (dy/dt)(b, tk‚àí1 ). (VWR SUHVHQWD XQD GL√ÄFXOWDG SRUTXH QR
VH FRQRFH XQD UHSUHVHQWDFLyQ H[SOtFLWD SDUD y(b, t) VRODPHQWH FRQRFHPRV ORV YDORUHV
y(b, t0 ), y(b, t1 ), . . . , y(b, tk‚àí1 ).
6XSRQJDTXHUHHVFULELPRVHOSUREOHPDGHYDORULQLFLDO  DOHQIDWL]DUTXHODVROXFLyQGHSHQGHWDQWRGHxFRPRGHOSDUiPHWURt

y (x, t) = f (x, y(x, t), y (x, t)), para a ‚â§ x ‚â§ b, con y(a, t) = Œ± y y (a, t) = t.


11.2 El m√©todo de disparo para problemas no lineales

Paso 1 Determine h = (b ‚àí a)/N ;
k = 1;
TK = (Œ≤ ‚àí Œ±)/(b ‚àí a).

515

(Nota: TK tambi√©n podr√≠a ser una entrada.)

Paso 2 Mientras (k ‚â§ M) haga los pasos 3‚Äì10.
Paso 3 Determine w 1,0 = Œ±;
w 2,0 = TK ;
u 1 = 0;
u 2 = 1.
Paso 4 Para i = 1, . . . , N haga los pasos 5 y 6.
(Se utiliza el m√©todo Runge-Kutta para sistemas en los pasos 5 y 6.)
Paso 5 Determine x = a + (i ‚àí 1)h.
Paso 6 Determine k1,1 = hw 2,i‚àí1 ;
k1,2 = h f (x, w 1,i‚àí1 , w 2,i‚àí1 );
k2,1 = h w 2,i‚àí1 + 12 k1,2 ;
k2,2 = h f x + h/2, w 1,i‚àí1 + 12 k1,1 , w 2,i‚àí1 + 12 k1,2 ;
k3,1 = h w 2,i‚àí1 + 12 k2,2 ;
k3,2 = h f x + h/2, w 1,i‚àí1 + 12 k2,1 , w 2,i‚àí1 + 12 k2,2 ;
k4,1 = h(w 2,i‚àí1 + k3,2 );
k4,2 = h f (x + h, w 1,i‚àí1 + k3,1 , w 2,i‚àí1 + k3,2 );
w 1,i = w 1,i‚àí1 + (k1,1 + 2k2,1 + 2k3,1 + k4,1 )/6;
w 2,i = w 2,i‚àí1 + (k1,2 + 2k2,2 + 2k3,2 + k4,2 )/6;
k1,1 = hu 2 ;
k1,2 = h[ f y (x, w 1,i‚àí1 , w 2,i‚àí1 )u 1
+ f y (x, w 1,i‚àí1 , w 2,i‚àí1 )u 2 ];
k2,1 = h u 2 + 12 k1,2 ;
k2,2 = h f y (x + h/2, w 1,i‚àí1 , w 2,i‚àí1 ) u 1 + 12 k1,1
+ f y (x + h/2, w 1,i‚àí1 , w 2,i‚àí1 ) u 2 + 12 k1,2 ;
k3,1 = h u 2 + 12 k2,2 ;
k3,2 = h f y (x + h/2, w 1,i‚àí1 , w 2,i‚àí1 ) u 1 + 12 k2,1
+ f y (x + h/2, w 1,i‚àí1 , w 2,i‚àí1 ) u 2 + 12 k2,2 ;
k4,1 = h(u 2 + k3,2 );
k4,2 = h f y (x + h, w 1,i‚àí1 , w 2,i‚àí1 ) u 1 + k3,1
+ f y (x + h, w 1,i‚àí1 , w 2,i‚àí1 ) u 2 + k3,2 ;
u 1 = u 1 + 16 [k1,1 + 2k2,1 + 2k3,1 + k4,1 ];
u 2 = u 2 + 16 [k1,2 + 2k2,2 + 2k3,2 + k4,2 ].
Paso 7 Si |w 1,N ‚àí Œ≤| ‚â§ TOL entonces haga los pasos 8 y 9.
Paso 8 Para i = 0, 1, . . . , N
determine x = a + i h;
SALIDA (x, w 1,i , w 2,i ).
Paso 9 (El procedimiento est√° completo.)
PARE.

516

CAP√çTULO 11

Problemas de valor en la frontera para ecuaciones diferenciales ordinarias

w 1,N ‚àí Œ≤
;
u1
(Se utiliza el m√©todo de Newton para calcular TK.)
k = k + 1.
Paso 11 SALIDA (‚ÄòN√∫mero m√°ximo de iteraciones excedido‚Äô);
(El procedimiento no fue exitoso.)
PARE.
Paso 0

Determine TK = TK ‚àí

(O YDORU t 0 5 TK VHOHFFLRQDGR HQ HO SDVR  HV OD SHQGLHQWH GH OD UHFWD TXH SDVD SRU
(aŒ± \ bŒ≤ 6LHOSUREOHPDVDWLVIDFHODKLSyWHVLVGHOWHRUHPDFXDOTXLHUVHOHFFLyQ
de t 0SURSRUFLRQDUiFRQYHUJHQFLDSHURXQDEXHQDVHOHFFLyQGHt 0PHMRUDUiODFRQYHUJHQFLD
\HOSURFHGLPLHQWRIXQFLRQDUiSDUDPXFKRVSUREOHPDVTXHQRVDWLVIDFHQHVWDKLSyWHVLV8Q
HMHPSORGHHVWHWLSRVHSXHGHHQFRQWUDUHQHOHMHUFLFLRG 
Ejemplo 1

$SOLTXHHOPpWRGRGHGLVSDURFRQHOPpWRGRGH1HZWRQDOSUREOHPDGHYDORUHQODIURQWHUD

y =

1
(32 + 2x 3 ‚àí yy ),
8

para 1 ‚â§ x ‚â§ 3, con y(1) = 17 y y(3) =

43
.
3

8VH N = 20, M = 10, y TOL = 10‚àí5 \ FRPSDUH ORV UHVXOWDGRV FRQ OD VROXFLyQ H[DFWD
y(x) = x 2 + 16/x.
Soluci√≥n 1HFHVLWDPRVDSUR[LPDUODVVROXFLRQHVGHORVSUREOHPDVGHYDORULQLFLDO

y =

1
(32 + 2x 3 ‚àí yy ),
8

para 1 ‚â§ x ‚â§ 3, con y(1) = 17 y y (1) = tk ,

y
z =

‚àÇf
1
‚àÇf
z = ‚àí (y z + yz ),
z+
‚àÇy
‚àÇy
8

para 1‚â§ x ‚â§ 3, con z(1) = 0 y z (1) = 1,

HQFDGDSDVRHQODLWHUDFLyQ6LODWpFQLFDGHSDURHQHODOJRULWPRUHTXLHUH

|w 1,N (tk ) ‚àí y(3)| ‚â§ 10‚àí5 ,
HQWRQFHVQHFHVLWDPRVFXDWURLWHUDFLRQHV\t 5 2/RVUHVXOWDGRVREWHQLGRVSDUD
HVWHYDORUGHtVHPXHVWUDQHQODWDEOD

Tabla 11.2

xi

w 1,i

y(xi )

|w 1,i ‚àí y(xi )|

1.0
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
2.0
2.1
2.2
2.3
2.4
2.5
2.6
2.7
2.8
2.9
3.0

17.000000
15.755495
14.773389
13.997752
13.388629
12.916719
12.560046
12.301805
12.128923
12.031081
12.000023
12.029066
12.112741
12.246532
12.426673
12.650004
12.913847
13.215924
13.554282
13.927236
14.333327

17.000000
15.755455
14.773333
13.997692
13.388571
12.916667
12.560000
12.301765
12.128889
12.031053
12.000000
12.029048
12.112727
12.246522
12.426667
12.650000
12.913845
13.215926
13.554286
13.927241
14.333333

4.06 √ó 10‚àí5
5.60 √ó 10‚àí5
5.94 √ó 10‚àí5
5.71 √ó 10‚àí5
5.23 √ó 10‚àí5
4.64 √ó 10‚àí5
4.02 √ó 10‚àí5
3.14 √ó 10‚àí5
2.84 √ó 10‚àí5
2.32 √ó 10‚àí5
1.84 √ó 10‚àí5
1.40 √ó 10‚àí5
1.01 √ó 10‚àí5
6.68 √ó 10‚àí6
3.61 √ó 10‚àí6
9.17 √ó 10‚àí7
1.43 √ó 10‚àí6
3.46 √ó 10‚àí6
5.21 √ó 10‚àí6
6.69 √ó 10‚àí6

11.3 M√©todos de diferencias Ô¨Ånitas para problemas lineales

517

$SHVDUGHTXHHOPpWRGRGH1HZWRQTXHVHXVyFRQODWpFQLFDGHGLVSDURUHTXLHUHODVROXFLyQGHXQSUREOHPDGHYDORULQLFLDODGLFLRQDOSRUORJHQHUDOGDUiFRQYHUJHQFLDPiVUiSLGD
TXHHOPpWRGRGHVHFDQWH6LQHPEDUJRDPERVPpWRGRVVyORVRQORFDOPHQWHFRQYHUJHQWHV
SRUTXHUHTXLHUHQEXHQDVDSUR[LPDFLRQHVLQLFLDOHV
3DUDXQDQiOLVLVJHQHUDOGHODFRQYHUJHQFLDGHODVWpFQLFDVGHGLVSDURSDUDORVSUREOHPDV
QROLQHDOHVVHUH√ÄHUHDOOHFWRUDOH[FHOHQWHOLEURGH.HOOHU>.HOOHU+@(QHVDUHIHUHQFLDVH
DQDOL]DQFRQGLFLRQHVGHIURQWHUDPiVJHQHUDOHV7DPELpQVHGHEHREVHUYDUTXHODWpFQLFDGH
GLVSDURSDUDORVSUREOHPDVQROLQHDOHVHVVHQVLEOHDORVHUURUHVGHUHGRQGHRHQHVSHFLDOVLODV
VROXFLRQHVy(x \z(xt VRQIXQFLRQHVGHxTXHFUHFHQUiSLGDPHQWHHQ>ab@
La secci√≥n Conjunto de ejercicios 11.2 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

11.3 M√©todos de diferencias Ô¨Ånitas para problemas lineales
/RVPpWRGRVGHGLVSDUROLQHDO\QROLQHDOSDUDORVSUREOHPDVGHYDORUHQODIURQWHUDSXHGHQ
SUHVHQWDUSUREOHPDVGHLQHVWDELOLGDG/RVPpWRGRVHQHVWDVHFFLyQWLHQHQPHMRUHVFDUDFWHUtVWLFDVGHHVWDELOLGDGSHURHQJHQHUDOUHTXLHUHQPiVFiOFXORVSDUDREWHQHUXQDH[DFWLWXG
HVSHFt√ÄFD
/RVPpWRGRVTXHLPSOLFDQGLIHUHQFLDV√ÄQLWDVSDUDUHVROYHUORVSUREOHPDVGHYDORUHQOD
IURQWHUDUHHPSOD]DQFDGDXQDGHODVGHULYDGDVHQODHFXDFLyQGLIHUHQFLDOFRQXQDDSUR[LPDFLyQGHFRFLHQWHGHGLIHUHQFLDDGHFXDGDGHOWLSRFRQVLGHUDGRHQODVHFFLyQ(OFRFLHQWHGH
GLIHUHQFLDSDUWLFXODU\ODORQJLWXGGHSDVRhVHVHOHFFLRQDQSDUDPDQWHQHUXQRUGHQHVSHFt√ÄFRGHHUURUGHWUXQFDPLHQWR6LQHPEDUJRhQRVHSXHGHHOHJLUGHPDVLDGRSHTXHxDGHELGRD
ODLQHVWDELOLGDGJHQHUDOGHODVDSUR[LPDFLRQHVGHODGHULYDGD

Aproximaci√≥n discreta
(OPpWRGRGHGLIHUHQFLDV√ÄQLWDVSDUDHOSUREOHPDOLQHDOGHYDORUHQODIURQWHUDGHVHJXQGR
RUGHQ

y = p(x)y + q(x)y + r (x), para a ‚â§ x ‚â§ b, con y(a) = Œ± y y(b) = Œ≤,



UHTXLHUHTXHVHXVHQDSUR[LPDFLRQHVGHFRFLHQWHGHGLIHUHQFLDSDUDDSUR[LPDUWDQWRy9 como y0
3ULPHURVHOHFFLRQDPRVXQHQWHURN .\GLYLGLPRVHOLQWHUYDOR>ab@HQ N 1 VXELQWHUYDORVLJXDOHVFX\RVH[WUHPRVVRQORVSXQWRVGHPDOODxi = a + i h, para i = 0, 1, . . . , N + 1,
donde h 5 (b 2 a) (N 1 6HOHFFLRQDUXQWDPDxRGHORQJLWXGhGHHVWDIRUPDIDFLOLWDOD
DSOLFDFLyQGHXQDOJRULWPRPDWULFLDOGHOFDStWXORORFXDOUHVXHOYHXQVLVWHPDOLQHDOTXH
LPSOLFDXQDPDWUL]N 3 N
(Q ORV SXQWRV GH PDOOD LQWHULRU xi , para i = 1, 2, . . . , N , OD HFXDFLyQ GLIHUHQFLDO D
DSUR[LPDUHV

y (xi ) = p(xi )y (xi ) + q(xi )y(xi ) + r (xi ).



$O H[SDQGLU y HQ HO WHUFHU SROLQRPLR GH 7D\ORU DOUHGHGRU GH xi HYDOXDGR HQ xi+1 y xi‚àí1 ,
WHQHPRVVXSRQLHQGRTXHy ‚àà C 4 [xi‚àí1 , xi+1 ],

y(xi+1 ) = y(xi + h) = y(xi ) + hy (xi ) +
SDUDDOJXQDŒæi+ en(xi , xi+1 ), \

y(xi‚àí1 ) = y(xi ‚àí h) = y(xi ) ‚àí hy (xi ) +

h2
h3
h 4 (4) +
y (xi ) + y (xi ) +
y (Œæi ),
2
6
24
h2
h3
h 4 (4) ‚àí
y (xi ) ‚àí y (xi ) +
y (Œæi ),
2
6
24

518

CAP√çTULO 11

Problemas de valor en la frontera para ecuaciones diferenciales ordinarias

SDUDDOJXQDŒæi‚àí en(xi‚àí1 , xi ). 6LVHVXPDQHVWDVHFXDFLRQHVWHQHPRV

y(xi+1 ) + y(xi‚àí1 ) = 2y(xi ) + h 2 y (xi ) +

h 4 (4) +
[y (Œæi ) + y (4) (Œæi‚àí )],
24

\DOUHVROYHUSDUDy (xi )REWHQHPRV

1
h 2 (4) +
[y(x
[y (Œæi ) + y (4) (Œæi‚àí )].
)
‚àí
2y(x
)
+
y(x
)]
‚àí
i+1
i
i‚àí1
h2
24
3RGHPRVXVDUHOWHRUHPDGHYDORULQWHUPHGLRSDUDVLPSOL√ÄFDUHOWpUPLQRGHHUURU
para proporcionar
y (xi ) =

y (xi ) =

1
h 2 (4)
y (Œæi ),
[y(x
)
‚àí
2y(x
)
+
y(x
)]
‚àí
i+1
i
i‚àí1
h2
12



SDUDDOJXQDŒæi en(xi‚àí1 , xi+1 ).(VWRUHFLEHHOQRPEUHGHf√≥rmula de diferencia centrada
para y (xi ).
UnaIyUPXODGHGLIHUHQFLDFHQWUDGDSDUD y (xi )VHREWLHQHGHIRUPDVLPLODU ORVGHWDOOHV
VHFRQVLGHUDURQHQODVHFFLyQ ORFXDOUHVXOWDHQ

y (xi ) =

1
h2
[y(xi+1 ) ‚àí y(xi‚àí1 )] ‚àí y (Œ∑i ),
2h
6



SDUDDOJXQDŒ∑i en (xi‚àí1 , xi+1 ).
(OXVRGHHVWDVIyUPXODVGHGLIHUHQFLDFHQWUDGDHQODHFXDFLyQ  JHQHUDODHFXDFLyQ

y(xi+1 ) ‚àí 2y(xi ) + y(xi‚àí1 )
y(xi+1 ) ‚àí y(xi‚àí1 )
= p(xi )
+ q(xi )y(xi )
2
h
2h
+ r (xi ) ‚àí

h2
2 p(xi )y (Œ∑i ) ‚àí y (4) (Œæi ) .
12

8QPpWRGRGHGLIHUHQFLDV√ÄQLWDVFRQHUURUGHWUXQFDPLHQWRGHRUGHQO (h UHVXOWDGHO
XVR GH HVWD HFXDFLyQ FRQ ODV FRQGLFLRQHV GH IURQWHUD y(a) = Œ± y y(b) = Œ≤ SDUD GH√ÄQLU HO
VLVWHPDGHHFXDFLRQHVOLQHDOHV

w 0 = Œ±,

w N +1 = Œ≤

y
‚àíw i+1 + 2w i ‚àí w i‚àí1
h2

+ p(xi )

w i+1 ‚àí w i‚àí1
2h

+ q(xi )w i = ‚àír (xi ),



para cada i = 1, 2, . . . , N .
(QODIRUPDTXHFRQVLGHUDUHPRVODHFXDFLyQ  VHUHHVFULEHFRPR

‚àí 1+

h
h
p(xi ) w i‚àí1 + 2 + h 2 q(xi ) w i ‚àí 1 ‚àí p(xi ) w i+1 = ‚àíh 2r (xi ),
2
2

\HOVLVWHPDGHHFXDFLRQHVUHVXOWDQWHVHH[SUHVDHQIRUPDGHODPDWUL]WULGLDJRQDON 3 N

Aw = b,

donde



11.3 M√©todos de diferencias Ô¨Ånitas para problemas lineales

519

‚é§
h
2
. .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 0.
p(x
q(x
)
‚àí1
+
)
0
2
+
h
1
1
....
‚é•
‚é¢
..
2
..
‚é•
‚é¢
..
‚é•
‚é¢‚àí1 ‚àí h p(x ) 2 + h 2 q(x ) ‚àí1 + h p(x ) . . . . . . .
2
2
2
‚é•
‚é¢
.
.
.
.
.
.
.
..
....
....
2 ....
2 ....
‚é•
‚é¢
....
....
....
....
‚é•
‚é¢
.
....
.
....
....
....
‚é•
‚é¢
.. 0
....
....
....
A =‚é¢
‚é•,
....
0
.
....
....
....
‚é•
‚é¢
....
.
.
.
.
.
.
.
.
‚é•
‚é¢
....
....
....
....
..
....
‚é•
‚é¢
....
....
h
..
....
‚é•
‚é¢
....
....
‚àí1
+
)
p(x
N
‚àí1
....
....
....
‚é•
‚é¢
.
2
....
....
..
....
‚é¶
‚é£
h
.
.
.
.
.
2
0 .............................. 0
‚àí1 ‚àí p(x N ) . 2 + h q(x N )
2
‚é°
‚é§
h
‚é°
‚é§
‚àíh 2r (x1 ) + 1 + p(x1 ) w 0
‚é¢
‚é•
2
w1
‚é¢
‚é•
2
‚é¢ w2 ‚é•
‚é¢
‚é•
‚àíh r (x2 )
‚é¢
‚é•
‚é¢
‚é•
‚é¢
‚é•
‚é¢
‚é•
..
..
w=‚é¢
,
y
b
=
‚é•
‚é¢
‚é•.
.
.
‚é¢
‚é•
‚é¢
‚é•
‚é¢
‚é•
‚é£ w N ‚àí1 ‚é¶
2
‚àíh r (x N ‚àí1 )
‚é¢
‚é•
‚é£
‚é¶
wN
h
‚àíh 2r (x N ) + 1 ‚àí p(x N ) w N +1
2
‚é°

(O VLJXLHQWH WHRUHPD HVWDEOHFH ODV FRQGLFLRQHV HQ ODV FXDOHV HO VLVWHPD OLQHDO WULGLDJRQDO
 WLHQHXQD~QLFDVROXFLyQ6XGHPRVWUDFLyQHVXQDFRQVHFXHQFLDGHOWHRUHPDHQ
ODSiJLQD\VHFRQVLGHUDHQHOHMHUFLFLR
Teorema 11.3

6XSRQJD TXH p, q \ r VRQ FRQWLQXDV HQ >a b@ 6L q(x) ‚â• 0 HQ >a b@ HQWRQFHV HO VLVWHPD OLQHDO WULGLDJRQDO   WLHQH XQD ~QLFD VROXFLyQ VLHPSUH TXH h < 2/L, donde
L = m√°xa‚â§x‚â§b | p(x)|.
6HGHEHUtDREVHUYDUTXHODKLSyWHVLVGHOWHRUHPDJDUDQWL]DXQDVROXFLyQ~QLFDSDUD
HOSUREOHPDGHYDORUHQODIURQWHUD  SHURQRJDUDQWL]DTXHy ‚àà C 4 [a, b]. 1HFHVLWDPRV
HVWDEOHFHUTXHy  HVFRQWLQXDHQ>ab@SDUDJDUDQWL]DUTXHHOHUURUGHWUXQFDPLHQWRWLHQH
orden O(h 
(ODOJRULWPRLPSOHPHQWDHOPpWRGRGHGLIHUHQFLDV√ÄQLWDVOLQHDO

ALGORITMO

11.3

Diferencia Ô¨Ånita lineal
3DUDDSUR[LPDUODVROXFLyQGHOSUREOHPDGHYDORUHQODIURQWHUD

y = p(x)y + q(x)y + r (x),

para a ‚â§ x ‚â§ b, con y(a) = Œ± y y(b) = Œ≤ :

ENTRADA H[WUHPRVa, bFRQGLFLRQHVGHIURQWHUDŒ±Œ≤HQWHURN ‚â•
SALIDA DSUR[LPDFLRQHVwi para y(xi) para cada i 5   N 1

Paso 1 Determine h = (b ‚àí a)/(N + 1);
x = a + h;
a1 = 2 + h 2 q(x);
b1 = ‚àí1 + (h/2) p(x);
d1 = ‚àíh 2r (x) + (1 + (h/2) p(x))Œ±.
Paso 2 Para i = 2, . . . , N ‚àí 1
determine x = a + i h;
ai = 2 + h 2 q(x);
bi = ‚àí1 + (h/2) p(x);
ci = ‚àí1 ‚àí (h/2) p(x);
di = ‚àíh 2r (x).

520

CAP√çTULO 11

Problemas de valor en la frontera para ecuaciones diferenciales ordinarias

Paso 3 Determine x = b ‚àí h;
a N = 2 + h 2 q(x);
c N = ‚àí1 ‚àí (h/2) p(x);
d N = ‚àíh 2r (x) + (1 ‚àí (h/2) p(x))Œ≤.
Paso 4 Determine l1 = a1 ;

(Los pasos 4-8 resuelven un sistema lineal tridiagonal
usando el algoritmo 6.7.)
u 1 = b1 /a1 ;
z 1 = d1 /l1 .

Paso 5 Para i = 2, . . . , N ‚àí 1 determine li = ai ‚àí ci u i‚àí1 ;
u i = bi /li ;
z i = (di ‚àí ci z i‚àí1 )/li .
Paso 6 Determine l N = a N ‚àí c N u N ‚àí1 ;
z N = (d N ‚àí c N z N ‚àí1 )/l N .
Paso 7 Determine w 0 = Œ±;
w N +1 = Œ≤.
w N = zN .
Paso 8 Para i = N ‚àí 1, . . . , 1 determine w i = z i ‚àí u i w i+1 .
Paso 9 Para i = 0, . . . , N + 1 determine x = a + i h;
SALIDA (x, w i ).
Paso 10 PARE. (El procedimiento est√° completo.)

Ejemplo 1

8VHHODOJRULWPRFRQN 5SDUDDSUR[LPDUODVROXFLyQGHOSUREOHPDOLQHDOGHYDORUHQ
ODIURQWHUD

2
2
sen(ln x)
y =‚àí y + 2y+
,
x
x
x2

para 1 ‚â§ x ‚â§ 2, con y(1) = 1 y y(2) = 2,

\FRPSDUHORVUHVXOWDGRVFRQORVREWHQLGRVFRQHOPpWRGRGHGLVSDURHQHOHMHPSORGHOD
VHFFLyQ
Soluci√≥n 3DUDHVWHHMHPSORXVDUHPRVN 5SRUORTXHh 5\WHQHPRVHOPLVPRHVSD-

FLDGRTXHHQHOHMHPSORGHODVHFFLyQ/RVUHVXOWDGRVFRPSOHWRVVHHQXPHUDQHQOD
WDEOD

Tabla 11.3

xi

wi

y(xi )

|w i ‚àí y(xi )|

1.0
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
2.0

1.00000000
1.09260052
1.18704313
1.28333687
1.38140205
1.48112026
1.58235990
1.68498902
1.78888175
1.89392110
2.00000000

1.00000000
1.09262930
1.18708484
1.28338236
1.38144595
1.48115942
1.58239246
1.68501396
1.78889853
1.89392951
2.00000000

2.88 √ó 10‚àí5
4.17 √ó 10‚àí5
4.55 √ó 10‚àí5
4.39 √ó 10‚àí5
3.92 √ó 10‚àí5
3.26 √ó 10‚àí5
2.49 √ó 10‚àí5
1.68 √ó 10‚àí5
8.41 √ó 10‚àí6

(VWRVUHVXOWDGRVVRQFRQVLGHUDEOHPHQWHPHQRVH[DFWRVTXHORVREWHQLGRVHQHOHMHPSOR
GHODVHFFLyQ(VWRVHGHEHDTXHHOPpWRGRTXHVHXVyHQHVHHMHPSORLPSOLFDEDXQD
WpFQLFD5XQJH.XWWDFRQHUURUGHWUXQFDPLHQWRORFDOGHRUGHQO(h PLHQWUDVHOPpWRGRGH
GLIHUHQFLDTXHVHXVDDTXtWLHQHXQHUURUGHWUXQFDPLHQWRORFDOGHRUGHQO(h 

11.3 M√©todos de diferencias Ô¨Ånitas para problemas lineales

521

3DUDREWHQHUXQPpWRGRGHGLIHUHQFLDFRQPD\RUH[DFWLWXGSRGHPRVSURFHGHUGHGLIHUHQWHVIRUPDV8VDQGRODVHULHGH7D\ORUGHTXLQWRRUGHQSDUDDSUR[LPDU y (xi ) y y (xi ) en
XQWpUPLQRGHHUURUGHWUXQFDPLHQWRTXHLPSOLFDh6LQHPEDUJRHVWHSURFHVRUHTXLHUHXVDU
P~OWLSORVQRVylo de y(xi+1 ) y y(xi‚àí1 )VLQRWDPELpQGH y(xi+2 ) y y(xi‚àí2 )HQODVIyUPXODV
GHDSUR[LPDFLyQSDUD y (xi ) y y (xi ). (VWRFRQGXFHDODGL√ÄFXOWDGHQi 5 NSRUTXHQRFRQRFHPRV w ‚àí1\HQi 5 NSRUTXHQRFRQRFHPRV w N +2 . $GHPiVHOVLVWHPDGHHFXDFLRQHV
DQiORJDVUHVXOWDQWHSDUD  QRHVGHIRUPDWULGLDJRQDO\ODVROXFLyQGHOVLVWHPDUHTXLHUH
PXFKRVPiVFiOFXORV

Uso de la extrapolaci√≥n de Richardson
(QOXJDUGHLQWHQWDUREWHQHUXQPpWRGRGHGLIHUHQFLDFRQXQHUURUGHWUXQFDPLHQWRGHRUGHQ
VXSHULRUGHHVWDIRUPDHQJHQHUDOHVPiVVDWLVIDFWRULRFRQVLGHUDUXQDUHGXFFLyQGHOWDPDxR
GHORQJLWXG$GHPiVODWpFQLFDGHH[WUDSRODFLyQGH5LFKDUGVRQVHSXHGHXVDUGHPDQHUD
HIHFWLYDSDUDHVWHPpWRGRSRUTXHHOWpUPLQRGHHUURUHVWiH[SUHVDGRHQSRWHQFLDVSDUHVGHh
FRQFRH√ÄFLHQWHVLQGHSHQGLHQWHVGHh,VLHPSUH\FXDQGRyVHDVX√ÄFLHQWHPHQWHGLIHUHQFLDEOH
FRQVXOWHSRUHMHPSOR>.HOOHU+@S 
(OHMHUFLFLRGDDOJXQDVLGHDVUHVSHFWRDODIRUPDGHOHUURUGHWUXQFDPLHQWR\ODMXVWL√ÄFDFLyQSDUDXVDUODH[WUDSRODFLyQ
Ejemplo 2

$SOLTXHODH[WUDSRODFLyQGH5LFKDUGVRQSDUDDSUR[LPDUODVROXFLyQGHOSUREOHPDGHYDORU
HQODIURQWHUD

2
2
sen(ln x)
y =‚àí y + 2y+
, para 1 ‚â§ x ‚â§ 2, con y(1) = 1 y y(2) = 2,
x
x
x2
XVDQGRh 5\
Soluci√≥n

/RVUHVXOWDGRVVHPXHVWUDQHQODWDEOD/DSULPHUDH[WUDSRODFLyQHV

Ext1i =

4w i (h = 0.05) ‚àí w i (h = 0.1)
,
3

ODVHJXQGDH[WUDSRODFLyQHV

Ext2i =

4w i (h = 0.025) ‚àí w i (h = 0.05)
,
3

\ODH[WUDSRODFLyQ√ÄQDOHV

Ext3i =

16Ext2i ‚àí Ext1i
.
15

Tabla 11.4
xi

w i (h = 0.05)

w i (h = 0.025)

Ext1i

Ext2i

Ext3i

1.0
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
2.0

1.00000000
1.09262207
1.18707436
1.28337094
1.38143493
1.48114959
1.58238429
1.68500770
1.78889432
1.89392740
2.00000000

1.00000000
1.09262749
1.18708222
1.28337950
1.38144319
1.48115696
1.58239042
1.68501240
1.78889748
1.89392898
2.00000000

1.00000000
1.09262925
1.18708477
1.28338230
1.38144589
1.48115937
1.58239242
1.68501393
1.78889852
1.89392950
2.00000000

1.00000000
1.09262930
1.18708484
1.28338236
1.38144595
1.48115941
1.58239246
1.68501396
1.78889853
1.89392951
2.00000000

1.00000000
1.09262930
1.18708484
1.28338236
1.38144595
1.48115942
1.58239246
1.68501396
1.78889853
1.89392951
2.00000000

522

CAP√çTULO 11

Problemas de valor en la frontera para ecuaciones diferenciales ordinarias

6HRPLWHQORVYDORUHVGHwi (h 5 HQODWDEODSDUDDKRUUDUHVSDFLRSHURHVWiQOLVWDGRVHQODWDEOD/RVUHVXOWDGRVSDUDwi (h 5 VRQH[DFWRVHQDSUR[LPDGDPHQWH
 3 2 6LQ HPEDUJR ORV UHVXOWDGRV GH (FWi VRQ FRUUHFWRV SDUD ORV OXJDUHV GHFLPDOHV
HQXPHUDGRV 'H KHFKR VL VH XVDQ VX√ÄFLHQWHV GtJLWRV HVWD DSUR[LPDFLyQ FRQFRUGDUtD FRQ
OD VROXFLyQ H[DFWD FRQ HUURU Pi[LPR GH 3 2 HQ ORV SXQWRV GH PDOOD XQD PHMRUD
LPSUHVLRQDQWH
La secci√≥n Conjunto de ejercicios 11.3 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

11.4 M√©todos de diferencias Ô¨Ånitas para problemas no lineales
3DUDHOSUREOHPDQROLQHDOJHQHUDOGHYDORUHQODIURQWHUD

y = f (x, y, y ), para a ‚â§ x ‚â§ b, con y(a) = Œ± y y(b) = Œ≤,
HOPpWRGRGHGLIHUHQFLDVHVVLPLODUDOPpWRGRDSOLFDGRSDUDSUREOHPDVOLQHDOHVHQODVHFFLyQ
6LQHPEDUJRDTXtHOVLVWHPDGHHFXDFLRQHVQRVHUiOLQHDOSRUORTXHVHUHTXLHUHXQ
SURFHVRLWHUDWLYRSDUDUHVROYHUOR
3DUDHOGHVDUUROORGHOSURFHGLPLHQWRVXSRQHPRVVLHPSUHTXHfVDWLVIDFHODVVLJXLHQWHV
FRQGLFLRQHV
‚Ä¢ f \ODVGHULYDGDVSDUFLDOHVf y\ f y VRQWRGDVFRQWLQXDVHQ

D = { (x, y, y ) | a ‚â§ x ‚â§ b, con ‚àí‚àû < y < ‚àû y ‚àí‚àû < y < ‚àû };
‚Ä¢ f y (x, y, y ) ‚â• Œ¥ en DSDUDDOJ~QŒ¥ > 0;
¬á ([LVWHQFRQVWDQWHVk\LFRQ

k=

m√°x

(x,y,y )‚ààD

| f y (x, y, y )| y L =

m√°x

(x,y,y )‚ààD

| f y (x, y, y )|.

(VWRJDUDQWL]DPHGLDQWHHOWHRUHPDTXHH[LVWHXQDVROXFLyQ~QLFD
&RPRHQHOFDVROLQHDOGLYLGLPRV>ab@HQ N 1 VXELQWHUYDORVLJXDOHVFX\RVH[WUHPRVHVWiQHQ xi = a + i h, para i = 0, 1, . . . , N + 1. 6XSRQHUTXHODVROXFLyQH[DFWDWLHQH
XQDFXDUWDGHULYDGDHQODIURQWHUDQRVSHUPLWHUHHPSOD]DUy (xi )\y (xi )HQFDGDXQDGHODV
HFXDFLRQHV

y (x ) = f (x , y(x ), y (x ))
PHGLDQWHODIyUPXODGHGLIHUHQFLDFHQWUDGDDGHFXDGDSURYLVWDHQODVHFXDFLRQHV  \
 HQODSiJLQDUHVSHFWLYDPHQWH(VWRGDSDUDFDGDi = 1, 2, . . . , N ,

y(xi+1 ) ‚àí 2y(xi ) + y(xi‚àí1 )
y(xi+1 ) ‚àí y(xi‚àí1 ) h 2
h 2 (4)
= f xi , y(xi ),
‚àí y (Œ∑i ) +
y (Œæi ),
2
h
2h
6
12
SDUDDOJXQDVŒæi\Œ∑iHQHOLQWHUYDOR(xi‚àí1 , xi+1 ).
&RPRHQHOFDVROLQHDOHOPpWRGRGHGLIHUHQFLDVUHVXOWDGHHOLPLQDUORVWpUPLQRVGHHUURU
\HPSOHDUODVFRQGLFLRQHVGHIURQWHUD

w 0 = Œ±,

w N +1 = Œ≤,

11.4

M√©todos de diferencias Ô¨Ånitas para problemas no lineales

523

y
‚àí

w i+1 ‚àí 2w i + w i‚àí1
+ f
h2

xi , w i ,

w i+1 ‚àí w i‚àí1
2h

= 0,

para cada i 5   N
(OVLVWHPDQROLQHDON 3 NREWHQLGRDSDUWLUGHHVWHPpWRGR

2w 1 ‚àí w 2 + h 2 f x1 , w 1 ,

w2 ‚àí Œ±
2h

‚àíw 1 + 2w 2 ‚àí w 3 + h 2 f x2 , w 2 ,

‚àí Œ± = 0,

w3 ‚àí w1
2h

= 0,
..
.

‚àíw N ‚àí2 + 2w N ‚àí1 ‚àí w N + h 2 f x N ‚àí1 , w N ‚àí1 ,
‚àíw N ‚àí1 + 2w N + h 2 f x N , w N ,

w N ‚àí w N ‚àí2
2h

Œ≤ ‚àí w N ‚àí1
2h



= 0,

‚àí Œ≤ = 0,

WLHQHXQDVROXFLyQ~QLFDVLHPSUH\FXDQGRh < 2/L, FRPRVHPXHVWUDHQ>.HOOHU+@S
7DPELpQFRQVXOWHHOHMHUFLFLR

M√©todo de Newton para iteraciones
8VDPRV HO PpWRGR GH 1HZWRQ SDUD ORV VLVWHPDV QR OLQHDOHV TXH VH DQDOL]DURQ HQ OD VHFFLyQSDUDDSUR[LPDUODVROXFLyQGHHVWHVLVWHPD6HJHQHUDXQDVXFHVLyQGHLWHUDFLRt
QHV w 1(k) , w 2(k) , . . . , w (k)
TXHFRQYHUJHDODVROXFLyQGHOVLVWHPD  VLHPSUHTXH
N
t

 VHD VX√ÄFLHQWHPHQWH FHUFDQD D OD VROXFLyQ
OD DSUR[LPDFLyQ LQLFLDO w 1(0) , w 2(0) , . . . , w (0)
N
t
(w 1 , w 2 , . . . , w N ) \ODPDWUL]MDFRELQDSDUDTXHHOVLVWHPDVHDQRVLQJXODU3DUDHOVLVWHPD
 ODPDWUL]MDFRELQDJ (w 1 , . . . , w N )HVWULGLDJRQDOFRQi j-pVLPDHQWUDGD

‚éß
w i+1 ‚àí w i‚àí1
h
‚é™
‚é™
, para i = j ‚àí 1 y j = 2, . . . , N ,
‚àí1 + f y xi , w i ,
‚é™
‚é™
‚é™
2
2h
‚é™
‚é™
‚é™
‚é®
w i+1 ‚àí w i‚àí1
J (w 1 , . . . , w N )i j = 2 + h 2 f y xi , w i ,
, para i = j y j = 1, . . . , N ,
‚é™
2h
‚é™
‚é™
‚é™
‚é™
‚é™
h
w i+1 ‚àí w i‚àí1
‚é™
‚é™
, para i = j + 1 y j = 1, . . . , N ‚àí 1,
‚é©‚àí1 ‚àí f y xi , w i ,
2
2h
donde w 0 = Œ± y w N +1 = Œ≤.
(OPpWRGRGH1HZWRQSDUDORVVLVWHPDVQROLQHDOHVUHTXLHUHTXHHQFDGDLWHUDFLyQVH
UHVXHOYDHOVLVWHPDOLQHDON 3 N

J (w 1 , . . . , w N )(v1 , . . . , vn )t
= ‚àí 2w 1 ‚àí w 2 ‚àí Œ± + h 2 f

x1 , w 1 ,

w2 ‚àí Œ±
2h

‚àí w 1 + 2w 2 ‚àí w 3 + h 2 f

x2 , w 2 ,

w3 ‚àí w1
2h

,
,... ,

524

CAP√çTULO 11

Problemas de valor en la frontera para ecuaciones diferenciales ordinarias

‚àí w N ‚àí2 + 2w N ‚àí1 ‚àí w N + h 2 f

x N ‚àí1 , w N ‚àí1 ,

w N ‚àí w N ‚àí2
2h

‚àí w N ‚àí1 + 2w N + h 2 f

Œ≤ ‚àí w N ‚àí1
2h

‚àíŒ≤

xN , w N ,

,

t

para v1 , v2 , . . . , v N \DTXH

w i(k) = w i(k‚àí1) + vi , para cada i = 1, 2, . . . , N .
3XHVWR TXH J HV WULGLDJRQDO HVWR QR HV XQ SUREOHPD WDQ GLItFLO FRPR SRGUtD SDUHFHU (Q
SDUWLFXODUVHSXHGHDSOLFDUODIDFWRUL]DFLyQGH&URXWHODOJRULWPRHQODSiJLQD(O
SURFHVRVHGHWDOODHQHODOJRULWPR

ALGORITMO

11.4

Diferencia Ô¨Ånita no lineal
3DUDDSUR[LPDUODVROXFLyQGHOSUREOHPDGHYDORUHQODIURQWHUDQROLQHDO

y = f (x, y, y ),

para a ‚â§ x ‚â§ b, con y(a) = Œ± y y(b) = Œ≤ :

ENTRADA H[WUHPRVa, bFRQGLFLRQHVGHIURQWHUDŒ±Œ≤HQWHURN ‚â•WROHUDQFLDTOL
Q~PHURPi[LPRGHLWHUDFLRQHVM
SALIDA DSUR[LPDFLRQHVwi para y(xi) para cada i 5   N 1RXQPHQVDMHTXHLQFD
TXHVHKDH[FHGLGRHOQ~PHURPi[LPRGHLWHUDFLRQHV

Paso 1 Determine h = (b ‚àí a)/(N + 1);
w 0 = Œ±;
w N +1 = Œ≤.
Paso 2 Para i = 1, . . . , N determine w i = Œ± + i

Œ≤ ‚àíŒ±
b‚àía

h.

Paso 3 Determine k = 1.
Paso 4 Mientras k ‚â§ M haga los pasos 5‚Äìl6.
Paso 5 Determine x = a + h;
t = (w 2 ‚àí Œ±)/(2h);
a1 = 2 + h 2 f y (x, w 1 , t);
b1 = ‚àí1 + (h/2) f y (x, w 1 , t);
d1 = ‚àí 2w 1 ‚àí w 2 ‚àí Œ± + h 2 f (x, w 1 , t) .

Paso 6 Para i = 2, . . . , N ‚àí 1
determine x = a + i h;
t = (w i+1 ‚àí w i‚àí1 )/(2h);
ai = 2 + h 2 f y (x, w i , t);
bi = ‚àí1 + (h/2) f y (x, w i , t);
ci = ‚àí1 ‚àí (h/2) f y (x, w i , t);
di = ‚àí 2w i ‚àí w i+1 ‚àí w i‚àí1 + h 2 f (x, w i , t) .
Paso 7 Determine x = b ‚àí h;
t = (Œ≤ ‚àí w N ‚àí1 )/(2h);
a N = 2 + h 2 f y (x, w N , t);
c N = ‚àí1 ‚àí (h/2) f y (x, w N , t);
d N = ‚àí 2w N ‚àí w N ‚àí1 ‚àí Œ≤ + h 2 f (x, w N , t) .

11.4

M√©todos de diferencias Ô¨Ånitas para problemas no lineales

Paso 8 Determine l1 = a1 ;
u 1 = b1 /a1 ;
z 1 = d1 /l1 .

525

(Los pasos 8-12 resuelven un sistema lineal
tridiagonal mediante el algoritmo 6.7.)

Paso 9 Para i = 2, . . . , N ‚àí 1 determine li = ai ‚àí ci u i‚àí1 ;
u i = bi /li ;
z i = (di ‚àí ci z i‚àí1 )/li .
Paso 10 Determine l N = a N ‚àí c N u N ‚àí1 ;
z N = (d N ‚àí c N z N ‚àí1 )/l N .
Paso 11 Determine v N = z N ;
w N = w N + vN .
Paso 12 Para i = N ‚àí 1, . . . , 1 determine vi = z i ‚àí u i vi+1 ;
w i = w i + vi .
Paso 13 Si v

TOL entonces haga los pasos 14 y 15.

Paso 14 Para i = 0, . . . , N + 1 determine x = a + i h;
SALIDA (x, wi ).
Paso 15 PARE.

( El procedimiento fue exitoso.)

Paso 16 Determine k = k + 1.
Paso 17 SALIDA (‚ÄòN√∫mero m√°ximo de iteraciones excedido‚Äô);
(El procedimiento no fue exitoso.)
PARE.

6HSXHGHPRVWUDU FRQVXOWH>,.@S TXHHVWHPpWRGRGHGLIHUHQFLD√ÄQLWDQROLQHDO
HVGHRUGHQO(h 
6HUHTXLHUHXQDEXHQDDSUR[LPDFLyQLQLFLDOFXDQGRQRVHSXHGHYHUL√ÄFDUTXHVHFXPSOHQODVFRQGLFLRQHV    \  GDGDVDOSULQFLSLRGHHVWDSUHVHQWDFLyQSRUORTXHVHGHEH
HVSHFL√ÄFDUXQDFRWDVXSHULRUSDUDHOQ~PHURGHLWHUDFLRQHV\VLVHH[FHGHVHFRQVLGHUDXQD
DSUR[LPDFLyQLQLFLDOQXHYDRXQDUHGXFFLyQGHODORQJLWXG$PHQRVTXHH[LVWDLQIRUPDFLyQ
FRQWUDGLFWRULDHVUD]RQDEOHFRPHQ]DUHOSURFHGLPLHQWRDODVXPLUTXHODVROXFLyQHVOLQHDO
3RUORTXHODVDSUR[LPDFLRQHVLQLFLDOHVw i(0) para w i , para cada i = 1, 2, . . . , N , VHREWLHQH
HQHOSDVRDOSDVDUXQDOtQHDUHFWDDWUDYpVGHORVH[WUHPRVFRQRFLGRV aŒ± \ bŒ≤ \DO
HYDOXDUHQxi

Ejemplo 1

$SOLTXHHODOJRULWPRFRQh 5SDUDHOSUREOHPDQROLQHDOGHYDORUHQODIURQWHUD

y =

1
(32 + 2x 3 ‚àí yy ),
8

para 1 ‚â§ x ‚â§ 3, con y(1) = 17 y y(3) =

43
,
3

\FRPSDUHORVUHVXOWDGRVFRQORVREWHQLGRVHQHOHMHPSORGHODVHFFLyQ
Soluci√≥n (O SURFHGLPLHQWR GH SDUR R GHWHQFLyQ XWLOL]DGR HQ HO DOJRULWPR  IXH LWHUDU

KDVWDTXHORVYDORUHVGHLWHUDFLRQHVVXFHVLYDVGL√ÄHUDQSRUPHQRVGH26HORJUyFRQFXDWURLWHUDFLRQHV(VWRGDORVUHVXOWDGRVHQODWDEOD6RQPHQRVH[DFWRVTXHORVREWHQLGRV
XVDQGRHOPpWRGRGHGLVSDURQROLQHDOORFXDOGLRORVUHVXOWDGRVGHODSDUWHPHGLDGHODWDEOD
H[DFWRVHQHORUGHQGH2

526

CAP√çTULO 11

Tabla 11.5

Problemas de valor en la frontera para ecuaciones diferenciales ordinarias
xi

wi

y(xi )

|w i ‚àí y(xi )|

1.0
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
2.0
2.1
2.2
2.3
2.4
2.5
2.6
2.7
2.8
2.9
3.0

17.000000
15.754503
14.771740
13.995677
13.386297
12.914252
12.557538
12.299326
12.126529
12.028814
11.997915
12.027142
12.111020
12.245025
12.425388
12.648944
12.913013
13.215312
13.553885
13.927046
14.333333

17.000000
15.755455
14.773333
13.997692
13.388571
12.916667
12.560000
12.301765
12.128889
12.031053
12.000000
12.029048
12.112727
12.246522
12.426667
12.650000
12.913846
13.215926
13.554286
13.927241
14.333333

9.520 √ó 10‚àí4
1.594 √ó 10‚àí3
2.015 √ó 10‚àí3
2.275 √ó 10‚àí3
2.414 √ó 10‚àí3
2.462 √ó 10‚àí3
2.438 √ó 10‚àí3
2.360 √ó 10‚àí3
2.239 √ó 10‚àí3
2.085 √ó 10‚àí3
1.905 √ó 10‚àí3
1.707 √ó 10‚àí3
1.497 √ó 10‚àí3
1.278 √ó 10‚àí3
1.056 √ó 10‚àí3
8.335 √ó 10‚àí4
6.142 √ó 10‚àí4
4.006 √ó 10‚àí4
1.953 √ó 10‚àí4

Uso de la extrapolaci√≥n de Richardson
(OSURFHGLPLHQWRGHH[WUDSRODFLyQGH5LFKDUGVRQWDPELpQVHSXHGHXVDUSDUDHOPpWRGRGH
GLIHUHQFLDV√ÄQLWDVQROLQHDO/DWDEODHQXPHUDORVUHVXOWDGRVFXDQGRVHDSOLFDHVWHPpWRGRDQXHVWURHMHPSORXVDQGRh 5\ FRQFXDWURLWHUDFLRQHVHQFDGDFDVR6H
RPLWHQORVYDORUHVGHw i (h = 0.1)GHODWDEODSDUDDKRUUDUHVSDFLRSHURVHOLVWDQHQODWDEOD
/RVYDORUHVGHw i (h = 0.25)VRQH[DFWRVGHQWURGHDSUR[LPDGDPHQWH326LQ
HPEDUJRORVYDORUHVGH([WiVRQH[DFWRVSDUDORVOXJDUHVHQXPHUDGRVFRQXQHUURUPi[LPR
DFWXDOGH32

Tabla 11.6

xi

w i (h = 0.05)

w i (h = 0.025)

Ext1i

Ext2i

Ext3i

1.0
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
2.0
2.1
2.2
2.3
2.4
2.5
2.6
2.7
2.8
2.9
3.0

17.00000000
15.75521721
14.77293601
13.99718996
13.38800424
12.91606471
12.55938618
12.30115670
12.12830042
12.03049438
11.99948020
12.02857252
12.11230149
12.24614846
12.42634789
12.64973666
12.91362828
13.21577275
13.55418579
13.92719268
14.33333333

17.00000000
15.75539525
14.77323407
13.99756690
13.38842973
12.91651628
12.55984665
12.30161280
12.12874287
12.03091316
11.99987013
12.02892892
12.11262089
12.24642848
12.42658702
12.64993420
12.91379422
13.21588765
13.55426075
13.92722921
14.33333333

17.00000000
15.75545543
14.77333479
13.99769413
13.38857346
12.91666881
12.56000217
12.30176684
12.12899094
12.03105457
12.00000179
12.02902924
12.11272872
12.24652299
12.42666773
12.65000086
12.91384683
13.21592641
13.55428603
13.92724153
14.33333333

17.00000000
15.75545460
14.77333342
13.99769242
13.38857156
12.91666680
12.56000014
12.30176484
12.12888902
12.03105275
12.00000011
12.02904772
12.11272736
12.24652182
12.42666673
12.65000005
12.91384620
13.21592596
13.55428573
13.92724139
14.33333333

17.00000000
15.75545455
14.77333333
13.99769231
13.38857143
12.91666667
12.56000000
12.30176471
12.12888889
12.03105263
12.00000000
12.02904762
12.11272727
12.24652174
12.42666667
12.65000000
12.91384615
13.21592593
13.55428571
13.92724138
14.33333333

La secci√≥n Conjunto de ejercicios 11.4 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

11.5 El m√©todo de Rayleigh-Ritz

527

11.5 El m√©todo de Rayleigh-Ritz
-RKQ:LOOLDP6WUXWW/RUG
5D\OHLJK ¬≤ XQItVLFR
PDWHPiWLFRTXHVHLQWHUHVDED
HVSHFLDOPHQWHHQODSURSDJDFLyQ
GHRQGDVUHFLELyHO3UHPLR
1REHOGHItVLFDHQ
:DOWHU5LW] ¬≤ XQ
ItVLFRWHyULFRHQOD*|WWLJHQ
8QLYHUVLW\SXEOLFyXQDUWtFXOR
VREUHXQSUREOHPDGHYDULDFLyQ
HQ>5L@0XULyGH
WXEHUFXORVLVDORVDxRV

(OPpWRGRGHOGLVSDURSDUDDSUR[LPDUODVROXFLyQGHXQSUREOHPDGHYDORUHQODIURQWHUD
UHHPSOD]yDOSUREOHPDGHYDORUHQODIURQWHUDFRQXQSDUGHSUREOHPDVGHYDORULQLFLDO(O
HQIRTXHGHGLIHUHQFLD√ÄQLWDUHHPSOD]DODRSHUDFLyQFRQWLQXDGHGLIHUHQFLDFLyQFRQODRSHUDFLyQGLVFUHWDGHGLIHUHQFLDV√ÄQLWDV(OPpWRGR5D\OHLJK5LW]HVXQDWpFQLFDGHYDULDFLyQTXH
DERUGDHOSUREOHPDGHVGHXQWHUFHUHQIRTXH(OSUREOHPDGHYDORUHQODIURQWHUDSULPHURVH
UHIRUPXODFRPRXQSUREOHPDGHVHOHFFLyQGHOFRQMXQWRGHWRGDVODVIXQFLRQHVVX√ÄFLHQWHPHQWHGLIHUHQFLDEOHVTXHVDWLVIDFHQODVFRQGLFLRQHVGHIURQWHUDVHVHOHFFLRQDODIXQFLyQSDUD
PLQLPL]DU FLHUWD LQWHJUDO$ FRQWLQXDFLyQ VH UHGXFH HO WDPDxR GHO FRQMXQWR GH IXQFLRQHV
YLDEOHV\VHHQFXHQWUDXQDDSUR[LPDFLyQDSDUWLUGHHVWHFRQMXQWRSDUDPLQLPL]DUODLQWHJUDO
(VWRQRVSURSRUFLRQDXQDDSUR[LPDFLyQSDUDODVROXFLyQGHOSUREOHPDGHYDORUHQODIURQWHUD
3DUDGHVFULELUHOPpWRGR5D\OHLJK5LW]FRQVLGHUDPRVODDSUR[LPDFLyQGHXQDVROXFLyQ
VROXFLyQGHXQSUREOHPDGHYDORUHQODIURQWHUDHQGRVSXQWRVOLQHDOGHVGHHODQiOLVLVGH
WHQVLyQGHXQDYLJD(VWHSUREOHPDGHYDORUHQODIURQWHUDVHGHVFULEHPHGLDQWHODHFXDFLyQ
GLIHUHQFLDO

‚àí

d
dx

p(x)

dy
dx

+ q(x)y = f (x),

para 0 ‚â§ x ‚â§ 1,



FRQODVFRQGLFLRQHVGHIURQWHUD

y(0) = y(1) = 0.



(VWDHFXDFLyQGLIHUHQFLDOGHVFULEHODGH√ÅH[LyQy(x GHXQDYLJDGHORQJLWXGFRQVHFFLyQ
WUDQVYHUVDOYDULDEOHUHSUHVHQWDGDSRUq(x /DGH√ÅH[LyQVHGHEHDODVWHQVLRQHVDxDGLGDVp(x)
\f (x (QORVHMHUFLFLRV\VHFRQVLGHUDQFRQGLFLRQHVGHIURQWHUDPiVJHQHUDOHV
(QHOVLJXLHQWHDQiOLVLVVXSRQHPRVTXH p ‚àà C 1 [0, 1] y q, f ‚àà C[0, 1]. $GHPiVVXSRQHPRVTXHH[LVWHXQDFRQVWDQWHŒ¥ .WDOTXH

p(x) ‚â• Œ¥, \TXHq(x) ‚â• 0,

para cada xHQ>@

(VWDVVXSRVLFLRQHVVRQVX√ÄFLHQWHVSDUDJDUDQWL]DUTXHHOSUREOHPDGHYDORUHQODIURQWHUD
SURYLVWRHQODVHFXDFLRQHV  \  WLHQHXQDVROXFLyQ~QLFD FRQVXOWH>%6:@ 

Problemas de variaciones
$OLJXDOTXHHQHOFDVRGHORVSUREOHPDVGHYDORUHQODIURQWHUDTXHGHVFULEHQIHQyPHQRV
ItVLFRVODVROXFLyQGHODHFXDFLyQGHODYLJDVDWLVIDFHODSURSLHGDGvariacional de minimi]DFLyQLQWHJUDO(OSULQFLSLRYDULDFLRQDOSDUDODHFXDFLyQGHODYLJDHVIXQGDPHQWDOSDUDHO
GHVDUUROORGHOPpWRGR5D\OHLJK5LW]\FDUDFWHUL]DODVROXFLyQSDUDGLFKDHFXDFLyQFRPROD
IXQFLyQTXHPLQLPL]DXQDLQWHJUDOVREUHWRGDVODVIXQFLRQHV C02 [0, 1], HOFRQMXQWRGHHVDV
IXQFLRQHVu en C  >@FRQODSURSLHGDG u(0) = u(1) = 0. (OVLJXLHQWHWHRUHPDHVWDEOHFH
ODFDUDFWHUL]DFLyQ
Teorema 11.4

Si p ‚àà C 1 [0, 1], q, f ‚àà C[0, 1], y

p(x) ‚â• Œ¥ > 0,

q(x) ‚â• 0, para 0‚â§ x ‚â§ 1.

/DIXQFLyQ y ‚àà C02 [0, 1] HVOD~QLFDVROXFLyQSDUDODHFXDFLyQGLIHUHQFLDO

‚àí

d
dx

p(x)

dy
dx

+ q(x)y = f (x), para 0 ‚â§ x ‚â§ 1,



VL\VyORVLyHVOD~QLFDIXQFLyQHQC02 [0, 1]TXHPLQLPL]DODLQWHJUDO
1

I [u] =
0

{ p(x)[u (x)]2 + q(x)[u(x)]2 ‚àí 2 f (x)u(x)} d x.



528

CAP√çTULO 11

Problemas de valor en la frontera para ecuaciones diferenciales ordinarias

/RVGHWDOOHVGHODGHPRVWUDFLyQGHHVWHWHRUHPDVHSXHGHQHQFRQWUDUHQ>6KXO@S
6XSURFHGLPLHQWRFRQVWDGHWUHVSDVRV3ULPHURVHPXHVWUDTXHXQDIXQFLyQyHVXQDVROXFLyQGHODVROXFLyQGHODHFXDFLyQ  VL\VyORVLVDWLVIDFHODHFXDFLyQ
1

‚Ä¢

1

f (x)u(x)d x =

0

p(x)y (x)u (x) + q(x)y(x)u(x)d x,



0

 SDUDWRGDu ‚àà C02 [0.1].
¬á (OVHJXQGRSDVRPXHVWUDTXHy ‚àà C02 [0, 1] HVXQDVROXFLyQSDUDODHFXDFLyQ  VL\
VyORVLODHFXDFLyQ  VHPDQWLHQHSDUDWRGDu ‚àà C02 [0, 1].
¬á (OSDVR√ÄQDOPXHVWUDTXH  WLHQHXQDVROXFLyQ~QLFD(VWD~OWLPDWDPELpQVHUiXQD
VROXFLyQSDUD  \SDUD  GHWDOIRUPDTXHODVVROXFLRQHVSDUDODVHFXDFLRQHV
 \  VRQLGpQWLFDV
(OPpWRGR5D\OHLJK5LW]DSUR[LPDODVROXFLyQ\DOPLQLPL]DUODLQWHJUDOQRVREUHWRGDV
ODVIXQFLRQHVHQ C02 [0, 1]VLQRVREUHXQFRQMXQWRSHTXHxRGHIXQFLRQHVTXHFRQVLVWHQHQ
FRPELQDFLRQHV OLQHDOHV GH FLHUWDV IXQFLRQHV EDVH œÜ1 , œÜ2 , . . . , œÜn . /DV IXQFLRQHV EDVH VRQ
OLQHDOPHQWHLQGHSHQGLHQWHV\VDWLVIDFHQ

œÜi (0) = œÜi (1) = 0, para cada i = 1, 2, . . . , n.
n
ci œÜi (x) SDUD OD VROXFLyQ y (x)
$ FRQWLQXDFLyQ VH REWLHQH XQD DSUR[LPDFLyQ œÜ(x) = i=1
GHODHFXDFLyQ  DOHQFRQWUDUODVFRQVWDQWHV c1 , c2 , . . . , cnSDUDPLQLPL]DUODLQWHJUDO
n
I
i=1 ci œÜi .
$SDUWLUGHODHFXDFLyQ 
n

I [œÜ] = I

ci œÜi



i=1
1

=

n

0

n

2

p(x)

ci œÜi (x)
i=1

n

2

+ q(x)

ci œÜi (x)

‚àí 2 f (x)

i=1

ci œÜi (x) d x,
i=1

\ SDUD TXH VH SUHVHQWH XQ PtQLPR HV QHFHVDULR DO FRQVLGHUDU I FRPR XQD IXQFLyQ GH
c1 , c2 , . . . , cnWHQHU

‚àÇI
= 0, para cada j = 1, 2, . . . , n.
‚àÇc j



$OGHULYDU  REWHQHPRV

‚àÇI
=
‚àÇc j

1

n

2 p(x)

0

n

ci œÜi (x)œÜ j (x) + 2q(x)
i=1

ci œÜi (x)œÜ j (x) ‚àí 2 f (x)œÜ j (x)

d x,

i=1

\DOVXVWLWXLUHQODHFXDFLyQ  VHREWLHQH
n

1

0=
i=1

0

{ p(x)œÜi (x)œÜ j (x) + q(x)œÜi (x)œÜ j (x)} d x ci ‚àí

1

f (x)œÜ j (x) d x,

0



para cada j = 1, 2, . . . , n.
/DVecuaciones normalesGHVFULWDVHQODHFXDFLyQ  JHQHUDQXQVLVWHPDOLQHDOn 3 n
Ac = bHQODVYDULDEOHVc1 , c2 , . . . , cnGRQGHODPDWUL]VLPpWULFDA HVWiGDGD
1

ai j =
0

[ p(x)œÜi (x)œÜ j (x) + q(x)œÜi (x)œÜ j (x)] d x,

11.5 El m√©todo de Rayleigh-Ritz

529

\bVHGH√ÄQHFRPR
1

bi =

f (x)œÜi (x) d x.

0

Base lineal por tramos
/D RSFLyQ PiV VLPSOH GH ODV IXQFLRQHV EDVH LPSOLFD SROLQRPLRV OLQHDOHV SRU WUDPRV (O
SULPHUSDVRHVIRUPDUXQDSDUWLFLyQGH>@DOVHOHFFLRQDUORVSXQWRVx0 , x1 , . . . , xn+1 con

0 = x0 < x1 < ¬∑ ¬∑ ¬∑ < xn < xn+1 = 1.
$OXWLOL]DU h i = xi+1 ‚àí xi , para cada i = 0, 1, . . . , n, GH√ÄQLPRVODVIXQFLRQHVEDVH œÜ1 (x),
œÜ2 (x), . . . , œÜn (x)PHGLDQWH
‚éß
‚é™
0,
si 0 ‚â§ x ‚â§ xi‚àí1 ,
‚é™
‚é™
‚é™
‚é™
‚é™
1
‚é™
‚é™
‚é™
‚é® h (x ‚àí xi‚àí1 ), si xi‚àí1 < x ‚â§ xi ,
i‚àí1
œÜi (x) =

‚é™1
‚é™
‚é™
(x
‚àí
x),
si
x
<
x
‚â§
x
,
‚é™
i+1
i
i+1
‚é™
‚é™
hi
‚é™
‚é™
‚é™
‚é©
0,
si xi+1 < x ‚â§ 1,
para cada i = 0, 1, . . . , n &RQVXOWHOD√ÄJXUD
(QHOHMHUFLFLRVHPXHVWUDTXHODVIXQFLRQHVEDVHVRQOLQHDOPHQWHLQGHSHQGLHQWHV
Figura 11.4
y

y

y

1

1

1

y = fi (x)

y = f1(x)

y = fn(x)

0

0
x1

x2

1 x

0
x i21

xi

x i11

1

x

x n21

xn

1

x

/DVIXQFLRQHV œÜiVRQOLQHDOHVSRUWUDPRVSRUORTXHODVGHULYDGDV œÜi , DXQTXHQRVHDQ
FRQWLQXDVVRQFRQVWDQWHVHQ(x j , x j+1 ), para cada j = 0, 1, . . . , n, \
‚éß
‚é™
0,
si 0 < x < xi‚àí1 ,
‚é™
‚é™
‚é™
‚é™
‚é™
1
‚é™
‚é™
‚é™
, si xi‚àí1 < x < xi ,
‚é®
h i‚àí1
œÜi (x) =

‚é™
1
‚é™
‚é™
,
si
x
<
x
<
x
,
‚àí
‚é™
i
i+1
‚é™
‚é™
hi
‚é™
‚é™
‚é™
‚é©
0,
si xi+1 < x < 1,
para cada i 5   n
3XHVWRTXHœÜi \œÜi VRQGLIHUHQWHVDFHURVylo en (xi‚àí1 , xi+1 ),

œÜi (x)œÜ j (x) ‚â° 0

y

œÜi (x)œÜ j (x) ‚â° 0,

530

CAP√çTULO 11

Problemas de valor en la frontera para ecuaciones diferenciales ordinarias

H[FHSWR FXDQGR j HV i 2  i R i 1  &RPR FRQVHFXHQFLD HO VLVWHPD OLQHDO GDGR SRU OD
HFXDFLyQ  VHUHGXFHDXQVLVWHPDOLQHDOWULGLDJRQDOn 3 n/DVHQWUDGDVGLIHUHQWHVD
cero en AVRQ
1

aii =

p(x)[œÜi (x)]2 + q(x)[œÜi (x)]2

0

=

xi

2

1
h i‚àí1

xi‚àí1

xi

2

1

+

p(x) d x +

h i‚àí1

xi‚àí1

‚àí1
hi

dx
xi+1

2

p(x) d x

xi

xi+1

2

1
hi

(x ‚àí xi‚àí1 )2 q(x) d x +

xi

(xi+1 ‚àí x)2 q(x) d x,

para cada i 5   n
1

ai,i+1 =
0

{ p(x)œÜi (x)œÜi+1 (x) + q(x)œÜi (x)œÜi+1 (x)} d x
xi+1

2

1
hi

=‚àí

xi

p(x) d x +

xi+1

2

1
hi

xi

(xi+1 ‚àí x)(x ‚àí xi )q(x) d x,

para cada i 5   n 2\
1

ai,i‚àí1 =
0

{ p(x)œÜi (x)œÜi‚àí1 (x) + q(x)œÜi (x)œÜi‚àí1 (x)} d x

=‚àí

xi

2

1
h i‚àí1

xi‚àí1

p(x) d x +

xi

2

1
h i‚àí1

xi‚àí1

(xi ‚àí x)(x ‚àí xi‚àí1 )q(x) d x,

para cada i 5   n/DVHQWUDGDVGHbVRQ
1

bi =

f (x)œÜi (x) d x =

0

1

xi

h i‚àí1

xi‚àí1

(x ‚àí xi‚àí1 ) f (x) d x +

1
hi

xi+1
xi

(xi+1 ‚àí x) f (x) d x,

para cada i 5   n.
([LVWHQVHLVWLSRVGHLQWHJUDOHVTXHGHEHQHYDOXDUVH

Q 1,i =

1
hi

Q 2,i =

1

xi

1
hi

Q 4,i =

1

(xi+1 ‚àí x)(x ‚àí xi )q(x) d x,

xi

2

h i‚àí1

Q 3,i =

Q 5,i =

xi+1

2

(x ‚àí xi‚àí1 )2 q(x) d x,

para cada i = 1, 2, . . . , n,

(xi+1 ‚àí x)2 q(x) d x,

para cada i = 1, 2, . . . , n,

xi‚àí1
xi+1

2

xi
xi

2

h i‚àí1

xi‚àí1

1

xi

h i‚àí1

xi‚àí1

para cada i = 1, 2, . . . , n ‚àí 1,

p(x) d x,

para cada i = 1, 2, . . . , n + 1,

(x ‚àí xi‚àí1 ) f (x) d x,

para cada i = 1, 2, . . . , n,

y
Q 6,i =

1
hi

xi+1
xi

(xi+1 ‚àí x) f (x) d x,

para cada i = 1, 2, . . . , n.

11.5 El m√©todo de Rayleigh-Ritz

531

/DPDWUL]A\HOYHFWRUbHQHOVLVWHPDOLQHDOAc 5 bWLHQHODVHQWUDGDV

ai,i = Q 4,i + Q 4,i+1 + Q 2,i + Q 3,i ,

para cada i = 1, 2, . . . , n,

ai,i+1 = ‚àíQ 4,i+1 + Q 1,i ,

para cada i = 1, 2, . . . , n ‚àí 1,

ai,i‚àí1 = ‚àíQ 4,i + Q 1,i‚àí1 ,

para cada i = 2, 3, . . . , n,

y
bi = Q 5,i + Q 6,i ,

para cada i = 1, 2, . . . , n.

/DVHQWUDGDVHQcVRQORVFRH√ÄFLHQWHVGHVFRQRFLGRVc1 , c2 , . . . , cn , DSDUWLUGHORVFXDOHV
n

VHFRQVWUX\HODDSUR[LPDFLyQGH5D\OHLJK5LW]œÜGDGDSRUœÜ(x) =

ci œÜi (x),
i=1

(OXVRGHHVWHPpWRGRUHTXLHUHTXHVHHYDO√∫ennLQWHJUDOHV\DVHDGLUHFWDPHQWHRPHGLDQWHXQDIyUPXODGHFXDGUDWXUDFRPRODUHJODFRPSXHVWDGH6LPSVRQ
8QHQIRTXHDOWHUQDWLYRSDUDODHYDOXDFLyQLQWHJUDOHVDSUR[LPDUFDGDXQDGHODVIXQFLRQHVp, q \ fFRQVXSROLQRPLRGHLQWHUSRODFLyQOLQHDOSRUWUDPRV\DFRQWLQXDFLyQLQWHJUDPRV
ODDSUR[LPDFLyQ
&RQVLGHUHSRUHMHPSORODLQWHJUDOQ 1,i . /DLQWHUSRODFLyQOLQHDOSRUWUDPRVGHqHV
n+1

Pq (x) =

q(xi )œÜi (x),
i=0

donde œÜ1 , . . . , œÜnVHGH√ÄQHQHQODHFXDFLyQ  \

‚éß
x ‚àíx
‚é™
‚é® 1
, si
‚â§ x ‚â§ x1
x1
œÜ0 (x) =
‚é™
‚é©0,
en otro caso

‚éß
x ‚àí xn
‚é™
‚é®
, si xn ‚â§ x ‚â§ 1
œÜn+1 (x) = 1 ‚àí xn
‚é™
‚é©0,
en otro caso.

y

(OLQWHUYDORGHLQWHJUDFLyQHV[xi , xi+1 ], GHWDOIRUPDTXHHOSROLQRPLRSRUWUDPRVPq (x VH
UHGXFHD

Pq (x) = q(xi )œÜi (x) + q(xi+1 )œÜi+1 (x).
eVWHHVHOSROLQRPLRLQWHUSRODQWHGHSULPHUJUDGRHQODVHFFLyQ0HGLDQWHHOWHRUHPD
HQODSiJLQD

|q(x) ‚àí Pq (x)| = O(h i2 ),

para xi ‚â§ x ‚â§ xi+1 ,

VLq ‚àà C 2 [xi , xi+1 ]. 3DUDi = 1, 2, . . . , n ‚àí 1, ODDSUR[LPDFLyQSDUDQiVHREWLHQHDOLQWHJUDUODDSUR[LPDFLyQSDUDHOLQWHJUDQGR

1
hi

2

Q 1,i =

1
hi

2

‚âà

xi+1
xi
xi+1
xi

(xi+1 ‚àí x)(x ‚àí xi )q(x) d x
(xi+1 ‚àí x)(x ‚àí xi )

hi
[q(xi ) + q(xi+1 )].
=
12

q(xi )(xi+1 ‚àí x) q(xi+1 )(x ‚àí xi )
+
dx
hi
hi

532

CAP√çTULO 11

Problemas de valor en la frontera para ecuaciones diferenciales ordinarias

$GHPiVVLq ‚àà C 2 [xi , xi+1 ], HQWRQFHV

Q 1,i ‚àí

hi
[q(xi ) + q(xi+1 )] = O(h i3 ).
12

/DVDSUR[LPDFLRQHVSDUDODVRWUDVLQWHJUDOHVVHGHULYDQGHIRUPDVLPLODU\HVWiQGDGDVSRU

h i‚àí1
[3q(xi ) + q(xi‚àí1 )],
12
1
[ p(xi ) + p(xi‚àí1 )],
Q 4,i ‚âà
2h i‚àí1

hi
[3q(xi ) + q(xi+1 )],
12
h i‚àí1
[2 f (xi ) + f (xi‚àí1 )],
Q 5,i ‚âà
6

Q 2,i ‚âà

Q 3,i ‚âà

y
Q 6,i ‚âà

hi
[2 f (xi ) + f (xi+1 )].
6

(ODOJRULWPRHVWDEOHFHHOVLVWHPDOLQHDOWULGLDJRQDOHLQFOX\HHODOJRULWPRGHIDFWRUL]DFLyQGH&URXWSDUDUHVROYHUHOVLVWHPD/DVLQWHJUDOHVQ 1,i , . . . , Q 6,iVHSXHGHQFDOFXODU
PHGLDQWHDOJXQRGHORVPpWRGRVDQWHVPHQFLRQDGRV
ALGORITMO

11.5

Rayleigh-Ritz lineal por tramos
3DUDDSUR[LPDUODVROXFLyQDOSUREOHPDGHYDORUHQODIURQWHUD

‚àí

d
dx

p(x)

dy
dx

+ q(x)y = f (x),

para 0 ‚â§ x ‚â§ 1, con y(0) = 0 y y(1) = 0

con la funci√≥n lineal por tramos
n

œÜ(x) =

ci œÜi (x) :
i=1

ENTRADA entero n ‚â• 1; puntos x0 = 0 < x1 < ¬∑ ¬∑ ¬∑ < xn < xn+1 = 1.
SALIDA coeficientes c1 , . . . , cn .
Paso 1 Para i = 0, . . . , n determine h i = xi+1 ‚àí xi .
Paso 2 Para i = 1, . . . , n definir la base lineal por tramos œÜi por
‚éß
‚é™
0,
0 ‚â§ x ‚â§ xi‚àí1 ,
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™ x ‚àí xi‚àí1 , xi‚àí1 < x ‚â§ xi ,
‚é™
‚é® h
i‚àí1
œÜi (x) =
‚é™ xi+1 ‚àí x
‚é™
‚é™
, xi < x ‚â§ xi+1 ,
‚é™
‚é™
hi
‚é™
‚é™
‚é™
‚é©0,
xi+1 < x ‚â§ 1.
Paso 3 Para cada i = 1, 2, . . . , n ‚àí 1 calcule Q 1,i , Q 2,i , Q 3,i , Q 4,i , Q 5,i , Q 6,i ;
Calcule Q 2,n , Q 3,n , Q 4,n , Q 4,n+1 , Q 5,n , Q 6,n .
Paso 4 Para cada i = 1, 2, . . . , n ‚àí 1, determine Œ±i = Q 4,i + Q 4,i+1 + Q 2,i + Q 3,i ;
Œ≤i = Q 1,i ‚àí Q 4,i+1 ;
bi = Q 5,i + Q 6,i .
Paso 5 Determine Œ±n = Q 4,n + Q 4,n+1 + Q 2,n + Q 3,n ;
bn = Q 5,n + Q 6,n .
Paso 6 Determine a1 = Œ±1 ;
Œ∂1 = Œ≤1 /Œ±1 ;
z 1 = b1 /a1 .

(Los pasos 1-6 resuelven el sistema lineal tridiagonal
sim√©trico usando el algoritmo 6.7.)

11.5 El m√©todo de Rayleigh-Ritz

533

Paso 7 Para i = 2, . . . , n ‚àí 1 determine ai = Œ±i ‚àí Œ≤i‚àí1 Œ∂i‚àí1 ;
Œ∂i = Œ≤i /ai ;
z i = (bi ‚àí Œ≤i‚àí1 z i‚àí1 )/ai .
Paso 8 Determine an = Œ±n ‚àí Œ≤n‚àí1 Œ∂n‚àí1 ;
z n = (bn ‚àí Œ≤n‚àí1 z n‚àí1 )/an .
Paso 9 Determine cn = z n ;
SALIDA (cn ).
Paso 10 Para i = n ‚àí 1, . . . , 1 determine ci = z i ‚àí Œ∂i ci+1 ;
SALIDA (ci ).
Paso 11 PARE. (El procedimiento est√° completo.)
/DVLJXLHQWHLOXVWUDFLyQXVDHODOJRULWPR'HELGRDODQDWXUDOH]DIXQGDPHQWDOGH
HVWHHMHPSORODVLQWHJUDOHVHQORVSDVRV\VHHQFRQWUDURQGLUHFWDPHQWH
Ilustraci√≥n

&RQVLGHUHHOSUREOHPDGHYDORUHQODIURQWHUD

‚àíy + œÄ 2 y = 2œÄ 2 sen(œÄ x), para 0 ‚â§ x ‚â§ 1, con y(0) = y(1) = 0.
Sea h i = h = 0.1, GHWDOIRUPDTXHxi = 0.1i, para cada i = 0, 1, . . . , 9. /DVLQWHJUDOHVVRQ
0.1i+0.1

Q 1,i = 100

(0.1i + 0.1 ‚àí x)(x ‚àí 0.1i)œÄ 2 d x =

0.1i
0.1i

Q 2,i = 100

(x ‚àí 0.1i + 0.1)2 œÄ 2 d x =

0.1i‚àí0.1
0.1i+0.1

Q 3,i = 100

œÄ2
,
30

(0.1i + 0.1 ‚àí x)2 œÄ 2 d x =

0.1i
0.1i

Q 4,i = 100

œÄ2
,
60

œÄ2
,
30

d x = 10,

0.1i‚àí0.1

Q 5,i = 10

0.1i

(x ‚àí 0.1i + 0.1)2œÄ 2 sen œÄ x d x

0.1i‚àí0.1

= ‚àí2œÄ cos 0.1œÄi + 20[sen(0.1œÄi) ‚àí sen((0.1i ‚àí 0.1)œÄ )],

y
0.1i+0.1

Q 6,i =10

(0.1i + 0.1 ‚àí x)2œÄ 2 sen œÄ x d x

0.1i

=2œÄ cos 0.1œÄi ‚àí 20[sen((0.1i + 0.1)œÄ ) ‚àí sen(0.1œÄi)].
(OVLVWHPDOLQHDOAc 5 bWLHQH

ai,i = 20 +

œÄ2
,
15

para cada i = 1, 2, . . . , 9,

ai,i+1 = ‚àí10 +

œÄ2
,
60

para cada i = 1, 2, . . . , 8,

ai,i‚àí1 = ‚àí10 +

œÄ2
,
60

para cada i = 2, 3, . . . , 9,

534

CAP√çTULO 11

Problemas de valor en la frontera para ecuaciones diferenciales ordinarias

y
bi = 40 sen(0.1œÄi)[1 ‚àí cos 0.1œÄ ],

para cada i = 1, 2, . . . , 9.

/DVROXFLyQSDUDHOVLVWHPDOLQHDOWULGLDJRQDOHV

c9 = 0.3102866742, c8 = 0.5902003271, c7 = 0.8123410598,
c6 = 0.9549641893, c5 = 1.004108771, c4 = 0.9549641893,
c3 = 0.8123410598, c2 = 0.5902003271, c1 = 0.3102866742.
/DDSUR[LPDFLyQOLQHDOSRUWUDPRVHV
9

œÜ(x) =

ci œÜi (x),
i=1

\ODVROXFLyQUHDOSDUDHOSUREOHPDGHYDORUHQODIURQWHUDHV y(x) = sen œÄ x. /DWDEOD
LQFOX\HHOHUURUHQODDSUR[LPDFLyQHQxiSDUDFDGDi = 1, . . . , 9.

Tabla 11.7

i

xi

œÜ(xi )

y(xi )

|œÜ(xi ) ‚àí y(xi )|

1
2
3
4
5
6
7
8
9

0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9

0.3102866742
0.5902003271
0.8123410598
0.9549641896
1.0041087710
0.9549641893
0.8123410598
0.5902003271
0.3102866742

0.3090169943
0.5877852522
0.8090169943
0.9510565162
1.0000000000
0.9510565162
0.8090169943
0.5877852522
0.3090169943

0.00127
0.00241
0.00332
0.00390
0.00411
0.00390
0.00332
0.00241
0.00127

6HSXHGHPRVWUDUTXHODPDWUL]WULGLDJRQDOASURYLVWDSRUODVIXQFLRQHVEDVHOLQHDOHV
SRUWUDPRVHVGH√ÄQLGDSRVLWLYD FRQVXOWHHOHMHUFLFLR SRUORTXHSRUHOWHRUHPDHQ
ODSiJLQDHOVLVWHPDOLQHDOHVHVWDEOHUHVSHFWRDOHUURUGHUHGRQGHR'HDFXHUGRFRQOD
KLSyWHVLVSUHVHQWDGDDOLQLFLRGHHVWDVHFFLyQWHQHPRV

|œÜ(x) ‚àí y(x)| = O(h 2 ), SDUDFDGD[HQ>@
8QDSUXHEDGHHVWHUHVXOWDGRVHSXHGHHQFRQWUDUHQ>6FKXO@S

Base B spline
(OXVRGHODVIXQFLRQHVEDVHOLQHDOHVSRUWUDPRVUHVXOWDHQXQDVROXFLyQDSUR[LPDGDSDUDODV
HFXDFLRQHV  \  TXHHVFRQWLQXDSHURQRGLIHUHQFLDEOHHQ>@6HUHTXLHUHXQ
FRQMXQWRPiVVR√ÄVWLFDGRGHIXQFLRQHVEDVHC02 [0, 1]SDUDFRQVWUXLUXQDDSUR[LPDFLyQ(VWDV
IXQFLRQHVEDVHVRQVLPLODUHVDORVVSOLQHVF~ELFRVLQWHUSRODQWHVDQDOL]DGRVHQODVHFFLyQ
5HFXHUGHTXHHOVSOLQHF~ELFRinterpolante SHQORVFLQFRQRGRV x0 , x1 , x2 , x3 \x para
XQDIXQFLyQfHVWiGH√ÄQLGRSRU

a)

S(x) es un polinomio c√∫bico, que se denota S j (x), en el subintervalo [ x j , x j+1 ]
para cada j = 0, 1, 2, 3, 4;

b)

S j (x j ) = f (x j ) y S j (x j+1 ) = f (x j+1 ) para cada j = 0, 1, 2;

c)

S j+1 (x j+1 ) = S j (x j+1 ) para cada j = 0, 1, 2; (implicado por (b).)

d)

S j+1 (x j+1 ) = S j (x j+1 ) para cada j = 0, 1, 2;

11.5 El m√©todo de Rayleigh-Ritz

(Q,-6FKRHQEHUJ>6FKR@
SUHVHQWyORV% SDUD¬¥EDVH¬µ 
SHURSRUPiVGHXQDGpFDGD
IXHURQGLItFLOHVGHFDOFXODU
(Q&DUOGH%RRU ¬≤ 
>'HE@GHVFULELyODVIyUPXODV
SDUDODHYDOXDFLyQTXHPHMRUDURQ
VXHVWDELOLGDG\XWLOLGDG

535

e)

S j+1 (x j+1 ) = S j (x j+1 ) para cada j = 0, 1, 2;

f)

Se debe satisfacer uno de los siguientes conjuntos de condiciones de frontera:
i)

S (x0 ) = S (xn ) = 0

ii)

S (x0 ) = f (x0 )

(frontera natural (o libre))

y S (xn ) = f (xn )

(frontera fijada).

3XHVWRTXHODVLQJXODULGDGGHODVROXFLyQUHTXLHUHTXHHOQ~PHURGHFRQVWDQWHVHQa)
VHDLJXDODOQ~PHURGHFRQGLFLRQHVHQb)DWUDYpVGHf)VyORVHSXHGHHVSHFL√ÄFDUXQDGHODV
FRQGLFLRQHVGHIURQWHUDHQf)SDUDORVVSOLQHVF~ELFRVLQWHUSRODQWHV
/DVIXQFLRQHVGHVSOLQHF~ELFRTXHXVDUHPRVSDUDQXHVWUDVIXQFLRQHVEDVHUHFLEHQHO
nombre de B-splines o splines con forma de campana(VWRVGL√ÄHUHQGHORVVSOLQHVLQWHUSRODQWHVHQTXHVHVDWLVIDFHQDPERVFRQMXQWRVGHFRQGLFLRQHVGHIURQWHUDHQ(f)(VWRUHTXLHUH
TXHODVGRVFRQGLFLRQHVHQb) a e VH√ÅH[LELOLFHQ3XHVWRTXHHOVSOLQHGHEHWHQHUGRVGHULYDGDVFRQWLQXDVHQ[x0 , x4 ], ERUUDPRVGRVGHODVFRQGLFLRQHVGHLQWHUSRODFLyQDSDUWLUGHOD
GHVFULSFLyQGHORVVSOLQHVLQWHUSRODQWHV(QHVSHFLDOPRGL√ÄFDPRVODFRQGLFLyQb) por
b.

S(x j ) = f (x j ) para j = 0, 2, 4.

3RUHMHPSORHO%VSOLQHEiVLFRSTXHVHGH√ÄQHDFRQWLQXDFLyQ\TXHVHPXHVWUDHQOD
√ÄJXUD  XVD ORV QRGRV LJXDOPHQWH HVSDFLDGRV x0 = ‚àí2, x1 = ‚àí1, x2 = 0, x3 = 1 \
x 56DWLVIDFHODVFRQGLFLRQHVLQWHUSRODQWHV
b.

S(x0 ) = 0,

S(x2 ) = 1,

S(x4 ) = 0

DVtFRPRDPERVFRQMXQWRVGHFRQGLFLRQHV

i) S (x0 ) = S (x4 ) = 0

y

ii) S (x0 ) = S (x4 ) = 0.

Figura 11.5
y
1
y = S(x)

‚´∫2

‚´∫1

1

2

x

3RUFRQVLJXLHQWHS ‚àà C02 (‚àí‚àû, ‚àû), \HVWiGDGRHVSHFt√ÄFDPHQWHFRPR

‚éß
‚é™
0,
si x ‚â§ ‚àí2,
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™ 14 (2 + x)3 ,
si ‚àí 2 ‚â§ x ‚â§ ‚àí1,
‚é™
‚é™
‚é™
‚é™1
3
3
‚é®
(2 + x) ‚àí 4(1 + x) , si ‚àí 1 < x ‚â§ 0,
S(x) = 4
1
‚é™
(2 ‚àí x)3 ‚àí 4(1 ‚àí x)3 , si 0 < x ‚â§ 1,
‚é™
‚é™
4
‚é™
‚é™1
‚é™
‚é™
(2 ‚àí x)3 ,
si 1 < x ‚â§ 2,
‚é™
4
‚é™
‚é™
‚é™
‚é©0,
si 2 < x.



$KRUD XVDUHPRV HO %VSOLQH EiVLFR SDUD FRQVWUXLU ODV IXQFLRQHV EDVH œÜi en C02 [0, 1].
3ULPHURGLYLGLPRV>@DOVHOHFFLRQDUXQHQWHURSRVLWLYRn\GH√ÄQLU h = 1/(n + 1). (VWR

536

CAP√çTULO 11

Problemas de valor en la frontera para ecuaciones diferenciales ordinarias

SURGXFHQRGRVLJXDOPHQWHHVSDFLDGRVxi = i h, para cada i = 0, 1, . . . , n + 1. $KRUDGH√Än+1
QLPRVODVIXQFLRQHVEDVH{œÜi }i=0
como

‚éß
‚é™
‚é™
S
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™S
‚é™
‚é™
‚é™
‚é™
‚é™
‚é®
œÜi (x) = S
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™S
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é©S

x
h

x +h
h

‚àí 4S

x ‚àíh
h

‚àíS

x ‚àí ih
h

,

x ‚àí nh
h

‚àíS

,

x +h
h

si i = 0,
,

si i = 1,
si

x ‚àí (n + 1)h
h

x ‚àí (n + 2)h
h
‚àí 4S

,

x ‚àí (n + 2)h
h

2 ‚â§ i ‚â§ n ‚àí 1,

si i = n,
,

si i = n + 1.

n+1
1RHVGLItFLOPRVWUDUTXH{œÜi }i=0
HVXQFRQMXQWROLQHDOPHQWHLQGHSHQGLHQWHGHVSOLQHVF~ELFRVTXHVDWLVIDFHQœÜi (0) = œÜi (1) = 0, para cada i = 0, 1, . . . , n, n + 1  FRQVXOWHHOHMHUFLFLR /DVJUi√ÄFDVGHœÜi , para 2 ‚â§ i ‚â§ n ‚àí 1, VHPXHVWUDQHQOD√ÄJXUD\ODVJUi√ÄFDV
de œÜ0 , œÜ1 , œÜn , y œÜn+1HVWiQHQOD√ÄJXUD

Figura 11.6

y
y 5 fi (x) cuando i 5 2, ‚Ä¶ , n 2 1

1

x i22

x i21

xi

x i11

x i12

x

3XHVWRTXHœÜi (x) y œÜi (x)VRQGLIHUHQWHVGHFHURVylo para x ‚àà [xi‚àí2 , xi+2 ], ODPDWUL]HQ
ODDSUR[LPDFLyQ5D\OHLJK5LW]HVXQDPDWUL]GHEDQGDFRQDQFKRGHEDQGDPi[LPRGHVLHWH

‚é§
a00 a01 a02 a03
0 .. .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 0.
.
..
...
‚é•
‚é¢
...
..
‚é•
‚é¢ a10 a11 a12 a13 a14
.
.
.
‚é•
‚é¢
...
.
.
.
‚é•
‚é¢ a20 a21 a22 a23 a24
.
a25
..
...
‚é•
‚é¢
.
.
..
...
‚é•
‚é¢
...
a36. .
‚é•
‚é¢ a30. . a31. . a32. . . a33. . . a34. . . a35. . .
.
.
.
.
.
...
...
‚é•
‚é¢
.
.
.
.
.
.
...
...
... ..... ..... ..... .....
...
‚é•
.
0
0
A =‚é¢
.
.
...
...
...
...
...
.
‚é•
‚é¢ .. . . . . . . .
.
.
.
.
.
.
.
.
.
.
.
.
‚é•
‚é¢ ..
.
.
...
.
.
.
.
an‚àí2,n+1 ‚é•
‚é¢ .
... ..... ..... ...... ..... ..... .....
‚é•
‚é¢ ..
.
.
.
.
...
...
.
‚é¢ .
. . . . . . . . . . . . . . . . . an‚àí1,n+1 ‚é•
... ..... ......
‚é•
‚é¢ ..
...
.
.
...
.
.
... ..... ....
‚é•
‚é¢ .
... ..... .....
.
.
a
‚é£ ..
...
...
n,n+1 ‚é¶
...
...
.
..
..
.. ...
..
0 . . . . . . . . . . . . . . . . . . . . . . . . . 0 an+1,n‚àí2 an+1,n‚àí1 an+1,n an+1,n+1
‚é°

11.5 El m√©todo de Rayleigh-Ritz

537

Figura 11.7

y

y

1

1
y 5 f1(x)

y 5 f0(x)
x1

x2

x3

x1

x

y

y

1

1
y 5 fn(x)

xn22

x2

x3

x

1

x

y 5 fn11(x)
xn

xn21

1

x

xn21

xn

donde
1

ai j =
0

{ p(x)œÜi (x)œÜ j (x) + q(x)œÜi (x)œÜ j (x)} d x,

para cada i, j = 0, 1, . . . , n + 1. (OYHFWRUbWLHQHODVHQWUDGDV
1

bi =

f (x)œÜi (x)d x.

0

/DPDWUL]AHVGH√ÄQLGDSRVLWLYD FRQVXOWHHOHMHUFLFLR SRUORTXHHOVLVWHPDOLQHDOAc 5 b
VHSXHGHUHVROYHUPHGLDQWHHODOJRULWPRGH&KROHVN\RODHOLPLQDFLyQJDXVVLDQD(ODOJRULWPRGHVFULEHGHWDOODGDPHQWHODFRQVWUXFFLyQGHODDSUR[LPDFLyQGHOVSOLQHF~ELFR
œÜ (x PHGLDQWHHOPpWRGRGH5D\OHLJK5LW]SDUDORVSUREOHPDVGHYDORUHQODIURQWHUD  
\  HQXQFLDGRVDOSULQFLSLRGHHVWDVHFFLyQ

ALGORITMO

11.6

M√©todo Rayleigh-Ritz de spline c√∫bico
3DUDDSUR[LPDUODVROXFLyQGHOSUREOHPDGHYDORUHQODIURQWHUD

‚àí

d
dx

p(x)

dy
dx

+ q(x)y = f (x),

para 0 ‚â§ x ‚â§ 1, con y(0) = 0 y y(1) = 0

con la suma de splines c√∫bicos
n+1

ci œÜi (x) :

œÜ(x) =
i=0

ENTRADA

entero n ‚â• 1.

SALIDA coeficientes c0 , . . . , cn+1 .

538

CAP√çTULO 11

Problemas de valor en la frontera para ecuaciones diferenciales ordinarias

Paso 1 Determine h = 1/(n + 1).
Paso 2 Para i = 0, . . . , n + 1 establezca xi = i h.
Determine x‚àí2 = x‚àí1 = 0; xn+2 = xn+3 = 1.
Paso 3 Defina la‚éßfunci√≥n S mediante
‚é™
0,
‚é™
‚é™
‚é™
‚é™
1
‚é™
‚é™
(2 + x)3 ,
‚é™
4
‚é™
‚é™
‚é™
‚é® 1 (2 + x)3 ‚àí 4(1 + x)3 ,
S(x) = 4
1
‚é™
(2 ‚àí x)3 ‚àí 4(1 ‚àí x)3 ,
‚é™
‚é™
4
‚é™
‚é™
‚é™
1
‚é™
(2 ‚àí x)3 ,
‚é™
4
‚é™
‚é™
‚é™
‚é©0,

x ‚â§ ‚àí2,
‚àí2 < x ‚â§ ‚àí1,
‚àí1 < x ‚â§ 0,
0 < x ‚â§ 1,
1 < x ‚â§ 2,
2<x

n+1
mediante
Paso 4 Defina la base del spline c√∫bico {œÜi }i=0

œÜ0 (x) = S

x
h

œÜ1 (x) = S

x ‚àí x1
h

œÜi (x) = S

x ‚àí xi
, para i = 2, . . . , n ‚àí 1,
h

œÜn (x) = S

x ‚àí xn
h

œÜn+1 (x) = S

x +h
h

‚àí 4S

‚àíS

‚àíS

x ‚àí xn+1
h

,

x +h
h

,

x ‚àí (n + 2)h
h
‚àí 4S

,

x ‚àí (n + 2)h
h

.

Paso 5 Para i = 0, . . . , n + 1 haga los pasos 6‚Äì9.
(Nota: Las integrales en los pasos 6 y 9 se pueden evaluar usando un
procedimiento de integraci√≥n num√©rica.)
Paso 6 Para j = i, i + 1, . . . , m√≠n{i + 3, n + 1}
determine L = m√°x{x j‚àí2 , 0};
U = m√≠n{xi+2 , 1};
ai j =

U
L

p(x)œÜi (x)œÜ j (x) + q(x)œÜi (x)œÜ j (x) d x;

si i = j, entonces determine a ji = ai j .

(Puesto que A es sim√©trica.)

Paso 7 Si i ‚â• 4 entonces para j = 0, . . . , i ‚àí 4 determine ai j = 0.
Paso 8 Si i ‚â§ n ‚àí 3 entonces para j = i + 4, . . . , n + 1 determine ai j = 0.
Paso 9 Determine L = m√°x{xi‚àí2 , 0};
U = m√≠n{xi+2 , 1};
bi =

U
L

f (x)œÜi (x) d x.

Paso 10 Resolver el sistema lineal Ac = b, donde A = (ai j ), b = (b0 , . . . , bn+1 )t y
c = (c0 , . . . , cn+1 )t .
Paso 11 Para i = 0, . . . , n + 1
SALIDA (ci ).
Paso 12 PARE. (El procedimiento est√° completo.)

11.5 El m√©todo de Rayleigh-Ritz

Ilustraci√≥n

539

&RQVLGHUHHOSUREOHPDGHYDORUHQODIURQWHUD

‚àíy + œÄ 2 y = 2œÄ 2 sen(œÄ x),

para 0 ‚â§ x ‚â§ 1, con y(0) = y(1) = 0.

(QODLOXVWUDFLyQGHVSXpVGHODOJRULWPRKDFHPRVh 5\JHQHUDPRVDSUR[LPDFLRQHV
XVDQGRIXQFLRQHVEDVHOLQHDOHVSRUWUDPRV/DWDEODPXHVWUDORVUHVXOWDGRVREWHQLGRVDO
DSOLFDUORV%VSOLQHVFRPRVHGHVFULEHHQHODOJRULWPRFRQODPLVPDVHOHFFLyQGHQRGRV

Tabla 11.8

i

ci
‚àí5

0.50964361 √ó 10
0.20942608
0.39835678
0.54828946
0.64455358
0.67772340
0.64455370
0.54828951
0.39835730
0.20942593
0.74931285 √ó 10‚àí5

0
1
2
3
4
5
6
7
8
9
10

xi

œÜ(xi )

y(xi )

|y(xi ) ‚àí œÜ(xi )|

0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0

0.00000000
0.30901644
0.58778549
0.80901687
0.95105667
1.00000002
0.95105713
0.80901773
0.58778690
0.30901810
0.00000000

0.00000000
0.30901699
0.58778525
0.80901699
0.95105652
1.00000000
0.95105652
0.80901699
0.58778525
0.30901699
0.00000000

0.00000000
0.00000055
0.00000024
0.00000012
0.00000015
0.00000020
0.00000061
0.00000074
0.00000165
0.00000111
0.00000000

5HFRPHQGDPRVTXHODVLQWHJUDFLRQHVHQORVSDVRV\VHUHDOLFHQHQGRVSDUWHV3ULPHURFRQVWUXLPRVSROLQRPLRVLQWHUSRODQWHVGHVSOLQHF~ELFRSDUDp, q \ fXVDQGRORVPpWRGRV
SUHVHQWDGRVHQODVHFFLyQ$FRQWLQXDFLyQDSUR[LPDPRVORVLQWHJUDQGRVSRUSURGXFWRV
GHVSOLQHVF~ELFRVRGHULYDGDVGHVSOLQHVF~ELFRV$KRUDORVLQWHJUDQGRVVRQSROLQRPLRVSRU
WUDPRV\VHSXHGHQLQWHJUDUH[DFWDPHQWHHQFDGDVXELQWHUYDOR\GHVSXpVVXPDUVH(VWROOHYD
DDSUR[LPDFLRQHVH[DFWDVGHODVLQWHJUDOHV
/DVKLSyWHVLVDVXPLGDVDOSULQFLSLRGHHVWDVHFFLyQVRQVX√ÄFLHQWHVSDUDJDUDQWL]DUTXH
1

1/2

|y(x) ‚àí œÜ(x)|2 d x

= O(h 4 ),

si

0 ‚â§ x ‚â§ 1.

0

%RULV*ULJRULHYLFK*DOHUNLQ
¬≤ UHDOL]yWUDEDMRV
IXQGDPHQWDOHVDODSOLFDUODV
WpFQLFDVGHDSUR[LPDFLyQSDUD
UHVROYHUSUREOHPDVGHYDORU
HQODIURQWHUDUHODFLRQDGRVFRQ
SUREOHPDVGHLQJHQLHUtDFLYLO6X
DUWtFXORLQLFLDOVREUHDQiOLVLVGH
HOHPHQWRV√ÄQLWRVIXHSXEOLFDGR
HQ\VXPDQXVFULWR
IXQGDPHQWDOVREUHSODFDV
HOiVWLFDVGHOJDGDVHQ

3DUDXQDFRPSUREDFLyQGHHVWHUHVXOWDGRFRQVXOWH>6FKXOO@S
/RV%VSOLQHVWDPELpQVHSXHGHQGH√ÄQLUSDUDQRGRVQRHVSDFLDGRVHTXLWDWLYDPHQWHSHUR
ORV GHWDOOHV VRQ PiV FRPSOLFDGRV 8QD SUHVHQWDFLyQ GH OD WpFQLFD VH SXHGH HQFRQWUDU HQ
>6FKXOO@S2WUDEDVHTXHVHXVDGHPDQHUDFRP~QHVHOSROLQRPLRGH+HUPLWHF~ELFRSRU
WUDPRV3DUDXQDH[FHOHQWHSUHVHQWDFLyQGHHVWHPpWRGRFRQVXOWHGHQXHYR>6FKXOO@SII
2WURV PpWRGRV TXH UHFLEHQ FRQVLGHUDEOH DWHQFLyQ VRQ *DOHUNLQ R PpWRGRV GH ¬¥IRUPD
GpELO¬µ3DUDHOSUREOHPDGHYDORUHQODIURQWHUDTXHKHPRVFRQVLGHUDGR

‚àí

d
dx

p(x)

dy
dx

+ q(x)y = f (x),

para 0 ‚â§ x ‚â§ 1, con y(0) = 0 y y(1) = 0,

EDMR ODV VXSRVLFLRQHV OLV PHQFLRQDGDV DO SULQFLSLR GH HVWD VHFFLyQ ORV PpWRGRV GH
*DOHUNLQ\5D\OHLJK5LW]VHGHWHUPLQDQFRQODHFXDFLyQ  6LQHPEDUJRpVWHQRHVHO
FDVRGHXQSUREOHPDGHYDORUHQODIURQWHUDDUELWUDULR8QWUDWDPLHQWRVREUHODVVLPLOLWXGHV
\ ODV GLIHUHQFLDV HQ ORV GRV PpWRGRV \ XQ DQiOLVLV GH OD DPSOLD DSOLFDFLyQ GHO PpWRGR GH
*DOHUNLQVHSXHGHHQFRQWUDUHQ>6FKXOO@\HQ>6)@

540

CAP√çTULO 11

Problemas de valor en la frontera para ecuaciones diferenciales ordinarias

/DUDt]GHODSDODEUD
¬¥FRORFDFLyQ¬µSURYLHQHGHO
/DWtQ¬¥FR¬¥\¬¥ORFXV¬µORFXDO
LQGLFD¬¥MXQWRFRQ¬µ\¬¥OXJDU¬µ(V
HTXLYDOHQWHDORTXHOODPDPRV
LQWHUSRODFLyQ

2WUDWpFQLFDSRSXODUSDUDUHVROYHUSUREOHPDVGHYDORUHQODIURQWHUDHVHOm√©todo de
colocaci√≥n
(VWHSURFHGLPLHQWRFRPLHQ]DVHOHFFLRQDQGRXQFRQMXQWRIXQFLRQHVEDVH{œÜ1 , . . . , œÜ N },
XQFRQMXQWRGHQ~PHURV{xi , . . . , xn }HQ>@\UHTXLHUHQTXHXQDDSUR[LPDFLyQ
N

ci œÜi (x)
i=1

VDWLVIDJDODHFXDFLyQGLIHUHQFLDOGHFDGDQ~PHUR x j , para 1 ‚â§ j ‚â§ n. 6LDGHPiVVHUHTXLHUH TXH œÜi (0) = œÜi (1) = 0, para 1‚â§ i ‚â§ N ,  HQWRQFHV ODV FRQGLFLRQHV GH IURQWHUD VH
VDWLVIDFHQDXWRPiWLFDPHQWH6HKDSUHVWDGRPXFKDDWHQFLyQHQODOLWHUDWXUDDODVHOHFFLyQ
GHQ~PHURV {x j }\ODVIXQFLRQHVEDVH {œÜi }. 8QDVHOHFFLyQFRP~QHVSHUPLWLUTXHœÜ iVHDOD
IXQFLyQEDVHSDUDODVIXQFLRQHVVSOLQHUHODWLYDVDXQDSDUWLFLyQ>@\TXHORVQRGRV {x j }
VHDQSXQWRVJDXVVLDQRVRUDtFHVGHFLHUWRVSROLQRPLRVRUWRJRQDOHVWUDQVIRUPDGRVHQHOVXELQWHUYDORDGHFXDGR
8QDFRPSDUDFLyQGHGLIHUHQWHVPpWRGRVGHFRORFDFLyQ\PpWRGRVGHGLIHUHQFLD√ÄQLWD
VHHQFXHQWUDHQ>5X@/DFRQFOXVLyQHVTXHORVPpWRGRVGHFRORFDFLyQTXHXWLOL]DQVSOLQHV
GHJUDGRVXSHULRUFRPSLWHQFRQODVWpFQLFDVGHGLIHUHQFLD√ÄQLWDTXHXWLOL]DQH[WUDSRODFLyQ
2WUDVUHIHUHQFLDVSDUDPpWRGRVGHFRORFDFLyQVRQ>'HE6@\>/5@
La secci√≥n Conjunto de ejercicios 11.5 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

11.6 Software num√©rico
/DELEOLRWHFD,06/WLHQHPXFKDVVXEUXWLQDVSDUDSUREOHPDVGHYDORUHQODIURQWHUD([LVWHQ
WDQWRPpWRGRVGHGLVSDURFRPRGHGLIHUHQFLD√ÄQLWD/RVPpWRGRVGHGLVSDURXVDQODWpFQLFD
5XQJH.XWWD9HUQHUSDUDUHVROYHUORVSUREOHPDVGHYDORULQLFLDOUHODFLRQDGRV
/DELEOLRWHFD1$*WDPELpQWLHQHXQDPXOWLWXGGHVXEUXWLQDVSDUDUHVROYHUSUREOHPDVGH
YDORUHQODIURQWHUD$OJXQRVGHHVWRVVRQXQPpWRGRGHGLVSDURTXHXVDHOPpWRGRGHYDORU
LQLFLDO5XQJH.XWWD0HUVRQMXQWRFRQHOPpWRGRGH1HZWRQXQPpWRGRGHGLIHUHQFLD√ÄQLWD
FRQHOGH1HZWRQSDUDUHVROYHUXQVLVWHPDOLQHDO\XQRGHGLIHUHQFLD√ÄQLWDOLQHDOFRQEDVH
HQODFRORFDFLyQ
([LVWHQVXEUXWLQDVHQHOSDTXHWH2'(FRQWHQLGRHQODELEOLRWHFD1HWOLESDUDUHVROYHU
SUREOHPDVGHYDORUHQODIURQWHUDGHGRVSXQWRVQROLQHDOHV\OLQHDOHVUHVSHFWLYDPHQWH(VWDV
UXWLQDVHVWiQEDVDGDVHQPpWRGRVGHP~OWLSOHVGLVSDURV
Las secciones Preguntas de an√°lisis, Conceptos clave y Revisi√≥n del cap√≠tulo est√°n disponibles en l√≠nea. Encuentre la ruta de acceso en las p√°ginas preliminares.

CAP√çTULO

12

Soluciones num√©ricas para ecuaciones
diferenciales parciales
Introducci√≥n
Un cuerpo es isotr√≥pico si la conductividad t√©rmica en cada uno de sus puntos es indeSHQGLHQWHGHODGLUHFFLyQGHO√ÅXMRGHFDORUDWUDYpVGHOSXQWR6XSRQJDTXHk, c y œÅ son
funciones de (x, y, z) y representan, respectivamente, la conductividad t√©rmica, el calor esSHFt√ÄFR\ODGHQVLGDGGHXQFXHUSRLVRWUySLFRHQHOSXQWR x, y, z (QWRQFHVODWHPSHUDWXUD
u ‚â° u(x, y, z, t), en el cuerpo se puede encontrar al resolver la ecuaci√≥n diferencial parcial

‚àÇ
‚àÇx

k

‚àÇu
‚àÇx

+

‚àÇ
‚àÇy

k

‚àÇu
‚àÇy

+

‚àÇ
‚àÇz

k

‚àÇu
‚àÇz

= cœÅ

‚àÇu
.
‚àÇt

Cuando k, c y œÅ son constantes, a esta ecuaci√≥n se le conoce como ecuaci√≥n de calor tridimensional y se expresa como

‚àÇ 2u
‚àÇ 2u
‚àÇ 2u
cœÅ ‚àÇu
.
+
+
=
2
2
2
‚àÇx
‚àÇy
‚àÇz
k ‚àÇt
6LODIURQWHUDGHOFXHUSRHVUHODWLYDPHQWHVLPSOHODVROXFLyQGHHVWDHFXDFLyQVHSXHGHHQFRQWUDUSRUPHGLRGHVHULHVGH)RXULHU
(QPXFKDVVLWXDFLRQHVFXDQGRk, c y œÅ no son constantes o cuando la frontera es irreJXODUODVROXFLyQGHODHFXDFLyQGLIHUHQFLDOSDUFLDOVHGHEHREWHQHUSRUPHGLRGHWpFQLFDVGH
DSUR[LPDFLyQ(QHVWHFDStWXORVHSUHVHQWDXQDLQWURGXFFLyQDHVWDVWpFQLFDV

Ecuaciones el√≠pticas
*HQHUDOPHQWH ODV HFXDFLRQHV GLIHUHQFLDOHV SDUFLDOHV VH FODVL√ÄFDQ GH PDQHUD VLPLODU D ODV
VHFFLRQHV FyQLFDV /D HFXDFLyQ GLIHUHQFLDOSDUFLDO TXHFRQVLGHUDUHPRV HQ ODVHFFLyQ
involucra u x x (x, y)+ u yy (x, y) y es una ecuaci√≥n el√≠ptica$ODHFXDFLyQHOtSWLFDSDUWLFXODU
TXHFRQVLGHUDUHPRVVHOHFRQRFHFRPRecuaci√≥n de Poisson:

‚àÇ 2u
‚àÇ 2u
(x,
y)
+
(x, y) = f (x, y).
‚àÇx2
‚àÇ y2
(QHVWDHFXDFLyQVXSRQHPRVTXHfGHVFULEHODHQWUDGDSDUDHOSUREOHPDHQXQDUHJLyQSODQDR
con frontera S/DVHFXDFLRQHVGHHVWHWLSRVXUJHQHQHOHVWXGLRGHGLIHUHQWHVSUREOHPDVItVLFRV
LQGHSHQGLHQWHVGHOWLHPSRFRPRODGLVWULEXFLyQGHHVWDGRHVWDEOHGHOFDORUHQXQDUHJLyQ
SODQD\SUREOHPDVGHHVWDGRHVWDEOHELGLPHQVLRQDOHVTXHLPSOLFDQ√ÅXLGRVLQFRPSUHVLEOHV

541

542

CAP√çTULO 12

6LPpRQ'HQLV3RLVVRQ
¬≤ HUDXQHVWXGLDQWHGH
/DSODFH\/HJHQGUHGXUDQWHORV
DxRVQDSROHyQLFRVHQ)UDQFLD
M√°s adelante, asumi√≥ la c√°tedra
en la eFROH3RO\WHFKQLTXH
GRQGHWUDEDMycon ecuaciones
diferenciales parciales y
ordinarias y, despu√©s, en la teor√≠a
GHODSUREDELOLGDGGHODYLGD

Soluciones num√©ricas para ecuaciones diferenciales parciales

'HEHQLPSRQHUVHUHVWULFFLRQHVDGLFLRQDOHVSDUDREWHQHUXQDVROXFLyQ~QLFDSDUDODHFXDFLyQGH3RLVVRQ3RUHMHPSORHOHVWXGLRGHODGLVWULEXFLyQGHHVWDGRHVWDEOHGHFDORUHQXQD
UHJLyQSODQDUHTXLHUHTXH f (x, y) ‚â° 0, ORFXDOUHVXOWDHQXQDVLPSOL√ÄFDFLyQSDUDODecuaci√≥n
de Laplace

‚àÇ 2u
‚àÇ 2u
(x,
y)
+
(x, y) = 0.
‚àÇx2
‚àÇ y2
6LODWHPSHUDWXUDGHQWURGHODUHJLyQVHGHWHUPLQDPHGLDQWHODGLVWULEXFLyQGHWHPSHUDWXUD HQ OD IURQWHUD GH OD UHJLyQ ODV UHVWULFFLRQHV UHFLEHQ HO QRPEUH GH condiciones de
frontera de Dirichlet, dadas por

u(x, y) = g(x, y),
para todas las (x, y) en SODIURQWHUDGHODUHJLyQR. &RQVXOWHOD√ÄJXUD
Figura 12.1
3LHUUH¬≤6LPRQ/DSODFH
¬≤ WUDEDMyHQPXFKDV
iUHDVPDWHPiWLFDVSURGXMR
art√≠culos fundamentales
VREUHSUREDELOLGDG\ItVLFD
PDWHPiWLFD3XEOLFyVXWUDEDMR
PiVLPSRUWDQWHVREUHODWHRUtD
del calor durante el periodo de
¬≤

-RKDQQ3HWHU*XVWDY/HMHXQH
'LULFKOHW ¬≤ KL]R
LPSRUWDQWHVFRQWULEXFLRQHVDODV
√°reas de la teor√≠a num√©rica y la
FRQYHUJHQFLDGHODVVHULHV'H
KHFKRSRGUtDVHUFRQVLGHUDGR
como el fundador de las series
GH)RXULHU\DTXHGHDFXHUGR
con Riemann, fue el primero en
HVFULELUXQDUWtFXORSURIXQGR
VREUHHVWHWHPD

y

S
R

(x, y): La temperatura se
mantiene constante
en g(x, y) grados
x

Ecuaciones parab√≥licas
(QODVHFFLyQFRQVLGHUDPRVODVROXFLyQQXPpULFDGHOSUREOHPDTXHLQYROXFUDXQDHFXDci√≥n diferencial parcial parab√≥lica de la forma

‚àÇu
‚àÇ 2u
(x, t) ‚àí Œ± 2 2 (x, t) = 0.
‚àÇt
‚àÇx
(OSUREOHPDItVLFRFRQVLGHUDGRDTXtDERUGDHO√ÅXMRGHFDORUDORODUJRGHXQDYDULOOD REDUUD 
GHORQJLWXGl FRQVXOWHOD√ÄJXUD TXHWLHQHXQDWHPSHUDWXUDXQLIRUPHHQFDGDVHFFLyQ
WUDQVYHUVDO(VWRUHTXLHUHTXHODYDULOODHVWpSHUIHFWDPHQWHDLVODGDHQVXVXSHU√ÄFLHODWHUDO6H
VXSRQHTXHODFRQVWDQWHŒ±HVLQGHSHQGLHQWHGHODSRVLFLyQHQODYDULOOD6HGHWHUPLQDPHGLDQWHODVSURSLHGDGHVFRQGXFWRUDVGHFDORUGHOPDWHULDOGHOTXH√©sta se compone

Figura 12.2
0

l

x

8QRGHORVFRQMXQWRVFRPXQHVGHODVUHVWULFFLRQHVSDUDXQSUREOHPDGH√ÅXMRGHFDORUGH
HVWHWLSRHVHVSHFL√ÄFDUODGLVWULEXFLyQLQLFLDOGHFDORUHQODYDULOOD

u(x, 0) = f (x),

Introducci√≥n

543

\GHVFULELUODFRQGXFWDHQVXVH[WUHPRV3RUHMHPSORVLORVH[WUHPRVVHPDQWLHQHQDWHPSHraturas constantes U y U, las condiciones en la frontera tienen la forma

u(0, t) = U1

y

u(l, t) = U2 ,

\ODGLVWULEXFLyQGHFDORUVHDSUR[LPDDODGLVWULEXFLyQOtPLWHGHWHPSHUDWXUD

l√≠m u(x, t) = U1 +

t‚Üí‚àû

U2 ‚àí U1
x.
l

6LSRUHOFRQWUDULRODYDULOODVHDtVODGHWDOIRUPDTXHHOFDORUQR√ÅX\HDWUDYpVGHORVH[WUHmos, las condiciones en la frontera son

‚àÇu
(0, t) = 0
‚àÇx

y

‚àÇu
(l, t) = 0.
‚àÇx

(QWRQFHVHOFDORUQRHVFDSDGHODYDULOOD\HQHOFDVROtPLWHODWHPSHUDWXUDHQODYDULOODHV
FRQVWDQWH/DHFXDFLyQGLIHUHQFLDOSDUFLDOSDUDEyOLFDWDPELpQHVLPSRUWDQWHHQHOHVWXGLRGH
ODGLIXVLyQGHJDVGHKHFKRHVFRQRFLGDHQDOJXQRVFtUFXORVFRPRecuaci√≥n de difusi√≥n

Ecuaciones hiperb√≥licas
(OSUREOHPDHVWXGLDGRHQODVHFFLyQHVODecuaci√≥n de onda \HVXQHMHPSORGHXQD
ecuaci√≥n diferencial parcial hiperb√≥lica6XSRQJDXQDFXHUGDHOiVWLFDGHORQJLWXGl se estira
HQWUHGRVVRSRUWHVHQHOPLVPRQLYHOKRUL]RQWDO FRQVXOWHOD√ÄJXUD 
Figura 12.3
u(x, t)

l

x, tiempo fijo t

6LODFXHUGDVHFRQ√ÄJXUDSDUDYLEUDUHQXQSODQRYHUWLFDOHOGHVSOD]DPLHQWRYHUWLFDOu(x, t)
de un punto x en el tiempo t satisface la ecuaci√≥n diferencial parcial

Œ±2

‚àÇ 2u
‚àÇ 2u
(x, t) ‚àí 2 (x, t) = 0,
2
‚àÇx
‚àÇt

para 0 < x < l

y

0 < t,

VLHPSUH\FXDQGRVHLJQRUHQORVHIHFWRVGHDPRUWLJXDPLHQWR\ODDPSOLWXGQRVHDWDQJUDQGH3DUDLPSRQHUUHVWULFFLRQHVVREUHHVWHSUREOHPDVXSRQHPRVTXHODSRVLFLyQLQLFLDO\OD
velocidad de la cuerda est√°n dadas por

u(x, 0) = f (x) y

‚àÇu
(x, 0) = g(x),
‚àÇt

para 0 ‚â§ x ‚â§ l.

6LORVH[WUHPRVHVWiQ√ÄMRVWDPELpQWHQHPRVu t) 5\u(l, t) 5
2WURV SUREOHPDV ItVLFRV LPSOLFDQ OD SUHVHQFLD GH OD HFXDFLyQ GLIHUHQFLDO SDUFLDO
KLSHUEyOLFDHQHOHVWXGLRGHYLJDVTXHYLEUDQFRQXQRRDPERVH[WUHPRVVXMHWDGRV\HQOD
WUDQVPLVLyQGHHOHFWULFLGDGHQXQDOtQHDODUJDGRQGHH[LVWHDOJXQDIXJDGHFRUULHQWHKDFLD
HOSLVR

CAP√çTULO 12

Soluciones num√©ricas para ecuaciones diferenciales parciales

12.1 Ecuaciones diferenciales parciales el√≠pticas
/DHFXDFLyQGLIHUHQFLDOSDUFLDOel√≠pticaTXHFRQVLGHUDPRVHVODHFXDFLyQGH3RLVVRQ

‚àá 2 u(x, y) ‚â°

‚àÇ 2u
‚àÇ 2u
(x, y) + 2 (x, y) = f (x, y),
2
‚àÇx
‚àÇy



en R = { (x, y) | a < x < b, c < y < d }, con u(x, y) = g(x, y) para (x, y) ‚àà S, donde S
denota la frontera de R6Lf y g son continuas en sus dominios, entonces existe una ~QLFD
VROXFLyQSDUDHVWDHFXDFLyQ

Selecci√≥n de una cuadr√≠cula
(OPpWRGRXWLOL]DGRHVXQDDGDSWDFLyQELGLPHQVLRQDOGHOPpWRGRGHGLIHUHQFLDV√ÄQLWDVSDUD
SUREOHPDVOLQHDOHVGHYDORUHQODIURQWHUDDQDOL]DGRVHQODVHFFLyQ(OSULPHUSDVRHV
seleccionar enteros n y mSDUDGH√ÄQLUORVWDPDxRVGHSDVRh 5 (b 2 a) n y k 5 (d 2 c) m
/DGLYLVLyQGHOLQWHUYDOR>a, b] en nSDUWHVLJXDOHVGHDQFKRh\HOLQWHUYDOR>c, d] en m partes
LJXDOHVGHDQFKRk FRQVXOWHOD√ÄJXUD 
Figura 12.4

...
...
...

...

...

y2

...

ym  d

...

544

. . .
. . .

y1
y0  c
x 0  a x1

x2

x3

x4

b  xn

x

&RORTXHXQDFXDGUtFXODVREUHHOUHFWiQJXORRDOWUD]DUOtQHDVYHUWLFDOHV\KRUL]RQWDOHVD
trav√©s de los puntos con coordenadas (x i , y j ), donde

xi = a + i h, para cada i = 0, 1, . . . , n, y

y j = c + jk, para cada j = 0, 1, . . . , m.

/DVOtQHDVx 5 x i y y 5 y j son l√≠neas de cuadr√≠cula y sus intersecciones son los puntos de
mallaGHODFXDGUtFXOD3DUDFDGDSXQWRGHPDOODHQHOLQWHULRUGHODFXDGUtFXOD x i, y j ), para
i 5   , n 2\j 5   , m 2SRGHPRVXVDUODVHULHGH7D\ORUHQODYDULDEOHx
alrededor de x iSDUDJHQHUDUODIyUPXODGHGLIHUHQFLDVFHQWUDGDV

‚àÇ 2u
u(xi+1 , y j ) ‚àí 2u(xi , y j ) + u(xi‚àí1 , y j ) h 2 ‚àÇ 4 u
(x
,
y
)
=
‚àí
(Œæi , y j ),
i
j
‚àÇx2
h2
12 ‚àÇ x 4



donde Œæi ‚àà (xi‚àí1 , xi+1 ). 7DPELpQSRGHPRVXVDUODVHULHGH7D\ORUHQODYDULDEOHy alrededor
de y jSDUDJHQHUDUODIyUPXODGHGLIHUHQFLDVFHQWUDGDV

u(xi , y j+1 ) ‚àí 2u(xi , y j ) + u(xi , y j‚àí1 )
k2 ‚àÇ 4u
‚àÇ 2u
(xi , y j ) =
‚àí
(xi , Œ∑ j ),
2
2
‚àÇy
k
12 ‚àÇ y 4
donde Œ∑ j ‚àà (y j‚àí1 , y j+1 ).



12.1 Ecuaciones diferenciales parciales el√≠pticas

545

3RU PHGLR GH HVWDV IyUPXODV HQ OD HFXDFLyQ   SRGHPRV H[SUHVDU OD HFXDFLyQ GH
3RLVVRQHQORVSXQWRV x i, y j ) como

u(xi+1 , y j ) ‚àí 2u(xi , y j ) + u(xi‚àí1 , y j ) u(xi , y j+1 ) ‚àí 2u(xi , y j ) + u(xi , y j‚àí1 )
+
h2
k2
= f (xi , y j ) +

h2 ‚àÇ 4u
k2 ‚àÇ 4u
(Œæ
,
y
)
+
(xi , Œ∑ j ),
i
j
12 ‚àÇ x 4
12 ‚àÇ y 4

para cada i 5   , n 2\j 5   , m 2/DVFRQGLFLRQHVGHIURQWHUDVRQ

u(x0 , y j ) = g(x0 , y j )

y

u(xn , y j ) = g(xn , y j ),

para cada j = 0, 1, . . . , m;

u(xi , y0 ) = g(xi , y0 )

y

u(xi , ym ) = g(xi , ym ),

para cada i = 1, 2, . . . , n ‚àí 1.

M√©todo de diferencias Ô¨Ånitas
(QODIRUPDGHHFXDFLyQGHGLIHUHQFLDHVWRUHVXOWDHQHOm√©todo de diferenciaV√ÄQLWDV:

2

h
k

2

2

h
k

+ 1 w i j ‚àí (w i+1, j + w i‚àí1, j ) ‚àí

(w i, j+1 + w i, j‚àí1 ) = ‚àíh 2 f (xi , y j ),


para cada i 5   , n 2\j 5   , m 2\

w 0 j = g(x0 , y j )

y w n j = g(xn , y j ),

para cada j = 0, 1, . . . , m;

w i0 = g(xi , y0 )

y w im = g(xi , ym ),

para cada i = 1, 2, . . . , n ‚àí 1;



donde wi j aproxima u(x i, y j (VWHPpWRGRWLHQHHUURUGHWUXQFDPLHQWRORFDOGHRUGHQO(h
1 k 
/DHFXDFLyQFRP~QHQ  LPSOLFDDSUR[LPDFLRQHVSDUDu(x, y) en los puntos

(xi‚àí1 , y j ),

(xi , y j ),

(xi+1 , y j ),

(xi , y j‚àí1 ),

y

(xi , y j+1 ).

5HSURGXFLUODSDUWHGHODFXDGUtFXODHQODTXHVHORFDOL]DQHVWRVSXQWRV FRQVXOWHOD√ÄJXUD PXHVWUDTXHFDGDHFXDFLyQLPSOLFDDSUR[LPDFLRQHVHQODUHJLyQHQIRUPDGHHVWUHOOD
alrededor de la x en (x i, y j 

Figura 12.5
y
d
y j1
yj
y j1

x
x x x
x

c
a

x i1 x i x i1

b

x

8VDPRV OD LQIRUPDFLyQ D SDUWLU GH ODV FRQGLFLRQHV GH IURQWHUD   VLHPSUH TXH
VHD DSURSLDGR HQ HO VLVWHPD GDGR SRU OD HFXDFLyQ   HV GHFLU HQ WRGRV ORV SXQ-

546

CAP√çTULO 12

Soluciones num√©ricas para ecuaciones diferenciales parciales

tos (x i, y j DG\DFHQWHVDXQSXQWRGHPDOODHQODIURQWHUD(VWRSURGXFHXQVLVWHPDOLQHDO
(n ‚àí 1)(m ‚àí 1) √ó (n ‚àí 1)(m ‚àí 1)FRQODVLQFyJQLWDVFRPRDSUR[LPDFLRQHVwi j para u(x i, y j )
HQORVSXQWRVGHPDOODLQWHULRUHV
(OVLVWHPDOLQHDOTXHFRQWLHQHHVWDVLQFyJQLWDVVHH[SUHVDSDUDORVFiOFXORVGHPDWUL]
GHIRUPDPiVH√ÄFLHQWHVLVHLQWURGXFHHOUHHWLTXHWDGRGHORVSXQWRVGHPDOODLQWHULRUHV8Q
HWLTXHWDGRUHFRPHQGDGRGHHVWRVSXQWRV FRQVXOWH>9DU@S HVKDFHU

Pl = (xi , y j )

y w l = w i, j ,

donde l 5 i 1 (m 22 j)(n 2 SDUDFDGDi 5   , n 2 \j 5m 2 
(VWRHWLTXHWDORVSXQWRVGHPDOODGHIRUPDFRQVHFXWLYDGHVGHODL]TXLHUGDKDVWDODGHUHFKD
\GHVGHODSDUWHVXSHULRUKDVWDODSDUWHLQIHULRU(WLTXHWDUORVSXQWRVGHHVWDIRUPDJDUDQWL]D
TXHHOVLVWHPDQHFHVDULRSDUDGHWHUPLQDUwi, jHVXQDPDWUL]FRQXQDQFKRGHEDQGDFRPR
Pi[LPRn 2
3RUHMHPSORFRQn 5\m 5HOUHHWLTXHWDGRUHVXOWDHQXQDFXDGUtFXODFX\RVSXQWRV
VHPXHVWUDQHQOD√ÄJXUD
Figura 12.6
y
y5
y4

P1

P2

P3

y3

P4

P5

P6

y2

P7

P8

P9

y1

P10 P11 P12

y0
x0

Ejemplo 1

x1

x2

x3

x4

x

'HWHUPLQHODGLVWULEXFLyQGHFDORUHQHVWDGRHVWDEOHHQXQDSODFDGHPHWDOFXDGUDGD\GHOJDGDFRQGLPHQVLRQHVGHPSRUPXVDQGRn 5 m 5'RVIURQWHUDVDG\DFHQWHVVH
PDQWLHQHQD&\HOFDORUHQODVRWUDVIURQWHUDVVHLQFUHPHQWDGHPDQHUDOLQHDOGHVGH¬û&HQ
XQDHVTXLQDKDVWD&GRQGHVHXQHQORVODGRV
Soluci√≥n &RORTXHORVODGRVFRQODVFRQGLFLRQHVGHIURQWHUDFHURDORODUJRGHORVHMHVx y y
(QWRQFHVHOSUREOHPDVHH[SUHVDFRPR

‚àÇ 2u
‚àÇ 2u
(x, y) + 2 (x, y) = 0,
2
‚àÇx
‚àÇy
para (x, y HQHOFRQMXQWR R = { (x, y) | 0 < x < 0.5, 0 < y < 0.5 }. /DVFRQGLFLRQHVGH
frontera son

u(0, y) = 0, u(x, 0) = 0, u(x, 0.5) = 200x, y u(0.5, y) = 200y.
6Ln 5 m 5HOSUREOHPDWLHQHODFXDGUtFXODTXHVHPXHVWUDHQOD√ÄJXUD\ODHFXDFLyQ
GHGLIHUHQFLDV  HV

4w i, j ‚àí w i+1, j ‚àí w i‚àí1, j ‚àí w i, j‚àí1 ‚àí w i, j+1 = 0,
para cada i 5\j 5

12.1 Ecuaciones diferenciales parciales el√≠pticas

547

Figura 12.7
y
0.5

u(0, y)  0

u(x, 0.5)  200x
P1

P2

P3

P4

P5

P6

P7

P8

P9

u(x, 0)  0

u(0.5, y)  200y

0.5

x

([SUHVDU HVWR HQ WpUPLQRV GH SXQWRV GH FXDGUtFXOD LQWHULRU UHHWLTXHWDGRV w i = u(Pi )
LPSOLFDTXHODVHFXDFLRQHVHQORVSXQWRVP i son

P1 :
4w 1 ‚àí w 2 ‚àí w 4 = w 0,3 + w 1,4 ,
4w 2 ‚àí w 3 ‚àí w 1 ‚àí w 5 = w 2,4 ,
P2 :
4w 3 ‚àí w 2 ‚àí w 6 = w 4,3 + w 3,4 ,
P3 :
4w 4 ‚àí w 5 ‚àí w 1 ‚àí w 7 = w 0,2 ,
P4 :
P5 : 4w 5 ‚àí w 6 ‚àí w 4 ‚àí w 2 ‚àí w 8 = 0,
P6 :
4w 6 ‚àí w 5 ‚àí w 3 ‚àí w 9 = w 4,2 ,
P7 :
4w 7 ‚àí w 8 ‚àí w 4 = w 0,1 + w 1,0 ,
4w 8 ‚àí w 9 ‚àí w 7 ‚àí w 5 = w 2,0 ,
P8 :
4w 9 ‚àí w 8 ‚àí w 6 = w 3,0 + w 4,1 ,
P9 :
GRQGHORVODGRVGHUHFKRVGHODVHFXDFLRQHVVHREWLHQHQDSDUWLUGHODVFRQGLFLRQHVGHIURQWHUD
'HKHFKRODVFRQGLFLRQHVGHIURQWHUDLPSOLFDQTXH

w 1,0 = w 2,0 = w 3,0 = w 0,1 = w 0,2 = w 0,3 = 0,
w 1,4 = w 4,1 = 25,

w 2,4 = w 4,2 = 50,

y w 3,4 = w 4,3 = 75.

Tabla 12.1
i

wi

1
2
3
4
5
6
7
8

18.75
37.50
56.25
12.50
25.00
37.50
6.25
12.50

9

18.75

3RUORTXHHOVLVWHPDOLQHDODVRFLDGRFRQHVWHSUREOHPDWLHQHODIRUPD
‚é§ ‚é°
‚é§
‚é°
‚é§‚é°
25
w1
4 ‚àí1
0 ‚àí1
0
0
0
0
0
‚é• ‚é¢
‚é•
‚é¢
‚é¢ ‚àí1
4 ‚àí1
0 ‚àí1
0
0
0
0 ‚é•
‚é• ‚é¢ w 2 ‚é• ‚é¢ 50 ‚é•
‚é¢
‚é• ‚é¢ w 3 ‚é• ‚é¢ 150 ‚é•
‚é¢ 0 ‚àí1
4
0
0
‚àí1
0
0
0
‚é• ‚é¢
‚é•
‚é¢
‚é•‚é¢
‚é• ‚é¢
‚é•
‚é¢
‚é¢ ‚àí1
0
0
4 ‚àí1
0 ‚àí1
0
0 ‚é•
‚é• ‚é¢ w4 ‚é• ‚é¢ 0 ‚é•
‚é¢
‚é• ‚é¢ w5 ‚é• = ‚é¢ 0 ‚é• .
‚é¢ 0 ‚àí1
0
‚àí1
4
‚àí1
0
‚àí1
0
‚é• ‚é¢
‚é•
‚é•‚é¢
‚é¢
‚é• ‚é¢
‚é•
‚é¢ 0
‚é¢
0 ‚àí1
0 ‚àí1
4
0
0 ‚àí1 ‚é•
‚é¢
‚é• ‚é¢ w 6 ‚é• ‚é¢ 50 ‚é•
‚é•
‚é¢
‚é•
‚é¢
‚é¢ 0
0
0 ‚àí1
0
0
4 ‚àí1
0 ‚é• ‚é¢ w7 ‚é• ‚é¢ 0 ‚é•
‚é•
‚é¢
‚é£ 0
0
0
0 ‚àí1
0 ‚àí1
4 ‚àí1 ‚é¶ ‚é£ w 8 ‚é¶ ‚é£ 0 ‚é¶
25
w9
0
0
0
0
0 ‚àí1
0 ‚àí1
4
/RVYDORUHVGHw, w,   , wHQFRQWUDGRVDODSOLFDUHOPpWRGR*DXVV6HLGHODHVWDPDWUL]
VHHVWDEOHFHQHQODWDEOD

548

CAP√çTULO 12

Soluciones num√©ricas para ecuaciones diferenciales parciales

(VWDVUHVSXHVWDVVRQH[DFWDVSRUTXHODYHUGDGHUDVROXFLyQu(x, y) 5xy, tiene

‚àÇ 4u
‚àÇ 4u
=
‚â° 0,
‚àÇx4
‚àÇ y4
\HOHUURUGHWUXQFDPLHQWRHVFHURHQFDGDSDVR
(OSUREOHPDFRQVLGHUDGRHQHOHMHPSORWLHQHHOPLVPRWDPDxRGHPDOODHQ
FDGDHMH\UHTXLHUHUHVROYHUVRODPHQWHXQVLVWHPDOLQHDO3(VWRVLPSOL√ÄFDODVLWXDFLyQ
\QRLQWURGXFHORVSUREOHPDVFRPSXWDFLRQDOHVSUHVHQWHVFXDQGRHOVLVWHPDHVPiVJUDQGH
(ODOJRULWPRXVDHOPpWRGRLWHUDWLYRGH*DXVV6HLGHOSDUDUHVROYHUHOVLVWHPDOLQHDOTXH
UHVXOWD\SHUPLWHWDPDxRVGHPDOODGHVLJXDOHVHQORVHMHV

ALGORITMO

12.1

Diferencia Ô¨Ånita de la ecuaci√≥n de Poisson
3DUDDSUR[LPDUODVROXFLyQGHODHFXDFLyQGH3RLVVRQ

‚àÇ 2u
‚àÇ 2u
(x,
y)
+
(x, y) = f (x, y),
‚àÇx2
‚àÇ y2

a ‚â§ x ‚â§ b,

c ‚â§ y ‚â§ d,

sujeta a las condiciones de frontera
u(x, y) = g(x, y)

si x = a o x = b

y c‚â§y‚â§d

u(x, y) = g(x, y)

si y = c o y = d

y a‚â§x ‚â§b:

y

ENTRADA extremos a, b, c, d; enteros m ‚â• 3, n ‚â• 3; tolerancia TOL; n√∫mero m√°ximo
de iteraciones N .
SALIDA aproximaciones w i, j para u(xi , y j ) para cada i = 1, . . . , n ‚àí 1 y para cada
j = 1, . . . , m ‚àí 1 o un mensaje que indica que se excedi√≥ el n√∫mero m√°ximo de iteraciones.
Paso 1 Determine h = (b ‚àí a)/n;
k = (d ‚àí c)/m.
Paso 2 Para i = 1, . . . , n ‚àí 1 determine xi = a + i h.

(Los pasos 2 y 3 construyen
puntos de malla.)

Paso 3 Para j = 1, . . . , m ‚àí 1 determine y j = c + jk.
Paso 4 Para i = 1, . . . , n ‚àí 1
para j = 1, . . . , m ‚àí 1 determine w i, j = 0.
Paso 5 Determine Œª = h 2 /k 2 ;
Œº = 2(1 + Œª);
l = 1.
Paso 6 Mientras l ‚â§ N haga los pasos 7‚Äì20.

(Los pasos 7‚Äì20 realizan iteraciones de
Gauss-Seidel.)

Paso 7 Determine z = (‚àíh 2 f (x1 , ym‚àí1 ) + g(a, ym‚àí1 ) + Œªg(x1 , d)
+ Œªw 1,m‚àí2 + w 2,m‚àí1 )/Œº;
NORM = |z ‚àí w 1,m‚àí1 |;
w 1,m‚àí1 = z.
Paso 8 Para i = 2, . . . , n ‚àí 2

12.1 Ecuaciones diferenciales parciales el√≠pticas

549

determine z = (‚àíh 2 f (xi , ym‚àí1 ) + Œªg(xi , d) + w i‚àí1,m‚àí1
+w i+1,m‚àí1 + Œªw i,m‚àí2 /Œº;
si |w i,m‚àí1 ‚àí z| > NORM entonces establezca NORM = |w i,m‚àí1 ‚àí z|;
determine w i,m‚àí1 = z.
Paso 9 Determine z = (‚àíh 2 f (xn‚àí1 , ym‚àí1 ) + g(b, ym‚àí1 ) + Œªg(xn‚àí1 , d)
+w n‚àí2,m‚àí1 + Œªw n‚àí1,m‚àí2 /Œº;
si |w n‚àí1,m‚àí1 ‚àí z| > NORM entonces determine NORM = |w n‚àí1,m‚àí1 ‚àí z|;
determine w n‚àí1,m‚àí1 = z.
Paso 10 Para j = m ‚àí 2, . . . , 2 haga los pasos 11, 12, y 13.
Paso 11 Determine z = (‚àíh 2 f (x1 , y j ) + g(a, y j ) + Œªw 1, j+1
+Œªw 1, j‚àí1 + w 2, j )/Œº;
si |w 1, j ‚àí z| > NORM entonces determine NORM = |w 1, j ‚àí z|;
determine w 1, j = z.
Paso 12 Para i = 2, . . . , n ‚àí 2
determine z = (‚àíh 2 f (xi , y j ) + w i‚àí1, j + Œªw i, j+1
+w i+1, j + Œªw i, j‚àí1 )/Œº;
si |w i, j ‚àí z| > NORM entonces determine NORM = |w i, j ‚àí z|;
determine w i, j = z.
Paso 13 Determine z = (‚àíh 2 f (xn‚àí1 , y j ) + g(b, y j ) + w n‚àí2, j
+Œªw n‚àí1, j+1 + Œªw n‚àí1, j‚àí1 /Œº;
si |w n‚àí1, j ‚àí z| > NORM entonces determine NORM = |w n‚àí1, j ‚àí z|;
determine w n‚àí1, j = z.
Paso 14 Determine z = (‚àíh 2 f (x1 , y1 ) + g(a, y1 ) + Œªg(x1 , c) + Œªw 1,2 + w 2,1 )/Œº;
si |w 1,1 ‚àí z| > NORM entonces determine NORM = |w 1,1 ‚àí z|;
determine w 1,1 = z.
Paso 15 Para i = 2, . . . , n ‚àí 2
determine z = (‚àíh 2 f (xi , y1 ) + Œªg(xi , c) + w i‚àí1,1 + Œªw i,2 + w i+1,1 )/Œº;
si |w i,1 ‚àí z| > NORM entonces determine NORM = |w i,1 ‚àí z|;
determine w i,1 = z.
Paso 16 Determine z = (‚àíh 2 f (xn‚àí1 , y1 ) + g(b, y1 ) + Œªg(xn‚àí1 , c)
+w n‚àí2,1 + Œªw n‚àí1,2 )/Œº;
si |w n‚àí1,1 ‚àí z| > NORM entonces determine NORM = |w n‚àí1,1 ‚àí z|;
determine w n‚àí1,1 = z.
Paso 17 Si NORM ‚â§ TOL entonces hacer los pasos 18 y 19.
Paso 18 Para i = 1, . . . , n ‚àí 1
para j = 1, . . . , m ‚àí 1 SALIDA (xi , y j , w i, j ).
Paso 19 PARE.

(El procedimiento fue exitoso.)

Paso 20 Determine l = l + 1.
Paso 21 SALIDA (‚ÄòN√∫mero m√°ximo de iteracciones excedido‚Äô);
(El procedimiento no fue exitoso.)
PARE.
$SHVDUGHTXHHOSURFHGLPLHQWRLWHUDWLYRGH*DXVV6HLGHOVHLQFRUSRUDHQHODOJRULWPR
SDUDVLPSOLFLGDGHVDFRQVHMDEOHXVDUXQDWpFQLFDGLUHFWDFRPRHOLPLQDFLyQJDXVVLDQD
FXDQGRHOVLVWHPDHVSHTXHxRGHORUGHQGHRPHQRVSRUTXHHOFDUiFWHUGH√ÄQLGDSRVLWLYD

550

CAP√çTULO 12

Soluciones num√©ricas para ecuaciones diferenciales parciales

JDUDQWL]DHVWDELOLGDGUHVSHFWRDHUURUHVGHUHGRQGHR(VSHFLDOPHQWHXQDJHQHUDOL]DFLyQGHO
DOJRULWPRGHIDFWRUL]DFLyQGH&URXW FRQVXOWH>9DU@S HVH√ÄFLHQWHSDUDUHVROYHU
HVWHVLVWHPDSRUTXHODPDWUL]HVGHODIRUPDWULGLDJRQDOVLPpWULFDSRUEORTXHV

‚é°

‚é§
0 .. .. . . . . . . . . . . . . . 0.
...
.
.. ‚é•
‚é¢C
.
‚é¢ 1 A2 . . . C 2 . . . . . . .
.. ‚é•
‚é¢
‚é•
...
... ..
‚é¢ 0. . . C2. . . . . . .
. . . ... ‚é•
‚é¢ . ..
‚é•,
.
.
.
.
.
‚é¢ .. . . . . . . . . . . . . .
‚é•
‚é¢ .
... ... 0 ‚é•
... ...
‚é¢ ..
‚é•
.
... ...
.
‚é£ .
. . . . . . . . . . Cm‚àí1 ‚é¶
.
..
.
0 . . . . . . . . . . . . . . 0 Cm‚àí1 Am‚àí1
A1

C1

FRQEORTXHVFXDGUDGRVGHWDPDxR n 2 3 (n 2 

Selecci√≥n del m√©todo iterativo
3DUDVLVWHPDVJUDQGHVVHGHEHUtDXVDUXQPpWRGRLWHUDWLYRHVSHFt√ÄFDPHQWHHOPpWRGR625
DQDOL]DGRHQHODOJRULWPR/DVHOHFFLyQGHvTXHHVySWLPDHQHVWDVLWXDFLyQSURYLHQH
GHOKHFKRGHTXHFXDQGRAVHGHVFRPSRQHHQVXGLDJRQDOD\SDUWHVWULDQJXODUVXSHULRU\
WULDQJXODULQIHULRUU y L,

A = D ‚àí L ‚àí U,
y BHVODPDWUL]SDUDHOPpWRGRGH-DFREL

B = D ‚àí1 (L + U ),
entonces, el radio espectral de BHV FRQVXOWH>9DU@

œÅ(B) =

1
cos
2

œÄ
m

œÄ
n

+ cos

.

(OYDORUGHvTXHVHYDDXVDUHVSRUFRQVLJXLHQWH

œâ=

2
1+

1 ‚àí [œÅ(B)]2

4

=
2+

4 ‚àí cos

œÄ
m

+ cos

œÄ
n

2

.

8QDWpFQLFDGHEORTXHVHSXHGHLQFRUSRUDUDODOJRULWPRSDUDFRQYHUJHQFLDPiVUiSLGDGHO
SURFHGLPLHQWR6253DUDXQDSUHVHQWDFLyQGHHVWDWpFQLFDFRQVXOWH>9DU@S¬≤
Ejemplo 2

8VHHOPpWRGRGHGLIHUHQFLDV√ÄQLWDVGH3RLVVRQFRQn 5m 5\XQDWROHUDQFLDGH2
para aproximar la soluci√≥n de

‚àÇ 2u
‚àÇ 2u
(x,
y)
+
(x, y) = xe y ,
‚àÇx2
‚àÇ y2

0 < x < 2,

0 < y < 1,

con las condiciones de frontera

u(0, y) = 0,

u(2, y) = 2e y ,

0 ‚â§ y ‚â§ 1,

u(x, 0) = x,

u(x, 1) = ex,

0 ‚â§ x ‚â§ 2,

y compare los resultados con la soluci√≥n exacta u(x, y) = xe y .

12.2 Ecuaciones diferenciales parciales parab√≥licas

551

Soluci√≥n 8VDQGRHODOJRULWPRFRQXQQ~PHURPi[LPRGHLWHUDFLRQHVHVWDEOHFLGDVHQ

N 5SURSRUFLRQDORVUHVXOWDGRVHQODWDEOD(OFULWHULRGHGHWHQHUHOPpWRGR*DXVV¬≤
6HLGHOHQHOSDVRUHTXLHUHTXH

w i(l)j ‚àí w i(l‚àí1)
‚â§ 10‚àí10 ,
j
para cada i 5   \j 5   /DVROXFLyQGHODHFXDFLyQGHGLIHUHQFLDVVHREWXYR
con exactitud y el procedimiento se detuvo en l 5/RVUHVXOWDGRVMXQWRFRQORVYDORUHV
FRUUHFWRVVHSUHVHQWDQHQODWDEOD

Tabla 12.2

i

j

xi

yj

w i,(61)
j

u(xi , y j )

u(xi , y j ) ‚àí w i,(61)
j

1
1
1
1
2
2
2
2
3
3
3
3
4
4
4
4
5
5
5
5

1
2
3
4
1
2
3
4
1
2
3
4
1
2
3
4
1
2
3
4

0.3333
0.3333
0.3333
0.3333
0.6667
0.6667
0.6667
0.6667
1.0000
1.0000
1.0000
1.0000
1.3333
1.3333
1.3333
1.3333
1.6667
1.6667
1.6667
1.6667

0.2000
0.4000
0.6000
0.8000
0.2000
0.4000
0.6000
0.8000
0.2000
0.4000
0.6000
0.8000
0.2000
0.4000
0.6000
0.8000
0.2000
0.4000
0.6000
0.8000

0.40726
0.49748
0.60760
0.74201
0.81452
0.99496
1.2152
1.4840
1.2218
1.4924
1.8227
2.2260
1.6290
1.9898
2.4302
2.9679
2.0360
2.4870
3.0375
3.7097

0.40713
0.49727
0.60737
0.74185
0.81427
0.99455
1.2147
1.4837
1.2214
1.4918
1.8221
2.2255
1.6285
1.9891
2.4295
2.9674
2.0357
2.4864
3.0369
3.7092

1.30 √ó 10‚àí4
2.08 √ó 10‚àí4
2.23 √ó 10‚àí4
1.60 √ó 10‚àí4
2.55 √ó 10‚àí4
4.08 √ó 10‚àí4
4.37 √ó 10‚àí4
3.15 √ó 10‚àí4
3.64 √ó 10‚àí4
5.80 √ó 10‚àí4
6.24 √ó 10‚àí4
4.51 √ó 10‚àí4
4.27 √ó 10‚àí4
6.79 √ó 10‚àí4
7.35 √ó 10‚àí4
5.40 √ó 10‚àí4
3.71 √ó 10‚àí4
5.84 √ó 10‚àí4
6.41 √ó 10‚àí4
4.89 √ó 10‚àí4

La secci√≥n Conjunto de ejercicios 12.1 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

12.2 Ecuaciones diferenciales parciales parab√≥licas
/DHFXDFLyQGLIHUHQFLDOSDUFLDOparab√≥licaTXHFRQVLGHUDPRVHVODHFXDFLyQGHFDORURGLfusi√≥n

‚àÇu
‚àÇ 2u
(x, t) = Œ± 2 2 (x, t),
‚àÇt
‚àÇx

0 < x < l,

t > 0,



VXMHWDDODVFRQGLFLRQHV

u(0, t) = u(l, t) = 0,

t > 0,

y

u(x, 0) = f (x),

0 ‚â§ x ‚â§ l.

(O HQIRTXH TXH XVDPRV SDUD DSUR[LPDU OD VROXFLyQ GH HVWH SUREOHPD LPSOLFD GLIHUHQFLDV
√ÄQLWDV\HVVLPLODUDOPpWRGRTXHVHXVyHQODVHFFLyQ
(QSULPHUOXJDUVHOHFFLRQHXQHQWHURm .\GH√ÄQDHOWDPDxRGHORQJLWXGGHSDVRGHO
HMHx h = l/m. $FRQWLQXDFLyQVHOHFFLRQHXQWDPDxRGHORQJLWXGGHSDVRGHWLHPSRk/RV
puntos de cuadr√≠cula para esta situaci√≥n son (x i, tj ), donde x i = ih, para i 5   , m, y
t j 5 jk, para j 5   

552

CAP√çTULO 12

Soluciones num√©ricas para ecuaciones diferenciales parciales

M√©todo de diferencias progresivas
1RVRWURVREWHQHPRVHOPpWRGRGHGLIHUHQFLDVPHGLDQWHODVHULHGH7D\ORUHQt para formar el
cociente de diferencias

‚àÇu
u(xi , t j + k) ‚àí u(xi , t j ) k ‚àÇ 2 u
(xi , t j ) =
‚àí
(xi , Œº j ),
‚àÇt
k
2 ‚àÇt 2



SDUDDOJXQDŒº j ‚àà (t j , t j+1 ), \ODVHULHGH7D\ORUHQx para formar el cociente de diferencias

‚àÇ 2u
u(xi + h, t j ) ‚àí 2u(xi , t j ) + u(xi ‚àí h, t j ) h 2 ‚àÇ 4 u
(xi , t j ) =
‚àí
(Œæi , t j ),
2
‚àÇx
h2
12 ‚àÇ x 4



donde Œæi ‚àà (xi‚àí1 , xi+1 ).
/DHFXDFLyQGLIHUHQFLDOSDUFLDOSDUDEyOLFD  LPSOLFDTXHSDUDORVSXQWRVHQHOLQWHrior de la malla (x i, t j ), para cada i 5   , m 2\j 5   , tenemos

‚àÇu
‚àÇ 2u
(xi , t j ) ‚àí Œ± 2 2 (xi , t j ) = 0,
‚àÇt
‚àÇx
DVtTXHHOPpWRGRXVDORVFRFLHQWHVGHGLIHUHQFLDV  \  HV

w i, j+1 ‚àí w i j
w i+1, j ‚àí 2w i j + w i‚àí1, j
= 0,
‚àí Œ±2
k
h2



donde wi j aproxima a u(x i , tj )
(OHUURUGHWUXQFDPLHQWRORFDOSDUDHVWDHFXDFLyQGHGLIHUHQFLDVHV

œÑi j =

2 4
k ‚àÇ 2u
2h ‚àÇ u
(x
,
Œº
)
‚àí
Œ±
(Œæi , t j ).
i
j
2 ‚àÇt 2
12 ‚àÇ x 4



5HVROYLHQGRODHFXDFLyQ  SDUDw i, j+1REWHQHPRV

w i, j+1 =

1‚àí

2Œ± 2 k
h2

w i j + Œ±2

k
(w i+1, j + w i‚àí1, j ),
h2

para cada i = 1, 2, . . . , m ‚àí 1 y j = 1, 2, . . . .
$VtREWHQHPRV

w 0,0 = f (x0 ),

w 1,0 = f (x1 ),

. . . w m,0 = f (xm ).

/XHJRJHQHUDPRVODVLJXLHQWHt√ÄODSRU

w 0,1 =u(0, t1 ) = 0;
w 1,1 = 1 ‚àí

2Œ± 2 k
h2

w 1,0 + Œ± 2

k
(w 2,0 + w 0,0 );
h2

w 2,1 = 1 ‚àí

2Œ± 2 k
h2

w 2,0 + Œ± 2

k
(w 3,0 + w 1,0 );
h2

2Œ± 2 k
h2

w m‚àí1,0 + Œ± 2

..
.
w m‚àí1,1 = 1 ‚àí

w m,1 =u(m, t1 ) = 0.

k
(w m,0 + w m‚àí2,0 );
h2



12.2 Ecuaciones diferenciales parciales parab√≥licas

553

$KRUDSRGHPRVXVDUORVYDORUHVw i,1SDUDJHQHUDUWRGRVORVYDORUHVGHODIRUPDw i,2
/DQDWXUDOH]DH[SOtFLWDGHOPpWRGRGHGLIHUHQFLDVLPSOLFDTXHODPDWUL](m ‚àí 1) √ó (m ‚àí 1)
DVRFLDGDFRQHVWHVLVWHPDSXHGHHVFULELUVHHQIRUPDWULGLDJRQDO
‚é°
‚é§
(1 ‚àí 2Œª)
Œª
0.. .. .. . . . . . . . . . . 0.
.
.
.
...
‚é¢
‚é•
.
Œª . . . . (1 ‚àí 2 Œª)
‚é¢
‚é•
. . . Œª . . . . . . . . . . . ...
....
‚é¢
‚é•
.
...
...
....
‚é•,
.
.
.
0
0
A=‚é¢
.
.
...
...
.. . . . .
‚é¢
‚é•
....
...
..
....
‚é¢
‚é•
.
..
.
.
.
.
.
.
Œª
.
‚é£
‚é¶
.
.
...
...
....
..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
0
Œª
(1 ‚àí 2Œª)
donde Œª = Œ± 2 (k/ h 2 ). 6LKDFHPRV

w(0) = ( f (x1 ), f (x2 ), . . . , f (xm‚àí1 ))t
y

w( j) = (w 1 j , w 2 j , . . . , w m‚àí1, j )t

para cada j 5   ,

entonces, la soluci√≥n aproximada est√° dada por

w( j) = Aw( j‚àí1) , para cada j 5   ,
SRUORTXH w( j)VHREWLHQHDSDUWLUGH w( j‚àí1)PHGLDQWHXQDVLPSOHPXOWLSOLFDFLyQGHPDWUL]
$HVWRVHOHFRQRFHFRPRHOm√©todo de diferencias progresivas y la aproximaci√≥n en el
SXQWRD]XOPRVWUDGRHQOD√ÄJXUDXVDLQIRUPDFLyQGHRWURVSXQWRVPDUFDGRVHQHVD√ÄJXUD6LODVROXFLyQGHODHFXDFLyQGLIHUHQFLDOSDUFLDOWLHQHFXDWURGHULYDGDVSDUFLDOHVHQx y dos
en tHQWRQFHVODHFXDFLyQ  LPSOLFDTXHHOPpWRGRHVGHRUGHQO(k 1 h 
Figura 12.8
t

t j11
tj

x
x x x

M√©todo de
diferencia
progresiva

l

xi
x i21 x i11

Ejemplo 1

x

Use los tama√±os de paso a) h 5\k 5\b) h 5\k 5 SDUDDSUR[LPDUOD
soluci√≥n de la ecuaci√≥n de calor

‚àÇu
‚àÇ 2u
(x, t) ‚àí 2 (x, t) = 0,
‚àÇt
‚àÇx

0 < x < 1,

con condiciones de frontera

u(0, t) = u(1, t) = 0,

0 < t,

0 ‚â§ t,

554

CAP√çTULO 12

Soluciones num√©ricas para ecuaciones diferenciales parciales

y condiciones iniciales

u(x, 0) = sen(œÄ x),

0 ‚â§ x ‚â§ 1.

Compare los resultados en t 5FRQODVROXFLyQH[DFWD

u(x, t) = e‚àíœÄ t sen(œÄ x).
2

a) (O PpWRGR GH GLIHUHQFLDV SURJUHVLYDV FRQ h 5  k 5  \ Œª 5
    ) 5GDORVUHVXOWDGRVHQODWHUFHUDFROXPQDGHODWDEOD&RPRVH
SXHGHREVHUYDUDSDUWLUGHODFXDUWDFROXPQDHVWRVUHVXOWDGRVVRQEDVWDQWHH[DFWRV
b)(OPpWRGRGHGLIHUHQFLDVSURJUHVLYDVFRQh 5k 5\Œª 5     ) 5
GDORVUHVXOWDGRVHQODTXLQWDFROXPQDGHODWDEOD&RPRVHSXHGHREVHUYDUDSDUWLUGH
ODVH[WDFROXPQDHVWRVUHVXOWDGRVVRQLQ~WLOHV
Soluci√≥n

Tabla 12.3
xi

u(xi , 0.5)

w i,1000
k = 0.0005

0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0

0
0.00222241
0.00422728
0.00581836
0.00683989
0.00719188
0.00683989
0.00581836
0.00422728
0.00222241
0

0
0.00228652
0.00434922
0.00598619
0.00703719
0.00739934
0.00703719
0.00598619
0.00434922
0.00228652
0

w i,50
k = 0.01

|u(xi , 0.5) ‚àí w i,1000 |

|u(xi , 0.5) ‚àí w i,50 |

0
8.19876 √ó 107
‚àí1.55719 √ó 108
2.13833 √ó 108
‚àí2.50642 √ó 108
2.62685 √ó 108
‚àí2.49015 √ó 108
2.11200 √ó 108
‚àí1.53086 √ó 108
8.03604 √ó 107
0

6.411 √ó 10‚àí5
1.219 √ó 10‚àí4
1.678 √ó 10‚àí4
1.973 √ó 10‚àí4
2.075 √ó 10‚àí4
1.973 √ó 10‚àí4
1.678 √ó 10‚àí4
1.219 √ó 10‚àí4
6.511 √ó 10‚àí5

8.199 √ó 107
1.557 √ó 108
2.138 √ó 108
2.506 √ó 108
2.627 √ó 108
2.490 √ó 108
2.112 √ó 108
1.531 √ó 108
8.036 √ó 107

Consideraciones de estabilidad
(QHOHMHPSORVHHVSHUDXQHUURUGHWUXQFDPLHQWRGHRUGHQO(k 1 h $SHVDUGHTXHHVWR
VHREWLHQHFRQh 5\k 5VLQGXGDDOJXQDQRVHREWLHQHFXDQGRh 5\k 5
3DUDH[SOLFDUODGL√ÄFXOWDGQHFHVLWDPRVREVHUYDUODHVWDELOLGDGGHOPpWRGRGHGLIHUHQFLDVSURJUHVLYDV
t
(0)
al representar los datos
6XSRQJDTXHVHFRPHWHXQHUURU e(0) = e1(0) , e2(0) , . . . , em‚àí1
iniciales

w(0) = f (x1 ), f (x2 ), . . . , f (xm‚àí1 )

t

RHQFXDOTXLHUSDVRSDUWLFXODUODVHOHFFLyQGHOSDVRLQLFLDOHVVLPSOHPHQWHSRUFRQYHQLHQFLD 8QHUURUGHAe(0)VHSURSDJDHQw(1)SRUTXH

w(1) = A w(0) + e(0) = Aw(0) + Ae(0) .
(VWHSURFHVRFRQWLQ~DHQHOHQpVLPRSDVRHOHUURUHQw(n)GHELGRDe(0) es An e(0) . 3RUFRQVLJXLHQWHHOPpWRGRHVHVWDEOHSUHFLVDPHQWHFXDQGRHVWRVHUURUHVQRFUHFHQFRQIRUPHn se
LQFUHPHQWD3HURHVWRQRHVYHUGDGHURVL\V√≥ORVLSDUDFXDOTXLHUHUURULQLFLDO e(0), tenemos
An e(0) ‚â§ e(0) para todas las n3RUORWDQWRGHEHPRVWHQHU ||An || ‚â§ 1, una condici√≥n
TXHPHGLDQWHHOWHRUHPDHQODSiJLQDUHTXLHUHTXHœÅ( An ) = (œÅ(A))n ‚â§ 1. (OPpWRGRGHGLIHUHQFLDVSURJUHVLYDVHVSRUORWDQWRHVWDEOHVL\V√≥lo si œÅ(A) ‚â§ 1.
(VSRVLEOHGHPRVWUDUTXHORVHLJHQYDORUHVGHA FRQVXOWHHOHMHUFLFLR VRQ

Œºi = 1 ‚àí 4Œª sen

iœÄ
2m

2

,

para cada i = 1, 2, . . . , m ‚àí 1.

12.2 Ecuaciones diferenciales parciales parab√≥licas

555

3RUORWDQWRODFRQGLFLyQSDUDHVWDELOLGDGVHUHGXFHDOGHWHUPLQDUVL

œÅ(A) = m√°x

1‚â§i‚â§m‚àí1

1 ‚àí 4Œª sen

iœÄ
2m

2

‚â§ 1,

\HVWRVHVLPSOL√ÄFDSDUD

0 ‚â§ Œª sen

iœÄ
2m

2

1
,
2

‚â§

para cada i = 1, 2, . . . , m ‚àí 1.

/DHVWDELOLGDGUHTXLHUHTXHHVWDFRQGLFLyQGHGHVLJXDOGDGVHPDQWHQJDFXDQGRh ‚Üí 0, o, de
IRUPDHTXLYDOHQWHFXDQGRm ‚Üí ‚àû.(OKHFKRGHTXH

l√≠m

m‚Üí‚àû

(m ‚àí 1)œÄ
2m

sen

2

=1

VLJQL√ÄFDTXHVHSUHVHQWDUiHVWDELOLGDGV√≥lo si 0 ‚â§ Œª ‚â§ 12 .
3RUGH√ÄQLFLyQ Œª = Œ± 2 (k/ h 2 ), SRUORWDQWRHVWDGHVLJXDOGDGUHTXLHUHVHOHFFLRQDUh y k
GHWDOIRUPDTXH

Œ±2

k
1
‚â§ .
h2
2

(QHOHMHPSORWHQHPRVŒ± 5SRUORTXHHVWDFRQGLFLyQVHVDWLVIDFHFXDQGRh 5\
k 53HURFXDQGRVHLQFUHPHQWykDVLQHODXPHQWRFRUUHVSRQGLHQWHHQh, la
UD]yQIXH

1
0.01
=1> ,
(0.1)2
2
\ORVSUREOHPDVGHHVWDELOLGDGVHYROYLHURQLQPHGLDWDPHQWHGUiVWLFRV
&RQVLVWHQWHFRQODWHUPLQRORJtDGHOFDStWXOROODPDPRVDOPpWRGRGHGLIHUHQFLDVSURJUHVLYDVcondicionalmente estable(OPpWRGRFRQYHUJHHQODVROXFLyQGHODHFXDFLyQ  
FRQODYHORFLGDGGHFRQYHUJHQFLDO(k 1 h), siempre y cuando

Œ±2

k
1
‚â§
2
h
2

\ODVFRQGLFLRQHVGHFRQWLQXLGDGUHTXHULGDVSDUDODVROXFLyQVHFXPSODQ 3DUDXQDSUXHED
GHWDOODGDGHHVWHKHFKRREVHUYH>,.@S¬≤

M√©todo de diferencias regresivas
3DUDREWHQHUXQPpWRGRTXHHVincondicionalmente estable, consideramos uno de diferenFLDVLPSOtFLWDVTXHUHVXOWDGHOXVRGHOFRFLHQWHGHGLIHUHQFLDVUHJUHVLYDVSDUD(‚àÇu/‚àÇt)(xi , t j )
en la forma

‚àÇu
u(xi , t j ) ‚àí u(xi , t j‚àí1 ) k ‚àÇ 2 u
(xi , t j ) =
+
(xi , Œº j ),
‚àÇt
k
2 ‚àÇt 2
donde Œº j est√° en (t j‚àí1 , t j ). $O VXVWLWXLU HVWD HFXDFLyQ MXQWR FRQ OD HFXDFLyQ   SDUD
‚àÇ 2 u/‚àÇ x 2 ,HQODHFXDFLyQGLIHUHQFLDOSDUFLDOREWHQHPRV

u(xi , t j ) ‚àí u(xi , t j‚àí1 )
u(xi+1 , t j ) ‚àí 2u(xi , t j ) + u(xi‚àí1 , t j )
‚àí Œ±2
k
h2
=‚àí

2 4
k ‚àÇ 2u
2h ‚àÇ u
(x
,
Œº
)
‚àí
Œ±
(Œæi , t j ),
i
j
2 ‚àÇt 2
12 ‚àÇ x 4

556

CAP√çTULO 12

Soluciones num√©ricas para ecuaciones diferenciales parciales

SDUDDOJXQDVŒæi ‚àà (xi‚àí1 , xi+1 ). (Om√©todo de diferencias regresivas resultante es

w i j ‚àí w i, j‚àí1
w i+1, j ‚àí 2w i j + w i‚àí1, j
‚àí Œ±2
= 0,
k
h2



para cada i 5   , m 2\j 5   
(OPpWRGRGHGLIHUHQFLDVUHJUHVLYDVLPSOLFDORVSXQWRVGHPDOOD(xi , t j‚àí1 ), (xi‚àí1 , t j ), y
(xi+1 , t j ) para aproximar el valor en (xi, tj FRPRVHLOXVWUDHQOD√ÄJXUD
Figura 12.9
t

tj

t j21

M√©todo de
diferencias
regresivas

x x x
x

xi
x i21 x i11

l

x

3XHVWR TXH ODV FRQGLFLRQHV GH IURQWHUD H LQLFLDOHV UHODFLRQDGDV FRQ HO SUREOHPD SURSRUFLRQDQLQIRUPDFLyQHQORVSXQWRVGHPDOODURGHDGRVSRUXQFtUFXOROD√ÄJXUDQRPXHVWUD
SURFHGLPLHQWRVH[SOtFLWRVTXHVHSXHGDQXWLOL]DUSDUDUHVROYHUODHFXDFLyQ  5HFXHUGH
TXHHQHOPpWRGRGHGLIHUHQFLDVSURJUHVLYDV FRQVXOWHOD√ÄJXUD ODVDSUR[LPDFLRQHV
en (xi‚àí1 , t j‚àí1 ), (xi , t j‚àí1 ), y (xi+1 , t j‚àí1 ) se usaron para encontrar la aproximaci√≥n en (xi, tj 
3RUORTXHVHSXHGHXVDUXQPpWRGRH[SOtFLWRSDUDHQFRQWUDUODVDSUR[LPDFLRQHVFRQEDVHHQ
ODLQIRUPDFLyQDSDUWLUGHODVFRQGLFLRQHVLQLFLDOHV\GHIURQWHUD
Figura 12.10
t

t j11
tj

x
x x x

xi
x i21 x i11

M√©todo de
diferencias
progresivas

l

x

6L GH QXHYR SHUPLWLPRV TXH Œª denote la cantidad Œ±(k h), el m√©todo de diferencia
UHJUHVLYDVHYXHOYH

(1 + 2Œª)w i j ‚àí Œªw i+1, j ‚àí Œªw i‚àí1, j = w i, j‚àí1 ,
para cada i 5   , m 2\j 5   3RUPHGLRGHOFRQRFLPLHQWRTXHw i,0 = f (xi ),
para cada i = 1, 2, . . . , m ‚àí 1 y w m, j = w 0, j = 0, para cada j 5   , este m√©todo de
diferencias tiene la representaci√≥n matricial

12.2 Ecuaciones diferenciales parciales parab√≥licas

‚é§
(1 + 2 Œª)
. . . ‚àíŒª . . .0.. .. .. . . . . . . . . . 0.
‚é§ ‚é°
‚é§
‚é°
.
.
.
.
w 1, j‚àí1
w 1, j
... ....
...
‚é•
‚é¢
.
.
‚àíŒª
.
.
‚é•‚é¢
‚é¢
...
.
‚é¢
‚é•
‚é• ‚é¢ w 2, j ‚é•
‚é¢
. . . . . . . . . . . . . . . . . ..
‚é• ‚é¢ w 2, j‚àí1 ‚é•
‚é•
‚é¢
.
.
.
0
0
.
=
.
.
.
‚é•
‚é¢
‚é•,
‚é¢
.
.
.
.
.
.. . . .
‚é•‚é£
‚é¢
..
..
‚é¶ ‚é£
‚é¶
... ..... ..... ...
‚é•
‚é¢
..
...
.
‚é¶
‚é£
..
. . . . . . . . . . ‚àíŒª
w m‚àí1, j
w m‚àí1, j‚àí1
0 . . . . . . . . . . . .0 ‚àíŒª (1 + 2Œª)

557

‚é°



o Aw( j) = w( j‚àí1) , para cada i = 1, 2, . . . .
3RU OR WDQWR DKRUD GHEHPRV UHVROYHU HO VLVWHPD OLQHDO SDUD REWHQHU w( j) a partir de
w
. 6LQHPEDUJRŒª .SRUORTXHODPDWUL]AHVGH√ÄQLGDSRVLWLYD\HVWULFWD\GLDJRQDOPHQWHGRPLQDQWHDVtFRPRWULGLDJRQDO3RUFRQVLJXLHQWHSRGHPRVXVDUWDQWRHODOJRULWPR
GH IDFWRUL]DFLyQ GH &URXW  FRPR HO DOJRULWPR 625  SDUD UHVROYHU HVWH VLVWHPD (O
DOJRULWPRUHVXHOYHODHFXDFLyQ  SRUPHGLRGHIDFWRUL]DFLyQGH&URXWORFXDOHV
DFHSWDEOHDPHQRVTXHmVHDJUDQGH(QHVWHDOJRULWPRVXSRQHPRVFRQHO√ÄQGHSDURTXH
se da una cota para t
( j‚àí1)

ALGORITMO

12.2

Diferencias regresivas de la ecuaci√≥n de calor
3DUDDSUR[LPDUODVROXFLyQGHODHFXDFLyQGLIHUHQFLDOSDUFLDOSDUDEyOLFD

‚àÇu
‚àÇ 2u
(x, t) ‚àí Œ± 2 2 (x, t) = 0,
‚àÇt
‚àÇx

0 < x < l,

0 < t < T,

VXMHWDDODVFRQGLFLRQHVGHIURQWHUD

u(0, t) = u(l, t) = 0,

0 < t < T,

y las condiciones iniciales

u(x, 0) = f (x),
ENTRADA
SALIDA

0‚â§x ‚â§l :

extremo lWLHPSRPi[LPRTFRQVWDQWHŒ±HQWHURVm ‚â• 3, N ‚â• 1.
aproximaciones w i, j para u(xi , t j ) para cada i 5   , m 2\j 5   , N

Paso 1 Determine h = l/m;
k = T /N ;
Œª = Œ± 2 k/ h 2 .
Paso 2 Para i = 1, . . . , m ‚àí 1 determine w i = f (i h). (Valores iniciales.)
(Los pasos 3‚Äì11 resuelven un sistema lineal tridiagonal por medio del algoritmo 6.7.)
Paso 3 Determine l1 = 1 + 2Œª;
u 1 = ‚àíŒª/l1 .
Paso 4 Para i = 2, . . . , m ‚àí 2 determine li = 1 + 2Œª + Œªu i‚àí1 ;
u i = ‚àíŒª/li .
Paso 5 Determine lm‚àí1 = 1 + 2Œª + Œªu m‚àí2 .
Paso 6 Para j = 1, . . . , N haga los pasos 7‚Äì11.
Paso 7 Determine t = jk; (Actual t j .)
z 1 = w 1 /l1 .
Paso 8 Para i = 2, . . . , m ‚àí 1 determine z i = (w i + Œªz i‚àí1 )/li .
Paso 9 Determine w m‚àí1 = z m‚àí1 .

558

CAP√çTULO 12

Soluciones num√©ricas para ecuaciones diferenciales parciales

Paso 10 Para i = m ‚àí 2, . . . , 1 determine w i = z i ‚àí u i w i+1 .
Paso 11 SALIDA (t); (Nota: t = t j .)
Para i = 1, . . . , m ‚àí 1 determine x = i h;
SALIDA (x, w i ). (Nota: w i = w i, j .)
Paso 12 PARE. (El procedimiento est√° completo.)
Ejemplo 2

8VHHOPpWRGRGHGLIHUHQFLDVUHJUHVLYDV DOJRULWPR FRQh 5\k 5SDUDDSUR[Lmar la soluci√≥n de la ecuaci√≥n de calor

‚àÇ 2u
‚àÇu
(x, t) ‚àí 2 (x, t) = 0,
‚àÇt
‚àÇx

0 < x < 1,

0 < t,

VXMHWDDODVUHVWULFFLRQHV

u(0, t) = u(1, t) = 0,

0 < t,

u(x, 0) = sen œÄ x,

0 ‚â§ x ‚â§ 1.

Soluci√≥n (VWHSUREOHPDVHFRQVLGHUyHQHOHMHPSORGRQGHHQFRQWUDPRVTXHVHOHFFLRQDU

h 5\k 5GDUHVXOWDGRVEDVWDQWHH[DFWRV6LQHPEDUJRFRQORVYDORUHVHQHVWH
HMHPSORh 5\k 5ORVUHVXOWDGRVIXHURQH[FHSFLRQDOPHQWHSREUHV3DUDGHPRVWUDU
OD HVWDELOLGDG LQFRQGLFLRQDO GHO PpWRGR GH GLIHUHQFLDV UHJUHVLYDV XWLOL]DUHPRV h 5  \
k 5\QXHYDPHQWHFRPSDUDPRVw i,50 con u(xi , 0.5), donde i 5   
/RVUHVXOWDGRVPRVWUDGRVHQODWDEODWLHQHQORVPLVPRVYDORUHVGHh y kTXHHQOD
TXLQWD\VH[WDFROXPQDVGHODWDEODORFXDOLOXVWUDODHVWDELOLGDGGHHVWHPpWRGR

Tabla 12.4

xi

w i,50

u(xi , 0.5)

|w i,50 ‚àí u(xi , 0.5)|

0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0

0
0.00289802
0.00551236
0.00758711
0.00891918
0.00937818
0.00891918
0.00758711
0.00551236
0.00289802
0

0
0.00222241
0.00422728
0.00581836
0.00683989
0.00719188
0.00683989
0.00581836
0.00422728
0.00222241
0

6.756 √ó 10‚àí4
1.285 √ó 10‚àí3
1.769 √ó 10‚àí3
2.079 √ó 10‚àí3
2.186 √ó 10‚àí3
2.079 √ó 10‚àí3
1.769 √ó 10‚àí3
1.285 √ó 10‚àí3
6.756 √ó 10‚àí4

/DUD]yQSRUODTXHHOPpWRGRGHGLIHUHQFLDVUHJUHVLYDVQRWLHQHORVSUREOHPDVGHHVWDELOLGDGGHOPpWRGRGHGLIHUHQFLDVSURJUHVLYDVVHSXHGHREVHUYDUDODQDOL]DUORVHLJHQYDORUHV
GHODPDWUL]A3DUDHOPpWRGRGHGLIHUHQFLDVUHJUHVLYDV FRQVXOWHHOHMHUFLFLR ORVHLJHQvalores son

Œºi = 1 + 4Œª sen

iœÄ
2m

2

,

para cada i = 1, 2, . . . , m ‚àí 1.

3XHVWR TXH Œª .  WHQHPRV Œºi > 1 para todas las i = 1, 2, . . . , m ‚àí 1.  3XHVWR TXH ORV
HLJHQYDORUHVGHA2 son los rec√≠procos de los de A, la relaci√≥n espectral de A‚àí1 , œÅ(A‚àí1 ) < 1.
(VWRLPSOLFDTXHA2HVXQDPDWL]FRQYHUJHQWH
Un error e(0) en los datos iniciales produce un error (A‚àí1 )n e(0) en el en√©simo paso del
PpWRGRGHGLIHUHQFLDVUHJUHVLYDV<DTXHA2HVFRQYHUJHQWH

l√≠m (A‚àí1 )n e(0) = 0.

n‚Üí‚àû

3RU OR TXH HO PpWRGR HV HVWDEOH LQGHSHQGLHQWHPHQWH GH OD VHOHFFLyQ GH Œª = Œ± 2 (k/ h 2 ).
(QODWHUPLQRORJtDGHOFDStWXOROODPDPRVDOPpWRGRGHGLIHUHQFLDVUHJUHVLYDVXQPpWRGR
incondicionalmente estable (O HUURU GH WUXQFDPLHQWR ORFDO SDUD HO PpWRGR HV GH RUGHQ

12.2 Ecuaciones diferenciales parciales parab√≥licas

559

O(k 1 h VLHPSUH\FXDQGRODVROXFLyQGHODHFXDFLyQGLIHUHQFLDOVDWLVIDJDODVFRQGLFLRQHV
GHGLIHUHQFLDELOLGDGXVXDO(QHVWHFDVRHOPpWRGRFRQYHUJHDODVROXFLyQGHODHFXDFLyQ
GLIHUHQFLDOSDUFLDOFRQHVWDPLVPDYHORFLGDGGHFRQYHUJHQFLD FRQVXOWH>,.@S 
/DGHELOLGDGGHOPpWRGRGHGLIHUHQFLDVUHJUHVLYDVUHVXOWDGHOKHFKRGHTXHHOHUURUGH
truncamiento local tiene uno de orden O(h) y otro de orden O(k (VWRUHTXLHUHKDFHUTXHORV
LQWHUYDORVGHWLHPSRVHDQPXFKRPiVSHTXHxRVTXHORVLQWHUYDORVGHOHMHx6HUtDFODUDPHQWH
GHVHDEOHWHQHUXQSURFHGLPLHQWRFRQHUURUGHWUXQFDPLHQWRORFDOGHRUGHQO(k 1 h (O
SULPHUSDVRHQHVWDGLUHFFLyQHVXVDUXQDHFXDFLyQGHGLIHUHQFLDVTXHWLHQHHUURUO(k) para
ut (x, t HQOXJDUGHORVTXHKHPRVXWLOL]DGRSUHYLDPHQWHFX\RHUURUHUDO(k (VWRVHSXHGH
KDFHUXVDQGRODVHULHGH7D\ORUHQt para la funci√≥n u (x, t) en el punto (xi , t j ) y al evaluar en
(xi , t j+1 ) y (xi , t j‚àí1 )SDUDREWHQHUODIyUPXODGHGLIHUHQFLDVFHQWUDGDV

‚àÇu
u(xi , t j+1 ) ‚àí u(xi , t j‚àí1 ) k 2 ‚àÇ 3 u
(xi , Œº j ),
(xi , t j ) =
+
‚àÇt
2k
6 ‚àÇt 3
/(5LFKDUGVRQTXLHQFRPR
REVHUYDPRVHVWiUHODFLRQDGR
FRQODH[WUDSRODFLyQUHDOL]y
XQWUDEDMRVXVWDQFLDOHQOD
aproximaci√≥n de ecuaciones
GLIHUHQFLDOHVSDUFLDOHV

donde Œº j ‚àà (t j‚àí1 , t j+1 ). (OPpWRGRGHGLIHUHQFLDVTXHUHVXOWDGHVXVWLWXLUHVWR\HOFRFLHQWH
de diferencias usual para (‚àÇ 2 u/‚àÇ x 2 ), ODHFXDFLyQ  HQODHFXDFLyQGLIHUHQFLDOUHFLEHHO
QRPEUHGHm√©todo de Richarson y est√° dado por

w i, j+1 ‚àí w i, j‚àí1
w i+1, j ‚àí 2w i j + w i‚àí1, j
= 0.
‚àí Œ±2
2k
h2



(VWHPpWRGRWLHQHHUURUGHWUXQFDPLHQWRORFDOGHRUGHQO(k 1 h SHURSRUGHVJUDFLDDOLJXDOTXHHOPpWRGRGHGLIHUHQFLDSURJUHVLYDWLHQHVHULRVSUREOHPDVGHHVWDELOLGDG
FRQVXOWHORVHMHUFLFLRV\ 

M√©todo de Crank-Nicolson
8QPpWRGRPiVSURPHWHGRUVHGHULYDDOSURPHGLDUHOPpWRGRGHGLIHUHQFLDVSURJUHVLYDVHQ
el j-√©simo paso en t,

w i, j+1 ‚àí w i, j
w i+1, j ‚àí 2w i, j + w i‚àí1, j
= 0,
‚àí Œ±2
k
h2
TXHWLHQHHUURUGHWUXQFDPLHQWRORFDO

œÑF =

k ‚àÇ 2u
(xi , Œº j ) + O(h 2 ),
2 ‚àÇt 2

\HOPpWRGRGHGLIHUHQFLDVUHJUHVLYDVHQHO j 1 √©simo paso en t,

w i+1, j+1 ‚àí 2w i, j+1 + w i‚àí1, j+1
w i, j+1 ‚àí w i, j
= 0,
‚àí Œ±2
k
h2
TXHWLHQHHUURUGHWUXQFDPLHQWRORFDO

œÑB = ‚àí
3DUDFRQWLQXDUVXWUDEDMRFRPR
f√≠sico matem√°tico durante
OD6HJXQGD*XHUUD0XQGLDO
-RKQ&UDQN ¬≤ 
UHDOL]yLQYHVWLJDFLRQHVVREUHOD
soluci√≥n num√©rica de ecuaciones
diferenciales parciales,
HQHVSHFLDOSUREOHPDVGH
FRQGXFFLyQGHFDORU(OPpWRGR
GH&UDQN¬≤1LFROVRQVHEDVDHQ
HOWUDEDMRUHDOL]DGRSRU3K\OOLV
1LFROVRQ ¬≤ XQItVLFR
HQOD8QLYHUVLGDG/HHGV6X
DUWtFXORRULJLQDOVREUHHOPpWRGR
DSDUHFLyHQ>&1@

k ‚àÇ 2u
(xi , uÃÇ j ) + O(h 2 ).
2 ‚àÇt 2

6LVXSRQHPRVTXH

‚àÇ 2u
‚àÇ 2u
(x
,
ŒºÃÇ
)
‚âà
(xi , Œº j ),
i
j
‚àÇt 2
‚àÇt 2
entonces, el m√©todo de diferencia promediado,

Œ± 2 w i+1, j ‚àí 2w i, j + w i‚àí1, j
w i, j+1 ‚àí w i j
w i+1, j+1 ‚àí 2w i, j+1 + w i‚àí1, j+1
‚àí
+
= 0,
k
2
h2
h2
tiene error de truncamiento local de orden O(k 1 h), siempre y cuando, por supuesto, se
VDWLVIDJDQODVFRQGLFLRQHVGHGLIHUHQFLDELOLGDGFRPXQHV

560

CAP√çTULO 12

Soluciones num√©ricas para ecuaciones diferenciales parciales

(VWRVHFRQRFHFRPRHOm√©todo de Crank-Nicolson\VHUHSUHVHQWDHQIRUPDGHPDWUL]

Aw( j+1) = Bw( j) , para cada j = 0, 1, 2, . . . ,



donde

Œª = Œ±2

k
,
h2

w( j) = (w 1, j , w 2, j , . . . , w m‚àí1, j )t ,

y las matrices A y B est√°n dadas por:

‚é°

‚é§
(1 + Œª)
. . . ‚àí Œª2 . . .0.. .. .. . . . . . . . . .0.
.
.
.
.
.
... ...
.
‚é¢
‚é•
. . . . . . ...
‚é¢ ‚àí Œª2 . . . . . . . .
‚é•
... .. .
‚é¢
‚é•
... ...
.
‚é¢
‚é•
... ...
...0
0. . . .
A=‚é¢
‚é•
.
.
.
... ...
.. . . .
‚é¢
... ... ‚àíŒª ‚é•
...
..
‚é£
..
. 2 ‚é¶
...
.
Œª
.
.
.
.
.
.
.
.
.
.
0
0 ‚àí 2 (1 + Œª)
y
‚é°
‚é¢
‚é¢
‚é¢
B=‚é¢
‚é¢
‚é¢
‚é£

(1 ‚àí Œª)
...

.

Œª
0. . . . . . . . . . . 0.
2 .... ....
.

‚é§

... ....
‚é•
. . . . . . ...
‚é•
... .. .
‚é•
...
...
.
‚é•.
... 0
...
0. . . .
...
‚é•
.
.
.
.. . . .
... ...
‚é•
...
... ... Œª
..
‚é¶
2
.
.
.
.
.
.
Œª
.
.
.
.
.
.
.
.
.
.
.
0
0
(1 ‚àí Œª)
2

...
Œª
...
2 ....
.

/DPDWUL]QRVLQJXODUAHVGH√ÄQLGDSRVLWLYDHVWULFWD\GLDJRQDOPHQWHGRPLQDQWH\WULGLDJRQDO6HSXHGHXVDUWDQWRODIDFWRUL]DFLyQGH&URXWFRPRHODOJRULWPR625SDUD
REWHQHU w( j+1) a partir de w( j) , para cada j = 0, 1, 2, . . . . (ODOJRULWPRLQFRUSRUDOD
IDFWRUL]DFLyQGH&URXWHQODWpFQLFD&UDQN1LFROVRQ&RPRHQHODOJRULWPRXQDORQJLWXG√ÄQLWDSDUDHOLQWHUYDORGHWLHPSRVHGHEHHVSHFL√ÄFDUSDUDGHWHUPLQDUXQSURFHGLPLHQWR
GHGHWHQFLyQ/DYHUL√ÄFDFLyQGHTXHHOPpWRGRGH&UDQN1LFROVRQHVLQFRQGLFLRQDOPHQWH
HVWDEOH\WLHQHRUGHQGHFRQYHUJHQFLDO(k 1 h VHSXHGHHQFRQWUDUHQ>,.@S¬≤
8QGLDJUDPDTXHPXHVWUDODLQWHUDFFLyQGHORVQRGRVSDUDGHWHUPLQDUXQDDSUR[LPDFLyQHQ
(xi , t j+1 )VHPXHVWUDHQOD√ÄJXUD

Figura 12.11
t

t j11
tj

x x x
x x x

xi
x i21 x i11

M√©todo
CrankNicolson

l

x

12.2 Ecuaciones diferenciales parciales parab√≥licas

ALGORITMO

12.3

561

M√©todo Crank-Nicolson
3DUDDSUR[LPDUODVROXFLyQGHODHFXDFLyQGLIHUHQFLDOSDUFLDOSDUDEyOLFD

‚àÇu
‚àÇ 2u
(x, t) ‚àí Œ± 2 2 (x, t) = 0,
‚àÇt
‚àÇx

0 < x < l,

0 < t < T,

VXMHWRDODVFRQGLFLRQHVGHIURQWHUD

u(0, t) = u(l, t) = 0,

0 < t < T,

y las condiciones iniciales

u(x, 0) = f (x),
ENTRADA
SALIDA

0‚â§x ‚â§l :

extremo lWLHPSRPi[LPRTFRQVWDQWHŒ±HQWHURVm ‚â• 3, N ‚â• 1.

aproximaciones w i, j para u(xi , t j ) para cada i = 1, . . . , m ‚àí 1 y j = 1, . . . , N .

Paso 1 Determine h = l/m;
k = T /N ;
Œª = Œ± 2 k/ h 2 ;
w m = 0.
Paso 2 Para i = 1, . . . , m ‚àí 1 determine w i = f (i h). (Valores iniciales. )
(Los pasos 3‚Äì11 resuelven un sistema lineal tridiagonal por medio del algoritmo 6.7.)
Paso 3 Determine l1 = 1 + Œª;
u 1 = ‚àíŒª/(2l1 ).
Paso 4 Para i = 2, . . . , m ‚àí 2 determine li = 1 + Œª + Œªu i‚àí1 /2;
u i = ‚àíŒª/(2li ).
Paso 5 Determine lm‚àí1 = 1 + Œª + Œªu m‚àí2 /2.
Paso 6 Para j = 1, . . . , N haga los pasos 7‚Äì11.
Paso 7 Determine t = jk;

(Actual t j .)

z 1 = (1 ‚àí Œª)w 1 +

Œª
w2
2

l1 .

Paso 8 Para i = 2, . . . , m ‚àí 1 determine
z i = (1 ‚àí Œª)w i +

Œª
(w i+1 + w i‚àí1 + z i‚àí1 )
2

li .

Paso 9 Determine w m‚àí1 = z m‚àí1 .
Paso 10 Para i = m ‚àí 2, . . . , 1 determine w i = z i ‚àí u i w i+1 .
Paso 11 SALIDA (t); (Nota: t = t j .)
Para i = 1, . . . , m ‚àí 1 establezca x = i h;
SALIDA (x, w i ). (Nota: wi = w i, j .)
Paso 12 PARE. (El procedimiento est√° completo.)

562

CAP√çTULO 12

Soluciones num√©ricas para ecuaciones diferenciales parciales

Ejemplo 3

8VHHOPpWRGR&UDQN1LFROVRQFRQh 5\k 5SDUDDSUR[LPDUODVROXFLyQGHOSUREOHPD

‚àÇu
‚àÇ 2u
(x, t) ‚àí 2 (x, t) = 0,
‚àÇt
‚àÇx

0<x <1

0 < t,

VXMHWRDODVFRQGLFLRQHV

u(0, t) = u(1, t) = 0,

0 < t,

y
u(x, 0) = sen(œÄ x), 0 ‚â§ x ‚â§ 1.
Soluci√≥n $OVHOHFFLRQDUh 5\k 5REWHQHPRVm 5N 5\Œª 5 HQHODOJRULWPR5HFXHUGHTXHHOPpWRGRGHGLIHUHQFLDSURJUHVLYDSURYHHUHVXOWDGRVGUiVWLFDPHQWHSREUHVSDUDHVWDVHOHFFLyQGHh y kSHURHOPpWRGRGHGLIHUHQFLDVUHJUHVLYDVGDUHVXOWDGRV
TXHHUDQH[DFWRVDOUHGHGRUGH32SDUDHQWUDGDVHQPHGLRGHODWDEOD/RVUHVXOWDGRVHQ
ODWDEODLQGLFDQHOLQFUHPHQWRGHH[DFWLWXGGHOPpWRGR&UDQN1LFROVRQVREUHHOPpWRGR
GHGLIHUHQFLDVUHJUHVLYDVODPHMRUGHODVGRVWpFQLFDVSUHYLDPHQWHDQDOL]DGDV

Tabla 12.5

xi

w i,50

u(xi , 0.5)

|w i,50 ‚àí u(xi , 0.5)|

0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0

0
0.00230512
0.00438461
0.00603489
0.00709444
0.00745954
0.00709444
0.00603489
0.00438461
0.00230512
0

0
0.00222241
0.00422728
0.00581836
0.00683989
0.00719188
0.00683989
0.00581836
0.00422728
0.00222241
0

8.271 √ó 10‚àí5
1.573 √ó 10‚àí4
2.165 √ó 10‚àí4
2.546 √ó 10‚àí4
2.677 √ó 10‚àí4
2.546 √ó 10‚àí4
2.165 √ó 10‚àí4
1.573 √ó 10‚àí4
8.271 √ó 10‚àí5

La secci√≥n Conjunto de ejercicios 12.2 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

12.3 Ecuaciones diferenciales parciales hiperb√≥licas
(QHVWDVHFFLyQFRQVLGHUDPRVODVROXFLyQQXPpULFDGHODecuaci√≥n de onda, XQHMHPSORGH
una ecuaci√≥n diferencial parcial hiperb√≥lica. /DHFXDFLyQGHRQGDHVWiGDGDSRUODHFXDFLyQ
diferencial

‚àÇ 2u
‚àÇ 2u
(x, t) ‚àí Œ± 2 2 (x, t) = 0,
2
‚àÇt
‚àÇx

0 < x < l,

t > 0,

VXMHWDDODVFRQGLFLRQHV

u(0, t) = u(l, t) = 0,
u(x, 0) = f (x), y

para t > 0,

‚àÇu
(x, 0) = g(x),
‚àÇt

para

0 ‚â§ x ‚â§ l,

donde Œ±HVXQDFRQVWDQWHGHSHQGLHQWHGHODVFRQGLFLRQHVItVLFDVGHOSUREOHPD



12.3 Ecuaciones diferenciales parciales hiperb√≥licas

563

6HOHFFLRQH XQ HQWHUR m .  SDUD GH√ÄQLU ORV SXQWRV GH FXDGUtFXOD GHO HMH x KDFLHQGR
h 5 l m$GHPiVVHOHFFLRQHXQWDPDxRGHORQJLWXGGHSDVRWLHPSRk ./RVSXQWRVGH
malla (xi , t j )HVWiQGH√ÄQLGRVSRU

xi = i h

y

t j = jk,

para cada i = 0, 1, . . . , m y j = 0, 1, . . . .
(QFXDOTXLHUSXQWRGHPDOODLQWHULRU(xi , t j ), la ecuaci√≥n de onda se vuelve
2
‚àÇ 2u
2‚àÇ u
(x
,
t
)
‚àí
Œ±
(xi , t j ) = 0.
i
j
‚àÇt 2
‚àÇx2



(OPpWRGRGHGLIHUHQFLDVVHREWLHQHDWUDYpVGHOFRFLHQWHGHGLIHUHQFLDVFHQWUDGDVSDUD
ODVVHJXQGDVGHULYDGDVSDUFLDOHVGDGDVSRU

‚àÇ 2u
u(xi , t j+1 ) ‚àí 2u(xi , t j ) + u(xi , t j‚àí1 )
k2 ‚àÇ 4u
(x
,
t
)
=
‚àí
(xi , Œº j ),
i
j
‚àÇt 2
k2
12 ‚àÇt 4
donde Œº j ‚àà (t j‚àí1 , t j+1 ), y
u(xi+1 , t j ) ‚àí 2u(xi , t j ) + u(xi‚àí1 , t j ) h 2 ‚àÇ 4 u
‚àÇ 2u
(x
,
t
)
=
‚àí
(Œæi , t j ),
i
j
‚àÇx2
h2
12 ‚àÇ x 4
donde Œæi ‚àà (xi‚àí1 , xi+1 ). $OVXVWLWXLU√©VWDVHQODHFXDFLyQ  REWHQHPRV

u(xi , t j+1 ) ‚àí 2u(xi , t j ) + u(xi , t j‚àí1 )
u(xi+1 , t j ) ‚àí 2u(xi , t j ) + u(xi‚àí1 , t j )
‚àí Œ±2
2
k
h2
=

4
1 2 ‚àÇ 4u
2 2‚àÇ u
(x
,
Œº
)
‚àí
Œ±
h
(Œæi , t j ) .
k
i
j
12
‚àÇt 4
‚àÇx4

$OLJQRUDUHOWpUPLQRGHHUURU

œÑi, j =

‚àÇ 4u
1 2 ‚àÇ 4u
k
(xi , Œº j ) ‚àí Œ± 2 h 2 4 (Œæi , t j ) ,
4
12
‚àÇt
‚àÇx



REWHQHPRVODHFXDFLyQGHGLIHUHQFLDV

w i+1, j ‚àí 2w i, j + w i‚àí1, j
w i, j+1 ‚àí 2w i, j + w i, j‚àí1
‚àí Œ±2
= 0.
2
k
h2
'H√ÄQDŒª = Œ±k/ h. (QWRQFHVSRGHPRVHVFULELUODHFXDFLyQGHGLIHUHQFLDFRPR

w i, j+1 ‚àí 2w i, j + w i, j‚àí1 ‚àí Œª2 w i+1, j + 2Œª2 w i, j ‚àí Œª2 w i‚àí1, j = 0
y resolver para w i, j+1 , ODDSUR[LPDFLyQGHORQJLWXGGHSDVRWLHPSRSDUDREWHQHU

w i, j+1 = 2(1 ‚àí Œª2 )w i, j + Œª2 (w i+1, j + w i‚àí1, j ) ‚àí w i, j‚àí1 .



(VWDHFXDFLyQVHPDQWLHQHSDUDFDGDi = 1, 2, . . . , m‚àí1 y j = 1, 2, . . . . /DVFRQGLFLRQHVGH
frontera proporcionan

w 0, j = w m, j = 0, para cada j = 1, 2, 3, . . . ,



\ODFRQGLFLyQLQLFLDOLPSOLFDTXH

w i,0 = f (xi ), para cada i = 1, 2, . . . , m ‚àí 1.



564

CAP√çTULO 12

Soluciones num√©ricas para ecuaciones diferenciales parciales

(VFULELUHVWHFRQMXQWRGHHFXDFLRQHVHQIRUPDPDWULFLDOQRVGD

‚é°
‚é°

‚é§
0.. .. .. . . . . . . . . . 0.
..
‚é§ ‚é°
‚é§
...
‚é•‚é°
...
..
w 1, j‚àí1
‚é• w 1, j
Œª. 2. .
2(1 ‚àí. .Œª. 2 ) Œª. 2.
.
‚é•‚é¢
. . . ..
.
...
...
‚é¢ w 2, j‚àí1 ‚é•
‚é•‚é¢ w 2, j ‚é•
.
... .....
.
‚é¢
‚é•
‚é•‚é¢ . ‚é•
...
...
0
0. . . . . . . . . . .
‚àí
‚é•
‚é•.
..
‚é•‚é£ . ‚é¶ ‚é¢
...
.
...
..
.
‚é£
‚é¶
.
.
.
.
‚é•
.
.
.
.
.
.
.
...
...
2
..
...
‚é•
Œª
.
.
.
.
w m‚àí1, j‚àí1
... ....
‚é¶ w m‚àí1, j
...
..
...
..
..
0 . . . . . . . . . . . . . . . . ... 0 . Œª2 2(1 ‚àí Œª2 )

2(1 ‚àí Œª2 )

‚é§ ‚é¢
w 1, j+1
‚é¢
‚é¢ w 2, j+1 ‚é• ‚é¢
‚é¢
‚é• ‚é¢
‚é¢
‚é•=‚é¢
..
‚é£
‚é¶ ‚é¢
.
‚é¢
‚é¢
w m‚àí1, j+1
‚é£

Œª2


/DVHFXDFLRQHV  \  LPSOLFDQTXHHO j 1 √©simoSDVRUHTXLHUHYDORUHVGHVde el j-√©simo y ( j 2 √©simoSDVRV &RQVXOWHOD√ÄJXUD (VWRSURGXFHXQSHTXHxR
SUREOHPDLQLFLDOSRUTXHORVYDORUHVGHj 5HVWiQGDGRVSRUODHFXDFLyQ  SHURORV
valores para j 5QHFHVDULRVHQODHFXDFLyQ  SDUDFDOFXODU w i,2 , GHEHQREWHQHUVHD
SDUWLUGHODFRQGLFLyQGHYHORFLGDGLQLFLDO

‚àÇu
(x, 0) = g(x),
‚àÇt

0 ‚â§ x ‚â§ l.

Figura 12.12
t
t j‚´π1
tj
t j‚´∫1

x
x x x
x

l

xi
x i‚´∫1 x i‚´π1

x

8QHQIRTXHHVUHHPSOD]DU‚àÇu/‚àÇtPHGLDQWHXQDDSUR[LPDFLyQGHGLIHUHQFLDVSURJUHVLYD

u(xi , t1 ) ‚àí u(xi , 0) k ‚àÇ 2 u
‚àÇu
(xi , ŒºÃÉi ),
(xi , 0) =
‚àí
‚àÇt
k
2 ‚àÇt 2



SDUDDOJ~QŒºÃÉi en (0, t1 )$OUHVROYHUSDUDu(xi , t1 ) HQODHFXDFLyQREWHQHPRV

u(xi , t1 ) = u(xi , 0) + k

‚àÇu
k2 ‚àÇ 2u
(xi , ŒºÃÉi )
(xi , 0) +
‚àÇt
2 ‚àÇt 2

= u(xi , 0) + kg(xi ) +

k2 ‚àÇ 2u
(xi , ŒºÃÉi ).
2 ‚àÇt 2

$OERUUDUHOWpUPLQRGHWUXQFDPLHQWRREWHQHPRVODDSUR[LPDFLyQ

w i,1 = w i,0 + kg(xi ), para cada i = 1, . . . , m ‚àí 1.



6LQHPEDUJRHVWDDSUR[LPDFLyQWLHQHHUURUGHWUXQFDPLHQWRGHV√≥lo O(k), mientras el error
GHWUXQFDPLHQWRHQODHFXDFLyQ  HVO(k 

12.3 Ecuaciones diferenciales parciales hiperb√≥licas

565

Mejora de la aproximaci√≥n inicial
3DUDREWHQHUXQDPHMRUDSUR[LPDFLyQSDUDu(xi , 0), expanda u(xi , t1 )HQXQVHJXQGRSROLQRmio Maclaurin en t(QWRQFHV

u(xi , t1 ) = u(xi , 0) + k

‚àÇu
k2 ‚àÇ 2u
k3 ‚àÇ 3u
(xi , 0) +
(x
,
0)
+
(xi , ŒºÃÇi ),
i
‚àÇt
2 ‚àÇt 2
6 ‚àÇt 3

SDUDDOJXQDVŒºÃÇi en (0, t1 ). 6Lf 0 existe, entonces

‚àÇ 2u
‚àÇ 2u
d2 f
(xi , 0) = Œ± 2 2 (xi , 0) = Œ± 2 2 (xi ) = Œ± 2 f (xi )
2
‚àÇt
‚àÇx
dx
y
u(xi , t1 ) = u(xi , 0) + kg(xi ) +

Œ±2k 2
k3 ‚àÇ 3u
(xi , ŒºÃÇi ).
f (xi ) +
2
6 ‚àÇt 3

(VWRSURGXFHXQDDSUR[LPDFLyQFRQHUURUO(k ):

w i1 = w i0 + kg(xi ) +

Œ±2k 2
f (xi ).
2

6L f ‚àà C 4 [0, 1] pero f (xi ) QR HVWi GLVSRQLEOH IiFLOPHQWH SRGHPRV XVDU OD HFXDFLyQ GH
GLIHUHQFLDVHQODHFXDFLyQ  SDUDHVFULELU

f (xi ) =

f (xi+1 ) ‚àí 2 f (xi ) + f (xi‚àí1 ) h 2 (4)
‚àí
f (ŒæÃÉi ),
h2
12

SDUDDOJXQDVŒæÃÉi en (xi‚àí1 , xi+1 ). (VWRLPSOLFDTXH

u(xi , t1 ) = u(xi , 0) + kg(xi ) +

k 2Œ±2
[ f (xi+1 ) ‚àí 2 f (xi ) + f (xi‚àí1 )] + O(k 3 + h 2 k 2 ).
2h 2

3XHVWRTXH Œª = kŒ±/ h, SRGHPRVHVFULELUHVWRFRPR

u(xi , t1 ) = u(xi , 0) + kg(xi ) +
= (1 ‚àí Œª2 ) f (xi ) +

Œª2
[ f (xi+1 ) ‚àí 2 f (xi ) + f (xi‚àí1 )] + O(k 3 + h 2 k 2 )
2

Œª2
Œª2
f (xi+1 ) +
f (xi‚àí1 ) + kg(xi ) + O(k 3 + h 2 k 2 ).
2
2

3RUORWDQWRODHFXDFLyQGHGLIHUHQFLDV

w i,1 = (1 ‚àí Œª2 ) f (xi ) +

Œª2
Œª2
f (xi+1 ) +
f (xi‚àí1 ) + kg(xi ),
2
2



se puede w i,1 usar para encontrar i = 1, 2, . . . , m‚àí1. 3DUDGHWHUPLQDUDSUR[LPDFLRQHVVXEVLJXLHQWHVXVDPRVHOVLVWHPDHQODHFXDFLyQ  
(ODOJRULWPRXVDODHFXDFLyQ  SDUDDSUR[LPDU w i,1 , DSHVDUGHTXHODHFXDFLyQ  WDPELpQSRGUtDXVDUVH6HVXSRQHTXHH[LVWHXQDFRWDVXSHULRUSDUDHOYDORUGHt
TXHVHSXHGHXWLOL]DUHQODWpFQLFDGHGHWHQFLyQ\TXHk = T /N , donde NWDPELpQHVWiGDGD

566

CAP√çTULO 12

ALGORITMO

12.4

Soluciones num√©ricas para ecuaciones diferenciales parciales

Diferencias Ô¨Ånitas para la ecuaci√≥n de onda
3DUDDSUR[LPDUODVROXFLyQGHODHFXDFLyQGHRQGD
2
‚àÇ 2u
2‚àÇ u
(x,
t)
‚àí
Œ±
(x, t) = 0,
‚àÇt 2
‚àÇx2

0 < x < l,

0 < t < T,

VXMHWDODVFRQGLFLRQHVGHIURQWHUD

u(0, t) = u(l, t) = 0,

0 < t < T,

y las condiciones iniciales

u(x, 0) = f (x), y
ENTRADA
SALIDA

‚àÇu
(x, 0) = g(x),
‚àÇt

para

0 ‚â§ x ‚â§ l,

extremo lWLHPSRPi[LPRTFRQVWDQWHŒ±HQWHURVm ‚â• 2, N ‚â• 2.

aproximaciones w i, j para u(xi , t j ) para cada i = 0, . . . , m y j = 0, . . . , N .

Paso 1 Determine h = l/m;
k = T /N ;
Œª = kŒ±/ h.
Paso 2 Para j = 1, . . . , N determine w 0, j = 0;
w m, j = 0;
Paso 3 Determine w 0,0 = f (0);
w m,0 = f (l).
Paso 4 Para i = 1, . . . , m ‚àí 1 (Inicialice para t = 0 y t = k.)
determine w i,0 = f (i h);
Œª2
w i,1 = (1 ‚àí Œª2 ) f (i h) + [ f ((i + 1)h) + f ((i ‚àí 1)h)] + kg(i h).
2
Paso 5 Para j = 1, . . . , N ‚àí 1 (Realice la multiplicaci√≥n de matriz. )
para i = 1, . . . , m ‚àí 1
determine w i, j+1 = 2(1 ‚àí Œª2 )w i, j + Œª2 (w i+1, j + w i‚àí1, j ) ‚àí w i, j‚àí1 .
Paso 6 Para j = 0, . . . , N
determine t = jk;
para i = 0, . . . , m
determine x = i h;
SALIDA (x, t, w i, j ).
Paso 7 PARE.

Ejemplo 1

(El procedimiento est√° completo.)

$SUR[LPHODVROXFLyQGHOSUREOHPDKLSHUEyOLFR

‚àÇ 2u
‚àÇ 2u
(x, t) ‚àí 4 2 (x, t) = 0,
2
‚àÇt
‚àÇx

0 < x < 1,

0 < t,

con condiciones de frontera

u(0, t) = u(1, t) = 0, para 0 < t,
y condiciones iniciales

u(x, 0) = sen(œÄ x), 0 ‚â§ x ‚â§ 1,

y

‚àÇu
(x, 0) = 0,
‚àÇt

0 ‚â§ x ‚â§ 1,

12.3 Ecuaciones diferenciales parciales hiperb√≥licas

567

Usando h 5\k 5&RPSDUHORVUHVXOWDGRVFRQODVROXFLyQH[DFWD

u(x, t) = sen œÄ x cos 2œÄ t.
Soluci√≥n $OVHOHFFLRQDUh 5\k 5GDŒª m 5\N 56HOHFFLRQDUHPRVXQ
tiempo m√°ximo T 5\DSOLFDUHPRVHODOJRULWPRGHGLIHUHQFLDV√ÄQLWDV(VWRSURGXFH
las aproximaciones w i,N a u(0.1i, 1) para i 5   (VWRVUHVXOWDGRVVHPXHVWUDQHQ
ODWDEOD\VRQFRUUHFWRVSDUDORVOXJDUHVGDGRV

Tabla 12.6
xi

w i,20

0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0

0.0000000000
0.3090169944
0.5877852523
0.8090169944
0.9510565163
1.0000000000
0.9510565163
0.8090169944
0.5877852523
0.3090169944
0.0000000000

/RVUHVXOWDGRVGHOHMHPSORHUDQPX\H[DFWRVPiVGHORTXHHOHUURUGHWUXQFDPLHQWR
O (k + h   QRV SHUPLWLUtD FUHHU (VWR HV SRUTXH OD YHUGDGHUD VROXFLyQ SDUD OD HFXDFLyQ HV
LQ√ÄQLWDPHQWHGLIHUHQFLDEOH&XDQGRpVWHHVHOFDVRODVHULHGH7D\ORUGD

u(xi+1 , t j ) ‚àí 2u(xi , t j ) + u(xi‚àí1 , t j )
h2
=

‚àÇ 2u
h4 ‚àÇ 6u
h2 ‚àÇ 4u
(xi , t j ) + 2
(xi , t j ) +
(xi , t j ) + ¬∑ ¬∑ ¬∑
2
4
‚àÇx
4! ‚àÇ x
6! ‚àÇ x 6

y
u(xi , t j+1 ) ‚àí 2u(xi , t j ) + u(xi , t j‚àí1 )
k2
k2 ‚àÇ 4u
‚àÇ 2u
h4 ‚àÇ 6u
(x
,
t
)
+
2
(x
,
t
)
+
(xi , t j ) + ¬∑ ¬∑ ¬∑ .
i
j
i
j
‚àÇt 2
4! ‚àÇt 4
6! ‚àÇt 6

=

3XHVWRTXHu(x, t ) satisface la ecuaci√≥n diferencial parcial,

u(xi , t j+1 ) ‚àí 2u(xi , t j ) + u(xi , t j‚àí1 )
u(xi+1 , t j ) ‚àí 2u(xi , t j ) + u(xi‚àí1 , t j )
‚àí Œ±2
k2
h2
=2

4
‚àÇ 4u
2 2‚àÇ u
(x
,
t
)
‚àí
Œ±
h
(xi , t j )
i
j
‚àÇt 4
‚àÇx4

1
4!

k2

1
6!

k4

+

6
‚àÇ 6u
2 4‚àÇ u
(x
,
t
)
‚àí
Œ±
h
(xi , t j ) + ¬∑ ¬∑ ¬∑ .
i
j
‚àÇt 6
‚àÇx6



6LQHPEDUJRDOGLIHUHQFLDUODHFXDFLyQGHRQGDREWHQHPRV

k2

‚àÇ 4u
‚àÇ2
‚àÇ 2u
‚àÇ 2 ‚àÇ 2u
(xi , t j ) = k 2 2 Œ± 2 2 (xi , t j ) = Œ± 2 k 2 2
(xi , t j )
4
‚àÇt
‚àÇt
‚àÇx
‚àÇx
‚àÇt 2
= Œ±2k 2

‚àÇ2
‚àÇ 2u
‚àÇ 4u
Œ± 2 2 (xi , t j ) = Œ± 4 k 2 4 (xi , t j ),
2
‚àÇx
‚àÇx
‚àÇx

\REVHUYDPRVTXHSXHVWRTXHŒª2 = (Œ± 2 k 2 / h 2 ) = 1, tenemos

‚àÇ 4u
Œ±2 2 2
‚àÇ 4u
1 2 ‚àÇ 4u
[Œ± k ‚àí h 2 ] 4 (xi , t j ) = 0.
k
(xi , t j ) ‚àí Œ± 2 h 2 4 (xi , t j ) =
4
4!
‚àÇt
‚àÇx
4!
‚àÇx
$OFRQWLQXDUGHHVWDIRUPDWRGRVORVWpUPLQRVHQHOODGRGHUHFKRGHODHFXDFLyQ  VRQ
ORTXHLPSOLFDTXHHOHUURUGHWUXQFDPLHQWRORFDOHV/RV~QLFRVHUURUHVHQHOHMHPSOR
VRQORVTXHVHGHEHQDODDSUR[LPDFLyQGHwi\DOGHUHGRQGHR
&RPRHQHOFDVRGHOPpWRGRGHGLIHUHQFLDVSURJUHVLYDVSDUDODHFXDFLyQGHFDORUHOPpWRGRGHGLIHUHQFLDV√ÄQLWDVH[SOtFLWDSDUDODHFXDFLyQGHRQGDWLHQHSUREOHPDVGHHVWDELOLGDG
'HKHFKRHVQHFHVDULRTXHŒª = Œ±k/ h ‚â§ 1 SDUDTXHHOPpWRGRVHDHVWDEOH &RQVXOWH>,.@
S (OPpWRGRH[SOtFLWRGDGRHQHODOJRULWPRFRQŒª ‚â§HVO(h  1 k  FRQYHUJHQWH
si f y gVRQVX√ÄFLHQWHPHQWHGLIHUHQFLDEOHV3DUDYHUL√ÄFDUORFRQVXOWH>,.@S

568

CAP√çTULO 12

Soluciones num√©ricas para ecuaciones diferenciales parciales

$SHVDUGHTXHQRORVDQDOL]DUHPRVH[LVWHQPpWRGRVLPSOtFLWRVTXHVRQLQFRQGLFLRQDOPHQWHHVWDEOHV8QDQiOLVLVGHHVWRVPpWRGRVVHSXHGHHQFRQWUDUHQ>$P@S>0L@R>6P%@
La secci√≥n Conjunto de ejercicios 12.3 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

12.4 Una introducci√≥n al m√©todo de elementos Ô¨Ånitos
/RVHOHPHQWRV√ÄQLWRV
FRPHQ]DURQHQODGpFDGD
GHHQODLQGXVWULDGH
ODVDHURQDYHV$OXVRGHODV
WpFQLFDVOHVLJXLyXQDUWtFXORGH
7XUQHU&ORXJK0DUWLQ\7RSS
>7&07@TXHIXHSXEOLFDGRHQ
/DDPSOLDDSOLFDFLyQGH
ORVPpWRGRVUHTXHUtDJUDQGHV
UHFXUVRVFRPSXWDFLRQDOHVTXH
QRHVWXYLHURQGLVSRQLEOHVKDVWD
SULQFLSLRVGHODGpFDGDGH

(OPpWRGRGHHOHPHQWRV√ÄQLWRVHVVLPLODUDOPpWRGRGH5D\OHLJK5LW]SDUDDSUR[LPDUOD
VROXFLyQ GH ORV SUREOHPDV GH YDORU HQ OD IURQWHUD SDUD GRV SXQWRV TXH VH SUHVHQWy HQ
ODVHFFLyQ)XHGHVDUUROODGRRULJLQDOPHQWHSDUDVXXVRHQLQJHQLHUtDFLYLOSHURDKRUD
VRQ~WLOHVHQODDSUR[LPDFLyQGHODVVROXFLRQHVSDUDODVHFXDFLRQHVGLIHUHQFLDOHVSDUFLDOHV
TXHVXUJHQHQWRGDVODViUHDVGHODVPDWHPiWLFDVDSOLFDGDV
8QDYHQWDMDTXHWLHQHHOPpWRGRGHHOHPHQWRV√ÄQLWRVVREUHORVPpWRGRVGHGLIHUHQFLDV
√ÄQLWDV HV OD UHODWLYD IDFLOLGDG FRQ OD TXH VH PDQHMDQ ODV FRQGLFLRQHV GH IURQWHUD GHO SUREOHPD0XFKRVSUREOHPDVItVLFRVWLHQHQFRQGLFLRQHVGHIURQWHUDTXHLPSOLFDQGHULYDGDV\
IURQWHUDVGHIRUPDVLUUHJXODUHV'LFKDVFRQGLFLRQHVVRQGLItFLOHVGHPDQHMDUFRQ t√©cnicas de
diferenciaV√ÄQLWDVSRUTXHFDGDFRQGLFLyQHQODIURQWHUDTXHLPSOLFDXQDGHULYDGDVHGHEH
aproximar mediante un cociente de diferencias en los puntos de cuadr√≠cula y una forma
LUUHJXODUGHODIURQWHUDKDFHTXHFRORFDUORVSXQWRVGHODFXDGUtFXODVHDGLItFLO(OPpWRGR
GHHOHPHQWRV√ÄQLWRVLQFOX\HODVFRQGLFLRQHVGHIURQWHUDFRPRLQWHJUDOHVHQXQDIXQFLyQTXH
VHHVWiPLQLPL]DQGRSRUORTXHHOSURFHGLPLHQWRGHFRQVWUXFFLyQHVLQGHSHQGLHQWHGHODV
FRQGLFLRQHVGHIURQWHUDSDUWLFXODUHVGHOSUREOHPD
(QQXHVWURDQ√°lisis, consideramos la ecuaci√≥n diferencial parcial

‚àÇ
‚àÇx

p(x, y)

‚àÇu
‚àÇx

+

‚àÇ
‚àÇu
q(x, y)
‚àÇy
‚àÇy

+ r (x, y)u(x, y) = f (x, y),



con (x, y) ‚àà D, donde DHVXQDUHJLyQSODQDFRQIURQWHUDS
/DVFRQGLFLRQHVGHIURQWHUDGHODIRUPD

u(x, y) = g(x, y)



se imponen en una parte, SGHODIURQWHUD(QHOUHVWRODIURQWHUDSVHUHTXLHUHODVROXFLyQ
u (x, y) para satisfacer

p(x, y)

‚àÇu
‚àÇu
(x, y) cos Œ∏1 + q(x, y) (x, y) cos Œ∏2 + g1 (x, y)u(x, y) = g2 (x, y),
‚àÇx
‚àÇy


donde Œ∏ y Œ∏VRQORViQJXORVGHGLUHFFLyQGHODQRUPDOH[WHULRUSDUDODIURQWHUDHQHOSXQWR
(x, y  &RQVXOWHOD√ÄJXUD 
Figura 12.13

y

Recta tangente
u2

Recta normal
u1

x

12.4

Una introducci√≥n al m√©todo de elementos Ô¨Ånitos

569

/RVSUREOHPDVItVLFRVHQODViUHDVGHODPHFiQLFDVyOLGD\ODHODVWLFLGDGWLHQHQHFXDFLRQHV GLIHUHQFLDOHV SDUFLDOHV UHODFLRQDGDV VLPLODUHV D OD HFXDFLyQ   (Q JHQHUDO OD
VROXFLyQGHOSUREOHPDGHHVWHWLSRPLQLPL]DFLHUWDIXQFLyQTXHLPSOLFDLQWHJUDOHVVREUHXQD
FODVHGHIXQFLRQHVGHWHUPLQDGDVSRUHOSUREOHPD
6XSRQJD TXH p, q, r y f son todas continuas en D ‚à™ S, \ TXH q tiene primeras derivadas parciales continuas y g y g son continuas en S 6XSRQJD DGHPiV TXH p(x, y)
0, q(x, y) > 0, r (x, y) ‚â§ 0, y g1 (x, y) > 0. (QWRQFHVXQDVROXFLyQGHODHFXDFLyQ  
VRODPHQWHPLQLPL]DODIXQFLyQ

I [w] =
+

2

D

1
‚àÇw
p(x, y)
2
‚àÇx

+ q(x, y)

‚àÇw
‚àÇy

S2

1
‚àíg2 (x, y)w + g1 (x, y)w 2
2

dS

2

‚àí r (x, y)w 2 + f (x, y)w

dx dy


VREUHWRGDVODVIXQFLRQHVGRVYHFHVFRQWLQXDPHQWHGLIHUHQFLDEOHVwTXHVDWLVIDFHQODHFXDFLyQ  HQS(OPpWRGRGHHOHPHQWRV√ÄQLWRVDSUR[LPDHVWDVROXFLyQDOPLQLPL]DUOD
funci√≥n IVREUHXQDFODVHPiVSHTXHxDGHIXQFLRQHVMXVWRFRPRHOPpWRGRGH5D\OHLJK5LW]
SDUDHOSUREOHPDGHYDORUHQODIURQWHUDFRQVLGHUDGRHQODVHFFLyQ

DeÔ¨Ånici√≥n de elementos
(OSULPHUSDVRHVGLYLGLUODUHJLyQHQXQQ~PHUR√ÄQLWRGHVHFFLRQHVRHOHPHQWRVGHXQD
IRUPDUHJXODU\DVHDUHFWiQJXORVRWULiQJXORV &RQVXOWHOD√ÄJXUD
Figura 12.14

(QJHQHUDOHOFRQMXQWRGHIXQFLRQHVTXHVHXVDSDUDODDSUR[LPDFLyQHVXQFRQMXQWRGH
SROLQRPLRVSRUWUDPRVGHJUDGR√ÄMRHQx y y\ODDSUR[LPDFLyQUHTXLHUHTXHORVSROLQRPLRV
VHUHFRQVWUX\DQGHWDOIRUPDTXHODIXQFLyQUHVXOWDQWHVHDFRQWLQXDFRQXQDSULPHUDRVHJXQGDGHULYDGDLQWHJUDEOHRFRQWLQXDHQWRGDODUHJLyQ/RVSROLQRPLRVGHWLSROLQHDOHQx y y,

œÜ(x, y) = a + bx + cy,
VH XVDQ FRP~QPHQWH FRQ HOHPHQWRV WULDQJXODUHV PLHQWUDV TXH ORV SROLQRPLRV GH WLSR
ELOLQHDOHQx y y,

œÜ(x, y) = a + bx + cy + d x y,
VHXWLOL]DQFRQHOHPHQWRVUHFWDQJXODUHV
6XSRQJDTXHODUHJLyQDVHKDVXEGLYLGLGRHQHOHPHQWRVWULDQJXODUHV(OFRQMXQWRGH
WULiQJXORVVHGHQRWDFRPRD\ORVYpUWLFHVGHHVWRVWULiQJXORVUHFLEHQHOQRPEUHGHnodos
(OPpWRGREXVFDXQDDSUR[LPDFLyQGHODIRUPD
m

œÜ(x, y) =

Œ≥i œÜi (x, y),
i=1



570

CAP√çTULO 12

Soluciones num√©ricas para ecuaciones diferenciales parciales

donde œÜ1 , œÜ2 , . . . , œÜm son polinomios lineales por tramos linealmente independientes y
Œ≥1 , Œ≥2 , . . . , Œ≥mVRQFRQVWDQWHV$OJXQDVGHHVWDVFRQVWDQWHVSRUHMHPSORŒ≥n+1 , Œ≥n+2 , . . . , Œ≥m ,
VHXVDQSDUDJDUDQWL]DUTXHODFRQGLFLyQGHIURQWHUD

œÜ(x, y) = g(x, y),
VHVDWLVIDJDHQS y las constantes restantes Œ≥1 , Œ≥2 , . . . , Œ≥n , VHXVDQSDUDPLQLPL]DUODIXQm
ci√≥n I
i=1 Œ≥i œÜi .
5HHPSOD]DQGRODIRUPDGHœÜ(x, y)GDGDHQODHFXDFLyQ  SDUDw en la ecuaci√≥n
 SURGXFH
m

I [œÜ] = I

Œ≥i œÜi
i=1

=

D

m

1
p(x, y)
2

Œ≥i
i=1

m

Œ≥i œÜi (x, y)

+ f (x, y)

i=1

S2

i=1

2
‚àÇœÜi
(x, y)
‚àÇy

Œ≥i œÜi (x, y) d y d x
i=1

m

+

Œ≥i

m

2

‚àí r (x, y)

m

2
‚àÇœÜi
(x, y) + q(x, y)
‚àÇx

‚àí g2 (x, y)

1
Œ≥i œÜi (x, y) + g1 (x, y)
2
i=1

m

2

Œ≥i œÜi (x, y)

d S.



i=1

Considere I como una funci√≥n de Œ≥1 , Œ≥2 , . . . , Œ≥n . 3DUD TXH VH SUHVHQWH XQ PtQLPR
GHEHPRVWHQHU

‚àÇI
= 0, para cada j = 1, 2, . . . , n.
‚àÇŒ≥ j
'HULYDQGR  REWHQHPRV
m

‚àÇI
=
‚àÇŒ≥ j

p(x, y)

D

Œ≥i
i=1

m

Œ≥i

+ q(x, y)
i=1

‚àÇœÜ j
‚àÇœÜi
(x, y)
(x, y)
‚àÇx
‚àÇx

‚àÇœÜ j
‚àÇœÜi
(x, y)
(x, y)
‚àÇy
‚àÇy

m

Œ≥i œÜi (x, y)œÜ j (x, y) + f (x, y)œÜ j (x, y) d x d y

‚àí r (x, y)
i=1

m

+

‚àí g2 (x, y)œÜ j (x, y) + g1 (x, y)

S2

Œ≥i œÜi (x, y)œÜ j (x, y) d S,
i=1

por lo que
m

0=

D

i=1

p(x, y)

‚àÇœÜ j
‚àÇœÜi
‚àÇœÜ j
‚àÇœÜi
(x, y)
(x, y) + q(x, y)
(x, y)
(x, y)
‚àÇx
‚àÇx
‚àÇy
‚àÇy

‚àí r (x, y)œÜi (x, y)œÜ j (x, y) d x d y
+
+

S2

g1 (x, y)œÜi (x, y)œÜ j (x, y) d S Œ≥i

D

f (x, y)œÜ j (x, y) d x d y ‚àí

S2

g2 (x, y)œÜ j (x, y) d S,

12.4

571

Una introducci√≥n al m√©todo de elementos Ô¨Ånitos

para cada j = 1, 2, . . . , n. (VWHFRQMXQWRGHHFXDFLRQHVVHSXHGHHVFULELUFRPRXQVLVWHPD
lineal:

Ac = b,
donde c = (Œ≥1 , . . . , Œ≥n )t y donde las matrices n 3 n A = (Œ±i j ) y b = (Œ≤1 , . . . , Œ≤n )t est√°n
GH√ÄQLGDVSRU

Œ±i j =

D

p(x, y)

‚àÇœÜ j
‚àÇœÜi
‚àÇœÜ j
‚àÇœÜi
(x, y)
(x, y) + q(x, y)
(x, y)
(x, y)
‚àÇx
‚àÇx
‚àÇy
‚àÇy

‚àí r (x, y)œÜi (x, y)œÜ j (x, y) d x d y +

S2



g1 (x, y)œÜi (x, y)œÜ j (x, y) d S,

para cada i = 1, 2, . . . , n y j = 1, 2, . . . , m,
m

Œ≤i = ‚àí

D

f (x, y)œÜi (x, y) d x d y +

S2

g2 (x, y)œÜi (x, y) d S ‚àí

Œ±ik Œ≥k ,



k=n+1

para cada i = 1, . . . , n.
/D VHOHFFLyQ SDUWLFXODU GH ODV IXQFLRQHV EDVH HV LPSRUWDQWH SRUTXH D PHQXGR OD VHOHFFLyQDGHFXDGDSXHGHFUHDUODPDWUL]GH√ÄQLGDSRVLWLYD\GHEDQGDA3DUDHOSUREOHPDGH
VHJXQGRRUGHQ  VXSRQHPRVTXHDHVSROLJRQDOGHWDOIRUPDTXHD 5 D\TXHS es
XQFRQMXQWRFRQWLJXRGHOtQHDVUHFWDV

Triangulaci√≥n de la regi√≥n
3DUD FRPHQ]DU HO SURFHGLPLHQWR GLYLGLPRV OD UHJLyQ D HQ XQ FRQMXQWR GH WULiQJXORV
T1 , T2 , . . . , TM , con el i-√©simoWULiQJXORFRQWUHVYpUWLFHVRQRGRVGHQRWDGRV
(i)
V j(i) = x (i)
j , y j , para j = 1, 2, 3.

3DUDVLPSOL√ÄFDUODQRWDFLyQHVFULELPRVV j(i) simplemente como V j = (x j , y j )FXDQGRWUDEDMDPRVFRQHOWULiQJXOR√ÄMRTi&RQFDGDYpUWLFHV j, asociamos el polinomio lineal

N (i)
j (x, y) ‚â° N j (x, y) = a j + b j x + c j y,

donde

N (i)
j (x k , yk ) =

1,
0,

si j = k,
si j = k.

(VWRSURGXFHVLVWHPDVOLQHDOHVGHODIRUPD
‚é°
‚é§‚é°
‚é§ ‚é° ‚é§
1 x1 y1
aj
0
‚é£ 1 x2 y2 ‚é¶ ‚é£ b j ‚é¶ = ‚é£ 1 ‚é¶ ,
0
1 x3 y3
cj
FRQHOHOHPHQWRSUHVHQWHHQODj-√©sima√ÄODHQHOYHFWRUGHODGHUHFKD DTXtj 5 
6HDQ E 1 , . . . , E nHWLTXHWDVGHORVQRGRVHQ D ‚à™ S. Con cada nodo Ek, asociamos una
funci√≥n œÜkTXHHVOLQHDOHQFDGDWULiQJXORWLHQHHOYDORUHQEk\HVHQFDGDXQRGHORV
RWURVQRGRV(VWDVHOHFFLyQKDFHTXHœÜk sea id√©ntico a N (i)
j HQHOWULiQJXORTi cuando el nodo
(i)
Ek es el v√©rtice denotado V j .
Ilustraci√≥n

6XSRQJDTXHXQSUREOHPDGHHOHPHQWRV√ÄQLWRVFRQWLHQHORVWULiQJXORVT y T mostrados en
OD√ÄJXUD

572

CAP√çTULO 12

Soluciones num√©ricas para ecuaciones diferenciales parciales

Figura 12.15
y
(‚´∫1, 2)

V (1)
2

2

V (1)
1
(1, 1)
V (2)
1

T1

T2
V (1)
3
‚´∫1

V (2)
3
V (2)
2

1

x

/DIXQFLyQOLQHDO N1(1) (x, y)DVXPHHOYDORUHQ  \HOYDORUWDQWRHQ  FRPRHQ
(2 VDWLVIDFH

a1(1) + b1(1) (1) + c1(1) (1) = 1,
a1(1) + b1(1) (‚àí1) + c1(1) (2) = 0,
y

a1(1) + b1(1) (0) + c1(1) (0) = 0.
/DVROXFLyQDHVWHVLVWHPDHVa1(1) = 0 , b1(1) = 23 , y c1(1) = 13 , SRUORTXH

N1(1) (x, y) =

2
1
x + y.
3
3

'HPDQHUDVLPLODUODIXQFLyQOLQHDON1(2) (x, y)WRPDHOYDORUHQ  \HOYDORUWDQWRHQ
 FRPRHQ  VDWLVIDFH

a1(2) + b1(2) (1) + c1(2) (1) = 1,
a1(2) + b1(2) (0) + c1(2) (0) = 0,
y
a1(2) + b1(2) (1) + c1(2) (0) = 0.
(VWRLPSOLFDTXH a1(2) = 0, b1(2) = 0, y c1(2) = 1. Como consecuencia, N1(2) (x, y) = y. Note

TXHN1(1) (x, y) = N1(2) (x, y)VREUHODIURQWHUDFRP~QGHT y TSXHVWRTXHy 5 x

&RQVLGHUHOD√ÄJXUDODSDUWHL]TXLHUGDVXSHULRUGHODUHJLyQPRVWUDGDHQOD√ÄJXUD
1RVRWURVJHQHUDUHPRVODVHQWUDGDVHQODPDWUL]ATXHFRUUHVSRQGHDORVQRGRVPRVWUDGRVHQHVWD√ÄJXUD

12.4

Una introducci√≥n al m√©todo de elementos Ô¨Ånitos

573

Figura 12.16
E2
E1

T1
T2

E3

E4

3DUD VLPSOLFLGDG VXSRQHPRV TXH E es uno de los nodos en S, donde la condici√≥n
de frontera u(x, y) 5 g(x, y HVLPSXHVWD/DUHODFLyQHQWUHORVQRGRV\ORVYpUWLFHVGHORV
WULiQJXORVSDUDHVWDSDUWHHV

E 1 = V3(1) = V1(2) , E 4 = V2(2) , E 3 = V2(1) = V3(2) , y

E 2 = V1(1) .

3XHVWRTXHœÜ y œÜVRQDPERVGLIHUHQWHVDFHURHQT y T, las entradas Œ± 5 Œ± se calculan
por medio de

Œ±1,3 =

D

=

p

‚àÇœÜ1 ‚àÇœÜ3
‚àÇœÜ1 ‚àÇœÜ3
+q
‚àí r œÜ1 œÜ3 d x d y
‚àÇx ‚àÇx
‚àÇy ‚àÇy

p

‚àÇœÜ1 ‚àÇœÜ3
‚àÇœÜ1 ‚àÇœÜ3
+q
‚àí r œÜ1 œÜ3 d x d y
‚àÇx ‚àÇx
‚àÇy ‚àÇy

T1

+

T2

p

‚àÇœÜ1 ‚àÇœÜ3
‚àÇœÜ1 ‚àÇœÜ3
+q
‚àí r œÜ1 œÜ3 d x d y.
‚àÇx ‚àÇx
‚àÇy ‚àÇy

(QHOWULiQJXORT,

œÜ1 (x, y) = N3(1) (x, y) = a3(1) + b3(1) x + c3(1) y
y
œÜ3 (x, y) = N2(1) (x, y) = a2(1) + b2(1) x + c2(1) y,
SRUORTXHSDUDWRGDVODV x, y),

‚àÇœÜ1
= b3(1) ,
‚àÇx

‚àÇœÜ1
= c3(1) ,
‚àÇy

‚àÇœÜ3
= b2(1) ,
‚àÇx

y

‚àÇœÜ3
= c2(1) .
‚àÇy

'HLJXDOIRUPDHQT,

œÜ1 (x, y) = N1(2) (x, y) = a1(2) + b1(2) x + c1(2) y
y
œÜ3 (x, y) = N3(2) (x, y) = a3(2) + b3(2) x + c3(2) y,

574

CAP√çTULO 12

Soluciones num√©ricas para ecuaciones diferenciales parciales

SRUORTXHWRGDVODV x, y),

‚àÇœÜ1
= b1(2) ,
‚àÇx

‚àÇœÜ1
= c1(2) ,
‚àÇy

‚àÇœÜ3
= b3(2) , y
‚àÇx

‚àÇœÜ3
= c3(2) .
‚àÇy

3RUORWDQWR

Œ±1,3 = b3(1) b2(1)
‚àí

T1

T1

T2

T1

q d x dy

r a3(1) + b3(1) x + c3(1) y a2(1) + b2(1) x + c2(1) y d x d y

+ b1(2) b3(2)
‚àí

p d x dy + c3(1) c2(1)

T2

p d x dy + c1(2) c3(2)

T2

q d x dy

r a1(2) + b1(2) x + c1(2) y a3(2) + b3(2) x + c3(2) y d x d y.

7RGDVODVLQWHJUDOHVGREOHVVREUHDUHGXFHQODVGREOHVLQWHJUDOHVVREUHORVWULiQJXORV(O
SURFHGLPLHQWRQRUPDOHVFDOFXODUWRGDVODVLQWHJUDOHVSRVLEOHVVREUHORVWULiQJXORV\DFXPX
larlas en la entrada correcta Œ±i j en A'HLJXDOIRUPDODVGREOHVLQWHJUDOHVGHODIRUPD
D

f (x, y)œÜi (x, y) d x d y

VHFDOFXODQVREUHWULiQJXORV\GHVSXpVVHDFXPXODQHQODHQWUDGDFRUUHFWDŒ≤i del vector b
3RUHMHPSORSDUDGHWHUPLQDUŒ≤, necesitamos

‚àí

D

f (x, y)œÜ1 (x, y) d x d y = ‚àí
‚àí

T1

T2

f (x, y) a3(1) + b3(1) x + c3(1) y d x d y
f (x, y) a1(2) + b1(2) x + c1(2) y d x d y.

3XHVWRTXHE es un v√©rtice tanto de T como de T, la parte de Œ≤HVWiFRQWULEXLGDSRUœÜ,
UHVWULQJLGDDT y el resto por œÜUHVWULQJLGDSRUT$GHPiVORVQRGRVTXHVHHQFXHQWUDQHQ
SWLHQHQLQFRUSRUDGDVLQWHJUDOHVOLQHDOHVDVXVHQWUDGDVHQA y b
(O DOJRULWPR  UHDOL]D HO PpWRGR GH HOHPHQWRV √ÄQLWRV HQ XQD HFXDFLyQ GLIHUHQFLDO
HOtSWLFDGHVHJXQGRRUGHQ(ODOJRULWPRHVWDEOHFHLQLFLDOPHQWHWRGRVORVYDORUHVGHODPDWUL]
A y el vector bHQ\GHVSXpVUHDOL]DWRGDVODVLQWHJUDFLRQHVHQWRGRVORVWULiQJXORVDxDGH
estos valores a las entradas adecuadas en A y b

ALGORITMO

12.5

M√©todo de elementos Ô¨Ånitos
3DUDDSUR[LPDUODVROXFLyQGHODHFXDFLyQGLIHUHQFLDOSDUFLDO

‚àÇu
‚àÇ
‚àÇu
‚àÇ
p(x, y)
+
q(x, y)
+ r (x, y)u = f (x, y),
‚àÇx
‚àÇx
‚àÇy
‚àÇy

(x, y) ‚àà D

VXMHWDDODVFRQGLFLRQHVGHIURQWHUD

u(x, y) = g(x, y),

(x, y) ‚àà S1

y
p(x, y)

‚àÇu
‚àÇu
(x, y) cos Œ∏1 + q(x, y) (x, y) cos Œ∏2 + g1 (x, y)u(x, y) = g2 (x, y),
‚àÇx
‚àÇy
(x, y) ‚àà S2 ,

12.4

575

Una introducci√≥n al m√©todo de elementos Ô¨Ånitos

donde S1 ‚à™ S2 es la frontera de D y Œ∏  y Œ∏ VRQORViQJXORVGHGLUHFFLyQGHODQRUPDOSDUD
la frontera:

Paso 0 Divida la regi√≥n D en tri√°ngulos T1 , . . . , TM de tal forma que:
T1 , . . . , TK son los tri√°ngulos sin bordes en S1 o S2 ;
(Nota: K = 0 implica que ning√∫n tri√°ngulo es interior para D .)
TK +1 , . . . , TN son tri√°ngulos con al menos un borde en S2 ;
TN +1 , . . . , TM son los tri√°ngulos restantes.
(Nota: M = N implica que todos los tri√°ngulos tienen bordes en S2 .)
Etiquete los tres v√©rtices del tri√°ngulo Ti mediante
x1(i) , y1(i) , x2(i) , y2(i) , y x3(i) , y3(i) .
Etiquete los nodos (vertices) E 1 , . . . , E m donde
E 1 , . . . , E n est√°n en D ‚à™ S2 y E n+1 , . . . , E m est√°n en S1 .
(Nota: n = m implica que S1 no contiene nodos.)

ENTRADA enteros K , N , M, n, m; v√©rtices x1(i) , y1(i) , x2(i) , y2(i) , x3(i) , y3(i)
para cada i = 1, . . . , M; nodos E j para cada j = 1, . . . , m.
xk(i) , yk(i)

(Nota: Todo lo que se necesita es un medio para corresponder a un v√©rtice
para un nodo E j = (x j , y j ).)

(i) (i)
SALIDA constantes Œ≥1 , . . . , Œ≥m ; a (i)
j , b j , c j para cada j = 1, 2 y i = 1, . . . , M.

Paso 1 Para l = n + 1, . . . , m determine Œ≥l = g(xl , yl ).

(Nota: El = (xl , yl ).)

Paso 2 Para i = 1, . . . , n
determine Œ≤i = 0;
para j = 1, . . . , n determine Œ±i, j = 0.
Paso 3 Para i = 1, . . . , M
1
determine

i = det

1
1

a1(i) =
a2(i) =
a3(i) =

x1(i)

y1(i)

x3(i)

y3(i)

x2(i)

y2(i) ;

x2(i) y3(i) ‚àí y2(i) x3(i)
i

x3(i) y1(i) ‚àí y3(i) x1(i)
i

x1(i) y2(i) ‚àí y1(i) x2(i)
i

;

b1(i) =

;

b2(i) =

;

b3(i) =

y2(i) ‚àí y3(i)
i

y3(i) ‚àí y1(i)
i

y1(i) ‚àí y2(i)
i

;

c1(i) =

;

c2(i) =

;

c3(i) =

x3(i) ‚àí x2(i)
i

x1(i) ‚àí x3(i)
i

x2(i) ‚àí x1(i)
i

;
;
;

para j = 1, 2, 3
(i)
(i)
(i)
deÔ¨Åna N (i)
j (x, y) = a j + b j x + c j y.
Paso 4 Para i = 1, . . . , M

(Las integrales en los pasos 4 y 5 se pueden evaluar
por medio de integraci√≥n num√©rica.)

para j = 1, 2, 3
para k = 1, . . . , j

(Calcule todas las integrales dobles sobre los tri√°ngulos.)

(i) (i)
determine z (i)
j,k = b j bk

‚àí
determine H (i)j = ‚àí

(i) (i)
Ti p(x, y) d x d y + c j ck

Ti q(x, y) d x d y

(i)
(i)
Ti r (x, y)N j (x, y)Nk (x, y) d x d y;
Tfi (x, y)N

(i)

j(x, y) d x d y.

Paso 5 Para i = K + 1, . . . , N (Calcule todas las integrales lineales. )
para j = 1, 2, 3
para k = 1, . . . , j

576

CAP√çTULO 12

Soluciones num√©ricas para ecuaciones diferenciales parciales
(i)
determine J j,k
=

determine I j(i) =

S2

S2

(i)
g1 (x, y)N (i)
j (x, y)Nk (x, y) d S;

g2 (x, y)N (i)
j (x, y) d S.

Paso 6 Para i = 1, . . . , M haga los pasos 7‚Äì12. ( Ensamble las integrales sobre cada
tri√°ngulo en el sistema lineal.)
Paso 7 Para k = 1, 2, 3 haga los pasos 8‚Äì12.
Paso 8 Encuentre l de modo que El = xk(i) , yk(i) .
Paso 9 Si k > 1 entonces para j = 1, . . . , k ‚àí 1 haga los pasos 10, 11.
(i)
Paso 10 Encuentre t de modo que E t = x (i)
.
j , yj
Paso 11 Si l ‚â§ n determine
(i)
si t ‚â§ n entonces determine Œ±lt = Œ±lt + z k,
j;
(i)
Œ±tl = Œ±tl + z k,
j

(i)
si no determine Œ≤l = Œ≤l ‚àí Œ≥t z k,
j

si no
(i)
si t ‚â§ n entonces determine Œ≤t = Œ≤t ‚àí Œ≥l z k,
j.
(i)
;
Paso 12 Si l ‚â§ n entonces determine all = Œ±ll + z k,k

Œ≤l = Œ≤l + Hk(i) .

Paso 13 Para i = K + 1, . . . , N haga los pasos 14‚Äì19. ( Ensamble las integrales lineales
en el sistema lineal.)
Paso 14 Para k = 1, 2, 3 haga los pasos 15‚Äì19.
Paso 15 Encuentre l de modo que El = xk(i) , yk(i) .
Paso 16 Si k > 1 entonces para j = 1, . . . , k ‚àí 1 haga los pasos 17, 18.
(i)
Paso 17 Encuentre t de modo que E t = x (i)
.
j , yj

Paso 18 Si l ‚â§ n entonces
si t ‚â§ n entonces determine Œ±lt = Œ±lt + Jk,(i)j ;
Œ±tl = Œ±tl + Jk,(i)j

si no determine Œ≤l = Œ≤l ‚àí Œ≥t Jk,(i)j

si no
si t ‚â§ n entonces determine Œ≤t = Œ≤t ‚àí Œ≥l Jk,(i)j .
(i)
;
Paso 19 Si l ‚â§ n entonces determine Œ±ll = Œ±ll + Jk,k

Œ≤l = Œ≤l + Ik(i) .

Paso 20 Resolver el sistema lineal Ac = b donde A = (Œ±l,t ), b = (Œ≤l ) y c = (Œ≥t ) para
1 ‚â§ l ‚â§ n y 1 ‚â§ t ‚â§ n.
Paso 21 SALIDA (Œ≥1 , . . . , Œ≥m ).

(i)
(i)
.
(Para cada k = 1, . . . , m sea œÜk = N (i)
j en Ti si E k = x j , y j
m
Entonces œÜ(x, y) = k=1 Œ≥k œÜk (x, y) aproxima u(x, y) en D ‚à™ S1 ‚à™ S2 .)

Paso 22 Para i = 1, . . . , M
para j = 1, 2, 3
Paso 23 PARE.

(i) (i)
SALIDA a (i)
.
j , bj , cj

(El procedimiento est√° completo .)

12.4

Ilustraci√≥n

Una introducci√≥n al m√©todo de elementos Ô¨Ånitos

577

/DWHPSHUDWXUDu(x, y HQXQDUHJLyQELGLPHQVLRQDODVDWLVIDFHODHFXDFLyQGH/DSODFH

‚àÇ 2u
‚àÇ 2u
(x, y) + 2 (x, y) = 0
2
‚àÇx
‚àÇy

en D

&RQVLGHUHODUHJLyQDPRVWUDGDHQOD√ÄJXUDFRQFRQGLFLRQHVHQODIURQWHUDGHWHUPLnadas por

u(x, y) = 4,

para (x, y) ‚àà L 6 y (x, y) ‚àà L 7 ;

‚àÇu
(x, y) = x,
‚àÇn
‚àÇu
(x, y) = y,
‚àÇn
x+y
‚àÇu
(x, y) = ‚àö ,
‚àÇn
2

para (x, y) ‚àà L 2 y (x, y) ‚àà L 4 ;
para (x, y) ‚àà L 5 ;
para (x, y) ‚àà L 1 y (x, y) ‚àà L 3 ,

donde ‚àÇu/‚àÇn denota la derivada direccional en la direcci√≥n de la normal n para la frontera
GHODUHJLyQD en el punto (x, y 

Figura 12.17
(0, 0.4)

n

L1

n
n

L7

(0.2, 0.2)

L2

(0.4, 0.2)
L3

D

n

n

(0.5, 0.1) L 4
L5

(0, 0)

n

(0.6, 0.1)

n
(0.6, 0)

L6

3ULPHURVXEGLYLGLPRVDHQWULiQJXORVFRQODHWLTXHWDVXJHULGDHQHOSDVRGHODOJRULWPR
3DUDHVWHHMHPSOR S1 = L 6 ‚à™ L 7 y S2 = L 1 ‚à™ L 2 ‚à™ L 3 ‚à™ L 4 ‚à™ L 5 . (OHWLTXHWDGRGHORVWULiQJXORVVHPXHVWUDHQOD√ÄJXUD
/DFRQGLFLyQHQODIURQWHUDu(x, y VREUHL y LLPSOLFDTXHŒ≥t = 4 cuando t 5
  HVGHFLUHQORVQRGRVE 6 , E 7 , . . . ,E 11 . 3DUDGHWHUPLQDUORVYDORUHVGHŒ≥l para l 5
   DSOLTXHORVSDVRVUHVWDQWHVGHODOJRULWPR\JHQHUHODPDWUL]

‚é°

2.5
‚é¢ 0
‚é¢
A=‚é¢
‚é¢ ‚àí1
‚é£ 0
0

0
1.5
‚àí1
‚àí0.5
0

‚àí1
‚àí1
4
0
0

0
‚àí0.5
0
2.5
‚àí0.5

‚é§
0
0 ‚é•
‚é•
0 ‚é•
‚é•
‚àí0.5 ‚é¶
1

578

CAP√çTULO 12

Soluciones num√©ricas para ecuaciones diferenciales parciales

Figura 12.18
E6

T3
E1

E7

E2
T4

T7
T1

T8

E3

T2

T5

E5
T6

T10

T9
E8

E4

E10

E9

E11

y el vector

‚é°

‚é§
6.0666ÃÑ
‚é¢ 0.0633ÃÑ ‚é•
‚é¢
‚é•
‚é•
b=‚é¢
‚é¢ 8.0000 ‚é• .
‚é£ 6.0566ÃÑ ‚é¶
2.0316ÃÑ
/DVROXFLyQGHODHFXDFLyQAc 5 b es
‚é°

‚é§
‚é°
‚é§
4.0383
Œ≥1
‚é¢ 4.0782 ‚é•
‚é¢ Œ≥2 ‚é•
‚é•
‚é¢
‚é•
‚é¢
‚é•
‚é•
‚é¢
c = ‚é¢ Œ≥3 ‚é• = ‚é¢
‚é¢ 4.0291 ‚é• .
‚é£ 4.0496 ‚é¶
‚é£ Œ≥4 ‚é¶
4.0565
Œ≥5

5HVROYHUHVWHVLVWHPDGDODVLJXLHQWHDSUR[LPDFLyQSDUDODVROXFLyQGHODHFXDFLyQGH/DSODFH\ODVFRQGLFLRQHVGHIURQWHUDHQORVWULiQJXORVUHVSHFWLYRV

T1 :

œÜ(x, y) = 4.0383(1 ‚àí 5x + 5y) + 4.0291(‚àí2 + 10x) + 4(2 ‚àí 5x ‚àí 5y),

T2 :

œÜ(x, y) = 4.0782(‚àí2 + 5x + 5y) + 4.0291(4 ‚àí 10x) + 4(‚àí1 + 5x ‚àí 5y),

T3 :

œÜ(x, y) = 4(‚àí1 + 5y) + 4(2 ‚àí 5x ‚àí 5y) + 4.0383(5x),

T4 :

œÜ(x, y) = 4.0383(1 ‚àí 5x + 5y) + 4.0782(‚àí2 + 5x + 5y) + 4.0291(2 ‚àí 10y),

T5 :

œÜ(x, y) = 4.0782(2 ‚àí 5x + 5y) + 4.0496(‚àí4 + 10x) + 4(3 ‚àí 5x ‚àí 5y),

T6 :

œÜ(x, y) = 4.0496(6 ‚àí 10x) + 4.0565(‚àí6 + 10x + 10y) + 4(1 ‚àí 10y),

T7 :

œÜ(x, y) = 4(‚àí5x + 5y) + 4.0383(5x) + 4(1 ‚àí 5y),

T8 :

œÜ(x, y) = 4.0383(5y) + 4(1 ‚àí 5x) + 4(5x ‚àí 5y),

T9 :

œÜ(x, y) = 4.0291(10y) + 4(2 ‚àí 5x ‚àí 5y) + 4(‚àí1 + 5x ‚àí 5y),

T10 :

œÜ(x, y) = 4.0496(10y) + 4(3 ‚àí 5x ‚àí 5y) + 4(‚àí2 + 5x ‚àí 5y).

/DVROXFLyQUHDOGHOSUREOHPDGHYDORUHQODIURQWHUDHVu(x, y) 5 xy 1/DWDEODFRPpara el valor de u con el valor de œÜ en Ei, para cada i 5   

12.4

Tabla 12.7

Una introducci√≥n al m√©todo de elementos Ô¨Ånitos

x

y

œÜ(x, y)

u(x, y)

|œÜ(x, y) ‚àí u(x, y)|

0.2
0.4
0.3
0.5
0.6

0.2
0.2
0.1
0.1
0.1

4.0383
4.0782
4.0291
4.0496
4.0565

4.04
4.08
4.03
4.05
4.06

0.0017
0.0018
0.0009
0.0004
0.0035

579

1RUPDOPHQWHHOHUURUGHORVSUREOHPDVHOtSWLFRVGHVHJXQGRRUGHQGHOWLSR  FRQ
IXQFLRQHVGHFRH√ÄFLHQWHVXDYHHVO(h), donde h es el di√°metro m√°ximo de los elementos
WULDQJXODUHV/DVIXQFLRQHVEDVHELOLQHDOHVSRUWUDPRVHQHOHPHQWRVUHFWDQJXODUHVWDPELpQ
VHHVSHUDTXHSURSRUFLRQHQUHVXOWDGRVO(h), donde hHVODORQJLWXGGLDJRQDOPi[LPDGHORV
HOHPHQWRVUHFWDQJXODUHV2WUDVFODVHVGHIXQFLRQHVEDVHVHSXHGHQXVDUSDUDSURSRUFLRQDU
resultados O(h SHURODFRQVWUXFFLyQHVPiVFRPSOHMD/RVWHRUHPDVGHHUURUH√ÄFLHQWHSDUD
ORVPpWRGRVGHHOHPHQWRV√ÄQLWRVVRQGLItFLOHVGHHVWDEOHFHU\GHDSOLFDU\DTXHODH[DFWLWXG
GHODDSUR[LPDFLyQGHSHQGHGHODUHJXODULGDGGHODIURQWHUDDVtFRPRGHODVSURSLHGDGHVGH
FRQWLQXLGDGGHODVROXFLyQ
(OPpWRGRGHHOHPHQWRV√ÄQLWRVWDPELpQVHSXHGHDSOLFDUDODVHFXDFLRQHVGLIHUHQFLDOHV
SDUFLDOHVSDUDEyOLFDVHKLSHUEyOLFDVSHURHOSURFHGLPLHQWRGHPLQLPL]DFLyQHVPiVGLItFLO
8QEXHQHVWXGLRVREUHODVYHQWDMDV\ODVWpFQLFDVGHOPpWRGRGHHOHPHQWRV√ÄQLWRVDSOLFDGR
DYDULRVSUREOHPDVItVLFRVVHSXHGHHQFRQWUDUHQHODUWtFXORGH>)L@3DUDXQDQiOLVLVPiV
H[WHQVRFRQVXOWH>6)@>=0@R>$%@
La secci√≥n Conjunto de ejercicios 12.4 est√° disponible en l√≠nea. Encuentre la ruta de
acceso en las p√°ginas preliminares.

12.5 Software num√©rico
8QD GH ODV VXEUXWLQDV D SDUWLU GH OD %LEOLRWHFD ,06/ VH XVD SDUD OD HFXDFLyQ GLIHUHQFLDO
parcial

‚àÇu
=F
‚àÇt

x, t, u,

‚àÇu ‚àÇ 2 u
,
‚àÇx ‚àÇx2

,

con condiciones de frontera

Œ±(x, t)u(x, t) + Œ≤(x, t)

‚àÇu
(x, t) = Œ≥ (x, t).
‚àÇx

/DUXWLQDHVWiEDVDGDHQFRORFDFLyQHQORVSXQWRVJDXVVLDQRVHQHOHMHx para cada valor de t
y usa splinesF~ELFRVGH+HUPLWHFRPRIXQFLRQHVEDVH2WUDVXEUXWLQDGH,06/VHXVDSDUD
UHVROYHUODHFXDFLyQGH3RLVVRQHQXQUHFWiQJXOR(OPpWRGRGHVROXFLyQHVWiEDVDGRHQXQD
VHOHFFLyQGHGLIHUHQFLDV√ÄQLWDVGHVHJXQGR\FXDUWRRUGHQHQXQDPDOODXQLIRUPH
/DOLEUHUtD1$*WLHQHXQQ~PHURGHVXEUXWLQDVSDUDHFXDFLRQHVGLIHUHQFLDOHVSDUFLDOHV
6HXWLOL]DXQDVXEUXWLQDSDUDODHFXDFLyQGH/DSODFHHQXQGRPLQLRDUELWUDULRHQHOSODQRxy y
RWURSDUDUHVROYHUXQDHFXDFLyQGLIHUHQFLDOSDUFLDOSDUDEyOLFDPHGLDQWHHOPpWRGRGHOtQHDV
([LVWHQ SDTXHWHV HVSHFLDOL]DGRV FRPR 1$675$1 TXH FRQVLVWHQ HQ FyGLJRV SDUD HO
PpWRGRGHHOHPHQWRV√ÄQLWRV(VWRVSDTXHWHVVRQSRSXODUHVHQDSOLFDFLRQHVGHLQJHQLHUtD(O
SDTXHWH),6+3$&.HQODELEOLRWHFD1HWOLEVHXVDSDUDUHVROYHUHFXDFLRQHVGLIHUHQFLDOHVSDUFLDOHVHOtSWLFDVVHSDUDEOHV/RVFyGLJRVJHQHUDOHVSDUDODVHFXDFLRQHVGLIHUHQFLDOHVSDUFLDOHV
VRQGLItFLOHVGHHVFULELUGHELGRDOSUREOHPDGHHVSHFL√ÄFDFLyQGHGRPLQLRVGLIHUHQWHVD√ÄJXUDVJHRPpWULFDVFRPXQHV$FWXDOPHQWHODLQYHVWLJDFLyQHQHOiUHDGHVROXFLyQGHHFXDFLRQHV
GLIHUHQFLDOHVSDUFLDOHVHVWiPX\DFWLYD
Las secciones Preguntas de an√°lisis, Conceptos clave y Revisi√≥n del cap√≠tulo est√°n disponibles en l√≠nea. Encuentre la ruta de acceso en las p√°ginas preliminares.

An√°lisis num√©rico, 10a. ed., se escribi√≥ para que los estudiantes de ingenier√≠a,
matem√°ticas, ciencias de la computaci√≥n puedan usarlo en los cursos sobre la teor√≠a
y la aplicaci√≥n de t√©cnicas de aproximaci√≥n num√©rica.
Pr√°cticamente todos los conceptos en el texto est√°n ilustrados con un ejemplo
y contiene m√°s de 2 500 ejercicios probados en clase que van desde aplicaciones
fundamentales de m√©todos y algoritmos hasta generalizaciones y extensiones de
la teor√≠a. Adem√°s, los conjuntos de ejercicios incluyen varios problemas aplicados
de diversas √°reas de la ingenier√≠a, as√≠ como de la f√≠sica, la inform√°tica, la biolog√≠a y las ciencias econ√≥micas y sociales. Las aplicaciones, seleccionadas de forma
clara y concisa, demuestran la manera en la que las t√©cnicas num√©ricas se aplican
en situaciones de la vida real.

Visite nuestro sitio en http://latinoamerica.cengage.com

